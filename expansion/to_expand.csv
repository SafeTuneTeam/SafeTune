ID,name,description,p,s,label,software
1,allocate_tokens_for_keyspace,Triggers automatic allocation of num_tokens tokens for this node. The allocation algorithm attempts to choose tokens in a way that optimizes replicated load over the nodes in the datacenter for the replication strategy used by the specified keyspace.,1,4,limited-side-effect,cassandra
2,authenticator,"Authentication backend, implementing IAuthenticator; used to identify users Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthenticator, PasswordAuthenticator}.",0,0,others,cassandra
3,authorizer,"Authorization backend, implementing IAuthorizer; used to limit access/provide permissions Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthorizer, CassandraAuthorizer}.",0,0,others,cassandra
4,auto_snapshot,Whether or not a snapshot is taken of the data before keyspace truncation or dropping of column families.,1,3,reliability-tradeoff,cassandra
5,back_pressure_enabled,"If enabled, the coordinator will apply the back-pressure strategy specified below to each mutation sent to replicas, with the aim of reducing pressure on overloaded replicas.",1,4,limited-side-effect,cassandra
6,batch_size_fail_threshold_in_kb,Fail any multiple-partition batch exceeding this value.,0,0,others,cassandra
7,batch_size_warn_threshold_in_kb,Log WARN on any multiple-partition batch size exceeding this value.,1,5,workload-specific,cassandra
9,broadcast_address,Address to broadcast to other Cassandra nodes Leaving this blank will set it to the same value as listen_address,0,0,others,cassandra
10,broadcast_rpc_address,RPC address to broadcast to drivers and other Cassandra nodes.,0,0,others,cassandra
12,cdc_enabled,Enable / disable CDC functionality on a per-node basis.,0,0,others,cassandra
13,cdc_free_space_check_interval_ms,"When we hit our cdc_raw limit and the CDCCompactor is either running behind or experiencing backpressure, we check at the following interval to see if any new space for cdc-tracked tables has been made available.",1,5,workload-specific,cassandra
14,cdc_raw_directory,CommitLogSegments are moved to this directory on flush if cdc_enabled: true and the segment contains mutations for a CDC-enabled table.,0,0,others,cassandra
17,column_index_cache_size_in_kb,Per sstable indexed key cache entries (the collation index in memory mentioned above) exceeding this size will not be held on heap.,1,1,resource,cassandra
18,column_index_size_in_kb,"Granularity of the collation index of rows within a partition. Increase if your rows are large, or if you have a very large number of rows per partition. The competing goals are these: a smaller granularity means more index entries are generated and looking up rows withing the partition by collation column is faste but, Cassandra will keep the collation index in memory for hot rows (as part of the key cache), so a larger granularity means you can cache more hot rows",1,5,workload-specific,cassandra
22,commitlog_segment_size_in_mb,"The size of the individual commitlog file segments. A commitlog segment may be archived, deleted, or recycled once all the data in it (potentially from each columnfamily in the system) has been flushed to sstables.",1,3,reliability-tradeoff,cassandra
24,commitlog_total_space_in_mb,"Total space to use for commit logs on disk.If space gets above this value, Cassandra will flush every dirty CF in the oldest segment and remove it. So a small total commitlog space will tend to cause more flush activity on less-active columnfamilies.",1,3,reliability-tradeoff,cassandra
26,compaction_throughput_mb_per_sec,Throttles compaction to the given total throughput across the entire system.,1,5,workload-specific,cassandra
27,concurrent_compactors,"Number of simultaneous compactions to allow, NOT including validation ""compactions"" for anti-entropy repair. Simultaneous compactions can help preserve read performance in a mixed read/write workload, by mitigating the tendency of small sstables to accumulate during a single long running compactions. The default is usually fine and if you experience problems with compaction running too slowly or too fast, you should look at compaction_throughput_mb_per_sec first.",1,5,workload-specific,cassandra
28,concurrent_materialized_view_writes,"For materialized view writes, as there is a read involved, so this should be limited by the less of concurrent reads or concurrent writes.",1,1,resource,cassandra
29,concurrent_reads,concurrent_reads should be set to (16 * number_of_drives) in order to allow the operations to enqueue low enough in the stack that the OS and drives can reorder them.,1,1,resource,cassandra
30,concurrent_writes,"since writes are almost never IO bound, the ideal number of concurrent_writes is dependent on the number of cores in your system; (8 * number_of_cores) is a good rule of thumb.",1,1,resource,cassandra
31,counter_cache_keys_to_save,"Number of keys from the counter cache to save. Disabled by default, meaning all keys are going to be saved",1,1,resource,cassandra
32,counter_cache_save_period,Duration in seconds after which Cassandra should save the counter cache (keys only). Caches are saved to saved_caches_directory as specified in this configuration file.,1,5,workload-specific,cassandra
34,counter_write_request_timeout_in_ms,How long the coordinator should wait for counter writes to complete,0,0,others,cassandra
35,credentials_update_interval_in_ms,"Refresh interval for credentials cache (if enabled). After this interval, cache entries become eligible for refresh.",1,5,workload-specific,cassandra
36,credentials_validity_in_ms,"Please note, credentials are cached in their encrypted form, so while activating this cache may reduce the number of queries made to the underlying table, it may not bring a significant reduction in the latency of individual authentication attempts.",1,2,security-tradeoff,cassandra
37,cross_node_timeout,"Enable operation timeout information exchange between nodes to accurately measure request timeouts. If disabled, replicas will assume that requests were forwarded to them instantly by the coordinator, which means that under overload conditions we will waste that much extra time processing already-timed-out requests.",1,4,limited-side-effect,cassandra
38,data_file_directories,Directories where Cassandra should store data on disk.,0,0,others,cassandra
39,disk_failure_policy,Policy for data disk failures,0,0,others,cassandra
40,dynamic_snitch_reset_interval_in_ms,"controls how often to reset all host scores, allowing a bad host to possibly recover",1,5,workload-specific,cassandra
41,dynamic_snitch_update_interval_in_ms,controls how often to perform the more expensive part of host score calculation,1,5,workload-specific,cassandra
42,enable_scripted_user_defined_functions,Enables scripted UDFs (JavaScript UDFs).,0,0,others,cassandra
43,enable_user_defined_functions,"If unset, all GC Pauses greater than gc_log_threshold_in_ms will log at INFO level. UDFs (user defined functions) are disabled by default.",1,6,function-tradeoff,cassandra
44,endpoint_snitch,Set this to a class that implements IEndpointSnitch.,0,0,others,cassandra
45,file_cache_size_in_mb,Maximum memory to use for sstable chunk cache and buffer pooling.,1,1,resource,cassandra
46,gc_log_threshold_in_ms,GC Pauses greater than 200 ms will be logged at INFO level This threshold can be adjusted to minimize logging if necessary,1,5,workload-specific,cassandra
47,gc_warn_threshold_in_ms,GC Pauses greater than gc_warn_threshold_in_ms will be logged at WARN level,1,5,workload-specific,cassandra
48,hinted_handoff_disabled_datacenters,"When hinted_handoff_enabled is true, a black list of data centers that will not perform hinted handoff",0,0,others,cassandra
50,hinted_handoff_throttle_in_kb,"Maximum throttle in KBs per second, per delivery thread. This will be reduced proportionally to the number of nodes in the cluster.",1,1,resource,cassandra
51,hints_compression,"Compression to apply to the hint files. If omitted, hints files will be written uncompressed. LZ4, Snappy, and Deflate compressors are supported.",1,4,limited-side-effect,cassandra
52,hints_directory,"Directory where Cassandra should store hints. If not set, the default directory is $CASSANDRA_HOME/data/hints.",0,0,others,cassandra
53,hints_flush_period_in_ms,How often hints should be flushed from the internal buffers to disk. Will not trigger fsync.,1,3,reliability-tradeoff,cassandra
54,ideal_consistency_level,Track a metric per keyspace indicating whether replication achieved the ideal consistency level for writes without timing out. This is different from the consistency level requested by each write which may be lower in order to facilitate availability.,1,6,function-tradeoff,cassandra
55,incremental_backups,Set to true to have Cassandra create a hard link to each sstable flushed or streamed locally in a backups/ subdirectory of the keyspace data. Removing these links is the operator's responsibility.,0,0,others,cassandra
56,index_summary_capacity_in_mb,A fixed memory pool size in MB for for SSTable index summaries.,1,1,resource,cassandra
58,initial_token,"initial_token allows you to specify tokens manually. While you can use it with vnodes (num_tokens > 1, above) in which case you should provide a comma-separated list it's primarily used when adding nodes to legacy clusters that do not have vnodes enabled.",0,0,others,cassandra
59,inter_dc_tcp_nodelay,"Enable or disable tcp_nodelay for inter-dc communication. Disabling it will result in larger (but fewer) network packets being sent, reducing overhead from the TCP protocol itself, at the cost of increasing latency if you block for cross-datacenter responses",1,4,limited-side-effect,cassandra
60,internode_authenticator,"Internode authentication backend, implementing IInternodeAuthenticator; used to allow/disallow connections from peer nodes.",0,0,others,cassandra
63,key_cache_save_period,"Duration in seconds after which Cassandra should save the key cache. Caches are saved to saved_caches_directory as specified in this configuration file. Saved caches greatly improve cold-start speeds, and is relatively cheap in terms of I/O for the key cache.",1,5,workload-specific,cassandra
64,key_cache_size_in_mb,"The key cache is fairly tiny for the amount of time it saves, so it's worthwhile to use it at large numbers. The row cache saves even more time, but must contain the entire row, so it is extremely space-intensive.",1,1,resource,cassandra
65,listen_address,Address or interface to bind to and tell other Cassandra nodes to connect to. You _must_ change this if you want multiple nodes to be able to communicate!,0,0,others,cassandra
66,listen_interface,"Set listen_address OR listen_interface, not both. Interfaces must correspond to a single address, IP aliasing is not supported.",0,0,others,cassandra
67,listen_interface_prefer_ipv6,If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address you can specify which should be chosen using listen_interface_prefer_ipv6.,0,0,others,cassandra
68,listen_on_broadcast_address,"When using multiple physical network interfaces, set this to true to listen on broadcast_address in addition to the listen_address, allowing nodes to communicate in both interfaces.",0,0,others,cassandra
69,max_hint_window_in_ms,"this defines the maximum amount of time a dead host will have hints generated. After it has been dead this long, new hints for it will not be created until it has been seen alive and gone down again.",0,0,others,cassandra
70,max_hints_delivery_threads,"Number of threads with which to deliver hints; Consider increasing this number when you have multi-dc deployments, since cross-dc handoff tends to be slower",1,5,workload-specific,cassandra
71,max_hints_file_size_in_mb,"Maximum size for a single hints file, in megabytes.",0,0,others,cassandra
72,max_value_size_in_mb,Maximum size of any value in SSTables. Safety measure to detect SSTable corruption early.,0,0,others,cassandra
73,memtable_allocation_type,Specify the way Cassandra allocates and manages memtable memory.,1,5,workload-specific,cassandra
74,memtable_cleanup_threshold,"Ratio of occupied non-flushing memtable size to total permitted size that will trigger a flush of the largest memtable. Larger mct will mean larger flushes and hence less compaction, but also less concurrent flush activity which can make it difficult to keep your disks fed under heavy write load.",1,5,workload-specific,cassandra
76,memtable_heap_space_in_mb,"Total permitted memory to use for memtables. Cassandra will stop accepting writes when the limit is exceeded until a flush completes, and will trigger a flush based on memtable_cleanup_threshold",1,3,reliability-tradeoff,cassandra
77,native_transport_max_concurrent_connections,The maximum number of concurrent client connections.,1,1,resource,cassandra
78,native_transport_max_concurrent_connections_per_ip,The maximum number of concurrent client connections per source ip.,1,1,resource,cassandra
79,native_transport_max_frame_size_in_mb,The maximum size of allowed frame. Frame (requests) larger than this will be rejected as invalid.,0,0,others,cassandra
80,native_transport_max_threads,The maximum threads for handling requests (note that idle threads are stopped after 30 seconds so there is not corresponding minimum setting).,1,1,resource,cassandra
82,native_transport_port_ssl,Setting native_transport_port_ssl to a different value from native_transport_port will use encryption for native_transport_port_ssl while keeping native_transport_port unencrypted.,1,2,security-tradeoff,cassandra
83,num_tokens,"This defines the number of tokens randomly assigned to this node on the ring The more tokens, relative to other nodes, the larger the proportion of data that this node will store.",0,0,others,cassandra
84,otc_backlog_expiration_interval_ms,How many milliseconds to wait between two expiration runs on the backlog (queue) of the OutboundTcpConnection.,0,0,others,cassandra
85,otc_coalescing_enough_coalesced_messages,Do not try to coalesce messages if we already got that many messages. This should be more than 2 and less than 128.,1,4,limited-side-effect,cassandra
86,otc_coalescing_window_us,How many microseconds to wait for coalescing. For fixed strategy this is the amount of time after the first message is received before it will be sent with any accompanying messages. For moving average this is the maximum amount of time that will be waited as well as the interval at which messages must arrive on average for coalescing to be enabled.,0,0,others,cassandra
87,partitioner,The partitioner is responsible for distributing groups of rows (by partition key) across nodes in the cluster. You should leave this alone for new clusters.,0,0,others,cassandra
88,permissions_update_interval_in_ms,"Refresh interval for permissions cache (if enabled). After this interval, cache entries become eligible for refresh. Upon next access, an async reload is scheduled and the old value returned until it completes.",0,0,others,cassandra
89,permissions_validity_in_ms,"Validity period for permissions cache (fetching permissions can be an expensive operation depending on the authorizer, CassandraAuthorizer is one example).",1,2,security-tradeoff,cassandra
90,prepared_statements_cache_size_mb,Maximum size of the native protocol prepared statement cache,1,1,resource,cassandra
91,range_request_timeout_in_ms,How long the coordinator should wait for seq or index scans to complete,0,0,others,cassandra
92,read_request_timeout_in_ms,How long the coordinator should wait for read operations to complete,0,0,others,cassandra
93,request_timeout_in_ms,"The default timeout for other, miscellaneous operations",0,0,others,cassandra
94,role_manager,"Part of the Authentication & Authorization backend, implementing IRoleManager; used to maintain grants and memberships between roles.",0,0,others,cassandra
95,roles_update_interval_in_ms,"Refresh interval for roles cache (if enabled). After this interval, cache entries become eligible for refresh. Upon next access, an async reload is scheduled and the old value returned until it completes.",1,2,security-tradeoff,cassandra
96,roles_validity_in_ms,"Validity period for roles cache (fetching granted roles can be an expensive operation depending on the role manager, CassandraRoleManager is one example) Granted roles are cached for authenticated sessions in AuthenticatedUser and after the period specified here, become eligible for (async) reload.",1,2,security-tradeoff,cassandra
98,row_cache_keys_to_save,"Number of keys from the row cache to save. Specify 0 (which is the default), meaning all keys are going to be saved",1,1,resource,cassandra
99,row_cache_save_period,"Duration in seconds after which Cassandra should save the row cache. Saved caches greatly improve cold-start speeds, and is relatively cheap in terms of I/O for the key cache. Row cache saving is much more expensive and has limited use.",1,5,workload-specific,cassandra
100,row_cache_size_in_mb,Maximum size of the row cache in memory.,1,1,resource,cassandra
104,rpc_keepalive,enable or disable keepalive on rpc/native connections,0,0,others,cassandra
105,saved_caches_directory,"saved caches If not set, the default directory is $CASSANDRA_HOME/data/saved_caches.",0,0,others,cassandra
106,slow_query_log_timeout_in_ms,How long before a node logs slow queries.,0,0,others,cassandra
108,ssl_storage_port,"SSL port, for encrypted communication.",0,0,others,cassandra
111,storage_port,"TCP port, for commands and data For security reasons, you should not expose this port to the internet. Firewall it if needed.",0,0,others,cassandra
113,transparent_data_encryption_options,Enables encrypting data at-rest (on disk).,1,2,security-tradeoff,cassandra
115,truncate_request_timeout_in_ms,How long the coordinator should wait for truncates to complete,0,0,others,cassandra
116,write_request_timeout_in_ms,How long the coordinator should wait for writes to complete,0,0,others,cassandra
117,--analyze,Run the static analyzer,1,6,function-tradeoff,clang
118,--analyzer-output,Static analyzer report output format (html|plist|plist-multi-file|plist-html|sarif|text).,0,0,others,clang
120,-arcmt-migrate-report-output,Output path for the plist report,1,6,function-tradeoff,clang
121,-B,Add dir to search path for binaries and object files used implicitly,0,0,others,clang
122,-C(captal),Include comments in preprocessed output,1,6,function-tradeoff,clang
123,-c,"Only run preprocess, compile, and assemble steps",1,6,function-tradeoff,clang
124,-CC,Include comments from within macros in preprocessed output,1,6,function-tradeoff,clang
126,-cl-fast-relaxed-math,"OpenCL only. Sets -cl-finite-math-only and -cl-unsafe-math-optimizations, and defines __FAST_RELAXED_MATH__.",1,4,limited-side-effect,clang
127,-cl-finite-math-only,OpenCL only. Allow floating-point optimizations that assume arguments and results are not NaNs or +-Inf.,1,4,limited-side-effect,clang
128,-cl-fp32-correctly-rounded-divide-sqrt,OpenCL only. Specify that single precision floating-point divide and sqrt used in the program source are correctly rounded.,0,0,others,clang
129,-cl-kernel-arg-info,OpenCL only. Generate kernel argument metadata.,0,0,others,clang
130,-cl-mad-enable,OpenCL only. Allow use of less precise MAD computations in the generated binary.,0,0,others,clang
131,-cl-no-signed-zeros,OpenCL only. Allow use of less precise no signed zeros computations in the generated binary.,0,0,others,clang
132,-cl-opt-disable,OpenCL only. This option disables all optimizations. By default optimizations are enabled.,1,4,limited-side-effect,clang
133,-cl-single-precision-constant,OpenCL only. Treat double precision floating-point constant as single precision constant.,0,0,others,clang
134,-cl-std=value,OpenCL language standard to compile for.,0,0,others,clang
135,-cl-strict-aliasing,OpenCL only. This option is added for compatibility with OpenCL 1.0.,0,0,others,clang
136,-cl-uniform-work-group-size,OpenCL only. Defines that the global work-size be a multiple of the work-group size specified to clEnqueueNDRangeKernel,0,0,others,clang
138,--config,Specifies configuration file,0,0,others,clang
139,--cuda-compile-host-device,Compile CUDA code for both host and device (default). Has no effect on non-CUDA compilations.,0,0,others,clang
140,--cuda-device-only,Compile CUDA code for device only,0,0,others,clang
141,--cuda-gpu-arch,CUDA GPU architecture (e.g. sm_35). May be specified more than once.,0,0,others,clang
142,--cuda-host-only,Compile CUDA code for host only. Has no effect on non-CUDA compilations.,0,0,others,clang
145,--cuda-path=value,CUDA installation path,0,0,others,clang
146,--cuda-path-ignore-env,Ignore environment variables to detect CUDA installation,0,0,others,clang
147,-cxx-isystem,Add directory to the C++ SYSTEM include search path,0,0,others,clang
148,-D,Define <macro> to <value> (or 1 if <value> omitted),0,0,others,clang
149,-dD,Print macro definitions in -E mode in addition to normal output,1,6,function-tradeoff,clang
150,-dependency-dot,Filename to write DOT-formatted header dependencies to,0,0,others,clang
152,-dI,Print include directives in -E mode in addition to normal output,1,6,function-tradeoff,clang
153,-dM,Print macro definitions in -E mode instead of normal output,1,6,function-tradeoff,clang
154,-E,Only run the preprocessor,0,0,others,clang
155,-emit-ast,Emit Clang AST files for source inputs,1,6,function-tradeoff,clang
157,-emit-llvm,Use the LLVM representation for assembler and object files,1,6,function-tradeoff,clang
158,-emit-merged-ifs,"Generate Interface Stub Files, emit merged text not binary.",1,6,function-tradeoff,clang
159,-enable-trivial-auto-var-init-zero-knowing-it-will-be-removed-from-clang,"Trivial automatic variable initialization to zero is only here for benchmarks, it'll eventually be removed, and I'm OK with that because I'm only using it to benchmark",1,6,function-tradeoff,clang
160,-F,Add directory to framework include search path,0,0,others,clang
161,-faddrsig,Emit an address-significance table,1,6,function-tradeoff,clang
162,-faligned-allocation,Enable C++17 aligned allocation functions,0,0,others,clang
163,-fallow-editor-placeholders,Treat editor placeholders as valid source code,0,0,others,clang
165,-fapple-kext,Use Apple's kernel extensions ABI,0,0,others,clang
166,-fapple-link-rtlib,Force linking the clang builtins runtime library,0,0,others,clang
167,-fapple-pragma-pack,Enable Apple gcc-compatible #pragma pack handling,0,0,others,clang
168,-fapplication-extension,Restrict code to those available for App Extensions,0,0,others,clang
169,-fblocks,Enable the 'blocks' language feature,0,0,others,clang
170,-fborland-extensions,Accept non-standard constructs supported by the Borland compiler,0,0,others,clang
171,-fbuild-session-file=file,Use the last modification time of <file> as the build session timestamp,0,0,others,clang
172,-fbuild-session-timestamp=value ,Time when the current build session started,0,0,others,clang
173,-fbuiltin-module-map,Load the clang builtins module map file.,0,0,others,clang
174,-fc++-static-destructors,Enable C++ static destructor registration (the default),0,0,others,clang
175,-fcall-saved-x10,Make the x10 register call-saved (AArch64 only),0,0,others,clang
176,-fcall-saved-x11,Make the x11 register call-saved (AArch64 only),0,0,others,clang
177,-fcall-saved-x12,Make the x12 register call-saved (AArch64 only),0,0,others,clang
178,-fcall-saved-x13,Make the x13 register call-saved (AArch64 only),0,0,others,clang
179,-fcall-saved-x14,Make the x14 register call-saved (AArch64 only),0,0,others,clang
180,-fcall-saved-x15,Make the x15 register call-saved (AArch64 only),0,0,others,clang
181,-fcall-saved-x18,Make the x18 register call-saved (AArch64 only),0,0,others,clang
182,-fcall-saved-x8,Make the x8 register call-saved (AArch64 only),0,0,others,clang
183,-fcall-saved-x9,Make the x9 register call-saved (AArch64 only),0,0,others,clang
184,-fcf-protection,"Instrument control-flow architecture protection. Options: return, branch, full, none.",1,2,security-tradeoff,clang
185,-fchar8_t,Enable C++ builtin type char8_t,0,0,others,clang
186,-fclang-abi-compat,Attempt to match the ABI of Clang <version>,0,0,others,clang
187,-fcolor-diagnostics,Use colors in diagnostics,0,0,others,clang
188,-fcomment-block-commands,Treat each comma separated argument in <arg> as a documentation comment block command,0,0,others,clang
189,-fcomplete-member-pointers,Require member pointer base types to be complete if they would be significant under the Microsoft ABI,0,0,others,clang
190,-fconvergent-functions,Assume functions may be convergent,0,0,others,clang
191,-fcoroutines-ts,Enable support for the C++ Coroutines TS,0,0,others,clang
192,-fcoverage-mapping,Generate coverage mapping to enable code coverage analysis,0,0,others,clang
193,-fcs-profile-generate,Generate instrumented code to collect context sensitive execution counts into default.profraw (overridden by LLVM_PROFILE_FILE env var),0,0,others,clang
196,-fcuda-flush-denormals-to-zero,Flush denormal floating point values to zero in CUDA device mode.,0,0,others,clang
197,-fcuda-short-ptr,Use 32-bit pointers for accessing const/local/shared address spaces.,0,0,others,clang
200,-fdebug-compilation-dir,The compilation directory to embed in the debug info.,0,0,others,clang
201,-fdebug-default-version,"Default DWARF version to use, if a -g option caused DWARF debug info to be produced",0,0,others,clang
202,-fdebug-info-for-profiling,Emit extra debug info to make sample profile more accurate.,1,6,function-tradeoff,clang
203,-fdebug-macro,Emit macro debug information,1,6,function-tradeoff,clang
204,-fdebug-prefix-map=value,remap file source paths in debug info,0,0,others,clang
205,-fdebug-ranges-base-address,Use DWARF base address selection entries in debug_ranges,0,0,others,clang
206,-fdebug-types-section,Place debug types in their own section (ELF Only),0,0,others,clang
207,-fdeclspec,Allow __declspec as a keyword,0,0,others,clang
208,-fdelayed-template-parsing,Parse templated function definitions at the end of the translation unit,0,0,others,clang
209,-fdelete-null-pointer-checks,Treat usage of null pointers as undefined behavior.,0,0,others,clang
210,-fdiagnostics-absolute-paths,Print absolute paths in diagnostics,1,6,function-tradeoff,clang
211,-fdiagnostics-hotness-threshold,Prevent optimization remarks from being output if they do not have at least this profile count,1,6,function-tradeoff,clang
212,-fdiagnostics-parseable-fixits,Print fix-its in machine parseable form,1,6,function-tradeoff,clang
213,-fdiagnostics-print-source-range-info,Print source range spans in numeric form,1,6,function-tradeoff,clang
214,-fdiagnostics-show-hotness,Enable profile hotness information in diagnostic line,1,6,function-tradeoff,clang
215,-fdiagnostics-show-note-include-stack,Display include stacks for diagnostic notes,0,0,others,clang
216,-fdiagnostics-show-option,Print option name with mappable diagnostics,0,0,others,clang
217,-fdiagnostics-show-template-tree,Print a template comparison tree for differing templates,1,6,function-tradeoff,clang
219,-fdiscard-value-names,Discard value names in LLVM IR,0,0,others,clang
220,-fdollars-in-identifiers,Allow '$' in identifiers,0,0,others,clang
221,-fdouble-square-bracket-attributes,Enable '[[]]' attributes in all C and C++ language modes,0,0,others,clang
223,-fembed-bitcode,Embed LLVM IR bitcode as data,0,0,others,clang
224,-fembed-bitcode-marker,Embed placeholder LLVM IR data as a marker,0,0,others,clang
225,-femit-all-decls,"Emit all declarations, even if unused",1,6,function-tradeoff,clang
226,-femulated-tls,Use emutls functions to access thread_local variables,0,0,others,clang
227,-fexceptions,Enable support for exception handling,0,0,others,clang
229,-fexperimental-new-constant-interpreter,Enable the experimental new constant interpreter,0,0,others,clang
230,-fexperimental-new-pass-manager,Enables an experimental new pass manager in LLVM.,0,0,others,clang
231,-ffast-math,"Allow aggressive, lossy floating-point optimizations",1,4,limited-side-effect,clang
232,-ffile-prefix-map=value,remap file source paths in debug info and predefined preprocessor macros,0,0,others,clang
233,-ffine-grained-bitfield-accesses,Use separate accesses for consecutive bitfield runs with legal widths and alignments.,0,0,others,clang
234,-ffixed-point,Enable fixed point types,0,0,others,clang
235,-ffixed-r19,Reserve register r19 (Hexagon only),0,0,others,clang
237,-ffixed-x1,Reserve the 1 register (AArch64/RISC-V only),0,0,others,clang
238,-ffixed-x10,Reserve the 10 register (AArch64/RISC-V only),0,0,others,clang
239,-ffixed-x11,Reserve the 11 register (AArch64/RISC-V only),0,0,others,clang
240,-ffixed-x12,Reserve the 12 register (AArch64/RISC-V only),0,0,others,clang
241,-ffixed-x13,Reserve the 13 register (AArch64/RISC-V only),0,0,others,clang
242,-ffixed-x14,Reserve the 14 register (AArch64/RISC-V only),0,0,others,clang
243,-ffixed-x15,Reserve the 15 register (AArch64/RISC-V only),0,0,others,clang
244,-ffixed-x16,Reserve the 16 register (AArch64/RISC-V only),0,0,others,clang
245,-ffixed-x17,Reserve the 17 register (AArch64/RISC-V only),0,0,others,clang
246,-ffixed-x18,Reserve the 18 register (AArch64/RISC-V only),0,0,others,clang
247,-ffixed-x19,Reserve the 19 register (AArch64/RISC-V only),0,0,others,clang
248,-ffixed-x2,Reserve the 2 register (AArch64/RISC-V only),0,0,others,clang
249,-ffixed-x20,Reserve the 20 register (AArch64/RISC-V only),0,0,others,clang
250,-ffixed-x21,Reserve the 21 register (AArch64/RISC-V only),0,0,others,clang
251,-ffixed-x22,Reserve the 22 register (AArch64/RISC-V only),0,0,others,clang
252,-ffixed-x23,Reserve the 23 register (AArch64/RISC-V only),0,0,others,clang
253,-ffixed-x24,Reserve the 24 register (AArch64/RISC-V only),0,0,others,clang
254,-ffixed-x25,Reserve the 25 register (AArch64/RISC-V only),0,0,others,clang
255,-ffixed-x26,Reserve the 26 register (AArch64/RISC-V only),0,0,others,clang
257,-ffixed-x28,Reserve the 28 register (AArch64/RISC-V only),0,0,others,clang
258,-ffixed-x29,Reserve the 29 register (AArch64/RISC-V only),0,0,others,clang
259,-ffixed-x3,Reserve the 3 register (AArch64/RISC-V only),0,0,others,clang
260,-ffixed-x30,Reserve the 30 register (AArch64/RISC-V only),0,0,others,clang
261,-ffixed-x31,Reserve the 31 register (AArch64/RISC-V only),0,0,others,clang
262,-ffixed-x4,Reserve the 4 register (AArch64/RISC-V only),0,0,others,clang
263,-ffixed-x5,Reserve the 5 register (AArch64/RISC-V only),0,0,others,clang
264,-ffixed-x6,Reserve the 6 register (AArch64/RISC-V only),0,0,others,clang
265,-ffixed-x7,Reserve the 7 register (AArch64/RISC-V only),0,0,others,clang
266,-ffixed-x8,Reserve the 8 register (AArch64/RISC-V only),0,0,others,clang
267,-ffixed-x9,Reserve the 9 register (AArch64/RISC-V only),0,0,others,clang
270,-fforce-enable-int128,Enable support for int128_t type,0,0,others,clang
271,-ffp-contract,Form fused FP ops (e.g. FMAs): fast (everywhere) | on (according to FP_CONTRACT pragma) | off (never fuse). Default is 'fast' for CUDA/HIP and 'on' otherwise.,1,4,limited-side-effect,clang
273,-ffp-model,Controls the semantics of floating-point calculations.,0,0,others,clang
274,-ffreestanding,Assert that the compilation takes place in a freestanding environment,0,0,others,clang
276,-fgnu89-inline,Use the gnu89 inline semantics,0,0,others,clang
277,-fgnuc-version,Sets various macros to claim compatibility with the given GCC version (default is 4.2.1),0,0,others,clang
278,-fgnu-keywords,Allow GNU-extension keywords regardless of language standard,0,0,others,clang
279,-fgnu-runtime,Generate output compatible with the standard GNU Objective-C runtime,1,6,function-tradeoff,clang
280,-fgpu-allow-device-init,Allow device side init function in HIP,0,0,others,clang
282,-fhip-new-launch-api,Use new kernel launching API for HIP.,0,0,others,clang
283,-fimplicit-module-maps,Implicitly search the file system for module map files.,0,0,others,clang
284,-finline-functions,Inline suitable functions,0,0,others,clang
285,-finline-hint-functions,Inline functions which are (explicitly or implicitly) marked inline,0,0,others,clang
286,-finstrument-function-entry-bare,"Instrument function entry only, after inlining, without arguments to the instrumentation call",0,0,others,clang
287,-finstrument-functions,Generate calls to instrument function entry and exit,0,0,others,clang
289,-fintegrated-as,Enable the integrated assembler,0,0,others,clang
290,-fintegrated-cc1,Run cc1 in-process,0,0,others,clang
291,-fkeep-static-consts,Keep static const variables even if unused,0,0,others,clang
292,-flax-vector-conversions,Enable implicit vector bit-casts,0,0,others,clang
293,-flto,Enable LTO in 'full' mode,0,0,others,clang
294,-flto-jobs,Controls the backend parallelism of -flto=thin (default of 0 means the number of threads will be derived from the number of CPUs detected),1,1,resource,clang
295,-fmacro-prefix-map=value,remap file source paths in predefined preprocessor macros,0,0,others,clang
296,-fmath-errno,Require math functions to indicate errors by setting errno,0,0,others,clang
297,-fmax-type-align,Specify the maximum alignment to enforce on pointers lacking an explicit alignment,0,0,others,clang
298,-fmerge-all-constants,Allow merging of constants,0,0,others,clang
299,-fmodule-file=[name=]file,"Specify the mapping of module name to precompiled module file, or load a module file if name is omitted.",0,0,others,clang
300,-fmodule-map-file=file,Load this module map file,0,0,others,clang
301,-fmodule-name=name,Specify the name of the module to build,0,0,others,clang
302,-fmodules,Enable the 'modules' language feature,0,0,others,clang
303,-fmodules-cache-path,Specify the module cache path,0,0,others,clang
304,-fmodules-decluse,Require declaration of modules used within a module,0,0,others,clang
305,-fmodules-disable-diagnostic-validation,Disable validation of the diagnostic options when loading the module,0,0,others,clang
307,-fmodules-prune-after=seconds,Specify the interval (in seconds) after which a module file will be considered unused,0,0,others,clang
308,-fmodules-prune-interval,Specify the interval (in seconds) between attempts to prune the module cache,0,0,others,clang
310,-fmodules-strict-decluse,Like -fmodules-decluse but requires all headers to be in modules,0,0,others,clang
312,-fmodules-user-build-path,Specify the module user build path,0,0,others,clang
313,-fmodules-validate-input-files-content,Validate PCM input files based on content if mtime differs,0,0,others,clang
315,-fmodules-validate-system-headers,Validate the system headers that a module depends on when loading the module,0,0,others,clang
316,-fms-compatibility,Enable full Microsoft Visual C++ compatibility,0,0,others,clang
317,-fms-compatibility-version,Dot-separated value representing the Microsoft compiler version number to report in _MSC_VER (0 = don't define it (default)),0,0,others,clang
319,-fms-extensions,Accept some non-standard constructs supported by the Microsoft compiler,0,0,others,clang
320,-fnew-alignment,Specifies the largest alignment guaranteed by '::operator new(size_t)',0,0,others,clang
321,-fno-access-control,Disable C++ access control,0,0,others,clang
322,-fno-addrsig,Don't emit an address-significance table,0,0,others,clang
323,-fno-assume-sane-operator-new,Don't assume that C++'s global operator new can't alias any pointer,0,0,others,clang
324,-fno-autolink,Disable generation of linker directives for automatic library linking,0,0,others,clang
325,-fno-builtin,Disable implicit builtin knowledge of functions,0,0,others,clang
326,-fno-builtin-value,Disable implicit builtin knowledge of a specific function,0,0,others,clang
327,-fno-c++-static-destructors,Disable C++ static destructor registration,0,0,others,clang
328,-fno-char8_t,Disable C++ builtin type char8_t,0,0,others,clang
329,-fno-common,Compile common globals like normal definitions,0,0,others,clang
330,-fno-complete-member-pointers,Do not require member pointer base types to be complete if they would be significant under the Microsoft ABI,0,0,others,clang
331,-fno-constant-cfstrings,Disable creation of CodeFoundation-type constant strings,0,0,others,clang
332,-fno-coverage-mapping,Disable code coverage analysis,0,0,others,clang
333,-fno-crash-diagnostics,Disable auto-generation of preprocessed source files and a script for reproduction during a clang crash,0,0,others,clang
334,-fno-debug-info-for-profiling,Do not emit extra debug info for sample profiler.,1,6,function-tradeoff,clang
335,-fno-debug-macro,Do not emit macro debug information,1,6,function-tradeoff,clang
336,-fno-declspec,Disallow __declspec as a keyword,0,0,others,clang
337,-fno-delayed-template-parsing,Disable delayed template parsing,0,0,others,clang
339,-fno-diagnostics-fixit-info,Do not include fixit information in diagnostics,1,6,function-tradeoff,clang
340,-fno-digraphs,"Disallow alternative token representations '<:',':>','<%','%>','%:','%:%:'",0,0,others,clang
342,-fno-dollars-in-identifiers,Disallow '$' in identifiers,0,0,others,clang
343,-fno-double-square-bracket-attributes,Disable '[[]]' attributes in all C and C++ language modes,0,0,others,clang
345,-fno-elide-type,Do not elide types when printing diagnostics,0,0,others,clang
346,-fno-experimental-isel,Disables the experimental global instruction selector,0,0,others,clang
348,-fno-fine-grained-bitfield-accesses,Use large-integer access for consecutive bitfield runs.,0,0,others,clang
349,-fno-fixed-point,Disable fixed point types,0,0,others,clang
350,-fno-force-dwarf-frame,Don't always emit a debug frame section,1,6,function-tradeoff,clang
351,-fno-force-enable-int128,Disable support for int128_t type,0,0,others,clang
352,-fno-gnu-inline-asm,Disable GNU style inline asm,0,0,others,clang
353,-fno-integrated-as,Disable the integrated assembler,0,0,others,clang
356,-fno-lto,Disable LTO mode (default),0,0,others,clang
357,-fno-merge-all-constants,Disallow merging of constants,0,0,others,clang
358,-fno-objc-infer-related-result-type,do not infer Objective-C related result type based on method family,1,6,function-tradeoff,clang
359,-fno-operator-names,Do not treat C++ operator name keywords as synonyms for operators,0,0,others,clang
360,-fno-plt,Do not use the PLT to make function calls,0,0,others,clang
361,-fno-preserve-as-comments,Do not preserve comments in inline assembly,0,0,others,clang
362,-fno-profile-generate,Disable generation of profile instrumentation.,0,0,others,clang
363,-fno-profile-instr-generate,Disable generation of profile instrumentation.,0,0,others,clang
364,-fno-profile-instr-use,Disable using instrumentation data for profile-guided optimization,0,0,others,clang
365,-fno-register-global-dtors-with-atexit,Don't use atexit or __cxa_atexit to register global destructors,0,0,others,clang
366,-fno-reroll-loops,Turn off loop reroller,0,0,others,clang
367,-fno-rtlib-add-rpath,Do not add -rpath with architecture-specific resource directory to the linker flags,0,0,others,clang
368,-fno-rtti,Disable generation of rtti information,0,0,others,clang
369,-fno-rtti-data,Control emission of RTTI data,0,0,others,clang
371,-fno-sanitize-address-use-after-scope,Disable use-after-scope detection in AddressSanitizer,0,0,others,clang
372,-fno-sanitize-address-use-odr-indicator,Disable ODR indicator globals,0,0,others,clang
373,-fno-sanitize-blacklist,Don't use blacklist file for sanitizers,0,0,others,clang
374,-fno-sanitize-cfi-canonical-jump-tables,Do not make the jump table addresses canonical in the symbol table,0,0,others,clang
376,-fno-sanitize-coverage,Disable specified features of coverage instrumentation for Sanitizers,0,0,others,clang
377,-fno-sanitize-memory-track-origins,Disable origins tracking in MemorySanitizer,0,0,others,clang
378,-fno-sanitize-memory-use-after-dtor,Disable use-after-destroy detection in MemorySanitizer,0,0,others,clang
379,-fno-sanitize-recover,Disable recovery for specified sanitizers,0,0,others,clang
380,-fno-sanitize-stats,Disable sanitizer statistics gathering.,1,6,function-tradeoff,clang
381,-fno-sanitize-thread-atomics,Disable atomic operations instrumentation in ThreadSanitizer,0,0,others,clang
382,-fno-sanitize-thread-func-entry-exit,Disable function entry/exit instrumentation in ThreadSanitizer,0,0,others,clang
383,-fno-sanitize-thread-memory-access,Disable memory access instrumentation in ThreadSanitizer,0,0,others,clang
384,-fno-sanitize-trap,Disable trapping for specified sanitizers,0,0,others,clang
385,-fno-short-wchar,Force wchar_t to be an unsigned int,0,0,others,clang
386,-fno-show-column,Do not include column number on diagnostics,0,0,others,clang
387,-fno-show-source-location,Do not include source location information with diagnostics,0,0,others,clang
388,-fno-signed-char,Char is unsigned,0,0,others,clang
389,-fno-signed-zeros,Allow optimizations that ignore the sign of floating point zeros,1,4,limited-side-effect,clang
390,-fno-spell-checking,Disable spell-checking,1,6,function-tradeoff,clang
391,-fno-stack-protector,Disable the use of stack protectors,1,2,security-tradeoff,clang
392,-fno-stack-size-section,Don't emit section containing metadata on function stack sizes,1,6,function-tradeoff,clang
393,-fno-standalone-debug,Limit debug information produced to reduce size of debug binary,1,6,function-tradeoff,clang
394,-fno-strict-float-cast-overflow,Relax language rules and try to match the behavior of the target's native float-to-int conversion instructions,0,0,others,clang
395,-fno-temp-file,Directly create compilation output files. This may lead to incorrect incremental builds if the compiler crashes,0,0,others,clang
396,-fno-threadsafe-statics,Do not emit code to make initialization of local statics thread safe,1,6,function-tradeoff,clang
397,-fno-trigraphs,Do not process trigraph sequences,0,0,others,clang
398,-fno-unroll-loops,Turn off loop unroller,1,4,limited-side-effect,clang
399,-fno-use-cxa-atexit,Don't use __cxa_atexit for calling destructors,0,0,others,clang
400,-fno-use-init-array,Don't use .init_array instead of .ctors,0,0,others,clang
401,-fobjc-arc,Synthesize retain and release calls for Objective-C pointers,0,0,others,clang
402,-fobjc-arc-exceptions,Use EH-safe code when synthesizing retains and releases in -fobjc-arc,0,0,others,clang
403,-fobjc-exceptions,Enable Objective-C exceptions,0,0,others,clang
404,-fobjc-runtime,Specify the target Objective-C runtime kind and version,0,0,others,clang
405,-fobjc-weak,Enable ARC-style weak references in Objective-C,0,0,others,clang
406,-fopenmp,Parse OpenMP pragmas and generate parallel code.,0,0,others,clang
408,-fopenmp-targets,Specify comma-separated list of triples OpenMP offloading targets to be supported,0,0,others,clang
409,-foptimization-record-file,"Specify the output name of the file containing the optimization remarks. Implies -fsave-optimization-record. On Darwin platforms, this cannot be used with multiple -arch <arch> options.",0,0,others,clang
410,-foptimization-record-passes,"Only include passes which match a specified regular expression in the generated optimization record (by default, include all passes)",0,0,others,clang
411,-forder-file-instrumentation,Generate instrumented code to collect order file into default.profraw file (overridden by '=' form of option or LLVM_PROFILE_FILE env var),0,0,others,clang
412,-fpack-struct,Specify the default maximum struct packing alignment,0,0,others,clang
414,-fpass-plugin=dsopath,Load pass plugin from a dynamic shared object file (only with new pass manager).,0,0,others,clang
415,-fpatchable-function-entry,Generate M NOPs before function entry and N-M NOPs after function entry,0,0,others,clang
416,-fpcc-struct-return,Override the default ABI to return all structs on the stack,0,0,others,clang
418,-fplt,Use the PLT to make function calls,0,0,others,clang
419,-fplugin,Load the named plugin (dynamic shared object),0,0,others,clang
420,-fprebuilt-module-path=directory,Specify the prebuilt module path,0,0,others,clang
421,-fprofile-exclude-files,Instrument only functions from files where names don't match all the regexes separated by a semi-colon,0,0,others,clang
422,-fprofile-filter-files,Instrument only functions from files where names match any regex separated by a semi-colon,0,0,others,clang
423,-fprofile-generate,Generate instrumented code to collect execution counts into default.profraw (overridden by LLVM_PROFILE_FILE env var),0,0,others,clang
424,-fprofile-generate=directory,Generate instrumented code to collect execution counts into <directory>/default.profraw (overridden by LLVM_PROFILE_FILE env var),0,0,others,clang
425,-fprofile-instr-generate,Generate instrumented code to collect execution counts into default.profraw file (overridden by '=' form of option or LLVM_PROFILE_FILE env var),0,0,others,clang
426,-fprofile-instr-generate=file,Generate instrumented code to collect execution counts into <file> (overridden by LLVM_PROFILE_FILE env var),0,0,others,clang
428,-fprofile-remapping-file=file,Use the remappings described in <file> to match the profile data against names in the program,0,0,others,clang
429,-fprofile-sample-accurate,Specifies that the sample profile is accurate,0,0,others,clang
430,-fprofile-sample-use,Enable sample-based profile guided optimizations,1,4,limited-side-effect,clang
431,-fprofile-use,"Use instrumentation data for profile-guided optimization. If pathname is a directory, it reads from <pathname>/default.profdata. Otherwise, it reads from file <pathname>.",0,0,others,clang
432,-freciprocal-math,Allow division operations to be reassociated,0,0,others,clang
433,-fregister-global-dtors-with-atexit,Use atexit or __cxa_atexit to register global destructors,0,0,others,clang
434,-freg-struct-return,Override the default ABI to return small structs in registers,0,0,others,clang
436,-freroll-loops,Turn on loop reroller,0,0,others,clang
437,-frtlib-add-rpath,Add -rpath with architecture-specific resource directory to the linker flags,0,0,others,clang
438,-fsanitize,Turn on runtime checks for various forms of undefined or suspicious behavior. See user manual for available checks,1,2,security-tradeoff,clang
440,-fsanitize-address-globals-dead-stripping,Enable linker dead stripping of globals in AddressSanitizer,0,0,others,clang
441,-fsanitize-address-poison-custom-array-cookie,Enable poisoning array cookies when using custom operator new[] in AddressSanitizer,0,0,others,clang
442,-fsanitize-address-use-after-scope,Enable use-after-scope detection in AddressSanitizer,0,0,others,clang
443,-fsanitize-address-use-odr-indicator,Enable ODR indicator globals to avoid false ODR violation reports in partially sanitized programs at the cost of an increase in binary size,0,0,others,clang
444,-fsanitize-blacklist=value,Path to blacklist file for sanitizers,0,0,others,clang
446,-fsanitize-cfi-cross-dso,Enable control flow integrity (CFI) checks for cross-DSO calls.,1,2,security-tradeoff,clang
447,-fsanitize-cfi-icall-generalize-pointers,Generalize pointers in CFI indirect call type signature checks,0,0,others,clang
448,-fsanitize-coverage,Specify the type of coverage instrumentation for Sanitizers,0,0,others,clang
449,-fsanitize-hwaddress-abi,"Select the HWAddressSanitizer ABI to target (interceptor or platform, default interceptor). This option is currently unused.",0,0,others,clang
450,-fsanitize-memory-track-origins,Enable origins tracking in MemorySanitizer,0,0,others,clang
452,-fsanitize-recover,Enable recovery for specified sanitizers,0,0,others,clang
454,-fsanitize-system-blacklist=value,Path to system blacklist file for sanitizers,0,0,others,clang
455,-fsanitize-thread-atomics,Enable atomic operations instrumentation in ThreadSanitizer (default),0,0,others,clang
456,-fsanitize-thread-func-entry-exit,Enable function entry/exit instrumentation in ThreadSanitizer (default),0,0,others,clang
457,-fsanitize-thread-memory-access,Enable memory access instrumentation in ThreadSanitizer (default),0,0,others,clang
458,-fsanitize-trap,Enable trapping for specified sanitizers,0,0,others,clang
459,-fsanitize-undefined-strip-path-components,"Strip (or keep only, if negative) a given number of path components when emitting check metadata.",0,0,others,clang
460,-fsave-optimization-record,Generate an optimization record file in a specific format,0,0,others,clang
461,-fseh-exceptions,Use SEH style exceptions,0,0,others,clang
462,-fshort-enums,Allocate to an enum type only as many bytes as it needs for the declared range of possible values,0,0,others,clang
463,-fshort-wchar,Force wchar_t to be a short unsigned int,0,0,others,clang
464,-fshow-overloads,Which overload candidates to show when overload resolution fails: best|all; defaults to all,0,0,others,clang
465,-fsized-deallocation,Enable C++14 sized global deallocation functions,0,0,others,clang
467,-fslp-vectorize,Enable the superword-level parallelism vectorization passes,0,0,others,clang
469,-fsplit-lto-unit,Enables splitting of the LTO unit.,0,0,others,clang
470,-fstack-protector,"Enable stack protectors for some functions vulnerable to stack smashing. This uses a loose heuristic which considers functions vulnerable if they contain a char (or 8bit integer) array or constant sized calls to alloca, which are of greater size than ssp-buffer-size (default: 8 bytes). All variable sized calls to alloca are considered vulnerable",1,2,security-tradeoff,clang
473,-fstack-size-section,Emit section containing metadata on function stack sizes,1,6,function-tradeoff,clang
474,-fstandalone-debug,Emit full debug info for all types used by the program,0,0,others,clang
475,-fstrict-enums,Enable optimizations based on the strict definition of an enum's value range,1,4,limited-side-effect,clang
476,-fstrict-float-cast-overflow,Assume that overflowing float-to-int casts are undefined (default),0,0,others,clang
477,-fstrict-return,Always treat control flow paths that fall off the end of a non-void function as unreachable,0,0,others,clang
478,-fstrict-vtable-pointers,Enable optimizations based on the strict rules for overwriting polymorphic C++ objects,1,4,limited-side-effect,clang
479,-fthin-link-bitcode=value,Write minimized bitcode to <file> for the ThinLTO thin link only,0,0,others,clang
480,-fthinlto-index,Perform ThinLTO importing using provided function summary index,0,0,others,clang
481,-ftime-trace,Turn on time profiler. Generates JSON file based on output filename.,0,0,others,clang
483,-ftrap-function,Issue call to specified function rather than a trap instruction,0,0,others,clang
484,-ftrapv,Trap on integer overflow,0,0,others,clang
485,-ftrapv-handler=function,name> Specify the function to be called on overflow,0,0,others,clang
486,-ftrigraphs,Process trigraph sequences,0,0,others,clang
487,-ftrivial-auto-var-init,Initialize trivial automatic stack variables: uninitialized (default) | pattern,0,0,others,clang
488,-funique-section-names,Use unique names for text and data sections (ELF Only),0,0,others,clang
489,-funroll-loops,Turn on loop unroller,0,0,others,clang
490,-fuse-init-array,Use .init_array instead of .ctors,0,0,others,clang
491,-fvalidate-ast-input-files-content,Compute and store the hash of input files used to build an AST. Files with mismatching mtime's are considered valid if both contents is identical,0,0,others,clang
493,-fvectorize,Enable the loop vectorization passes,0,0,others,clang
494,-fvirtual-function-elimination,Enables dead virtual function elimination optimization. Requires -flto=full,0,0,others,clang
495,-fvisibility,Set the default symbol visibility for all global declarations,0,0,others,clang
496,-fvisibility-global-new-delete-hidden,Give global C++ operator new and delete declarations hidden visibility,0,0,others,clang
497,-fvisibility-inlines-hidden,Give inline C++ member functions hidden visibility by default,0,0,others,clang
498,-fvisibility-ms-compat,Give global types 'default' visibility and global functions and variables 'hidden' visibility by default,0,0,others,clang
499,-fwasm-exceptions,Use WebAssembly style exceptions,0,0,others,clang
500,-fwhole-program-vtables,Enables whole-program vtable optimization. Requires -flto,1,4,limited-side-effect,clang
501,-fwrapv,Treat signed integer overflow as two's complement,0,0,others,clang
502,-fwritable-strings,Store string literals as writable data,0,0,others,clang
503,-fxray-always-emit-customevents,Determine whether to always emit __xray_customevent(...) calls even if the function it appears in is not always instrumented.,0,0,others,clang
504,-fxray-always-emit-typedevents,Determine whether to always emit __xray_typedevent(...) calls even if the function it appears in is not always instrumented.,0,0,others,clang
505,-fxray-always-instrument=,DEPRECATED: Filename defining the whitelist for imbuing the 'always instrument' XRay attribute.,0,0,others,clang
506,-fxray-attr-list=,Filename defining the list of functions/types for imbuing XRay attributes.,0,0,others,clang
507,-fxray-instruction-threshold=,Sets the minimum function size to instrument with XRay,1,5,workload-specific,clang
508,-fxray-instrument,Generate XRay instrumentation sleds on function entry and exit,0,0,others,clang
509,-fxray-link-deps,Tells clang to add the link dependencies for XRay.,0,0,others,clang
510,-fxray-modes=,List of modes to link in by default into XRay instrumented binaries.,0,0,others,clang
511,-fxray-never-instrument=,DEPRECATED: Filename defining the whitelist for imbuing the 'never instrument' XRay attribute.,0,0,others,clang
513,-g,Generate source-level debug information,1,6,function-tradeoff,clang
514,-G(captal),Put objects of at most <size> bytes into small data section (MIPS / Hexagon),0,0,others,clang
515,--gcc-toolchain=value,Use the gcc toolchain at the given directory,0,0,others,clang
516,-gcodeview,Generate CodeView debug information,1,6,function-tradeoff,clang
519,-gdwarf-2,Generate source-level debug information with dwarf version 2,0,0,others,clang
521,-gdwarf-4,Generate source-level debug information with dwarf version 4,0,0,others,clang
522,-gdwarf-5,Generate source-level debug information with dwarf version 5,0,0,others,clang
523,-gembed-source,Embed source text in DWARF debug sections,0,0,others,clang
525,-gline-tables-only,Emit debug line number tables only,1,6,function-tradeoff,clang
527,-gno-embed-source,Restore the default behavior of not embedding source text in DWARF debug sections,0,0,others,clang
528,-gno-inline-line-tables,Don't emit inline line tables,1,6,function-tradeoff,clang
529,--gpu-max-threads-per-block,Default max threads per block for kernel launch bounds for HIP,1,1,resource,clang
530,-gsplit-dwarf,Set DWARF fission mode to either 'split' or 'single',0,0,others,clang
531,-gz,DWARF debug sections compression type,1,6,function-tradeoff,clang
532,-H,Show header includes and nesting depth,0,0,others,clang
533,-help,Display available options,0,0,others,clang
534,--help-hidden,Display help for hidden options,0,0,others,clang
535,--hip-device-lib,HIP device library,0,0,others,clang
536,--hip-device-lib-path=value,HIP device library path,0,0,others,clang
537,--hip-link,Link clang-offload-bundler bundles for HIP,0,0,others,clang
538,-I,Add directory to include search path,0,0,others,clang
539,-I-,Restrict all prior -I flags to double-quoted inclusion and remove current directory from include path,0,0,others,clang
540,-idirafter,Add directory to AFTER include search path,0,0,others,clang
541,-iframework,Add directory to SYSTEM framework search path,0,0,others,clang
542,-imacros,Include macros from file before parsing,0,0,others,clang
543,-include,Include file before parsing,0,0,others,clang
544,-include-pch,Include precompiled header file,0,0,others,clang
545,-index-header-map,Make the next included directory (-I or -F) an indexer header map,0,0,others,clang
546,-iprefix,Set the -iwithprefix/-iwithprefixbefore prefix,0,0,others,clang
547,-iquote,Add directory to QUOTE include search path,0,0,others,clang
548,-isysroot,Set the system root directory (usually /),0,0,others,clang
549,-isystem,Add directory to SYSTEM include search path,0,0,others,clang
550,-isystem-after,Add directory to end of the SYSTEM include search path,0,0,others,clang
551,-ivfsoverlay,Overlay the virtual filesystem described by file over the real file system,0,0,others,clang
552,-iwithprefix,Set directory to SYSTEM include search path with prefix,0,0,others,clang
553,-iwithprefixbefore,Set directory to include search path with prefix,0,0,others,clang
554,-L,Add directory to library search path,0,0,others,clang
555,--libomptarget-nvptx-path=value,Path to libomptarget-nvptx libraries,0,0,others,clang
556,-M,"Like -MD, but also implies -E and writes to stdout by default",0,0,others,clang
557,-mabicalls,Enable SVR4-style position-independent code (Mips only),0,0,others,clang
558,-malign-double,Align doubles to two words in structs (x86 only),0,0,others,clang
559,-mbackchain,Link stack frames through backchain on System Z,0,0,others,clang
560,-mbranch-protection,Enforce targets of indirect branches and function returns,0,0,others,clang
561,-mcmodel=medany,"Equivalent to -mcmodel=medium, compatible with RISC-V gcc.",0,0,others,clang
563,-mcmse,Allow use of CMSE (Armv8-M Security Extensions),0,0,others,clang
564,-mcode-object-v3,Enable code object v3 (AMDGPU only),0,0,others,clang
565,-mcrc,Allow use of CRC instructions (ARM/Mips only),0,0,others,clang
566,-mcumode,CU wavefront execution mode is used (AMDGPU only),0,0,others,clang
568,-membedded-data,Place constants in the .rodata section instead of the .sdata section even if they meet the -G <size> threshold (MIPS),1,5,workload-specific,clang
569,-mexecute-only,Disallow generation of data access to code sections (ARM only),0,0,others,clang
571,-mfentry,Insert calls to fentry at function entry (x86/SystemZ only),0,0,others,clang
572,-mfix-cortex-a53-835769,Workaround Cortex-A53 erratum 835769 (AArch64 only),0,0,others,clang
574,-mfp64,Use 64-bit floating point registers (MIPS only),0,0,others,clang
575,-MG,Add missing headers to depfile,0,0,others,clang
576,-mgeneral-regs-only,Generate code which only uses the general purpose registers (AArch64 only),0,0,others,clang
577,-mglobal-merge,Enable merging of globals,0,0,others,clang
578,-mgpopt,Use GP relative accesses for symbols known to be in a small data section (MIPS),0,0,others,clang
579,-mhvx,Enable Hexagon Vector eXtensions,0,0,others,clang
580,-mhvx-length,Set Hexagon Vector Length,0,0,others,clang
581,-miamcu,Use Intel MCU ABI,0,0,others,clang
582,--migrate,Run the migrator,0,0,others,clang
583,-mincremental-linker-compatible,(integrated-as) Emit an object file which can be used with an incremental linker,0,0,others,clang
586,-MJ,Write a compilation database entry per input,0,0,others,clang
588,-mlocal-sdata,Extend the -G behaviour to object local data (MIPS),0,0,others,clang
589,-mlong-calls,"Generate branches with extended addressability, usually via indirect jumps.",0,0,others,clang
590,-mlong-double-128,Force long double to be 128 bits,0,0,others,clang
592,-mlong-double-80,"Force long double to be 80 bits, padded to 128 bits for storage",0,0,others,clang
594,-mmacosx-version-min,Set Mac OS X deployment target,0,0,others,clang
595,-mmadd4,"Enable the generation of 4-operand madd.s, madd.d and related instructions.",0,0,others,clang
597,-mmemops,Enable generation of memop instructions,0,0,others,clang
598,-mmsa,Enable MSA ASE (MIPS only),0,0,others,clang
599,-mms-bitfields,Set the default structure layout to be compatible with the Microsoft compiler standard,0,0,others,clang
600,-mmt,Enable MT ASE (MIPS only),0,0,others,clang
601,-mno-abicalls,Disable SVR4-style position-independent code (Mips only),0,0,others,clang
603,-mnocrc,Disallow use of CRC instructions (ARM only),0,0,others,clang
604,-mno-crc,Disallow use of CRC instructions (Mips only),0,0,others,clang
606,-mno-embedded-data,Do not place constants in the .rodata section instead of the .sdata if they meet the -G <size> threshold (MIPS),1,5,workload-specific,clang
607,-mno-execute-only,Allow generation of data access to code sections (ARM only),0,0,others,clang
609,-mno-fix-cortex-a53-835769,Don't workaround Cortex-A53 erratum 835769 (AArch64 only),0,0,others,clang
611,-mno-gpopt,Do not use GP relative accesses for symbols known to be in a small data section (MIPS),0,0,others,clang
612,-mno-hvx,Disable Hexagon Vector eXtensions,0,0,others,clang
613,-mno-implicit-float,Don't generate implicit floating point instructions,0,0,others,clang
614,-mno-incremental-linker-compatible,(integrated-as) Emit an object file which cannot be used with an incremental linker,0,0,others,clang
615,-mno-local-sdata,Do not extend the -G behaviour to object local data (MIPS),0,0,others,clang
618,-mno-memops,Disable generation of memop instructions,0,0,others,clang
620,-mno-msa,Disable MSA ASE (MIPS only),0,0,others,clang
622,-mno-mt,Disable MT ASE (MIPS only),0,0,others,clang
623,-mno-neg-immediates,Disallow converting instructions with negative immediates to their negation or inversion.,1,6,function-tradeoff,clang
624,-mno-nvj,Disable generation of new-value jumps,0,0,others,clang
625,-mno-nvs,Disable generation of new-value stores,0,0,others,clang
626,-mno-outline,Disable function outlining (AArch64 only),0,0,others,clang
627,-mno-packets,Disable generation of instruction packets,0,0,others,clang
628,-mnop-mcount,Generate mcount/__fentry__ calls as nops. To activate they need to be patched in.,0,0,others,clang
630,-mno-restrict-it,Allow generation of deprecated IT blocks for ARMv8. It is off by default for ARMv8 Thumb mode,0,0,others,clang
631,-mno-save-restore,Disable using library calls for save and restore,0,0,others,clang
632,-mno-sram-ecc,Disable SRAM ECC (AMDGPU only),0,0,others,clang
633,-mno-stack-arg-probe,Disable stack probes which are enabled by default,0,0,others,clang
634,-mno-tls-direct-seg-refs,Disable direct TLS access through segment registers,0,0,others,clang
635,-mno-unaligned-access,Force all memory accesses to be aligned (AArch32/AArch64 only),0,0,others,clang
636,-mno-wavefrontsize64,Wavefront size 32 is used,0,0,others,clang
637,-mno-xnack,Disable XNACK (AMDGPU only),0,0,others,clang
638,-mnvj,Enable generation of new-value jumps,0,0,others,clang
639,-mnvs,Enable generation of new-value stores,0,0,others,clang
640,-module-dependency-dir,Directory to dump module dependencies to,0,0,others,clang
641,-module-file-info,Provide information about a particular module file,0,0,others,clang
642,-momit-leaf-frame-pointer,Omit frame pointer setup for leaf functions,0,0,others,clang
643,-moutline,Enable function outlining (AArch64 only),0,0,others,clang
644,-MP,Create phony target for each dependency (other than main file),0,0,others,clang
645,-mpacked-stack,Use packed stack layout (SystemZ only).,0,0,others,clang
646,-mpackets,Enable generation of instruction packets,0,0,others,clang
647,-mpie-copy-relocations,Use copy relocations support for PIE builds,0,0,others,clang
648,-mprefer-vector-width,Specifies preferred vector width for auto-vectorization. Defaults to 'none' which allows target specific decisions.,0,0,others,clang
649,-MQ,Specify name of main file output to quote in depfile,0,0,others,clang
650,-mqdsp6-compat,Enable hexagon-qdsp6 backward compatibility,0,0,others,clang
651,-mrecord-mcount,Generate a __mcount_loc section entry for each __fentry__ call.,0,0,others,clang
652,-mrelax,Enable linker relaxation,0,0,others,clang
653,-mrelax-all,(integrated-as) Relax all machine instructions,0,0,others,clang
654,-mrestrict-it,Disallow generation of deprecated IT blocks for ARMv8. It is on by default for ARMv8 Thumb mode.,0,0,others,clang
655,-mrtd,Make StdCall calling convention the default,0,0,others,clang
656,-msave-restore,Enable using library calls for save and restore,0,0,others,clang
657,-msign-return-address=value,Select return address signing scope,0,0,others,clang
659,-msram-ecc,Enable SRAM ECC (AMDGPU only),0,0,others,clang
661,-mstack-arg-probe,Enable stack probes,0,0,others,clang
663,-mstackrealign,Force realign the stack at entry to every function,0,0,others,clang
664,-MT,Specify name of main file output in depfile,0,0,others,clang
665,-mtls-direct-seg-refs,Enable direct TLS access through segment registers (default),0,0,others,clang
666,-mtls-size,"Specify bit size of immediate TLS offsets (AArch64 ELF only): 12 (for 4KB) | 24 (for 16MB, default) | 32 (for 4GB) | 48 (for 256TB, needs -mcmodel=large)",0,0,others,clang
667,-mtp,Thread pointer access method (AArch32/AArch64 only),0,0,others,clang
668,-munaligned-access,Allow memory accesses to be unaligned (AArch32/AArch64 only),0,0,others,clang
669,-MV,Use NMake/Jom format for the depfile,0,0,others,clang
670,-mwavefrontsize64,Wavefront size 64 is used,0,0,others,clang
672,-nobuiltininc,Disable builtin #include directories,0,0,others,clang
673,--no-cuda-gpu-arch,Remove GPU architecture (e.g. sm_35) from the list of GPUs to compile for. 'all' resets the list to its default value.,0,0,others,clang
674,--no-cuda-include-ptx,Do not include PTX for the following GPU architecture (e.g. sm_35) or 'all'. May be specified more than once.,0,0,others,clang
675,--no-cuda-version-check,Don't error out if the detected version of the CUDA install is too low for the requested CUDA gpu architecture.,0,0,others,clang
676,-nogpulib,Do not link device library for CUDA/HIP device compilation,0,0,others,clang
677,-nostdinc++,Disable standard #include directories for the C++ standard library,0,0,others,clang
678,--no-system-header-prefix,Treat all #include paths starting with <prefix> as not including a system header.,0,0,others,clang
679,-o,Write output to <file>,0,0,others,clang
680,-ObjC,Treat source input files as Objective-C inputs,0,0,others,clang
681,-ObjC++,Treat source input files as Objective-C++ inputs,0,0,others,clang
682,-objcmt-atomic-property,Make migration to 'atomic' properties,0,0,others,clang
683,-objcmt-migrate-all,Enable migration to modern ObjC,0,0,others,clang
684,-objcmt-migrate-annotation,Enable migration to property and method annotations,0,0,others,clang
685,-objcmt-migrate-designated-init,Enable migration to infer NS_DESIGNATED_INITIALIZER for initializer methods,0,0,others,clang
686,-objcmt-migrate-instancetype,Enable migration to infer instancetype for method result type,0,0,others,clang
687,-objcmt-migrate-literals,Enable migration to modern ObjC literals,0,0,others,clang
688,-objcmt-migrate-ns-macros,Enable migration to NS_ENUM/NS_OPTIONS macros,0,0,others,clang
689,-objcmt-migrate-property,Enable migration to modern ObjC property,0,0,others,clang
690,-objcmt-migrate-property-dot-syntax,Enable migration of setter/getter messages to property-dot syntax,0,0,others,clang
691,-objcmt-migrate-protocol-conformance,Enable migration to add protocol conformance on classes,0,0,others,clang
692,-objcmt-migrate-readonly-property,Enable migration to modern ObjC readonly property,0,0,others,clang
693,-objcmt-migrate-readwrite-property,Enable migration to modern ObjC readwrite property,0,0,others,clang
694,-objcmt-migrate-subscripting,Enable migration to modern ObjC subscripting,0,0,others,clang
695,-objcmt-ns-nonatomic-iosonly,Enable migration to use NS_NONATOMIC_IOSONLY macro for setting property's 'atomic' attribute,0,0,others,clang
696,-objcmt-returns-innerpointer-property,Enable migration to annotate property with NS_RETURNS_INNER_POINTER,0,0,others,clang
697,-objcmt-whitelist-dir-path=value,Only modify files with a filename contained in the provided directory path,0,0,others,clang
698,-P,Disable linemarker output in -E mode,1,6,function-tradeoff,clang
699,-pg,Enable mcount instrumentation,0,0,others,clang
700,-pipe,"Use pipes between commands, when possible",0,0,others,clang
702,-print-effective-triple,Print the effective target triple,1,6,function-tradeoff,clang
703,-print-file-name,Print the full library path of <file>,1,6,function-tradeoff,clang
705,-print-libgcc-file-name,"Print the library path for the currently used compiler runtime library (""libgcc.a"" or ""libclang_rt.builtins.*.a"")",1,6,function-tradeoff,clang
706,-print-prog-name,Print the full program path of <name>,1,6,function-tradeoff,clang
707,-print-resource-dir,Print the resource directory pathname,1,6,function-tradeoff,clang
708,-print-search-dirs,Print the paths used for finding libraries and programs,1,6,function-tradeoff,clang
709,-print-supported-cpus,"Print supported cpu models for the given target (if target is not specified, it will print the supported cpus for the default target)",1,6,function-tradeoff,clang
710,-print-target-triple,Print the normalized target triple,1,6,function-tradeoff,clang
711,-pthread,Support POSIX threads in generated code,0,0,others,clang
712,--ptxas-path=value,Path to ptxas (used for compiling CUDA code),0,0,others,clang
713,-Qn,Do not emit metadata containing compiler name and version,0,0,others,clang
714,-Qunused-arguments,Don't emit warning for unused driver arguments,0,0,others,clang
715,-Qy,Emit metadata containing compiler name and version,0,0,others,clang
717,-rewrite-legacy-objc,Rewrite Legacy Objective-C source to C++,0,0,others,clang
719,-Rpass,Report transformations performed by optimization passes whose name matches the given POSIX regular expression,0,0,others,clang
720,-Rpass-analysis,Report transformation analysis from optimization passes whose name matches the given POSIX regular expression,1,6,function-tradeoff,clang
721,-Rpass-missed,Report missed transformations by optimization passes whose name matches the given POSIX regular expression,1,6,function-tradeoff,clang
722,-Rremark,Enable the specified remark,0,0,others,clang
723,-rtlib,Compiler runtime library to use,0,0,others,clang
724,-S,Only run preprocess and compilation steps,0,0,others,clang
725,-save-stats,Save llvm statistics.,1,6,function-tradeoff,clang
726,-save-temps,Save intermediate compilation results,1,6,function-tradeoff,clang
727,-serialize-diagnostics,Serialize compiler diagnostics to a file,0,0,others,clang
728,-shared-libsan,Dynamically link the sanitizer runtime,0,0,others,clang
730,-static-openmp,Use the static host OpenMP runtime while linking.,0,0,others,clang
732,-stdlib,C++ standard library to use,0,0,others,clang
733,-stdlib++-isystem,Use directory as the C++ standard library include path,0,0,others,clang
734,--system-header-prefix,Treat all #include paths starting with <prefix> as including a system header.,0,0,others,clang
735,-T,Specify <script> as linker script,0,0,others,clang
737,-Tbss,Set starting address of BSS to <addr>,0,0,others,clang
738,-Tdata,Set starting address of DATA to <addr>,0,0,others,clang
741,-trigraphs,Process trigraph sequences,0,0,others,clang
742,-Ttext,Set starting address of TEXT to <addr>,0,0,others,clang
743,-U,Undefine macro <macro>,0,0,others,clang
744,-undef,undef all system defines,0,0,others,clang
745,-unwindlib,Unwind library to use,0,0,others,clang
747,--verify-debug-info,Verify the binary representation of debug output,1,6,function-tradeoff,clang
748,-verify-pch,Load and verify that a pre-compiled header file is not stale,0,0,others,clang
750,-w,Suppress all warnings,1,6,function-tradeoff,clang
751,"-Wa,arg",Pass the comma separated arguments in <arg> to the assembler,0,0,others,clang
752,-Wdeprecated,Enable warnings for deprecated constructs and define __DEPRECATED,0,0,others,clang
753,"-Wl,arg",Pass the comma separated arguments in <arg> to the linker,0,0,others,clang
754,-working-directory,Resolve file paths relative to the specified directory,0,0,others,clang
756,-Wwarning,Enable the specified warning,1,6,function-tradeoff,clang
757,-x,Treat subsequent input files as having type <language>,0,0,others,clang
758,-Xanalyzer,Pass <arg> to the static analyzer,0,0,others,clang
759,-Xassembler,Pass <arg> to the assembler,0,0,others,clang
760,-Xclang,Pass <arg> to the clang compiler,0,0,others,clang
763,-Xlinker,Pass <arg> to the linker,0,0,others,clang
764,-Xopenmp-target,Pass <arg> to the target offloading toolchain identified by <triple>.,0,0,others,clang
765,-Xpreprocessor,Pass <arg> to the preprocessor,0,0,others,clang
766,-z,Pass -z <arg> to the linker,0,0,others,clang
768,dfs.ha.fencing.methods,List of fencing methods to use for service fencing. May contain builtin methods (eg shell and sshfence) or user-defined method.,0,0,others,core
769,dfs.ha.fencing.ssh.connect-timeout,"SSH connection timeout, in milliseconds, to use with the builtin sshfence fencer.",0,0,others,core
770,dfs.ha.fencing.ssh.private-key-files,The SSH private key files to use with the builtin sshfence fencer.,0,0,others,core
771,file.blocksize,Block size,1,5,workload-specific,core
773,file.client-write-packet-size,Packet size for clients to write,0,0,others,core
775,file.stream-buffer-size,"The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",1,1,resource,core
776,fs.AbstractFileSystem.file.impl,The AbstractFileSystem for file: uris.,0,0,others,core
777,fs.AbstractFileSystem.ftp.impl,The FileSystem for Ftp: uris.,0,0,others,core
780,fs.AbstractFileSystem.s3a.impl,The implementation class of the S3A AbstractFileSystem.,0,0,others,core
781,fs.AbstractFileSystem.swebhdfs.impl,The FileSystem for swebhdfs: uris.,0,0,others,core
782,fs.AbstractFileSystem.viewfs.impl,The AbstractFileSystem for view file system for viewfs: uris (ie client side mount table:).,0,0,others,core
783,fs.AbstractFileSystem.webhdfs.impl,The FileSystem for webhdfs: uris.,0,0,others,core
784,fs.adl.oauth2.access.token.provider,The class name of the OAuth2 access token provider.,0,0,others,core
785,fs.adl.oauth2.access.token.provider.type,"Defines Azure Active Directory OAuth2 access token provider type. Supported types are ClientCredential, RefreshToken, MSI, DeviceCode, and Custom. The ClientCredential type requires property fs.adl.oauth2.client.id, fs.adl.oauth2.credential, and fs.adl.oauth2.refresh.url. The RefreshToken type requires property fs.adl.oauth2.client.id and fs.adl.oauth2.refresh.token. The MSI type reads optional property fs.adl.oauth2.msi.port, if specified. The DeviceCode type requires property fs.adl.oauth2.devicecode.clientapp.id. The Custom type requires property fs.adl.oauth2.access.token.provider.",0,0,others,core
786,fs.adl.oauth2.client.id,The OAuth2 client id.,0,0,others,core
788,fs.adl.oauth2.devicecode.clientapp.id,The app id of the AAD native app in whose context the auth request should be made. Used by DeviceCode token provider.,0,0,others,core
789,fs.adl.oauth2.msi.port,"The localhost port for the MSI token service. This is the port specified when creating the Azure VM. The default, if this setting is not specified, is 50342. Used by MSI token provider.",0,0,others,core
790,fs.adl.oauth2.refresh.token,The OAuth2 refresh token.,0,0,others,core
791,fs.adl.oauth2.refresh.url,The OAuth2 token endpoint.,0,0,others,core
792,fs.automatic.close,"By default, FileSystem instances are automatically closed at program exit using a JVM shutdown hook. Setting this property to false disables this behavior. This is an advanced option that should only be used by server applications requiring a more carefully orchestrated shutdown sequence.",1,3,reliability-tradeoff,core
793,fs.azure.authorization,"Config flag to enable authorization support in WASB. Setting it to ""true"" enables authorization support to WASB. Currently WASB authorization requires a remote service to provide authorization that needs to be specified via fs.azure.authorization.remote.service.url configuration",1,2,security-tradeoff,core
795,fs.azure.local.sas.key.mode,"Works in conjuction with fs.azure.secure.mode. Setting this config to true results in fs.azure.NativeAzureFileSystem using the local SAS key generation where the SAS keys are generating in the same process as fs.azure.NativeAzureFileSystem. If fs.azure.secure.mode flag is set to false, this flag has no effect.",0,0,others,core
796,fs.azure.sas.expiry.period,"The default value to be used for expiration period for SAS keys generated. Can use the following suffix (case insensitive): ms(millis), s(sec), m(min), h(hour), d(day) to specify the time (such as 2s, 2m, 1h, etc.).",0,0,others,core
797,fs.azure.saskey.usecontainersaskeyforallaccess,Use container saskey for access to all blobs within the container. Blob-specific saskeys are not used when this setting is enabled. This setting provides better performance compared to blob-specific saskeys.,1,4,limited-side-effect,core
798,fs.azure.secure.mode,"Config flag to identify the mode in which fs.azure.NativeAzureFileSystem needs to run under. Setting it ""true"" would make fs.azure.NativeAzureFileSystem use SAS keys to communicate with Azure storage.",1,2,security-tradeoff,core
799,fs.azure.user.agent.prefix,"WASB passes User-Agent header to the Azure back-end. The default value contains WASB version, Java Runtime version, Azure Client library version, and the value of the configuration option fs.azure.user.agent.prefix.",0,0,others,core
800,fs.client.htrace.sampler.classes,The class names of the HTrace Samplers to use for Hadoop filesystem clients.,0,0,others,core
802,fs.client.resolve.topology.enabled,"Whether the client machine will use the class specified by property net.topology.node.switch.mapping.impl to compute the network distance between itself and remote machines of the FileSystem. Additional properties might need to be configured depending on the class specified in net.topology.node.switch.mapping.impl. For example, if org.apache.hadoop.net.ScriptBasedMapping is used, a valid script file needs to be specified in net.topology.script.file.name.",0,0,others,core
803,fs.default.name,Deprecated. Use (fs.defaultFS) property instead,0,0,others,core
804,fs.defaultFS,"The name of the default file system. A URI whose scheme and authority determine the FileSystem implementation. The uri's scheme determines the config property (fs.SCHEME.impl) naming the FileSystem implementation class. The uri's authority is used to determine the host, port, etc. for a filesystem.",0,0,others,core
805,fs.df.interval,Disk usage statistics refresh interval in msec.,0,0,others,core
806,fs.du.interval,File space usage statistics refresh interval in msec.,0,0,others,core
807,fs.ftp.data.connection.mode,"Set the FTPClient's data connection mode based on configuration. Valid values are ACTIVE_LOCAL_DATA_CONNECTION_MODE, PASSIVE_LOCAL_DATA_CONNECTION_MODE and PASSIVE_REMOTE_DATA_CONNECTION_MODE.",0,0,others,core
808,fs.ftp.host,FTP filesystem connects to this server,0,0,others,core
809,fs.ftp.host.port,FTP filesystem connects to fs.ftp.host on this port,0,0,others,core
811,fs.har.impl.disable.cache,Don't cache 'har' filesystem instances.,1,4,limited-side-effect,core
812,fs.permissions.umask-mode,"The umask used when creating files and directories. Can be in octal or in symbolic. Examples are: ""022"" (octal for u=rwx,g=r-x,o=r-x in symbolic), or ""u=rwx,g=rwx,o="" (symbolic for 007 in octal).",0,0,others,core
813,fs.protected.directories,A comma-separated list of directories which cannot be deleted even by the superuser unless they are empty. This setting can be used to guard important system directories against accidental deletion due to administrator error.,0,0,others,core
814,fs.s3.awsAccessKeyId,AWS access key ID used by S3 block file system.,0,0,others,core
815,fs.s3.awsSecretAccessKey,AWS secret key used by S3 block file system.,0,0,others,core
816,fs.s3.block.size,Block size to use when writing files to S3.,1,5,workload-specific,core
817,fs.s3.buffer.dir,Determines where on the local filesystem the s3:/s3n: filesystem should store files before sending them to S3 (or after retrieving them from S3).,0,0,others,core
818,fs.s3.maxRetries,"The maximum number of retries for reading or writing files to S3, before we signal failure to the application.",0,0,others,core
822,fs.s3a.attempts.maximum,How many times we should retry commands on transient errors.,0,0,others,core
823,fs.s3a.aws.credentials.provider,"Comma-separated class names of credential provider classes which implement com.amazonaws.auth.AWSCredentialsProvider. These are loaded and queried in sequence for a valid set of credentials. Each listed class must implement one of the following means of construction, which are attempted in order: 1. a public constructor accepting java.net.URI and org.apache.hadoop.conf.Configuration, 2. a public static method named getInstance that accepts no arguments and returns an instance of com.amazonaws.auth.AWSCredentialsProvider, or 3. a public default constructor. Specifying org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider allows anonymous access to a publicly accessible S3 bucket without any credentials. Please note that allowing anonymous access to an S3 bucket compromises security and therefore is unsuitable for most use cases. It can be useful for accessing public data sets without requiring AWS credentials. If unspecified, then the default list of credential provider classes, queried in sequence, is: 1. org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider: supports static configuration of AWS access key ID and secret access key. See also fs.s3a.access.key and fs.s3a.secret.key. 2. com.amazonaws.auth.EnvironmentVariableCredentialsProvider: supports configuration of AWS access key ID and secret access key in environment variables named AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY, as documented in the AWS SDK. 3. org.apache.hadoop.fs.s3a.SharedInstanceProfileCredentialsProvider: a shared instance of com.amazonaws.auth.InstanceProfileCredentialsProvider from the AWS SDK, which supports use of instance profile credentials if running in an EC2 VM. Using this shared instance potentially reduces load on the EC2 instance metadata service for multi-threaded applications.",0,0,others,core
824,fs.s3a.block.size,"Block size to use when reading files using s3a: file system. A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.",1,5,workload-specific,core
826,fs.s3a.connection.establish.timeout,Socket connection setup timeout in milliseconds.,0,0,others,core
827,fs.s3a.connection.maximum,Controls the maximum number of simultaneous connections to S3.,1,1,resource,core
828,fs.s3a.connection.ssl.enabled,Enables or disables SSL connections to S3.,1,2,security-tradeoff,core
829,fs.s3a.connection.timeout,Socket connection timeout in milliseconds.,0,0,others,core
830,fs.s3a.endpoint,"AWS S3 endpoint to connect to. An up-to-date list is provided in the AWS Documentation: regions and endpoints. Without this property, the standard region (s3.amazonaws.com) is assumed.",0,0,others,core
833,fs.s3a.fast.upload.buffer,"The buffering mechanism to use when using S3A fast upload (fs.s3a.fast.upload=true). Values: disk, array, bytebuffer. This configuration option has no effect if fs.s3a.fast.upload is false. ""disk"" will use the directories listed in fs.s3a.buffer.dir as the location(s) to save data prior to being uploaded. ""array"" uses arrays in the JVM heap ""bytebuffer"" uses off-heap memory within the JVM. Both ""array"" and ""bytebuffer"" will consume memory in a single stream up to the number of blocks set by: fs.s3a.multipart.size * fs.s3a.fast.upload.active.blocks. If using either of these mechanisms, keep this value low The total number of threads performing work across all threads is set by fs.s3a.threads.max, with fs.s3a.max.total.tasks values setting the number of queued work items.",0,0,others,core
834,fs.s3a.impl,The implementation class of the S3A Filesystem,0,0,others,core
837,fs.s3a.metadatastore.impl,"Fully-qualified name of the class that implements the MetadataStore to be used by s3a. The default class, NullMetadataStore, has no effect: s3a will continue to treat the backing S3 service as the one and only source of truth for file and directory metadata.",0,0,others,core
838,fs.s3a.multiobjectdelete.enable,"When enabled, multiple single-object delete requests are replaced by a single 'delete multiple objects'-request, reducing the number of requests. Beware: legacy S3-compatible object stores might not support this request.",0,0,others,core
839,fs.s3a.multipart.purge,"True if you want to purge existing multipart uploads that may not have been completed/aborted correctly. The corresponding purge age is defined in fs.s3a.multipart.purge.age. If set, when the filesystem is instantiated then all outstanding uploads older than the purge age will be terminated -across the entire bucket. This will impact multipart uploads by other applications and users. so should be used sparingly, with an age value chosen to stop failed uploads, without breaking ongoing operations.",1,3,reliability-tradeoff,core
840,fs.s3a.multipart.purge.age,Minimum age in seconds of multipart uploads to purge.,0,0,others,core
841,fs.s3a.multipart.size,"How big (in bytes) to split upload or copy operations up into. A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.",1,5,workload-specific,core
842,fs.s3a.multipart.threshold,"How big (in bytes) to split upload or copy operations up into. This also controls the partition size in renamed files, as rename() involves copying the source file(s). A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.",1,5,workload-specific,core
843,fs.s3a.paging.maximum,How many keys to request from S3 when doing directory listings at a time.,0,0,others,core
844,fs.s3a.path.style.access,Enable S3 path style access ie disabling the default virtual hosting behaviour. Useful for S3A-compliant storage providers as it removes the need to set up DNS for virtual hosting.,1,6,function-tradeoff,core
845,fs.s3a.proxy.domain,Domain for authenticating with proxy server.,0,0,others,core
846,fs.s3a.proxy.host,Hostname of the (optional) proxy server for S3 connections.,0,0,others,core
847,fs.s3a.proxy.password,Password for authenticating with proxy server.,0,0,others,core
848,fs.s3a.proxy.port,"Proxy server port. If this property is not set but fs.s3a.proxy.host is, port 80 or 443 is assumed (consistent with the value of fs.s3a.connection.ssl.enabled).",0,0,others,core
849,fs.s3a.proxy.username,Username for authenticating with proxy server.,0,0,others,core
850,fs.s3a.proxy.workstation,Workstation for authenticating with proxy server.,0,0,others,core
852,fs.s3a.s3guard.cli.prune.age,Default age (in milliseconds) after which to prune metadata from the metadatastore when the prune command is run. Can be overridden on the command-line.,0,0,others,core
853,fs.s3a.s3guard.ddb.background.sleep,Length (in milliseconds) of pause between each batch of deletes when pruning metadata. Prevents prune operations (which can typically be low priority background operations) from overly interfering with other I/O operations.,0,0,others,core
855,fs.s3a.s3guard.ddb.region,"AWS DynamoDB region to connect to. An up-to-date list is provided in the AWS Documentation: regions and endpoints. Without this property, the S3Guard will operate table in the associated S3 bucket region.",0,0,others,core
856,fs.s3a.s3guard.ddb.table,"The DynamoDB table name to operate. Without this property, the respective S3 bucket name will be used.",0,0,others,core
857,fs.s3a.s3guard.ddb.table.capacity.read,"Provisioned throughput requirements for read operations in terms of capacity units for the DynamoDB table. This config value will only be used when creating a new DynamoDB table, though later you can manually provision by increasing or decreasing read capacity as needed for existing tables. See DynamoDB documents for more information.",0,0,others,core
858,fs.s3a.s3guard.ddb.table.capacity.write,Provisioned throughput requirements for write operations in terms of capacity units for the DynamoDB table. Refer to related config fs.s3a.s3guard.ddb.table.capacity.read before usage.,0,0,others,core
859,fs.s3a.s3guard.ddb.table.create,"If true, the S3A client will create the table if it does not already exist.",0,0,others,core
860,fs.s3a.secret.key,AWS secret key used by S3A file system. Omit for IAM role-based or provider-based authentication.,0,0,others,core
861,fs.s3a.security.credential.provider.path,"Optional comma separated list of credential providers, a list which is prepended to that set in hadoop.security.credential.provider.path",0,0,others,core
862,fs.s3a.server-side-encryption.key,"Specific encryption key to use if fs.s3a.server-side-encryption-algorithm has been set to 'SSE-KMS' or 'SSE-C'. In the case of SSE-C, the value of this property should be the Base64 encoded key. If you are using SSE-KMS and leave this property empty, you'll be using your default's S3 KMS key, otherwise you should set this property to the specific KMS key id.",0,0,others,core
863,fs.s3a.server-side-encryption-algorithm,"Specify a server-side encryption algorithm for s3a: file system. Unset by default. It supports the following values: 'AES256' (for SSE-S3), 'SSE-KMS' and 'SSE-C'.",1,2,security-tradeoff,core
864,fs.s3a.session.token,"Session token, when using org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider as one of the providers.",0,0,others,core
866,fs.s3a.socket.recv.buffer,Socket receive buffer hint to amazon connector. Represented in bytes.,1,1,resource,core
867,fs.s3a.socket.send.buffer,Socket send buffer hint to amazon connector. Represented in bytes.,1,1,resource,core
868,fs.s3a.threads.keepalivetime,Number of seconds a thread can be idle before being terminated.,0,0,others,core
869,fs.s3a.threads.max,The total number of threads available in the filesystem for data uploads *or any other queued filesystem operation*.,1,1,resource,core
870,fs.s3a.user.agent.prefix,"Sets a custom value that will be prepended to the User-Agent header sent in HTTP requests to the S3 back-end by S3AFileSystem. The User-Agent header always includes the Hadoop version number followed by a string generated by the AWS SDK. An example is ""User-Agent: Hadoop 2.8.0, aws-sdk-java/1.10.6"". If this optional property is set, then its value is prepended to create a customized User-Agent. For example, if this configuration property was set to ""MyApp"", then an example of the resulting User-Agent would be ""User-Agent: MyApp, Hadoop 2.8.0, aws-sdk-java/1.10.6"".",0,0,others,core
871,fs.s3n.awsAccessKeyId,AWS access key ID used by S3 native file system.,0,0,others,core
872,fs.s3n.awsSecretAccessKey,AWS secret key used by S3 native file system.,0,0,others,core
873,fs.s3n.block.size,Block size to use when reading files using the native S3 filesystem (s3n: URIs).,1,5,workload-specific,core
876,fs.s3n.multipart.uploads.enabled,"Setting this property to true enables multiple uploads to native S3 filesystem. When uploading a file, it is split into blocks if the size is larger than fs.s3n.multipart.uploads.block.size.",1,4,limited-side-effect,core
877,fs.s3n.server-side-encryption-algorithm,"Specify a server-side encryption algorithm for S3. Unset by default, and the only other currently allowable value is AES256.",0,0,others,core
878,fs.swift.impl,The implementation class of the OpenStack Swift Filesystem,0,0,others,core
879,fs.trash.checkpoint.interval,"Number of minutes between trash checkpoints. Should be smaller or equal to fs.trash.interval. If zero, the value is set to the value of fs.trash.interval. Every time the checkpointer runs it creates a new checkpoint out of current and removes checkpoints created more than fs.trash.interval minutes ago.",0,0,others,core
881,fs.viewfs.rename.strategy,"Allowed rename strategy to rename between multiple mountpoints. Allowed values are SAME_MOUNTPOINT,SAME_TARGET_URI_ACROSS_MOUNTPOINT and SAME_FILESYSTEM_ACROSS_MOUNTPOINT.",0,0,others,core
882,fs.wasb.impl,The implementation class of the Native Azure Filesystem,0,0,others,core
883,fs.wasbs.impl,The implementation class of the Secure Native Azure Filesystem,0,0,others,core
885,ftp.bytes-per-checksum,The number of bytes per checksum. Must not be larger than ftp.stream-buffer-size,0,0,others,core
886,ftp.client-write-packet-size,Packet size for clients to write,0,0,others,core
887,ftp.replication,Replication factor,1,3,reliability-tradeoff,core
888,ftp.stream-buffer-size,"The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",1,1,resource,core
889,ha.failover-controller.cli-check.rpc-timeout.ms,"Timeout that the CLI (manual) FC waits for monitorHealth, getServiceState",0,0,others,core
890,ha.failover-controller.graceful-fence.connection.retries,FC connection retries for graceful fencing,0,0,others,core
892,ha.failover-controller.new-active.rpc-timeout.ms,Timeout that the FC waits for the new active to become active,0,0,others,core
893,ha.health-monitor.check-interval.ms,How often to check the service.,1,5,workload-specific,core
895,ha.health-monitor.rpc-timeout.ms,Timeout for the actual monitorHealth() calls.,0,0,others,core
896,ha.health-monitor.sleep-after-disconnect.ms,How long to sleep after an unexpected RPC error.,0,0,others,core
897,ha.zookeeper.acl,"A comma-separated list of ZooKeeper ACLs to apply to the znodes used by automatic failover. These ACLs are specified in the same format as used by the ZooKeeper CLI. If the ACL itself contains secrets, you may instead specify a path to a file, prefixed with the '@' symbol, and the value of this configuration will be loaded from within.",0,0,others,core
899,ha.zookeeper.parent-znode,"The ZooKeeper znode under which the ZK failover controller stores its information. Note that the nameservice ID is automatically appended to this znode, so it is not normally necessary to configure this, even in a federated environment.",0,0,others,core
900,ha.zookeeper.quorum,"A list of ZooKeeper server addresses, separated by commas, that are to be used by the ZKFailoverController in automatic failover.",0,0,others,core
901,ha.zookeeper.session-timeout.ms,"The session timeout to use when the ZKFC connects to ZooKeeper. Setting this value to a lower value implies that server crashes will be detected more quickly, but risks triggering failover too aggressively in the case of a transient error or network blip.",0,0,others,core
902,hadoop.caller.context.enabled,"When the feature is enabled, additional fields are written into name-node audit log records for auditing coarse granularity operations.",1,6,function-tradeoff,core
903,hadoop.caller.context.max.size,"The maximum bytes a caller context string can have. If the passed caller context is longer than this maximum bytes, client will truncate it before sending to server. Note that the server may have a different maximum size, and will truncate the caller context to the maximum size it allows.",0,0,others,core
905,hadoop.common.configuration.version,version of this configuration file,0,0,others,core
906,hadoop.htrace.span.receiver.classes,The class names of the Span Receivers to use for Hadoop.,0,0,others,core
908,hadoop.http.authentication.kerberos.keytab,Location of the keytab file with the credentials for the principal. Referring to the same keytab file Oozie uses for its Kerberos credentials for Hadoop.,0,0,others,core
909,hadoop.http.authentication.kerberos.principal,Indicates the Kerberos principal to be used for HTTP endpoint. The principal MUST start with 'HTTP/' as per Kerberos HTTP SPNEGO specification.,0,0,others,core
910,hadoop.http.authentication.signature.secret.file,The signature secret for signing the authentication tokens. The same secret should be used for JT/NN/DN/TT configurations.,0,0,others,core
911,hadoop.http.authentication.simple.anonymous.allowed,Indicates if anonymous requests are allowed when using 'simple' authentication.,0,0,others,core
912,hadoop.http.authentication.token.validity,Indicates how long (in seconds) an authentication token is valid before it has to be renewed.,0,0,others,core
914,hadoop.http.cross-origin.allowed-headers,Comma separated list of headers that are allowed for web services needing cross-origin (CORS) support.,0,0,others,core
915,hadoop.http.cross-origin.allowed-methods,Comma separated list of methods that are allowed for web services needing cross-origin (CORS) support.,0,0,others,core
917,hadoop.http.cross-origin.enabled,Enable/disable the cross-origin (CORS) filter.,1,2,security-tradeoff,core
918,hadoop.http.cross-origin.max-age,The number of seconds a pre-flighted request can be cached for web services needing cross-origin (CORS) support.,0,0,others,core
919,hadoop.http.filter.initializers,"A comma separated list of class names. Each class in the list must extend org.apache.hadoop.http.FilterInitializer. The corresponding Filter will be initialized. Then, the Filter will be applied to all user facing jsp and servlet web pages. The ordering of the list defines the ordering of the filters.",0,0,others,core
921,hadoop.http.staticuser.user,"The user name to filter as, on static web filters while rendering content. An example use is the HDFS web UI (user to be used for browsing files).",0,0,others,core
922,hadoop.jetty.logs.serve.aliases,Enable/Disable aliases serving from jetty,1,4,limited-side-effect,core
923,hadoop.kerberos.kinit.command,Used to periodically renew Kerberos credentials when provided to Hadoop. The default setting assumes that kinit is in the PATH of users running the Hadoop client. Change this to the absolute path to kinit if this is not the case.,0,0,others,core
924,hadoop.kerberos.min.seconds.before.relogin,"The minimum time between relogin attempts for Kerberos, in seconds.",0,0,others,core
925,hadoop.registry.jaas.context,Key to define the JAAS context. Used in secure mode,0,0,others,core
927,hadoop.registry.rm.enabled,"Is the registry enabled in the YARN Resource Manager? If true, the YARN RM will, as needed. create the user and system paths, and purge service records when containers, application attempts and applications complete. If false, the paths must be created by other means, and no automatic cleanup of service records will take place.",1,6,function-tradeoff,core
928,hadoop.registry.secure,"Key to set if the registry is secure. Turning it on changes the permissions policy from ""open access"" to restrictions on kerberos with the option of a user adding one or more auth key pairs down their own tree.",1,2,security-tradeoff,core
929,hadoop.registry.system.acls,"A comma separated list of Zookeeper ACL identifiers with system access to the registry in a secure cluster. These are given full access to all entries. If there is an ""@"" at the end of a SASL entry it instructs the registry client to append the default kerberos domain.",0,0,others,core
931,hadoop.registry.zk.quorum,List of hostname:port pairs defining the zookeeper quorum binding for the registry,0,0,others,core
932,hadoop.registry.zk.retry.ceiling.ms,"Zookeeper retry limit in milliseconds, during exponential backoff. This places a limit even if the retry times and interval limit, combined with the backoff policy, result in a long retry period",0,0,others,core
934,hadoop.registry.zk.root,The root zookeeper node for the registry,0,0,others,core
935,hadoop.rpc.protection,"A comma-separated list of protection values for secured sasl connections. Possible values are authentication, integrity and privacy. authentication means authentication only and no integrity or privacy; integrity implies authentication and integrity are enabled; and privacy implies all of authentication, integrity and privacy are enabled. hadoop.security.saslproperties.resolver.class can be used to override the hadoop.rpc.protection for a connection at the server side.",1,2,security-tradeoff,core
936,hadoop.rpc.socket.factory.class.ClientProtocol,"SocketFactory to use to connect to a DFS. If null or empty, use hadoop.rpc.socket.class.default. This socket factory is also used by DFSClient to create sockets to DataNodes.",0,0,others,core
937,hadoop.rpc.socket.factory.class.default,"Default SocketFactory to use. This parameter is expected to be formatted as ""package.FactoryClassName"".",0,0,others,core
939,hadoop.security.authentication,"Possible values are simple (no authentication), and kerberos",1,2,security-tradeoff,core
941,hadoop.security.credential.clear-text-fallback,true or false to indicate whether or not to fall back to storing credential password as clear text. The default value is true. This property only works when the password can't not be found from credential providers.,1,2,security-tradeoff,core
942,hadoop.security.credential.provider.path,A comma-separated list of URLs that indicates the type and location of a list of providers that should be consulted.,0,0,others,core
943,hadoop.security.credstore.java-keystore-provider.password-file,The path to a file containing the custom password for all keystores that may be configured in the provider path.,0,0,others,core
944,hadoop.security.crypto.buffer.size,The buffer size used by CryptoInputStream and CryptoOutputStream.,1,1,resource,core
945,hadoop.security.crypto.cipher.suite,Cipher suite for crypto codec.,1,2,security-tradeoff,core
948,hadoop.security.crypto.jce.provider,The JCE provider name used in CryptoCodec.,0,0,others,core
949,hadoop.security.crypto.jceks.key.serialfilter,"Enhanced KeyStore Mechanisms in JDK 8u171 introduced jceks.key.serialFilter. If jceks.key.serialFilter is configured, the JCEKS KeyStore uses it during the deserialization of the encrypted Key object stored inside a SecretKeyEntry. If jceks.key.serialFilter is not configured it will cause an error when recovering keystore file in KeyProviderFactory when recovering key from keystore file using JDK 8u171 or newer. The filter pattern uses the same format as jdk.serialFilter. The value of this property will be used as the following: 1. The value of jceks.key.serialFilter system property takes precedence over the value of this property. 2. In the absence of jceks.key.serialFilter system property the value of this property will be set as the value of jceks.key.serialFilter. 3. If the value of this property and jceks.key.serialFilter system property has not been set, org.apache.hadoop.crypto.key.KeyProvider sets a default value for jceks.key.serialFilter.",0,0,others,core
950,hadoop.security.dns.interface,"The name of the Network Interface from which the service should determine its host name for Kerberos login. e.g. eth2. In a multi-homed environment, the setting can be used to affect the _HOST substitution in the service Kerberos principal. If this configuration value is not set, the service will use its default hostname as returned by InetAddress.getLocalHost().getCanonicalHostName(). Most clusters will not require this setting.",0,0,others,core
951,hadoop.security.dns.log-slow-lookups.enabled,Time name lookups (via SecurityUtil) and log them if they exceed the configured threshold.,1,5,workload-specific,core
952,hadoop.security.dns.log-slow-lookups.threshold.ms,"If slow lookup logging is enabled, this threshold is used to decide if a lookup is considered slow enough to be logged.",1,5,workload-specific,core
953,hadoop.security.dns.nameserver,The host name or IP address of the name server (DNS) which a service Node should use to determine its own host name for Kerberos Login. Requires hadoop.security.dns.interface. Most clusters will not require this setting.,0,0,others,core
954,hadoop.security.group.mapping,"Class for user to group mapping (get groups for a given user) for ACL. The default implementation, org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, will determine if the Java Native Interface (JNI) is available. If JNI is available the implementation will use the API within hadoop to resolve a list of groups for a user. If JNI is not available then the shell implementation, ShellBasedUnixGroupsMapping, is used. This implementation shells out to the Linux/Unix environment with the bash -c groups command to resolve a list of groups for a user.",0,0,others,core
955,hadoop.security.group.mapping.ldap.base,"The search base for the LDAP connection. This is a distinguished name, and will typically be the root of the LDAP directory.",0,0,others,core
956,hadoop.security.group.mapping.ldap.bind.password,The password of the bind user. this property name is used as an alias to get the password from credential providers. If the password can not be found and hadoop.security.credential.clear-text-fallback is true LDAPGroupsMapping uses the value of this property for password.,0,0,others,core
957,hadoop.security.group.mapping.ldap.bind.password.file,"The path to a file containing the password of the bind user. If the password is not configured in credential providers and the property hadoop.security.group.mapping.ldap.bind.password is not set, LDAPGroupsMapping reads password from the file. IMPORTANT: This file should be readable only by the Unix user running the daemons and should be a local file.",0,0,others,core
958,hadoop.security.group.mapping.ldap.bind.user,The distinguished name of the user to bind as when connecting to the LDAP server. This may be left blank if the LDAP server supports anonymous binds.,0,0,others,core
959,hadoop.security.group.mapping.ldap.connection.timeout.ms,"This property is the connection timeout (in milliseconds) for LDAP operations. If the LDAP provider doesn't establish a connection within the specified period, it will abort the connect attempt. Non-positive value means no LDAP connection timeout is specified in which case it waits for the connection to establish until the underlying network times out.",0,0,others,core
960,hadoop.security.group.mapping.ldap.directory.search.timeout,The attribute applied to the LDAP SearchControl properties to set a maximum time limit when searching and awaiting a result. Set to 0 if infinite wait period is desired. Default is 10 seconds. Units in milliseconds.,0,0,others,core
961,hadoop.security.group.mapping.ldap.groupbase,"The search base for the LDAP connection for group search . This is a distinguished name, and its the root of the LDAP directory for groups. If not set, hadoop.security.group.mapping.ldap.base is used.",0,0,others,core
962,hadoop.security.group.mapping.ldap.posix.attr.gid.name,The attribute of posixAccount indicating the group id.,0,0,others,core
963,hadoop.security.group.mapping.ldap.posix.attr.uid.name,The attribute of posixAccount to use when groups for membership. Mostly useful for schemas wherein groups have memberUids that use an attribute other than uidNumber.,0,0,others,core
965,hadoop.security.group.mapping.ldap.search.attr.group.name,The attribute of the group object that identifies the group name. The default will usually be appropriate for all LDAP systems.,0,0,others,core
966,hadoop.security.group.mapping.ldap.search.attr.member,The attribute of the group object that identifies the users that are members of the group. The default will usually be appropriate for any LDAP installation.,0,0,others,core
967,hadoop.security.group.mapping.ldap.search.attr.memberof,"The attribute of the user object that identifies its group objects. By default, Hadoop makes two LDAP queries per user if this value is empty. If set, Hadoop will attempt to resolve group names from this attribute, instead of making the second LDAP query to get group objects. The value should be 'memberOf' for an MS AD installation.",0,0,others,core
968,hadoop.security.group.mapping.ldap.search.filter.group,An additional filter to use when searching for LDAP groups. This should be changed when resolving groups against a non-Active Directory installation. See the description of hadoop.security.group.mapping.ldap.search.filter.user to enable posixGroups support.,0,0,others,core
969,hadoop.security.group.mapping.ldap.search.filter.user,"An additional filter to use when searching for LDAP users. The default will usually be appropriate for Active Directory installations. If connecting to an LDAP server with a non-AD schema, this should be replaced with (&(objectClass=inetOrgPerson)(uid={0}). {0} is a special string used to denote where the username fits into the filter. If the LDAP server supports posixGroups, Hadoop can enable the feature by setting the value of this property to ""posixAccount"" and the value of the hadoop.security.group.mapping.ldap.search.filter.group property to ""posixGroup"".",0,0,others,core
970,hadoop.security.group.mapping.ldap.search.group.hierarchy.levels,The number of levels to go up the group hierarchy when determining which groups a user is part of. 0 Will represent checking just the group that the user belongs to. Each additional level will raise the time it takes to execute a query by at most hadoop.security.group.mapping.ldap.directory.search.timeout. The default will usually be appropriate for all LDAP systems.,0,0,others,core
971,hadoop.security.group.mapping.ldap.ssl,Whether or not to use SSL when connecting to the LDAP server.,1,2,security-tradeoff,core
972,hadoop.security.group.mapping.ldap.ssl.keystore,File path to the SSL keystore that contains the SSL certificate required by the LDAP server.,0,0,others,core
974,hadoop.security.group.mapping.ldap.ssl.keystore.password.file,"The path to a file containing the password of the LDAP SSL keystore. If the password is not configured in credential providers and the property hadoop.security.group.mapping.ldap.ssl.keystore.password is not set, LDAPGroupsMapping reads password from the file. IMPORTANT: This file should be readable only by the Unix user running the daemons and should be a local file.",0,0,others,core
975,hadoop.security.group.mapping.ldap.ssl.truststore,File path to the SSL truststore that contains the root certificate used to sign the LDAP server's certificate. Specify this if the LDAP server's certificate is not signed by a well known certificate authority.,0,0,others,core
976,hadoop.security.group.mapping.ldap.ssl.truststore.password.file,The path to a file containing the password of the LDAP SSL truststore. IMPORTANT: This file should be readable only by the Unix user running the daemons.,0,0,others,core
977,hadoop.security.group.mapping.ldap.url,The URL of the LDAP server to use for resolving user groups when using the LdapGroupsMapping user to group mapping.,0,0,others,core
978,hadoop.security.group.mapping.ldap.userbase,"The search base for the LDAP connection for user search query. This is a distinguished name, and its the root of the LDAP directory for users. If not set, hadoop.security.group.mapping.ldap.base is used.",0,0,others,core
979,hadoop.security.group.mapping.providers,Comma separated of names of other providers to provide user to group mapping. Used by CompositeGroupsMapping.,0,0,others,core
981,hadoop.security.groups.cache.background.reload,"Whether to reload expired user->group mappings using a background thread pool. If set to true, a pool of hadoop.security.groups.cache.background.reload.threads is created to update the cache in the background.",1,4,limited-side-effect,core
982,hadoop.security.groups.cache.background.reload.threads,Only relevant if hadoop.security.groups.cache.background.reload is true. Controls the number of concurrent background user->group cache entry refreshes. Pending refresh requests beyond this value are queued and processed when a thread is free.,1,1,resource,core
983,hadoop.security.groups.cache.secs,"This is the config controlling the validity of the entries in the cache containing the user->group mapping. When this duration has expired, then the implementation of the group mapping provider is invoked to get the groups of the user and then cached back.",1,5,workload-specific,core
984,hadoop.security.groups.cache.warn.after.ms,"If looking up a single user to group takes longer than this amount of milliseconds, we will log a warning message.",1,5,workload-specific,core
986,hadoop.security.groups.shell.command.timeout,"Used by the ShellBasedUnixGroupsMapping class, this property controls how long to wait for the underlying shell command that is run to fetch groups. Expressed in seconds (e.g. 10s, 1m, etc.), if the running command takes longer than the value configured, the command is aborted and the groups resolver would return a result of no groups found. A value of 0s (default) would mean an infinite wait (i.e. wait until the command exits on its own).",0,0,others,core
988,hadoop.security.instrumentation.requires.admin,"Indicates if administrator ACLs are required to access instrumentation servlets (JMX, METRICS, CONF, STACKS).",1,2,security-tradeoff,core
989,hadoop.security.java.secure.random.algorithm,The java secure random algorithm.,0,0,others,core
990,hadoop.security.key.provider.path,"The KeyProvider to use when managing zone keys, and interacting with encryption keys when reading and writing to an encryption zone. For hdfs clients, the provider path will be same as namenode's provider path.",0,0,others,core
991,hadoop.security.kms.client.authentication.retry-count,Number of time to retry connecting to KMS on authentication failure,0,0,others,core
992,hadoop.security.kms.client.encrypted.key.cache.expiry,"Cache expiry time for a Key, after which the cache Queue for this key will be dropped. Default = 12hrs",1,5,workload-specific,core
993,hadoop.security.kms.client.encrypted.key.cache.low-watermark,"If size of the EncryptedKeyVersion cache Queue falls below the low watermark, this cache queue will be scheduled for a refill",1,5,workload-specific,core
995,hadoop.security.kms.client.encrypted.key.cache.size,Size of the EncryptedKeyVersion cache Queue for each key,1,1,resource,core
996,hadoop.security.kms.client.failover.sleep.base.millis,"Expert only. The time to wait, in milliseconds, between failover attempts increases exponentially as a function of the number of attempts made so far, with a random factor of +/- 50%. This option specifies the base value used in the failover calculation. The first failover will retry immediately. The 2nd failover attempt will delay at least hadoop.security.client.failover.sleep.base.millis milliseconds. And so on.",0,0,others,core
997,hadoop.security.kms.client.failover.sleep.max.millis,"Expert only. The time to wait, in milliseconds, between failover attempts increases exponentially as a function of the number of attempts made so far, with a random factor of +/- 50%. This option specifies the maximum value to wait between failovers. Specifically, the time between two failover attempts will not exceed +/- 50% of hadoop.security.client.failover.sleep.max.millis milliseconds.",0,0,others,core
998,hadoop.security.kms.client.timeout,"Sets value for KMS client connection timeout, and the read timeout to KMS servers.",0,0,others,core
999,hadoop.security.random.device.file.path,OS security random device file path.,0,0,others,core
1000,hadoop.security.saslproperties.resolver.class,"SaslPropertiesResolver used to resolve the QOP used for a connection. If not specified, the full set of values specified in hadoop.rpc.protection is used while determining the QOP used for the connection. If a class is specified, then the QOP values returned by the class will be used while determining the QOP used for the connection.",0,0,others,core
1001,hadoop.security.secure.random.impl,Implementation of secure random.,0,0,others,core
1003,hadoop.security.service.user.name.key,"For those cases where the same RPC protocol is implemented by multiple servers, this configuration is required for specifying the principal name to use for the service when the client wishes to make an RPC call.",0,0,others,core
1004,hadoop.security.uid.cache.secs,This is the config controlling the validity of the entries in the cache containing the userId to userName and groupId to groupName used by NativeIO getFstat().,1,5,workload-specific,core
1006,hadoop.shell.missing.defaultFs.warning,Enable hdfs shell commands to display warnings if (fs.defaultFS) property is not set.,1,6,function-tradeoff,core
1007,hadoop.shell.safely.delete.limit.num.files,"Used by -safely option of hadoop fs shell -rm command to avoid accidental deletion of large directories. When enabled, the -rm command requires confirmation if the number of files to be deleted is greater than this limit. The default limit is 100 files. The warning is disabled if the limit is 0 or the -safely is not specified in -rm command.",0,0,others,core
1008,hadoop.socks.server,Address (host:port) of the SOCKS server to be used by the SocksSocketFactory.,0,0,others,core
1009,hadoop.ssl.client.conf,"Resource file from which ssl client keystore information will be extracted This file is looked up in the classpath, typically it should be in Hadoop conf/ directory.",0,0,others,core
1010,hadoop.ssl.enabled,Deprecated. Use dfs.http.policy and yarn.http.policy instead.,0,0,others,core
1012,hadoop.ssl.hostname.verifier,The hostname verifier to provide for HttpsURLConnections.,1,2,security-tradeoff,core
1013,hadoop.ssl.keystores.factory.class,The keystores factory to use for retrieving certificates.,0,0,others,core
1014,hadoop.ssl.require.client.cert,Whether client certificates are required,1,2,security-tradeoff,core
1015,hadoop.ssl.server.conf,"Resource file from which ssl server keystore information will be extracted. This file is looked up in the classpath, typically it should be in Hadoop conf/ directory.",0,0,others,core
1016,hadoop.tmp.dir,A base for other temporary directories.,0,0,others,core
1017,hadoop.token.files,List of token cache files that have delegation tokens for hadoop service,0,0,others,core
1018,hadoop.user.group.static.mapping.overrides,"Static mapping of user to groups. This will override the groups if available in the system for the specified user. In other words, groups look-up will not happen for these users, instead groups mapped in this configuration will be used. Mapping should be in this format. user1=group1,group2;user2=;user3=group2; Default, ""dr.who=;"" will consider ""dr.who"" as user without groups.",0,0,others,core
1020,hadoop.workaround.non.threadsafe.getpwuid,"Some operating systems or authentication modules are known to have broken implementations of getpwuid_r and getpwgid_r, such that these calls are not thread-safe. Symptoms of this problem include JVM crashes with a stack trace inside these functions. If your system exhibits this issue, enable this configuration parameter to include a lock around the calls as a workaround. An incomplete list of some systems known to have this issue is available at https://wiki.apache.org/hadoop/KnownBrokenPwuidImplementations",0,0,others,core
1021,hadoop.zk.acl,ACL's to be used for ZooKeeper znodes.,0,0,others,core
1022,hadoop.zk.address,Host:Port of the ZooKeeper server to be used.,0,0,others,core
1024,hadoop.zk.num-retries,Number of tries to connect to ZooKeeper.,0,0,others,core
1025,hadoop.zk.retry-interval-ms,Retry interval in milliseconds when connecting to ZooKeeper.,0,0,others,core
1026,io.bytes.per.checksum,The number of bytes per checksum. Must not be larger than io.file.buffer.size.,1,1,resource,core
1027,io.compression.codec.bzip2.library,"The native-code library to be used for compression and decompression by the bzip2 codec. This library could be specified either by by name or the full pathname. In the former case, the library is located by the dynamic linker, usually searching the directories specified in the environment variable LD_LIBRARY_PATH. The value of ""system-native"" indicates that the default system library should be used. To indicate that the algorithm should operate entirely in Java, specify ""java-builtin"".",1,6,function-tradeoff,core
1028,io.compression.codecs,"A comma-separated list of the compression codec classes that can be used for compression/decompression. In addition to any classes specified with this property (which take precedence), codec classes on the classpath are discovered using a Java ServiceLoader.",0,0,others,core
1029,io.file.buffer.size,"The size of buffer for use in sequence files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",1,1,resource,core
1030,io.map.index.interval,"MapFile consist of two files - data file (tuples) and index file (keys). For every io.map.index.interval records written in the data file, an entry (record-key, data-file-position) is written in the index file. This is to allow for doing binary search later within the index file to look up records by their keys and get their closest positions in the data file.",0,0,others,core
1031,io.map.index.skip,Number of index entries to skip between each entry. Zero by default. Setting this to values larger than zero can facilitate opening large MapFiles using less memory.,1,5,workload-specific,core
1033,io.mapfile.bloom.size,"The size of BloomFilter-s used in BloomMapFile. Each time this many keys is appended the next BloomFilter will be created (inside a DynamicBloomFilter). Larger values minimize the number of filters, which slightly increases the performance, but may waste too much space if the total number of keys is usually much smaller than this number.",1,1,resource,core
1036,io.seqfile.local.dir,The local directory where sequence file stores intermediate data files during merge. May be a comma-separated list of directories on different devices in order to spread disk i/o. Directories that do not exist are ignored.,0,0,others,core
1037,io.serializations,A list of serialization classes that can be used for obtaining serializers and deserializers.,0,0,others,core
1038,io.skip.checksum.errors,"If true, when a checksum error is encountered while reading a sequence file, entries are skipped, instead of throwing an exception.",0,0,others,core
1039,ipc.client.connect.max.retries,Indicates the number of retries a client will make to establish a server connection.,0,0,others,core
1040,ipc.client.connect.max.retries.on.timeouts,Indicates the number of retries a client will make on socket timeout to establish a server connection.,0,0,others,core
1041,ipc.client.connect.retry.interval,Indicates the number of milliseconds a client will wait for before retrying to establish a server connection.,0,0,others,core
1042,ipc.client.connect.timeout,Indicates the number of milliseconds a client will wait for the socket to establish a server connection.,0,0,others,core
1043,ipc.client.connection.maxidletime,The maximum time in msec after which a client will bring down the connection to the server.,0,0,others,core
1044,ipc.client.fallback-to-simple-auth-allowed,"When a client is configured to attempt a secure connection, but attempts to connect to an insecure server, that server may instruct the client to switch to SASL SIMPLE (unsecure) authentication. This setting controls whether or not the client will accept this instruction from the server. When false (the default), the client will not allow the fallback to SIMPLE authentication, and will abort the connection.",1,2,security-tradeoff,core
1046,ipc.client.kill.max,Defines the maximum number of clients to disconnect in one go.,0,0,others,core
1047,ipc.client.low-latency,Use low-latency QoS markers for IPC connections.,1,4,limited-side-effect,core
1050,ipc.client.tcpnodelay,Use TCP_NODELAY flag to bypass Nagle's algorithm transmission delays.,1,4,limited-side-effect,core
1051,ipc.maximum.data.length,This indicates the maximum IPC message length (bytes) that can be accepted by the server. Messages larger than this value are rejected by the immediately to avoid possible OOMs. This setting should rarely need to be changed.,0,0,others,core
1052,ipc.maximum.response.length,This indicates the maximum IPC message length (bytes) that can be accepted by the client. Messages larger than this value are rejected immediately to avoid possible OOMs. This setting should rarely need to be changed. Set to 0 to disable.,0,0,others,core
1053,ipc.ping.interval,"Timeout on waiting response from server, in milliseconds. The client will send ping when the interval is passed without receiving bytes, if ipc.client.ping is set to true.",0,0,others,core
1054,ipc.server.listen.queue.size,Indicates the length of the listen queue for servers accepting client connections.,0,0,others,core
1055,ipc.server.log.slow.rpc,This setting is useful to troubleshoot performance issues for various services. If this value is set to true then we log requests that fall into 99th percentile as well as increment RpcSlowCalls counter.,1,6,function-tradeoff,core
1057,net.topology.impl,The default implementation of NetworkTopology which is classic three layer one.,0,0,others,core
1058,net.topology.node.switch.mapping.impl,"The default implementation of the DNSToSwitchMapping. It invokes a script specified in net.topology.script.file.name to resolve node names. If the value for net.topology.script.file.name is not set, the default value of DEFAULT_RACK is returned for all node names.",0,0,others,core
1059,net.topology.script.file.name,"The script name that should be invoked to resolve DNS names to NetworkTopology names. Example: the script would take host.foo.bar as an argument, and return /rack1 as the output.",0,0,others,core
1060,net.topology.script.number.args,The max number of args that the script configured with net.topology.script.file.name should be run with. Each arg is an IP address.,0,0,others,core
1061,net.topology.table.file.name,"The file name for a topology file, which is used when the net.topology.node.switch.mapping.impl property is set to org.apache.hadoop.net.TableMapping. The file format is a two column text file, with columns separated by whitespace. The first column is a DNS or IP address and the second column specifies the rack where the address maps. If no entry corresponding to a host in the cluster is found, then /default-rack is assumed.",0,0,others,core
1062,nfs.exports.allowed.hosts,"By default, the export can be mounted by any client. The value string contains machine name and access privilege, separated by whitespace characters. The machine name format can be a single host, a Java regular expression, or an IPv4 address. The access privilege uses rw or ro to specify read/write or read-only access of the machines to exports. If the access privilege is not provided, the default is read-only. Entries are separated by "";"". For example: ""192.168.0.0/22 rw ; host.*\.example\.com ; host1.test.org ro;"". Only the NFS gateway needs to restart after this property is updated.",0,0,others,core
1063,rpc.metrics.percentiles.intervals,A comma-separated list of the granularity in seconds for the metrics which describe the 50/75/90/95/99th percentile latency for rpc queue/processing time. The metrics are outputted if rpc.metrics.quantile.enable is set to true.,0,0,others,core
1064,rpc.metrics.quantile.enable,"Setting this property to true and rpc.metrics.percentiles.intervals to a comma-separated list of the granularity in seconds, the 50/75/90/95/99th percentile latency for rpc queue/processing time in milliseconds are added to rpc metrics.",0,0,others,core
1065,s3.blocksize,Block size,1,5,workload-specific,core
1066,s3.bytes-per-checksum,The number of bytes per checksum. Must not be larger than s3.stream-buffer-size,0,0,others,core
1067,s3.client-write-packet-size,Packet size for clients to write,0,0,others,core
1068,s3.replication,Replication factor,1,3,reliability-tradeoff,core
1069,s3.stream-buffer-size,"The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",1,1,resource,core
1070,s3native.blocksize,Block size,1,5,workload-specific,core
1071,s3native.bytes-per-checksum,The number of bytes per checksum. Must not be larger than s3native.stream-buffer-size,0,0,others,core
1072,s3native.client-write-packet-size,Packet size for clients to write,0,0,others,core
1073,s3native.replication,Replication factor,1,3,reliability-tradeoff,core
1074,s3native.stream-buffer-size,"The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",1,1,resource,core
1076,seq.io.sort.mb,"The total amount of buffer memory to use while sorting files, while using SequenceFile.Sorter, in megabytes. By default, gives each merge stream 1MB, which should minimize seeks.",1,1,resource,core
1078,tfile.fs.output.buffer.size,Buffer size used for FSDataOutputStream in bytes.,1,1,resource,core
1079,tfile.io.chunk.size,Value chunk size in bytes. Default to 1MB. Values of the length less than the chunk size is guaranteed to have known value length in read time (See also TFile.Reader.Scanner.Entry.isValueLengthKnown()).,0,0,others,core
1080,akka.ask.callstack,"If true, call stack for asynchronous asks are captured. That way, when an ask fails (for example times out), you get a proper exception, describing to the original method call and call site. Note that in case of having millions of concurrent RPC calls, this may add to the memory footprint.",1,6,function-tradeoff,flink
1082,akka.client-socket-worker-pool.pool-size-factor,The pool size factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the pool-size-min and pool-size-max values.,1,1,resource,flink
1083,akka.client-socket-worker-pool.pool-size-max,Max number of threads to cap factor-based number to.,1,1,resource,flink
1084,akka.client-socket-worker-pool.pool-size-min,Min number of threads to cap factor-based number to.,1,1,resource,flink
1085,akka.fork-join-executor.parallelism-factor,The parallelism factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the parallelism-min and parallelism-max values.,1,1,resource,flink
1086,akka.fork-join-executor.parallelism-max,Max number of threads to cap factor-based parallelism number to.,1,1,resource,flink
1087,akka.fork-join-executor.parallelism-min,Min number of threads to cap factor-based parallelism number to.,1,1,resource,flink
1089,akka.jvm-exit-on-fatal-error,Exit JVM on fatal Akka errors.,0,0,others,flink
1090,akka.log.lifecycle.events,Turns on the Akka's remote logging of events. Set this value to 'true' in case of debugging.,1,6,function-tradeoff,flink
1091,akka.lookup.timeout,Timeout used for the lookup of the JobManager. The timeout value has to contain a time-unit specifier (ms/s/min/h/d).,0,0,others,flink
1092,akka.retry-gate-closed-for,Milliseconds a gate should be closed for after a remote connection was disconnected.,0,0,others,flink
1093,akka.server-socket-worker-pool.pool-size-factor,The pool size factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the pool-size-min and pool-size-max values.,1,1,resource,flink
1094,akka.server-socket-worker-pool.pool-size-max,Max number of threads to cap factor-based number to.,1,1,resource,flink
1096,akka.ssl.enabled,Turns on SSL for Akka's remote communication. This is applicable only when the global ssl flag security.ssl.enabled is set to true.,0,0,others,flink
1098,akka.tcp.timeout,"Timeout for all outbound connections. If you should experience problems with connecting to a TaskManager due to a slow network, you should increase this value.",0,0,others,flink
1099,akka.throughput,Number of messages that are processed in a batch before returning the thread to the pool. Low values denote a fair scheduling whereas high values can increase the performance at the cost of unfairness.,1,1,resource,flink
1100,akka.transport.heartbeat.interval,"Heartbeat interval for Akka's transport failure detector. Since Flink uses TCP, the detector is not necessary. Therefore, the detector is disabled by setting the interval to a very high value. In case you should need the transport failure detector, set the interval to some reasonable value. The interval value requires a time-unit specifier (ms/s/min/h/d).",0,0,others,flink
1101,akka.transport.heartbeat.pause,"Acceptable heartbeat pause for Akka's transport failure detector. Since Flink uses TCP, the detector is not necessary. Therefore, the detector is disabled by setting the pause to a very high value. In case you should need the transport failure detector, set the pause to some reasonable value. The pause value requires a time-unit specifier (ms/s/min/h/d).",0,0,others,flink
1102,akka.transport.threshold,"Threshold for the transport failure detector. Since Flink uses TCP, the detector is not necessary and, thus, the threshold is set to a high value.",1,5,workload-specific,flink
1103,blob.client.connect.timeout,The connection timeout in milliseconds for the blob client.,0,0,others,flink
1104,blob.client.socket.timeout,The socket timeout in milliseconds for the blob client.,0,0,others,flink
1105,blob.fetch.backlog,The config parameter defining the backlog of BLOB fetches on the JobManager.,1,5,workload-specific,flink
1106,blob.fetch.num-concurrent,The config parameter defining the maximum number of concurrent BLOB fetches that the JobManager serves.,1,5,workload-specific,flink
1107,blob.fetch.retries,The config parameter defining number of retires for failed BLOB fetches.,0,0,others,flink
1108,blob.offload.minsize,The minimum size for messages to be offloaded to the BlobServer.,1,5,workload-specific,flink
1109,blob.server.port,The config parameter defining the server port of the blob service.,0,0,others,flink
1110,blob.service.cleanup.interval,Cleanup interval of the blob caches at the task managers (in seconds).,0,0,others,flink
1112,blob.storage.directory,The config parameter defining the storage directory to be used by the blob server.,0,0,others,flink
1113,classloader.check-leaked-classloader,"Fails attempts at loading classes if the user classloader of a job is used after it has terminated. This is usually caused by the classloader being leaked by lingering threads or misbehaving libraries, which may also result in the classloader being used by other jobs. This check should only be disabled if such a leak prevents further jobs from running.",0,0,others,flink
1114,classloader.fail-on-metaspace-oom-error,Fail Flink JVM processes if 'OutOfMemoryError: Metaspace' is thrown while trying to load a user code class.,0,0,others,flink
1115,classloader.parent-first-patterns.additional,"A (semicolon-separated) list of patterns that specifies which classes should always be resolved through the parent ClassLoader first. A pattern is a simple prefix that is checked against the fully qualified class name. These patterns are appended to ""classloader.parent-first-patterns.default"".",0,0,others,flink
1118,client.retry-period,"The interval (in ms) between consecutive retries of failed attempts to execute commands through the CLI or Flink's clients, wherever retry is supported (default 2sec).",0,0,others,flink
1119,client.timeout,Timeout on the client side.,0,0,others,flink
1120,cluster.evenly-spread-out-slots,Enable the slot spread out allocation strategy. This strategy tries to spread out the slots evenly across all available TaskExecutors.,1,6,function-tradeoff,flink
1121,cluster.io-pool.size,The size of the IO executor pool used by the cluster to execute blocking IO operations (Master as well as TaskManager processes). By default it will use 4 * the number of CPU cores (hardware contexts) that the cluster process has access to. Increasing the pool size allows to run more IO operations concurrently.,1,1,resource,flink
1122,cluster.processes.halt-on-fatal-error,"Whether processes should halt on fatal errors instead of performing a graceful shutdown. In some environments (e.g. Java 8 with the G1 garbage collector), a regular graceful shutdown can lead to a JVM deadlock. See FLINK-16510 for details.",1,6,function-tradeoff,flink
1123,cluster.registration.error-delay,The pause made after an registration attempt caused an exception (other than timeout) in milliseconds.,0,0,others,flink
1124,cluster.registration.initial-timeout,Initial registration timeout between cluster components in milliseconds.,0,0,others,flink
1125,cluster.registration.max-timeout,Maximum registration timeout between cluster components in milliseconds.,0,0,others,flink
1126,cluster.registration.refused-registration-delay,The pause made after the registration attempt was refused in milliseconds.,0,0,others,flink
1127,cluster.services.shutdown-timeout,The shutdown timeout for cluster services like executors in milliseconds.,0,0,others,flink
1128,compiler.delimited-informat.max-line-samples,The maximum number of line samples taken by the compiler for delimited inputs. The samples are used to estimate the number of records. This value can be overridden for a specific input with the input format's parameters.,1,5,workload-specific,flink
1131,env.hadoop.conf.dir,Path to hadoop configuration directory. It is required to read HDFS and/or YARN configuration. You can also set it via environment variable.,0,0,others,flink
1132,env.hbase.conf.dir,Path to hbase configuration directory. It is required to read HBASE configuration. You can also set it via environment variable.,0,0,others,flink
1133,env.java.opts,Java options to start the JVM of all Flink processes with.,0,0,others,flink
1134,env.java.opts.client,Java options to start the JVM of the Flink Client with.,0,0,others,flink
1135,env.java.opts.historyserver,Java options to start the JVM of the HistoryServer with.,0,0,others,flink
1136,env.java.opts.jobmanager,Java options to start the JVM of the JobManager with.,0,0,others,flink
1137,env.java.opts.taskmanager,Java options to start the JVM of the TaskManager with.,0,0,others,flink
1138,env.log.dir,Defines the directory where the Flink logs are saved. It has to be an absolute path. (Defaults to the log directory under Flink's home),0,0,others,flink
1139,env.log.max,The maximum number of old log files to keep.,1,5,workload-specific,flink
1141,env.yarn.conf.dir,Path to yarn configuration directory. It is required to run flink on YARN. You can also set it via environment variable.,0,0,others,flink
1142,execution.attached,Specifies if the pipeline is submitted in attached or detached mode.,1,6,function-tradeoff,flink
1144,execution.checkpointing.externalized-checkpoint-retention,"Externalized checkpoints write their meta data out to persistent storage and are not automatically cleaned up when the owning job fails or is suspended (terminating with job status JobStatus#FAILED or JobStatus#SUSPENDED. In this case, you have to manually clean up the checkpoint state, both the meta data and actual program state. The mode defines how an externalized checkpoint should be cleaned up on job cancellation. If you choose to retain externalized checkpoints on cancellation you have to handle checkpoint clean up manually when you cancel the job as well (terminating with job status JobStatus#CANCELED). The target directory for externalized checkpoints is configured via state.checkpoints.dir.",0,0,others,flink
1145,execution.checkpointing.interval,Gets the interval in which checkpoints are periodically scheduled. This setting defines the base interval. Checkpoint triggering may be delayed by the settings execution.checkpointing.max-concurrent-checkpoints and execution.checkpointing.min-pause,0,0,others,flink
1146,execution.checkpointing.max-concurrent-checkpoints,"The maximum number of checkpoint attempts that may be in progress at the same time. If this value is n, then no checkpoints will be triggered while n checkpoint attempts are currently in flight. For the next checkpoint to be triggered, one checkpoint attempt would need to finish or expire.",1,5,workload-specific,flink
1147,execution.checkpointing.min-pause,"The minimal pause between checkpointing attempts. This setting defines how soon thecheckpoint coordinator may trigger another checkpoint after it becomes possible to triggeranother checkpoint with respect to the maximum number of concurrent checkpoints(see execution.checkpointing.max-concurrent-checkpoints). If the maximum number of concurrent checkpoints is set to one, this setting makes effectively sure that a minimum amount of time passes where no checkpoint is in progress at all.",0,0,others,flink
1149,execution.checkpointing.prefer-checkpoint-for-recovery,"If enabled, a job recovery should fallback to checkpoint when there is a more recent savepoint.",1,6,function-tradeoff,flink
1150,execution.checkpointing.snapshot-compression,Tells if we should use compression for the state snapshot data or not,1,4,limited-side-effect,flink
1151,execution.checkpointing.timeout,The maximum time that a checkpoint may take before being discarded.,0,0,others,flink
1152,execution.checkpointing.tolerable-failed-checkpoints,"The tolerable checkpoint failure number. If set to 0, that meanswe do not tolerance any checkpoint failure.",0,0,others,flink
1153,execution.checkpointing.unaligned,"Enables unaligned checkpoints, which greatly reduce checkpointing times under backpressure. Unaligned checkpoints contain data stored in buffers as part of the checkpoint state, which allows checkpoint barriers to overtake these buffers. Thus, the checkpoint duration becomes independent of the current throughput as checkpoint barriers are effectively not embedded into the stream of data anymore. Unaligned checkpoints can only be enabled if execution.checkpointing.mode is EXACTLY_ONCE and if execution.checkpointing.max-concurrent-checkpoints is 1",1,6,function-tradeoff,flink
1157,execution.savepoint.path,Path to a savepoint to restore the job from (for example hdfs:///flink/savepoint-1537).,0,0,others,flink
1158,execution.shutdown-on-attached-exit,"If the job is submitted in attached mode, perform a best-effort cluster shutdown when the CLI is terminated abruptly, e.g., in response to a user interrupt, such as typing Ctrl + C.",1,6,function-tradeoff,flink
1159,execution.target,"The deployment target for the execution. This can take one of the following values: remote, local, yarn-per-job, yarn-session, kubernetes-session",0,0,others,flink
1160,external-resource.resource_name.kubernetes.config-key,"If configured, Flink will add ""resources.limits.<config-key>"" and ""resources.requests.<config-key>"" to the main container of TaskExecutor and set the value to the value of external-resource.<resource_name>.amount.",0,0,others,flink
1161,external-resource.resource_name.yarn.config-key,"If configured, Flink will add this key to the resource profile of container request to Yarn. The value will be set to the value of external-resource.<resource_name>.amount.",0,0,others,flink
1162,fs.allowed-fallback-filesystems,"A (semicolon-separated) list of file schemes, for which Hadoop can be used instead of an appropriate Flink plugin. (example: s3;wasb)",0,0,others,flink
1163,fs.default-scheme,"The default filesystem scheme, used for paths that do not declare a scheme explicitly. May contain an authority, e.g. host:port in case of an HDFS NameNode.",0,0,others,flink
1164,fs.output.always-create-directory,"File writers running with a parallelism larger than one create a directory for the output file path and put the different result files (one per parallel writer task) into that directory. If this option is set to ""true"", writers with a parallelism of 1 will also create a directory and place a single result file into it. If the option is set to ""false"", the writer will directly create the file directly at the output path, without creating a containing directory.",0,0,others,flink
1165,fs.overwrite-files,"Specifies whether file output writers should overwrite existing files by default. Set to ""true"" to overwrite by default,""false"" otherwise.",0,0,others,flink
1166,heartbeat.interval,Time interval for requesting heartbeat from sender side.,0,0,others,flink
1167,heartbeat.timeout,Timeout for requesting and receiving heartbeat for both sender and receiver sides.,0,0,others,flink
1168,high-availability,"Defines high-availability mode used for the cluster execution. To enable high-availability, set this mode to ""ZOOKEEPER"" or specify FQN of factory class.",0,0,others,flink
1169,high-availability.cluster-id,"The ID of the Flink cluster, used to separate multiple Flink clusters from each other. Needs to be set for standalone clusters but is automatically inferred in YARN and Mesos.",0,0,others,flink
1170,high-availability.jobmanager.port,"The port (range) used by the Flink Master for its RPC connections in highly-available setups. In highly-available setups, this value is used instead of 'jobmanager.rpc.port'.A value of '0' means that a random free port is chosen. TaskManagers discover this port through the high-availability services (leader election), so a random port or a port range works without requiring any additional means of service discovery.",0,0,others,flink
1171,high-availability.kubernetes.leader-election.lease-duration,"Define the lease duration for the Kubernetes leader election. The leader will continuously renew its lease time to indicate its existence. And the followers will do a lease checking against the current time. ""renewTime + leaseDuration > now"" means the leader is alive.",0,0,others,flink
1172,high-availability.kubernetes.leader-election.renew-deadline,Defines the deadline duration when the leader tries to renew the lease. The leader will give up its leadership if it cannot successfully renew the lease in the given time.,0,0,others,flink
1173,high-availability.kubernetes.leader-election.retry-period,"Defines the pause duration between consecutive retries. All the contenders, including the current leader and all other followers, periodically try to acquire/renew the leadership if possible at this interval.",0,0,others,flink
1174,high-availability.storageDir,File system path (URI) where Flink persists metadata in high-availability setups.,0,0,others,flink
1175,high-availability.zookeeper.client.acl,Defines the ACL (open|creator) to be configured on ZK node. The configuration value can be set to 'creator' if the ZooKeeper server configuration has the 'authProvider' property mapped to use SASLAuthenticationProvider and the cluster is configured to run in secure mode (Kerberos).,0,0,others,flink
1176,high-availability.zookeeper.client.connection-timeout,Defines the connection timeout for ZooKeeper in ms.,0,0,others,flink
1179,high-availability.zookeeper.client.session-timeout,Defines the session timeout for the ZooKeeper session in ms.,0,0,others,flink
1181,high-availability.zookeeper.path.checkpoints,ZooKeeper root path (ZNode) for completed checkpoints.,0,0,others,flink
1182,high-availability.zookeeper.path.latch,Defines the znode of the leader latch which is used to elect the leader.,0,0,others,flink
1183,high-availability.zookeeper.path.leader,Defines the znode of the leader which contains the URL to the leader and the current leader session ID.,0,0,others,flink
1184,high-availability.zookeeper.path.mesos-workers,The ZooKeeper root path for persisting the Mesos worker information.,0,0,others,flink
1185,high-availability.zookeeper.path.root,The root path under which Flink stores its entries in ZooKeeper.,0,0,others,flink
1186,high-availability.zookeeper.quorum,"The ZooKeeper quorum to use, when running Flink in a high-availability mode with ZooKeeper.",0,0,others,flink
1187,historyserver.archive.clean-expired-jobs,Whether HistoryServer should cleanup jobs that are no longer present `historyserver.archive.fs.dir`.,1,6,function-tradeoff,flink
1188,historyserver.archive.fs.dir,Comma separated list of directories to fetch archived jobs from. The history server will monitor these directories for archived jobs. You can configure the JobManager to archive jobs to a directory via `jobmanager.archive.fs.dir`.,0,0,others,flink
1189,historyserver.archive.fs.refresh-interval,Interval in milliseconds for refreshing the archived job directories.,0,0,others,flink
1190,historyserver.archive.retained-jobs,"The maximum number of jobs to retain in each archive directory defined by `historyserver.archive.fs.dir`. If set to `-1`(default), there is no limit to the number of archives. If set to `0` or less than `-1` HistoryServer will throw an IllegalConfigurationException.",0,0,others,flink
1191,historyserver.web.address,Address of the HistoryServer's web interface.,0,0,others,flink
1192,historyserver.web.port,Port of the HistoryServers's web interface.,0,0,others,flink
1193,historyserver.web.refresh-interval,The refresh interval for the HistoryServer web-frontend in milliseconds.,0,0,others,flink
1194,historyserver.web.ssl.enabled,Enable HTTPs access to the HistoryServer web frontend. This is applicable only when the global SSL flag security.ssl.enabled is set to true.,1,6,function-tradeoff,flink
1195,historyserver.web.tmpdir,This configuration parameter allows defining the Flink web directory to be used by the history server web interface. The web interface will copy its static files into the directory.,0,0,others,flink
1196,io.tmp.dirs,"Directories for temporary files, separated by"","", ""|"", or the system's java.io.File.pathSeparator.",0,0,others,flink
1197,jmx.server.port,The port range for the JMX server to start the registry.,0,0,others,flink
1198,jobmanager.archive.fs.dir,Dictionary for JobManager to store the archives of completed jobs.,0,0,others,flink
1199,jobmanager.execution.attempts-history-size,The maximum number of prior execution attempts kept in history.,1,5,workload-specific,flink
1200,jobmanager.execution.failover-strategy,This option specifies how the job computation recovers from task failures.,0,0,others,flink
1201,jobmanager.memory.enable-jvm-direct-memory-limit,Whether to enable the JVM direct memory limit of the JobManager process (-XX:MaxDirectMemorySize). The limit will be set to the value of 'jobmanager.memory.off-heap.size' option.,1,6,function-tradeoff,flink
1202,jobmanager.memory.flink.size,"Total Flink Memory size for the JobManager. This includes all the memory that a JobManager consumes, except for JVM Metaspace and JVM Overhead. It consists of JVM Heap Memory and Off-heap Memory. See also 'jobmanager.memory.process.size' for total process memory size configuration.",1,1,resource,flink
1203,jobmanager.memory.heap.size,JVM Heap Memory size for JobManager. The minimum recommended JVM Heap size is 128.000mb (134217728 bytes).,1,1,resource,flink
1204,jobmanager.memory.jvm-metaspace.size,JVM Metaspace Size for the JobManager.,1,1,resource,flink
1205,jobmanager.memory.jvm-overhead.fraction,"Fraction of Total Process Memory to be reserved for JVM Overhead. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value.",1,1,resource,flink
1206,jobmanager.memory.jvm-overhead.max,"Max JVM Overhead size for the JobManager. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value.",1,1,resource,flink
1207,jobmanager.memory.jvm-overhead.min,"Min JVM Overhead size for the JobManager. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value.",1,1,resource,flink
1209,jobmanager.memory.process.size,"Total Process Memory size for the JobManager. This includes all the memory that a JobManager JVM process consumes, consisting of Total Flink Memory, JVM Metaspace, and JVM Overhead. In containerized setups, this should be set to the container memory. See also 'jobmanager.memory.flink.size' for Total Flink Memory size configuration.",1,1,resource,flink
1210,jobmanager.retrieve-taskmanager-hostname,"Flag indicating whether JobManager would retrieve canonical host name of TaskManager during registration. If the option is set to ""false"", TaskManager registration with JobManager could be faster, since no reverse DNS lookup is performed. However, local input split assignment (such as for HDFS files) may be impacted.",1,6,function-tradeoff,flink
1211,jobmanager.rpc.address,"The config parameter defining the network address to connect to for communication with the job manager. This value is only interpreted in setups where a single JobManager with static name or address exists (simple standalone setups, or container setups with dynamic service name resolution). It is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers.",0,0,others,flink
1212,jobmanager.rpc.port,"The config parameter defining the network port to connect to for communication with the job manager. Like jobmanager.rpc.address, this value is only interpreted in setups where a single JobManager with static name/address and port exists (simple standalone setups, or container setups with dynamic service name resolution). This config option is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers.",0,0,others,flink
1213,jobstore.cache-size,The job store cache size in bytes which is used to keep completed jobs in memory.,1,1,resource,flink
1214,jobstore.expiration-time,The time in seconds after which a completed job expires and is purged from the job store.,0,0,others,flink
1215,jobstore.max-capacity,The max number of completed jobs that can be kept in the job store.,1,5,workload-specific,flink
1216,kubernetes.cluster-id,"The cluster-id, which should be no more than 45 characters, is used for identifying a unique Flink cluster. The id must only contain lowercase alphanumeric characters and ""-"". The required format is [a-z]([-a-z0-9]*[a-z0-9]). If not set, the client will automatically generate it with a random ID.",0,0,others,flink
1217,kubernetes.config.file,The kubernetes config file will be used to create the client. The default is located at ~/.kube/config,0,0,others,flink
1219,kubernetes.container.image.pull-policy,The Kubernetes container image pull policy (IfNotPresent or Always or Never). The default policy is IfNotPresent to avoid putting pressure to image repository.,1,4,limited-side-effect,flink
1220,kubernetes.container.image.pull-secrets,A semicolon-separated list of the Kubernetes secrets used to access private image registries.,0,0,others,flink
1221,kubernetes.container-start-command-template,Template for the kubernetes jobmanager and taskmanager container start invocation.,0,0,others,flink
1222,kubernetes.context,The desired context from your Kubernetes config file used to configure the Kubernetes client for interacting with the cluster. This could be helpful if one has multiple contexts configured and wants to administrate different Flink clusters on different Kubernetes clusters/contexts.,0,0,others,flink
1223,kubernetes.entry.path,The entrypoint script of kubernetes in the image. It will be used as command for jobmanager and taskmanager container.,0,0,others,flink
1224,kubernetes.env.secretKeyRef,"The user-specified secrets to set env variables in Flink container. The value should be in the form of env:FOO_ENV,secret:foo_secret,key:foo_key;env:BAR_ENV,secret:bar_secret,key:bar_key.",0,0,others,flink
1225,kubernetes.flink.conf.dir,"The flink conf directory that will be mounted in pod. The flink-conf.yaml, log4j.properties, logback.xml in this path will be overwritten from config map.",0,0,others,flink
1226,kubernetes.flink.log.dir,The directory that logs of jobmanager and taskmanager be saved in the pod.,0,0,others,flink
1227,kubernetes.hadoop.conf.config-map.name,Specify the name of an existing ConfigMap that contains custom Hadoop configuration to be mounted on the JobManager(s) and TaskManagers.,0,0,others,flink
1230,kubernetes.jobmanager.labels,"The labels to be set for JobManager pod. Specified as key:value pairs separated by commas. For example, version:alphav1,deploy:test.",0,0,others,flink
1231,kubernetes.jobmanager.node-selector,"The node selector to be set for JobManager pod. Specified as key:value pairs separated by commas. For example, environment:production,disk:ssd.",0,0,others,flink
1232,kubernetes.jobmanager.owner.reference,"The user-specified Owner References to be set to the JobManager Deployment. When all the owner resources are deleted, the JobManager Deployment will be deleted automatically, which also deletes all the resources created by this Flink cluster. The value should be formatted as a semicolon-separated list of owner references, where each owner reference is a comma-separated list of `key:value` pairs. E.g., apiVersion:v1,blockOwnerDeletion:true,controller:true,kind:FlinkApplication,name:flink-app-name,uid:flink-app-uid;apiVersion:v1,kind:Deployment,name:deploy-name,uid:deploy-uid",0,0,others,flink
1233,kubernetes.jobmanager.service-account,"Service account that is used by jobmanager within kubernetes cluster. The job manager uses this service account when requesting taskmanager pods from the API server. If not explicitly configured, config option 'kubernetes.service-account' will be used.",0,0,others,flink
1234,kubernetes.jobmanager.tolerations,"The user-specified tolerations to be set to the JobManager pod. The value should be in the form of key:key1,operator:Equal,value:value1,effect:NoSchedule;key:key2,operator:Exists,effect:NoExecute,tolerationSeconds:6000",0,0,others,flink
1235,kubernetes.namespace,The namespace that will be used for running the jobmanager and taskmanager pods.,0,0,others,flink
1237,kubernetes.rest-service.exposed.type,The exposed type of the rest service (ClusterIP or NodePort or LoadBalancer). The exposed rest service could be used to access the Flink's Web UI and REST endpoint.,0,0,others,flink
1238,kubernetes.secrets,"The user-specified secrets that will be mounted into Flink container. The value should be in the form of foo:/opt/secrets-foo,bar:/opt/secrets-bar.",0,0,others,flink
1239,kubernetes.service-account,Service account that is used by jobmanager and taskmanager within kubernetes cluster. Notice that this can be overwritten by config options 'kubernetes.jobmanager.service-account' and 'kubernetes.taskmanager.service-account' for jobmanager and taskmanager respectively.,0,0,others,flink
1240,kubernetes.taskmanager.annotations,"The user-specified annotations that are set to the TaskManager pod. The value could be in the form of a1:v1,a2:v2",0,0,others,flink
1241,kubernetes.taskmanager.cpu,"The number of cpu used by task manager. By default, the cpu is set to the number of slots per TaskManager",1,1,resource,flink
1243,kubernetes.taskmanager.node-selector,"The node selector to be set for TaskManager pods. Specified as key:value pairs separated by commas. For example, environment:production,disk:ssd.",0,0,others,flink
1244,kubernetes.taskmanager.service-account,"Service account that is used by taskmanager within kubernetes cluster. The task manager uses this service account when watching config maps on the API server to retrieve leader address of jobmanager and resourcemanager. If not explicitly configured, config option 'kubernetes.service-account' will be used.",0,0,others,flink
1245,kubernetes.taskmanager.tolerations,"The user-specified tolerations to be set to the TaskManager pod. The value should be in the form of key:key1,operator:Equal,value:value1,effect:NoSchedule;key:key2,operator:Exists,effect:NoExecute,tolerationSeconds:6000",0,0,others,flink
1246,kubernetes.transactional-operation.max-retries,"Defines the number of Kubernetes transactional operation retries before the client gives up. For example, FlinkKubeClient#checkAndUpdateConfigMap.",0,0,others,flink
1247,mesos.constraints.hard.hostattribute,"Constraints for task placement on Mesos based on agent attributes. Takes a comma-separated list of key:value pairs corresponding to the attributes exposed by the target mesos agents. Example: az:eu-west-1a,series:t2",0,0,others,flink
1248,mesos.failover-timeout,"The failover timeout in seconds for the Mesos scheduler, after which running tasks are automatically shut down.",0,0,others,flink
1249,mesos.master,The Mesos master URL.,0,0,others,flink
1250,mesos.resourcemanager.artifactserver.port,The config parameter defining the Mesos artifact server port to use. Setting the port to 0 will let the OS choose an available port.,0,0,others,flink
1252,mesos.resourcemanager.declined-offer-refuse-duration,Amount of time to ask the Mesos master to not resend a declined resource offer again. This ensures a declined resource offer isn't resent immediately after being declined,0,0,others,flink
1253,mesos.resourcemanager.framework.name,Mesos framework name,0,0,others,flink
1254,mesos.resourcemanager.framework.principal,Mesos framework principal,0,0,others,flink
1255,mesos.resourcemanager.framework.role,Mesos framework role definition,0,0,others,flink
1256,mesos.resourcemanager.framework.secret,Mesos framework secret,0,0,others,flink
1257,mesos.resourcemanager.framework.user,Mesos framework user,0,0,others,flink
1258,mesos.resourcemanager.network.resource.name,Network resource name on Mesos cluster.,0,0,others,flink
1259,mesos.resourcemanager.tasks.bootstrap-cmd,A command which is executed before the TaskManager is started.,0,0,others,flink
1260,mesos.resourcemanager.tasks.container.docker.force-pull-image,Instruct the docker containerizer to forcefully pull the image rather than reuse a cached version.,1,6,function-tradeoff,flink
1261,mesos.resourcemanager.tasks.container.docker.parameters,"Custom parameters to be passed into docker run command when using the docker containerizer. Comma separated list of ""key=value"" pairs. The ""value"" may contain '='.",0,0,others,flink
1262,mesos.resourcemanager.tasks.container.image.name,Image name to use for the container.,0,0,others,flink
1263,mesos.resourcemanager.tasks.container.type,Type of the containerization used: 'mesos' or 'docker'.,0,0,others,flink
1264,mesos.resourcemanager.tasks.container.volumes,A comma separated list of [host_path:]container_path[:RO|RW]. This allows for mounting additional volumes into your container.,0,0,others,flink
1265,mesos.resourcemanager.tasks.cpus,CPUs to assign to the Mesos workers.,1,1,resource,flink
1266,mesos.resourcemanager.tasks.disk,Disk space to assign to the Mesos workers in MB.,1,1,resource,flink
1267,mesos.resourcemanager.tasks.gpus,GPUs to assign to the Mesos workers.,1,1,resource,flink
1268,mesos.resourcemanager.tasks.hostname,Optional value to define the TaskManager's hostname. The pattern _TASK_ is replaced by the actual id of the Mesos task. This can be used to configure the TaskManager to use Mesos DNS (e.g. _TASK_.flink-service.mesos) for name lookups.,0,0,others,flink
1269,mesos.resourcemanager.tasks.network.bandwidth,Network bandwidth to assign to the Mesos workers in MB per sec.,1,1,resource,flink
1270,mesos.resourcemanager.tasks.port-assignments,Comma-separated list of configuration keys which represent a configurable port. All port keys will dynamically get a port assigned through Mesos.,0,0,others,flink
1271,mesos.resourcemanager.tasks.uris,A comma separated list of URIs of custom artifacts to be downloaded into the sandbox of Mesos workers.,0,0,others,flink
1272,mesos.resourcemanager.unused-offer-expiration,Amount of time to wait for unused expired offers before declining them. This ensures your scheduler will not hoard unuseful offers.,0,0,others,flink
1273,metrics.fetcher.update-interval,Update interval for the metric fetcher used by the web UI in milliseconds. Decrease this value for faster updating metrics. Increase this value if the metric fetcher causes too much load. Setting this value to 0 disables the metric fetching completely.,0,0,others,flink
1274,metrics.internal.query-service.port,"The port range used for Flink's internal metric query service. Accepts a list of ports ('50100,50101'), ranges('50100-50200') or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple Flink components are running on the same machine. Per default Flink will pick a random port.",0,0,others,flink
1275,metrics.internal.query-service.thread-priority,"The thread priority used for Flink's internal metric query service. The thread is created by Akka's thread pool executor. The range of the priority is from 1 (MIN_PRIORITY) to 10 (MAX_PRIORITY). Warning, increasing this value may bring the main Flink components down.",0,0,others,flink
1276,metrics.latency.granularity,"Defines the granularity of latency metrics. Accepted values are: single - Track latency without differentiating between sources and subtasks. operator - Track latency while differentiating between sources, but not subtasks. subtask - Track latency while differentiating between sources and subtasks.",0,0,others,flink
1277,metrics.latency.history-size,Defines the number of measured latencies to maintain at each operator.,0,0,others,flink
1278,metrics.latency.interval,Defines the interval at which latency tracking marks are emitted from the sources. Disables latency tracking if set to 0 or a negative value. Enabling this feature can significantly impact the performance of the cluster.,1,5,workload-specific,flink
1279,metrics.reporter.name.class,The reporter class to use for the reporter named <name>.,0,0,others,flink
1280,metrics.reporter.name.interval,The reporter interval to use for the reporter named <name>.,0,0,others,flink
1281,metrics.reporter.name.parameter,Configures the parameter <parameter> for the reporter named <name>.,0,0,others,flink
1282,metrics.reporters,"An optional list of reporter names. If configured, only reporters whose name matches any of the names in the list will be started. Otherwise, all reporters that could be found in the configuration will be started.",0,0,others,flink
1283,metrics.scope.delimiter,Delimiter used to assemble the metric identifier.,0,0,others,flink
1284,metrics.scope.jm,Defines the scope format string that is applied to all metrics scoped to a JobManager.,0,0,others,flink
1286,metrics.scope.operator,Defines the scope format string that is applied to all metrics scoped to an operator.,0,0,others,flink
1289,metrics.scope.tm.job,Defines the scope format string that is applied to all metrics scoped to a job on a TaskManager.,0,0,others,flink
1290,metrics.system-resource,"Flag indicating whether Flink should report system resource metrics such as machine's CPU, memory or network usage.",1,6,function-tradeoff,flink
1292,pipeline.auto-generate-uids,"When auto-generated UIDs are disabled, users are forced to manually specify UIDs on DataStream applications. It is highly recommended that users specify UIDs before deploying to production since they are used to match state in savepoints to operators in a job. Because auto-generated ID's are likely to change when modifying a job, specifying custom IDs allow an application to evolve over time without discarding state.",0,0,others,flink
1294,pipeline.auto-watermark-interval,"The interval of the automatic watermark emission. Watermarks are used throughout the streaming system to keep track of the progress of time. They are used, for example, for time based windowing.",0,0,others,flink
1295,pipeline.cached-files,"Files to be registered at the distributed cache under the given name. The files will be accessible from any user-defined function in the (distributed) runtime under a local path. Files may be local files (which will be distributed via BlobServer), or files in a distributed file system. The runtime will copy the files temporarily to a local cache, if needed.",0,0,others,flink
1296,pipeline.classpaths,A semicolon-separated list of the classpaths to package with the job jars to be sent to the cluster. These have to be valid URLs.,0,0,others,flink
1297,pipeline.closure-cleaner-level,"Configures the mode in which the closure cleaner works where NONE disables the closure cleaner completely, TOP_LEVEL cleans only the top-level class without recursing into fields and RECURSIVE cleans all the fields recursively",0,0,others,flink
1298,pipeline.default-kryo-serializers,Semicolon separated list of pairs of class names and Kryo serializers class names to be used as Kryo default serializers,0,0,others,flink
1300,pipeline.force-kryo,"If enabled, forces TypeExtractor to use Kryo serializer for POJOS even though we could analyze as POJO. In some cases this might be preferable. For example, when using interfaces with subclasses that cannot be analyzed as POJO.",0,0,others,flink
1301,pipeline.generic-types,"If the use of generic types is disabled, Flink will throw an UnsupportedOperationException whenever it encounters a data type that would go through Kryo for serialization. Disabling generic types can be helpful to eagerly find and eliminate the use of types that would go through Kryo serialization during runtime. Rather than checking types individually, using this option will throw exceptions eagerly in the places where generic types are used. We recommend to use this option only during development and pre-production phases, not during actual production use. The application program and/or the input data may be such that new, previously unseen, types occur at some point. In that case, setting this option would cause the program to fail.",0,0,others,flink
1302,pipeline.global-job-parameters,"Register a custom, serializable user configuration object. The configuration can be accessed in operators",0,0,others,flink
1303,pipeline.jars,A semicolon-separated list of the jars to package with the job jars to be sent to the cluster. These have to be valid paths.,0,0,others,flink
1305,pipeline.name,The job name used for printing and logging.,0,0,others,flink
1306,pipeline.object-reuse,When enabled objects that Flink internally uses for deserialization and passing data to user-code functions will be reused. Keep in mind that this can lead to bugs when the user-code function of an operation is not aware of this behaviour.,0,0,others,flink
1308,pipeline.registered-kryo-types,"Semicolon separated list of types to be registered with the serialization stack. If the type is eventually serialized as a POJO, then the type is registered with the POJO serializer. If the type ends up being serialized with Kryo, then it will be registered at Kryo to make sure that only tags are written.",0,0,others,flink
1310,pipeline.time-characteristic,"The time characteristic for all created streams, e.g., processingtime, event time, or ingestion time. If you set the characteristic to IngestionTime or EventTime this will set a default watermark update interval of 200 ms. If this is not applicable for your application you should change it using pipeline.auto-watermark-interval.",0,0,others,flink
1311,queryable-state.client.network-threads,Number of network (Netty's event loop) Threads for queryable state client.,1,1,resource,flink
1312,queryable-state.enable,Option whether the queryable state proxy and server should be enabled where possible and configurable.,0,0,others,flink
1313,queryable-state.proxy.network-threads,Number of network (Netty's event loop) Threads for queryable state proxy.,1,1,resource,flink
1314,queryable-state.proxy.ports,"The port range of the queryable state proxy. The specified range can be a single port: ""9123"", a range of ports: ""50100-50200"", or a list of ranges and ports: ""50100-50200,50300-50400,51234"".",0,0,others,flink
1315,queryable-state.proxy.query-threads,Number of query Threads for queryable state proxy. Uses the number of slots if set to 0.,1,1,resource,flink
1316,queryable-state.server.network-threads,Number of network (Netty's event loop) Threads for queryable state server.,1,1,resource,flink
1317,queryable-state.server.ports,"The port range of the queryable state server. The specified range can be a single port: ""9123"", a range of ports: ""50100-50200"", or a list of ranges and ports: ""50100-50200,50300-50400,51234"".",0,0,others,flink
1318,queryable-state.server.query-threads,Number of query Threads for queryable state server. Uses the number of slots if set to 0.,1,1,resource,flink
1319,resourcemanager.job.timeout,Timeout for jobs which don't have a job manager as leader assigned.,0,0,others,flink
1320,resourcemanager.rpc.port,"Defines the network port to connect to for communication with the resource manager. By default, the port of the JobManager, because the same ActorSystem is used. Its not possible to use this configuration key to define port ranges.",0,0,others,flink
1322,resourcemanager.taskmanager-timeout,The timeout for an idle task manager to be released.,0,0,others,flink
1323,rest.address,The address that should be used by clients to connect to the server. Attention: This option is respected only if the high-availability configuration is NONE.,0,0,others,flink
1324,rest.await-leader-timeout,"The time in ms that the client waits for the leader address, e.g., Dispatcher or WebMonitorEndpoint",0,0,others,flink
1325,rest.bind-address,The address that the server binds itself.,0,0,others,flink
1326,rest.bind-port,"The port that the server binds itself. Accepts a list of ports ('50100,50101'), ranges ('50100-50200') or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple Rest servers are running on the same machine.",0,0,others,flink
1327,rest.client.max-content-length,The maximum content length in bytes that the client will handle.,1,5,workload-specific,flink
1328,rest.connection-timeout,The maximum time in ms for the client to establish a TCP connection.,0,0,others,flink
1329,rest.idleness-timeout,The maximum time in ms for a connection to stay idle before failing.,0,0,others,flink
1330,rest.port,"The port that the client connects to. If rest.bind-port has not been specified, then the REST server will bind to this port. Attention: This option is respected only if the high-availability configuration is NONE.",0,0,others,flink
1332,rest.retry.max-attempts,The number of retries the client will attempt if a retryable operations fails.,0,0,others,flink
1333,rest.server.max-content-length,The maximum content length in bytes that the server will handle.,1,5,workload-specific,flink
1335,rest.server.thread-priority,Thread priority of the REST server's executor for processing asynchronous requests. Lowering the thread priority will give Flink's main components more CPU time whereas increasing will allocate more time for the REST server's processing.,0,0,others,flink
1336,restart-strategy,"Defines the restart strategy to use in case of job failures. If checkpointing is disabled, the default value is none. If checkpointing is enabled, the default value is fixed-delay with Integer.MAX_VALUE restart attempts and '1 s' delay.",0,0,others,flink
1337,restart-strategy.failure-rate.delay,"Delay between two consecutive restart attempts if restart-strategy has been set to failure-rate. It can be specified using notation: ""1 min"", ""20 s""",0,0,others,flink
1338,restart-strategy.failure-rate.failure-rate-interval,"Time interval for measuring failure rate if restart-strategy has been set to failure-rate. It can be specified using notation: ""1 min"", ""20 s""",0,0,others,flink
1339,restart-strategy.failure-rate.max-failures-per-interval,Maximum number of restarts in given time interval before failing a job if restart-strategy has been set to failure-rate.,0,0,others,flink
1341,restart-strategy.fixed-delay.delay,"Delay between two consecutive restart attempts if restart-strategy has been set to fixed-delay. Delaying the retries can be helpful when the program interacts with external systems where for example connections or pending transactions should reach a timeout before re-execution is attempted. It can be specified using notation: ""1 min"", ""20 s""",0,0,others,flink
1342,security.kerberos.login.contexts,"A comma-separated list of login contexts to provide the Kerberos credentials to (for example, `Client,KafkaClient` to use the credentials for ZooKeeper authentication and for Kafka authentication)",0,0,others,flink
1343,security.kerberos.login.keytab,Absolute path to a Kerberos keytab file that contains the user credentials.,0,0,others,flink
1344,security.kerberos.login.principal,Kerberos principal name associated with the keytab.,0,0,others,flink
1345,security.kerberos.login.use-ticket-cache,Indicates whether to read from your Kerberos ticket cache.,1,6,function-tradeoff,flink
1346,security.ssl.algorithms,The comma separated list of standard SSL algorithms to be supported. Read more here,0,0,others,flink
1347,security.ssl.internal.cert.fingerprint,The sha1 fingerprint of the internal certificate. This further protects the internal communication to present the exact certificate used by Flink.This is necessary where one cannot use private CA(self signed) or there is internal firm wide CA is required,0,0,others,flink
1348,security.ssl.internal.close-notify-flush-timeout,The timeout (in ms) for flushing the `close_notify` that was triggered by closing a channel. If the `close_notify` was not flushed in the given timeout the channel will be closed forcibly. (-1 = use system default),0,0,others,flink
1350,security.ssl.internal.handshake-timeout,The timeout (in ms) during SSL handshake. (-1 = use system default),0,0,others,flink
1351,security.ssl.internal.key-password,"The secret to decrypt the key in the keystore for Flink's internal endpoints (rpc, data transport, blob server).",0,0,others,flink
1352,security.ssl.internal.keystore,"The Java keystore file with SSL Key and Certificate, to be used Flink's internal endpoints (rpc, data transport, blob server).",0,0,others,flink
1355,security.ssl.internal.session-timeout,The timeout (in ms) for the cached SSL session objects. (-1 = use system default),0,0,others,flink
1356,security.ssl.internal.truststore,"The truststore file containing the public CA certificates to verify the peer for Flink's internal endpoints (rpc, data transport, blob server).",0,0,others,flink
1357,security.ssl.internal.truststore-password,"The password to decrypt the truststore for Flink's internal endpoints (rpc, data transport, blob server).",0,0,others,flink
1359,security.ssl.provider,The SSL engine provider to use for the ssl transport.,0,0,others,flink
1360,security.ssl.rest.authentication-enabled,Turns on mutual SSL authentication for external communication via the REST endpoints.,1,2,security-tradeoff,flink
1361,security.ssl.rest.cert.fingerprint,The sha1 fingerprint of the rest certificate. This further protects the rest REST endpoints to present certificate which is only used by proxy serverThis is necessary where once uses public CA or internal firm wide CA,0,0,others,flink
1363,security.ssl.rest.key-password,The secret to decrypt the key in the keystore for Flink's external REST endpoints.,0,0,others,flink
1364,security.ssl.rest.keystore,"The Java keystore file with SSL Key and Certificate, to be used Flink's external REST endpoints.",0,0,others,flink
1365,security.ssl.rest.keystore-password,The secret to decrypt the keystore file for Flink's for Flink's external REST endpoints.,0,0,others,flink
1366,security.ssl.rest.truststore,The truststore file containing the public CA certificates to verify the peer for Flink's external REST endpoints.,0,0,others,flink
1368,security.ssl.verify-hostname,Flag to enable peer's hostname verification during ssl handshake.,1,2,security-tradeoff,flink
1369,slot.idle.timeout,The timeout in milliseconds for a idle slot in Slot Pool.,0,0,others,flink
1370,slot.request.timeout,The timeout in milliseconds for requesting a slot from Slot Pool.,0,0,others,flink
1373,state.backend,The state backend to be used to store and checkpoint state.,0,0,others,flink
1374,state.backend.async,"Option whether the state backend should use an asynchronous snapshot method where possible and configurable. Some state backends may not support asynchronous snapshots, or only support asynchronous snapshots, and ignore this option.",1,4,limited-side-effect,flink
1377,state.backend.incremental,"Option whether the state backend should create incremental checkpoints, if possible. For an incremental checkpoint, only a diff from the previous checkpoint is stored, rather than the complete checkpoint state. Once enabled, the state size shown in web UI or fetched from rest API only represents the delta checkpoint size instead of full checkpoint size. Some state backends may not support incremental checkpoints and ignore this option.",1,6,function-tradeoff,flink
1378,state.backend.local-recovery,"This option configures local recovery for this state backend. By default, local recovery is deactivated. Local recovery currently only covers keyed state backends. Currently, MemoryStateBackend does not support local recovery and ignore this option.",1,6,function-tradeoff,flink
1379,state.backend.rocksdb.block.blocksize,The approximate size (in bytes) of user data packed per block. RocksDB has default blocksize as '4KB'.,1,1,resource,flink
1380,state.backend.rocksdb.block.cache-size,The amount of the cache for data blocks in RocksDB. RocksDB has default block-cache size as '8MB'.,1,1,resource,flink
1382,state.backend.rocksdb.compaction.level.max-size-level-base,The upper-bound of the total size of level base files in bytes. RocksDB has default configuration as '256MB'.,1,1,resource,flink
1383,state.backend.rocksdb.compaction.level.target-file-size-base,"The target file size for compaction, which determines a level-1 file size. RocksDB has default configuration as '64MB'.",1,1,resource,flink
1384,state.backend.rocksdb.compaction.level.use-dynamic-size,"If true, RocksDB will pick target size of each level dynamically. From an empty DB, RocksDB would make last level the base level, which means merging L0 data into the last level, until it exceeds max_bytes_for_level_base. And then repeat this process for second last level and so on. RocksDB has default configuration as 'false'. For more information, please refer to RocksDB's doc.",1,6,function-tradeoff,flink
1386,state.backend.rocksdb.files.open,"The maximum number of open files (per TaskManager) that can be used by the DB, '-1' means no limit. RocksDB has default configuration as '-1'.",1,1,resource,flink
1387,state.backend.rocksdb.localdir,The local directory (on the TaskManager) where RocksDB puts its files.,0,0,others,flink
1388,state.backend.rocksdb.memory.fixed-per-slot,"The fixed total amount of memory, shared among all RocksDB instances per slot. This option overrides the 'state.backend.rocksdb.memory.managed' option when configured. If neither this option, nor the 'state.backend.rocksdb.memory.managed' optionare set, then each RocksDB column family state has its own memory caches (as controlled by the column family options).",1,1,resource,flink
1389,state.backend.rocksdb.memory.high-prio-pool-ratio,"The fraction of cache memory that is reserved for high-priority data like index, filter, and compression dictionary blocks. This option only has an effect when 'state.backend.rocksdb.memory.managed' or 'state.backend.rocksdb.memory.fixed-per-slot' are configured.",1,1,resource,flink
1390,state.backend.rocksdb.memory.managed,"If set, the RocksDB state backend will automatically configure itself to use the managed memory budget of the task slot, and divide the memory over write buffers, indexes, block caches, etc. That way, the three major uses of memory of RocksDB will be capped.",1,6,function-tradeoff,flink
1391,state.backend.rocksdb.memory.write-buffer-ratio,"The maximum amount of memory that write buffers may take, as a fraction of the total shared memory. This option only has an effect when 'state.backend.rocksdb.memory.managed' or 'state.backend.rocksdb.memory.fixed-per-slot' are configured.",1,1,resource,flink
1393,state.backend.rocksdb.metrics.background-errors,Monitor the number of background errors in RocksDB.,0,0,others,flink
1395,state.backend.rocksdb.metrics.block-cache-pinned-usage,Monitor the memory size for the entries being pinned in block cache.,1,6,function-tradeoff,flink
1396,state.backend.rocksdb.metrics.block-cache-usage,Monitor the memory size for the entries residing in block cache.,1,6,function-tradeoff,flink
1397,state.backend.rocksdb.metrics.column-family-as-variable,Whether to expose the column family as a variable.,0,0,others,flink
1398,state.backend.rocksdb.metrics.compaction-pending,"Track pending compactions in RocksDB. Returns 1 if a compaction is pending, 0 otherwise.",0,0,others,flink
1399,state.backend.rocksdb.metrics.cur-size-active-mem-table,Monitor the approximate size of the active memtable in bytes.,1,6,function-tradeoff,flink
1400,state.backend.rocksdb.metrics.cur-size-all-mem-tables,Monitor the approximate size of the active and unflushed immutable memtables in bytes.,1,6,function-tradeoff,flink
1401,state.backend.rocksdb.metrics.estimate-live-data-size,Estimate of the amount of live data in bytes.,1,6,function-tradeoff,flink
1402,state.backend.rocksdb.metrics.estimate-num-keys,Estimate the number of keys in RocksDB.,0,0,others,flink
1405,state.backend.rocksdb.metrics.is-write-stopped,"Track whether write has been stopped in RocksDB. Returns 1 if write has been stopped, 0 otherwise.",0,0,others,flink
1406,state.backend.rocksdb.metrics.mem-table-flush-pending,Monitor the number of pending memtable flushes in RocksDB.,1,6,function-tradeoff,flink
1409,state.backend.rocksdb.metrics.num-entries-active-mem-table,Monitor the total number of entries in the active memtable.,1,6,function-tradeoff,flink
1410,state.backend.rocksdb.metrics.num-entries-imm-mem-tables,Monitor the total number of entries in the unflushed immutable memtables.,1,6,function-tradeoff,flink
1411,state.backend.rocksdb.metrics.num-immutable-mem-table,Monitor the number of immutable memtables in RocksDB.,1,6,function-tradeoff,flink
1413,state.backend.rocksdb.metrics.num-running-compactions,Monitor the number of currently running compactions.,1,6,function-tradeoff,flink
1414,state.backend.rocksdb.metrics.num-running-flushes,Monitor the number of currently running flushes.,1,6,function-tradeoff,flink
1415,state.backend.rocksdb.metrics.num-snapshots,Monitor the number of unreleased snapshots of the database.,1,6,function-tradeoff,flink
1416,state.backend.rocksdb.metrics.size-all-mem-tables,"Monitor the approximate size of the active, unflushed immutable, and pinned immutable memtables in bytes.",1,6,function-tradeoff,flink
1417,state.backend.rocksdb.metrics.total-sst-files-size,Monitor the total size (bytes) of all SST files.WARNING: may slow down online queries if there are too many files.,1,6,function-tradeoff,flink
1418,state.backend.rocksdb.options-factory,"The options factory class for RocksDB to create DBOptions and ColumnFamilyOptions. The default options factory is org.apache.flink.contrib.streaming.state.DefaultConfigurableOptionsFactory, and it would read the configured options which provided in 'RocksDBConfigurableOptions'.",0,0,others,flink
1419,state.backend.rocksdb.predefined-options,"The predefined settings for RocksDB DBOptions and ColumnFamilyOptions by Flink community. Current supported candidate predefined-options are DEFAULT, SPINNING_DISK_OPTIMIZED, SPINNING_DISK_OPTIMIZED_HIGH_MEM or FLASH_SSD_OPTIMIZED. Note that user customized options and options from the RocksDBOptionsFactory are applied on top of these predefined ones.",0,0,others,flink
1421,state.backend.rocksdb.timer-service.factory,This determines the factory for timer service state implementation. Options are either HEAP (heap-based) or ROCKSDB for an implementation based on RocksDB.,0,0,others,flink
1422,state.backend.rocksdb.write-batch-size,"The max size of the consumed memory for RocksDB batch write, will flush just based on item count if this config set to 0.",1,1,resource,flink
1424,state.backend.rocksdb.writebuffer.number-to-merge,The minimum number of write buffers that will be merged together before writing to storage. RocksDB has default configuration as '1'.,1,1,resource,flink
1425,state.backend.rocksdb.writebuffer.size,The amount of data built up in memory (backed by an unsorted log on disk) before converting to a sorted on-disk files. RocksDB has default writebuffer size as '64MB'.,1,1,resource,flink
1427,state.checkpoints.num-retained,The maximum number of completed checkpoints to retain.,0,0,others,flink
1428,state.savepoints.dir,"The default directory for savepoints. Used by the state backends that write savepoints to file systems (MemoryStateBackend, FsStateBackend, RocksDBStateBackend).",0,0,others,flink
1429,task.cancellation.interval,Time interval between two successive task cancellation attempts in milliseconds.,0,0,others,flink
1432,taskmanager.data.port,The task manager's external port used for data exchange operations.,0,0,others,flink
1433,taskmanager.data.ssl.enabled,Enable SSL support for the taskmanager data transport. This is applicable only when the global flag for internal SSL (security.ssl.internal.enabled) is set to true,1,6,function-tradeoff,flink
1434,taskmanager.debug.memory.log,"Flag indicating whether to start a thread, which repeatedly logs the memory usage of the JVM.",1,6,function-tradeoff,flink
1435,taskmanager.debug.memory.log-interval,The interval (in ms) for the log thread to log the current memory usage.,0,0,others,flink
1436,taskmanager.host,"The external address of the network interface where the TaskManager is exposed. Because different TaskManagers need different values for this option, usually it is specified in an additional non-shared TaskManager-specific config file.",0,0,others,flink
1437,taskmanager.jvm-exit-on-oom,Whether to kill the TaskManager when the task thread throws an OutOfMemoryError.,1,6,function-tradeoff,flink
1438,taskmanager.memory.flink.size,"Total Flink Memory size for the TaskExecutors. This includes all the memory that a TaskExecutor consumes, except for JVM Metaspace and JVM Overhead. It consists of Framework Heap Memory, Task Heap Memory, Task Off-Heap Memory, Managed Memory, and Network Memory. See also 'taskmanager.memory.process.size' for total process memory size configuration.",1,1,resource,flink
1439,taskmanager.memory.framework.heap.size,"Framework Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for TaskExecutor framework, which will not be allocated to task slots.",1,1,resource,flink
1440,taskmanager.memory.framework.off-heap.size,"Framework Off-Heap Memory size for TaskExecutors. This is the size of off-heap memory (JVM direct memory and native memory) reserved for TaskExecutor framework, which will not be allocated to task slots. The configured value will be fully counted when Flink calculates the JVM max direct memory size parameter.",1,1,resource,flink
1441,taskmanager.memory.jvm-metaspace.size,JVM Metaspace Size for the TaskExecutors.,1,1,resource,flink
1443,taskmanager.memory.jvm-overhead.max,"Max JVM Overhead size for the TaskExecutors. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min/max size to the same value.",1,1,resource,flink
1445,taskmanager.memory.managed.consumer-weights,"Managed memory weights for different kinds of consumers. A slot's managed memory is shared by all kinds of consumers it contains, proportionally to the kinds' weights and regardless of the number of consumers from each kind. Currently supported kinds of consumers are DATAPROC (for RocksDB state backend in streaming and built-in algorithms in batch) and PYTHON (for Python processes).",1,1,resource,flink
1446,taskmanager.memory.managed.fraction,"Fraction of Total Flink Memory to be used as Managed Memory, if Managed Memory size is not explicitly specified.",1,1,resource,flink
1447,taskmanager.memory.managed.size,"Managed Memory size for TaskExecutors. This is the size of off-heap memory managed by the memory manager, reserved for sorting, hash tables, caching of intermediate results and RocksDB state backend. Memory consumers can either allocate memory from the memory manager in the form of MemorySegments, or reserve bytes from the memory manager and keep their memory usage within that boundary. If unspecified, it will be derived to make up the configured fraction of the Total Flink Memory.",1,1,resource,flink
1448,taskmanager.memory.network.fraction,"Fraction of Total Flink Memory to be used as Network Memory. Network Memory is off-heap memory reserved for ShuffleEnvironment (e.g., network buffers). Network Memory size is derived to make up the configured fraction of the Total Flink Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of Network Memory can be explicitly specified by setting the min/max size to the same value.",1,1,resource,flink
1450,taskmanager.memory.network.min,"Min Network Memory size for TaskExecutors. Network Memory is off-heap memory reserved for ShuffleEnvironment (e.g., network buffers). Network Memory size is derived to make up the configured fraction of the Total Flink Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of Network Memory can be explicitly specified by setting the min/max to the same value.",1,1,resource,flink
1451,taskmanager.memory.process.size,"Total Process Memory size for the TaskExecutors. This includes all the memory that a TaskExecutor consumes, consisting of Total Flink Memory, JVM Metaspace, and JVM Overhead. On containerized setups, this should be set to the container memory. See also 'taskmanager.memory.flink.size' for total Flink memory size configuration.",1,1,resource,flink
1452,taskmanager.memory.segment-size,Size of memory buffers used by the network stack and the memory manager.,1,1,resource,flink
1454,taskmanager.memory.task.off-heap.size,Task Off-Heap Memory size for TaskExecutors. This is the size of off heap memory (JVM direct memory and native memory) reserved for tasks. The configured value will be fully counted when Flink calculates the JVM max direct memory size parameter.,1,1,resource,flink
1455,taskmanager.network.bind-policy,"The automatic address binding policy used by the TaskManager if ""taskmanager.host"" is not set.",0,0,others,flink
1460,taskmanager.network.memory.floating-buffers-per-gate,"Number of extra network buffers to use for each outgoing/incoming gate (result partition/input gate). In credit-based flow control mode, this indicates how many floating credits are shared among all the input channels. The floating buffers are distributed based on backlog (real-time output buffers in the subpartition) feedback, and can help relieve back-pressure caused by unbalanced data distribution among the subpartitions. This value should be increased in case of higher round trip times between nodes and/or larger number of machines in the cluster.",1,1,resource,flink
1461,taskmanager.network.memory.max-buffers-per-channel,"Number of max buffers that can be used for each channel. If a channel exceeds the number of max buffers, it will make the task become unavailable, cause the back pressure and block the data processing. This might speed up checkpoint alignment by preventing excessive growth of the buffered in-flight data in case of data skew and high number of configured floating buffers. This limit is not strictly guaranteed, and can be ignored by things like flatMap operators, records spanning multiple buffers or single timer producing large amount of data.",1,1,resource,flink
1463,taskmanager.network.netty.client.numThreads,The number of Netty client threads.,1,1,resource,flink
1464,taskmanager.network.netty.num-arenas,The number of Netty arenas.,1,1,resource,flink
1465,taskmanager.network.netty.sendReceiveBufferSize,The Netty send and receive buffer size. This defaults to the system buffer size (cat /proc/sys/net/ipv4/tcp_[rw]mem) and is 4 MiB in modern Linux.,1,1,resource,flink
1466,taskmanager.network.netty.server.backlog,The netty server connection backlog.,1,3,reliability-tradeoff,flink
1467,taskmanager.network.netty.server.numThreads,The number of Netty server threads.,1,1,resource,flink
1469,taskmanager.network.request-backoff.initial,Minimum backoff in milliseconds for partition requests of input channels.,0,0,others,flink
1471,taskmanager.network.retries,The number of retry attempts for network communication. Currently it's only used for establishing input/output channel connections,0,0,others,flink
1472,taskmanager.network.sort-shuffle.min-buffers,"Minimum number of network buffers required per sort-merge blocking result partition. For large scale batch jobs, it is suggested to increase this config value to improve compression ratio and reduce small network packets. Note: to increase this config value, you may also need to increase the size of total network memory to avoid ""insufficient number of network buffers"" error.",1,1,resource,flink
1473,taskmanager.network.sort-shuffle.min-parallelism,"Parallelism threshold to switch between sort-merge blocking shuffle and the default hash-based blocking shuffle, which means for small parallelism, hash-based blocking shuffle will be used and for large parallelism, sort-merge blocking shuffle will be used. Note: sort-merge blocking shuffle uses unmanaged direct memory for shuffle data writing and reading so just increase the size of direct memory if direct memory OOM error occurs.",1,1,resource,flink
1475,taskmanager.registration.timeout,"Defines the timeout for the TaskManager registration. If the duration is exceeded without a successful registration, then the TaskManager terminates.",0,0,others,flink
1476,taskmanager.resource-id,"The TaskManager's ResourceID. If not configured, the ResourceID will be generated with the ""RpcAddress:RpcPort"" and a 6-character random string. Notice that this option is not valid in Yarn / Mesos and Native Kubernetes mode.",0,0,others,flink
1477,taskmanager.rpc.port,"The external RPC port where the TaskManager is exposed. Accepts a list of ports ('50100,50101'), ranges ('50100-50200') or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple TaskManagers are running on the same machine.",0,0,others,flink
1478,taskmanager.runtime.hashjoin-bloom-filters,"Flag to activate/deactivate bloom filters in the hybrid hash join implementation. In cases where the hash join needs to spill to disk (datasets larger than the reserved fraction of memory), these bloom filters can greatly reduce the number of spilled records, at the cost some CPU cycles.",1,6,function-tradeoff,flink
1479,taskmanager.runtime.large-record-handler,Whether to use the LargeRecordHandler when spilling. If a record will not fit into the sorting buffer. The record will be spilled on disk and the sorting will continue with only the key. The record itself will be read afterwards when merging.,1,6,function-tradeoff,flink
1480,taskmanager.runtime.max-fan,"The maximal fan-in for external merge joins and fan-out for spilling hash tables. Limits the number of file handles per operator, but may cause intermediate merging/partitioning, if set too small.",1,5,workload-specific,flink
1481,taskmanager.runtime.sort-spilling-threshold,A sort operation starts spilling when this fraction of its memory budget is full.,1,5,workload-specific,flink
1482,taskmanager.state.local.root-dirs,"The config parameter defining the root directories for storing file-based state for local recovery. Local recovery currently only covers keyed state backends. Currently, MemoryStateBackend does not support local recovery and ignore this option",0,0,others,flink
1483,web.access-control-allow-origin,Access-Control-Allow-Origin header for all responses from the web-frontend.,0,0,others,flink
1486,web.backpressure.num-samples,Number of samples to take to determine back pressure.,1,5,workload-specific,flink
1487,web.backpressure.refresh-interval,"Time, in milliseconds, after which available stats are deprecated and need to be refreshed (by resampling).",0,0,others,flink
1490,web.log.path,Path to the log file (may be in /log for standalone but under log directory when using YARN).,0,0,others,flink
1491,web.refresh-interval,Refresh interval for the web-frontend in milliseconds.,0,0,others,flink
1492,web.submit.enable,Flag indicating whether jobs can be uploaded and run from the web-frontend.,1,6,function-tradeoff,flink
1493,web.timeout,Timeout for asynchronous operations by the web monitor in milliseconds.,0,0,others,flink
1494,web.tmpdir,Flink web directory which is used by the webmonitor.,0,0,others,flink
1495,web.upload.dir,Directory for uploading the job jars. If not specified a dynamic directory will be used under the directory specified by JOB_MANAGER_WEB_TMPDIR_KEY.,0,0,others,flink
1496,yarn.application.id,The YARN application id of the running yarn cluster. This is the YARN cluster where the pipeline is going to be executed.,0,0,others,flink
1497,yarn.application.name,A custom name for your YARN application.,0,0,others,flink
1498,yarn.application.node-label,Specify YARN node label for the YARN application.,0,0,others,flink
1500,yarn.application.queue,The YARN queue on which to put the current pipeline.,0,0,others,flink
1501,yarn.application.type,A custom type for your YARN application..,0,0,others,flink
1502,yarn.application-attempt-failures-validity-interval,Time window in milliseconds which defines the number of application attempt failures when restarting the AM. Failures which fall outside of this window are not being considered. Set this value to -1 in order to count globally. See here for more information.,0,0,others,flink
1503,yarn.application-attempts,"Number of ApplicationMaster restarts. By default, the value will be set to 1. If high availability is enabled, then the default value will be 2. The restart number is also limited by YARN (configured via yarn.resourcemanager.am.max-attempts). Note that that the entire Flink cluster will restart and the YARN Client will lose the connection.",0,0,others,flink
1504,yarn.application-master.port,"With this configuration option, users can specify a port, a range of ports or a list of ports for the Application Master (and JobManager) RPC port. By default we recommend using the default value (0) to let the operating system choose an appropriate port. In particular when multiple AMs are running on the same physical host, fixed port assignments prevent the AM from starting. For example when running Flink on YARN on an environment with a restrictive firewall, this option allows specifying a range of allowed ports.",0,0,others,flink
1505,yarn.appmaster.vcores,The number of virtual cores (vcores) used by YARN application master.,1,1,resource,flink
1506,yarn.containers.vcores,"The number of virtual cores (vcores) per YARN container. By default, the number of vcores is set to the number of slots per TaskManager, if set, or to 1, otherwise. In order for this parameter to be used your cluster must have CPU scheduling enabled. You can do this by setting the org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.",1,1,resource,flink
1508,yarn.flink-dist-jar,The location of the Flink dist jar.,0,0,others,flink
1510,yarn.heartbeat.interval,Time between heartbeats with the ResourceManager in seconds.,0,0,others,flink
1511,yarn.per-job-cluster.include-user-jar,"Defines whether user-jars are included in the system class path for per-job-clusters as well as their positioning in the path. They can be positioned at the beginning (""FIRST""), at the end (""LAST""), or be positioned based on their name (""ORDER""). ""DISABLED"" means the user-jars are excluded from the system class path.",0,0,others,flink
1513,yarn.provided.lib.dirs,"A semicolon-separated list of provided lib directories. They should be pre-uploaded and world-readable. Flink will use them to exclude the local Flink jars(e.g. flink-dist, lib/, plugins/)uploading to accelerate the job submission process. Also YARN will cache them on the nodes so that they doesn't need to be downloaded every time for each application. An example could be hdfs://$namenode_address/path/of/flink/lib",0,0,others,flink
1514,yarn.security.kerberos.additionalFileSystems,"A comma-separated list of additional Kerberos-secured Hadoop filesystems Flink is going to access. For example, yarn.security.kerberos.additionalFileSystems=hdfs://namenode2:9002,hdfs://namenode3:9003. The client submitting to YARN needs to have access to these file systems to retrieve the security tokens.",0,0,others,flink
1515,yarn.security.kerberos.localized-keytab-path,"Local (on NodeManager) path where kerberos keytab file will be localized to. If yarn.security.kerberos.ship-local-keytab set to true, Flink willl ship the keytab file as a YARN local resource. In this case, the path is relative to the local resource directory. If set to false, Flink will try to directly locate the keytab from the path itself.",0,0,others,flink
1516,yarn.security.kerberos.ship-local-keytab,When this is true Flink will ship the keytab file configured via security.kerberos.login.keytab as a localized YARN resource.,1,2,security-tradeoff,flink
1517,yarn.ship-archives,"A semicolon-separated list of archives to be shipped to the YARN cluster. These archives will be un-packed when localizing and they can be any of the following types: "".tar.gz"", "".tar"", "".tgz"", "".dst"", "".jar"", "".zip"".",0,0,others,flink
1518,yarn.ship-files,A semicolon-separated list of files and/or directories to be shipped to the YARN cluster.,0,0,others,flink
1519,yarn.staging-directory,"Staging directory used to store YARN files while submitting applications. Per default, it uses the home directory of the configured file system.",0,0,others,flink
1521,-A predicate=answer,"Make an assertion with the predicate predicate and answer answer. This form is preferred to the older form -A predicate(answer), which is still supported, because it does not use shell special characters.",0,0,others,gcc
1522,-A -predicate=answer,Cancel an assertion with the predicate predicate and answer answer.,0,0,others,gcc
1523,aarch64-autovec-preference,"Force an ISA selection strategy for auto-vectorization. Accepts values from 0 to 4, inclusive.",1,4,limited-side-effect,gcc
1524,aarch64-double-recp-precision,The number of Newton iterations for calculating the reciprocal for double type. The precision of division is propotional to this param when division approximation is enabled. The default value is 2.,1,4,limited-side-effect,gcc
1525,aarch64-float-recp-precision,The number of Newton iterations for calculating the reciprocal for float type. The precision of division is proportional to this param when division approximation is enabled. The default value is 1.,1,4,limited-side-effect,gcc
1527,aarch64-sve-compare-costs,"When vectorizing for SVE, consider using npackedvectors for smaller elements and use the cost model to pick the cheapest approach. Also use the cost model to choose between SVE and Advanced SIMD vectorization.",1,4,limited-side-effect,gcc
1529,align-threshold,Select fraction of the maximal frequency of executions of a basic block in a function to align the basic block.,1,4,limited-side-effect,gcc
1530,analyzer-bb-explosion-factor,"The maximum number of after supernodeexploded nodes within the analyzer per supernode, before terminating analysis.",1,4,limited-side-effect,gcc
1532,analyzer-max-enodes-for-full-dump,The maximum depth of exploded nodes that should appear in a dot dump before switching to a less verbose format.,1,4,limited-side-effect,gcc
1533,analyzer-max-enodes-per-program-point,"The maximum number of exploded nodes per program point within the analyzer, before terminating analysis of that point.",1,4,limited-side-effect,gcc
1534,analyzer-max-infeasible-edges,The maximum number of infeasible edges to reject before declaring a diagnostic as infeasible.,1,4,limited-side-effect,gcc
1536,analyzer-max-svalue-depth,"The maximum depth of a symbolic value, before approximating the value as unknown.",1,5,workload-specific,gcc
1537,analyzer-min-snodes-for-call-summary,The minimum number of supernodes within a function for the analyzer to consider summarizing its effects at call sites.,1,5,workload-specific,gcc
1538,-ansi,"In C mode, this is equivalent to -std=c90. In C++ mode, it is equivalent to -std=c++98.",0,0,others,gcc
1539,asan-globals,Enable buffer overflow detection for global objects. This kind of protection is enabled by default if you are using -fsanitize=address option. To disable global objects protection use --param asan-globals=0.,1,4,limited-side-effect,gcc
1541,asan-instrumentation-with-call-threshold,"If number of memory accesses in function being instrumented is greater or equal to this number, use callbacks instead of inline checks. E.g. to disable inline code use --param asan-instrumentation-with-call-threshold=0.",1,4,limited-side-effect,gcc
1542,asan-instrument-reads,Enable buffer overflow detection for memory reads. This kind of protection is enabled by default when using -fsanitize=address. To disable memory reads protection use --param asan-instrument-reads=0.,1,4,limited-side-effect,gcc
1543,asan-instrument-writes,Enable buffer overflow detection for memory writes. This kind of protection is enabled by default when using -fsanitize=address. To disable memory writes protection use --param asan-instrument-writes=0 option.,1,4,limited-side-effect,gcc
1544,asan-memintrin,Enable detection for built-in functions. This kind of protection is enabled by default when using -fsanitize=address. To disable built-in functions protection use --param asan-memintrin=0.,1,4,limited-side-effect,gcc
1545,asan-stack,Enable buffer overflow detection for stack objects. This kind of protection is enabled by default when using -fsanitize=address. To disable stack protection use --param asan-stack=0 option.,1,4,limited-side-effect,gcc
1546,asan-use-after-return,Enable detection of use-after-return. This kind of protection is enabled by default when using the -fsanitize=address option. To disable it use --param asan-use-after-return=0.,1,4,limited-side-effect,gcc
1547,-aux-info filename,"Output to the given filename prototyped declarations for all functions declared and/or defined in a translation unit, including those in header files. This option is silently ignored in any language other than C.",1,6,function-tradeoff,gcc
1548,avg-loop-niter,Average number of iterations of a loop.,1,4,limited-side-effect,gcc
1550,-Bprefix,"This option specifies where to find the executables, libraries, include files, and data files of the compiler itself.",0,0,others,gcc
1551,builtin-expect-probability,Control the probability of the expression having the specified value. This parameter takes a percentage (i.e. 0 ... 100) as input.,1,4,limited-side-effect,gcc
1552,builtin-string-cmp-inline-length,The maximum length of a constant string for a builtin string cmp call eligible for inlining.,1,4,limited-side-effect,gcc
1553,-c,"Compile or assemble the source files, but do not link. The linking stage simply is not done. The ultimate output is in the form of an object file for each source file.",0,0,others,gcc
1554,-C(captal),"Do not discard comments. All comments are passed through to the output file, except for comments in processed directives, which are deleted along with the directive.",0,0,others,gcc
1555,case-values-threshold,"The smallest number of different values for which it is best to use a jump-table instead of a tree of conditional branches. If the value is 0, use the default for the machine.",1,4,limited-side-effect,gcc
1556,-CC,"Do not discard comments, including during macro expansion. This is like -C, except that comments contained within macros are also passed through to the output file where the macro is expanded.",0,0,others,gcc
1557,cgraph,"Dumps information about call-graph optimization, unused function removal, and inlining decisions.",1,6,function-tradeoff,gcc
1558,class,Dump class hierarchy information. Virtual table information is emitted unless limis specified. This option is applicable to C++ only.,1,6,function-tradeoff,gcc
1559,comdat-sharing-probability,Probability (in percent) that C++ inline function with comdat visibility are shared across multiple compilation units.,1,4,limited-side-effect,gcc
1560,common,Display the options that are common to all languages.,1,6,function-tradeoff,gcc
1561,--coverage,This option is used to compile and link code instrumented for coverage analysis. The option is a synonym for -fprofile-arcs -ftest-coverage (when compiling) and -lgcov (when linking). See the documentation for those options for more details.,0,0,others,gcc
1562,cxx-max-namespaces-for-diagnostic-help,The maximum number of namespaces to consult for suggestions when C++ name lookup fails for an identifier.,1,4,limited-side-effect,gcc
1563,-D name,"Predefine name as a macro, with definition 1.",0,0,others,gcc
1564,-D name=definition,"The contents of definition are tokenized and processed as if they appeared during translation phase three in a definedirective. In particular, the definition is truncated by embedded newline characters.",0,0,others,gcc
1565,-dA,Annotate the assembler output with miscellaneous debugging information.,1,6,function-tradeoff,gcc
1566,-dD,"Dump all macro definitions, at the end of preprocessing, in addition to normal output.",1,6,function-tradeoff,gcc
1568,-dI,Output includedirectives in addition to the result of preprocessing.,1,6,function-tradeoff,gcc
1569,diff-delete=,SGR substring for deleted lines within generated patches.,0,0,others,gcc
1570,diff-filename=,SGR substring for filename headers within generated patches.,0,0,others,gcc
1571,diff-hunk=,SGR substring for the starts of hunks within generated patches.,0,0,others,gcc
1572,diff-insert=,SGR substring for inserted lines within generated patches.,0,0,others,gcc
1573,-dletters,"Says to make debugging dumps during compilation as specified by letters. The flags documented here are those relevant to the preprocessor. Other letters are interpreted by the compiler proper, or reserved for future versions of GCC, and so are silently ignored. If you specify letters whose behavior conflicts, the result is undefined. See Developer Options, for more information.",1,6,function-tradeoff,gcc
1574,-dM,"Instead of the normal output, generate a list of #define directives for all the macros defined during the execution of the preprocessor, including predefined macros. This gives you a way of finding out what is predefined in your version of the preprocessor. Assuming you have no file foo.h, the command",0,0,others,gcc
1575,-dN,"Like -dD, but emit only the macro names, not their expansions.",1,6,function-tradeoff,gcc
1576,-dp,Annotate the assembler output with a comment indicating which pattern and alternative is used. The length and cost of each instruction are also printed.,1,6,function-tradeoff,gcc
1577,-dP(captal),Dump the RTL in the assembler output as a comment before each instruction. Also turns on -dp annotation.,1,6,function-tradeoff,gcc
1578,dse-max-alias-queries-per-store,Maximum number of queries into the alias oracle per store. Larger values result in larger compilation times and may result in more removed dead stores.,1,4,limited-side-effect,gcc
1579,dse-max-object-size,Maximum size (in bytes) of objects tracked bytewise by dead store elimination. Larger values may result in larger compilation times.,1,5,workload-specific,gcc
1581,-dumpbase dumpbase,"This option sets the base name for auxiliary and dump output files. It does not affect the name of the primary output file. Intermediate outputs, when preserved, are not regarded as primary outputs, but as auxiliary outputs:",0,0,others,gcc
1582,-dumpbase-ext auxdropsuf,"When forming the name of an auxiliary (but not a dump) output file, drop trailing auxdropsuf from dumpbase before appending any suffixes. If not specified, this option defaults to the suffix of a default dumpbase, i.e., the suffix of the input file when -dumpbase is not present in the command line, or dumpbase is combined with dumppfx.",0,0,others,gcc
1583,-dumpdir dumppfx,"When forming the name of an auxiliary or dump output file, use dumppfx as a prefix:",0,0,others,gcc
1584,-dumpfullversion,"Print the full compiler version and don't do anything else. The output is always three numbers separated by dots, major, minor and patchlevel version.",1,6,function-tradeoff,gcc
1585,-dumpmachine,"Print the compiler target machine (for example, i686-pc-linux-gnu and don't do anything else.",1,6,function-tradeoff,gcc
1586,-dumpspecs,Print the compiler's built-in specs nd don't do anything else. (This is used when GCC itself is being built.) See Spec Files.,1,6,function-tradeoff,gcc
1589,-E,"Stop after the preprocessing stage; do not run the compiler proper. The output is in the form of preprocessed source code, which is sent to the standard output.",0,0,others,gcc
1590,early-inlining-insns,Specify growth that the early inliner can make. In effect it increases the amount of inlining for code having a large abstraction penalty.,1,4,limited-side-effect,gcc
1591,eh,Enable showing the EH region number holding each statement.,1,6,function-tradeoff,gcc
1592,--entry=entry,Specify that the program entry point is entry. The argument is interpreted by the linker; the GNU linker accepts either a symbol name or an address.,0,0,others,gcc
1593,error=,SGR substring for error: markers.,0,0,others,gcc
1595,-fabi-compat-version=n,"On targets that support strong aliases, G++ works around mangling changes by creating an alias with the correct mangled name when defining a symbol with an incorrect mangled name. This switch specifies which ABI version to use for the alias.",0,0,others,gcc
1596,-fabi-version=n,Use version n of the C++ ABI. The default is version 0.,0,0,others,gcc
1597,-fada-spec-parent=unit,"In conjunction with -fdump-ada-spec[-slim] above, generate Ada specs as child units of parent unit.",0,0,others,gcc
1598,-faggressive-loop-optimizations,This option tells the loop optimizer to use language constraints to derive bounds for the number of iterations of a loop. This assumes that loop code does not invoke undefined behavior by for example causing signed integer overflows or out-of-bound array accesses. The bounds for the number of iterations of a loop are used to guide loop unrolling and peeling and loop exit test optimizations. This option is enabled by default.,1,4,limited-side-effect,gcc
1599,-faligned-new,"Enable support for C++17 new of types that require more alignment than void* ::operator new(std::size_t) provides. A numeric argument such as -faligned-new=32 can be used to specify how much alignment (in bytes) is provided by that function, but few users will need to override the default of alignof(std::max_align_t).",0,0,others,gcc
1600,-falign-functions=n:m:n2:m2,"Align the start of functions to the next power-of-two greater than or equal to n, skipping up to m-1 bytes. This ensures that at least the first m bytes of the function can be fetched by the CPU without crossing an n-byte alignment boundary.",1,4,limited-side-effect,gcc
1602,-falign-labels=n:m:n2:m2,Align all branch targets to a power-of-two boundary.,1,4,limited-side-effect,gcc
1603,-falign-loops=n:m:n2:m2,"Align loops to a power-of-two boundary. If the loops are executed many times, this makes up for any execution of the dummy padding instructions.",1,4,limited-side-effect,gcc
1604,-fallow-parameterless-variadic-functions,Accept variadic functions without named parameters.,0,0,others,gcc
1606,-fasan-shadow-offset=number,This option forces GCC to use custom shadow offset in AddressSanitizer checks. It is useful for experimenting with different shadow memory layouts in Kernel AddressSanitizer.,0,0,others,gcc
1607,-fassociative-math,"Allow re-association of operands in series of floating-point operations. This violates the ISO C and C++ language standard by possibly changing computation result. NOTE: re-ordering may change the sign of zero as well as ignore NaNs and inhibit or create underflow or overflow (and thus cannot be used on code that relies on rounding behavior like (x + 2**52) - 2**52. May also reorder floating-point comparisons and thus may not be used when ordered comparisons are required. This option requires that both -fno-signed-zeros and -fno-trapping-math be in effect. Moreover, it doesn't make much sense with -frounding-math. For Fortran the option is automatically enabled when both -fno-signed-zeros and -fno-trapping-math are in effect.",1,4,limited-side-effect,gcc
1609,-fauto-inc-dec,Combine increments or decrements of addresses with memory accesses. This pass is always skipped on architectures that do not have instructions to support this. Enabled by default at -O and higher on architectures that support this.,1,4,limited-side-effect,gcc
1610,-fauto-profile=path,"Enable sampling-based feedback-directed optimizations, and the following optimizations, many of which are generally profitable only with profile feedback available:",1,4,limited-side-effect,gcc
1611,-fbranch-probabilities,"After running a program compiled with -fprofile-arcs (see Instrumentation Options), you can compile it a second time using -fbranch-probabilities, to improve optimizations based on the number of times each branch was taken. When a program compiled with -fprofile-arcs exits, it saves arc execution counts to a file called sourcename.gcda for each source file. The information in this data file is very dependent on the structure of the generated code, so you must use the same source code and the same optimization options for both compilations.",1,4,limited-side-effect,gcc
1612,-fcaller-saves,"Enable allocation of values to registers that are clobbered by function calls, by emitting extra instructions to save and restore the registers around such calls. Such allocation is done only when it seems to result in better code.",1,4,limited-side-effect,gcc
1613,-fcallgraph-info=MARKERS,"Makes the compiler output callgraph information for the program, on a per-object-file basis. The information is generated in the common VCG format. It can be decorated with additional, per-node and/or per-edge information, if a list of comma-separated markers is additionally specified. When the su marker is specified, the callgraph is decorated with stack usage information; it is equivalent to -fstack-usage. When the da marker is specified, the callgraph is decorated with information about dynamically allocated objects.",0,0,others,gcc
1615,-fcall-used-reg,Treat the register named reg as an allocable register that is clobbered by function calls. It may be allocated for temporaries or variables that do not live across a call. Functions compiled this way do not save and restore the register reg.,0,0,others,gcc
1616,-fcf-protection=[full|branch|return|none|check],"Enable code instrumentation of control-flow transfers to increase program security by checking that target addresses of control-flow transfer instructions (such as indirect function call, function return, indirect jump) are valid. This prevents diverting the flow of control to an unexpected target. This is intended to protect against such threats as Return-oriented Programming (ROP), and similarly call/jmp-oriented programming (COP/JOP).",0,0,others,gcc
1617,-fchecking=n,Enable internal consistency checking. The default depends on the compiler configuration. -fchecking=2 enables further internal consistency checking that might affect code generation.,1,3,reliability-tradeoff,gcc
1619,-fcode-hoisting,"Perform code hoisting. Code hoisting tries to move the evaluation of expressions executed on all paths to the function exit as early as possible. This is especially useful as a code size optimization, but it often helps for code speed as well. This flag is enabled by default at -O2 and higher.",1,4,limited-side-effect,gcc
1620,-fcombine-stack-adjustments,Tracks stack adjustments (pushes and pops) and stack memory references and then tries to find ways to combine them.,1,4,limited-side-effect,gcc
1621,-fcommon,"In C code, this option controls the placement of global variables defined without an initializer, known as tentative definitions in the C standard. Tentative definitions are distinct from declarations of a variable with the extern keyword, which do not allocate storage.",0,0,others,gcc
1622,-fcompare-debug[=opts],"If no error occurs during compilation, run the compiler a second time, adding opts and -fcompare-debug-second to the arguments passed to the second compilation. Dump the final internal representation in both compilations, and print an error if they differ.",1,6,function-tradeoff,gcc
1623,-fcompare-debug-second,"This option is implicitly passed to the compiler for the second compilation requested by -fcompare-debug, along with options to silence warnings, and omitting other options that would cause the compiler to produce output to files or to standard output as a side effect. Dump files and preserved temporary files are renamed so as to contain the .gk additional extension during the second compilation, to avoid overwriting those generated by the first.",1,6,function-tradeoff,gcc
1624,-fcompare-elim,"After register allocation and post-register allocation instruction splitting, identify arithmetic instructions that compute processor flags similar to a comparison operation based on that arithmetic. If possible, eliminate the explicit comparison operation.",1,4,limited-side-effect,gcc
1626,-fcond-mismatch,Allow conditional expressions with mismatched types in the second and third arguments. The value of such an expression is void. This option is not supported for C++.,0,0,others,gcc
1628,-fconstant-string-class=class-name,"Use class-name as the name of the class to instantiate for each literal string specified with the syntax @"""". The default class name is NXConstantString if the GNU runtime is being used, and NSConstantString if the NeXT runtime is being used (see below). The -fconstant-cfstrings option, if also present, overrides the -fconstant-string-class setting and cause @"""" literals to be laid out as constant CoreFoundation strings.",0,0,others,gcc
1629,-fconstexpr-cache-depth=n,"Set the maximum level of nested evaluation depth for C++11 constexpr functions that will be cached to n. This is a heuristic that trades off compilation speed (when the cache avoids repeated calculations) against memory consumption (when the cache grows very large from highly recursive evaluations). The default is 8. Very few users are likely to want to adjust it, but if your code does heavy constexpr calculations you might want to experiment to find which value works best for you.",1,5,workload-specific,gcc
1630,-fconstexpr-depth=n,Set the maximum nested evaluation depth for C++11 constexpr functions to n. A limit is needed to detect endless recursion during constant expression evaluation. The minimum specified by the standard is 512.,0,0,others,gcc
1631,-fconstexpr-loop-limit=n,Set the maximum number of iterations for a loop in C++14 constexpr functions to n. A limit is needed to detect infinite loops during constant expression evaluation. The default is 262144 (1<<18).,1,5,workload-specific,gcc
1632,-fconstexpr-ops-limit=n,"Set the maximum number of operations during a single constexpr evaluation. Even when number of iterations of a single loop is limited with the above limit, if there are several nested loops and each of them has many iterations but still smaller than the above limit, or if in a body of some loop or even outside of a loop too many expressions need to be evaluated, the resulting constexpr evaluation might take too long. The default is 33554432 (1<<25).",1,5,workload-specific,gcc
1633,-fcoroutines,Enable support for the C++ coroutines extension (experimental).,0,0,others,gcc
1634,-fcprop-registers,"After register allocation and post-register allocation instruction splitting, perform a copy-propagation pass to try to reduce scheduling dependencies and occasionally eliminate the copy.",1,4,limited-side-effect,gcc
1636,-fcse-follow-jumps,"In common subexpression elimination (CSE), scan through jump instructions when the target of the jump is not reached by any other path. For example, when CSE encounters an if statement with an else clause, CSE follows the jump when the condition tested is false.",1,4,limited-side-effect,gcc
1637,-fcse-skip-blocks,"This is similar to -fcse-follow-jumps, but causes CSE to follow jumps that conditionally skip over blocks. When CSE encounters a simple if statement with no else clause, -fcse-skip-blocks causes CSE to follow the jump around the body of the if.",1,4,limited-side-effect,gcc
1638,-fcx-fortran-rules,"Complex multiplication and division follow Fortran rules. Range reduction is done as part of complex division, but there is no checking whether the result of a complex multiplication or division is NaN + I*NaN, with an attempt to rescue the situation in that case.",1,4,limited-side-effect,gcc
1639,-fcx-limited-range,"When enabled, this option states that a range reduction step is not needed when performing complex division. Also, there is no checking whether the result of a complex multiplication or division is NaN + I*NaN, with an attempt to rescue the situation in that case. The default is -fno-cx-limited-range, but is enabled by -ffast-math.",1,4,limited-side-effect,gcc
1641,-fdbg-cnt=counter-value-list,"Set the internal debug counter lower and upper bound. counter-value-list is a comma-separated list of name:lower_bound1-upper_bound1 [:lower_bound2-upper_bound2...] tuples which sets the name of the counter and list of closed intervals. The lower_bound is optional and is zero initialized if not set. For example, with -fdbg-cnt=dce:2-4:10-11,tail_call:10, dbg_cnt(dce) returns true only for second, third, fourth, tenth and eleventh invocation. For dbg_cnt(tail_call) true is returned for first 10 invocations.",0,0,others,gcc
1642,-fdbg-cnt-list,Print the name and the counter upper bound for all debug counters.,1,6,function-tradeoff,gcc
1643,-fdce,Perform dead code elimination (DCE) on RTL. Enabled by default at -O and higher.,1,4,limited-side-effect,gcc
1644,-fdebug-cpp,"This option is only useful for debugging GCC. When used from CPP or with -E, it dumps debugging information about location maps. Every token in the output is preceded by the dump of the map its location belongs to.",0,0,others,gcc
1645,-fdebug-prefix-map=old=new,"When compiling files residing in directory old, record debugging information describing them as if the files resided in directory new instead. This can be used to replace a build-time path with an install-time path in the debug info. It can also be used to change an absolute path to a relative path by using . for new. This can give more reproducible builds, which are location independent, but may require an extra command to tell GDB where to find the source files. See also -ffile-prefix-map.",0,0,others,gcc
1646,-fdebug-types-section,"When using DWARF Version 4 or higher, type DIEs can be put into their own .debug_types section instead of making them part of the .debug_info section. It is more efficient to put them in a separate comdat section since the linker can then remove duplicates. But not all DWARF consumers support .debug_types sections yet and on some objects .debug_types produces larger instead of smaller debugging information.",0,0,others,gcc
1647,-fdeclone-ctor-dtor,"The C++ ABI requires multiple entry points for constructors and destructors: one for a base subobject, one for a complete object, and one for a virtual destructor that calls operator delete afterwards. For a hierarchy with virtual bases, the base and complete variants are clones, which means two copies of the function. With this option, the base and complete variants are changed to be thunks that call a common implementation.",1,4,limited-side-effect,gcc
1648,-fdelayed-branch,"If supported for the target machine, attempt to reorder instructions to exploit instruction slots available after delayed branch instructions.",1,4,limited-side-effect,gcc
1649,-fdelete-dead-exceptions,"Consider that instructions that may throw exceptions but don't otherwise contribute to the execution of the program can be optimized away. This option is enabled by default for the Ada compiler, as permitted by the Ada language specification. Optimization passes that cause dead exceptions to be removed are enabled independently at different optimization levels.",1,4,limited-side-effect,gcc
1651,-fdevirtualize,"Attempt to convert calls to virtual functions to direct calls. This is done both within a procedure and interprocedurally as part of indirect inlining (-findirect-inlining) and interprocedural constant propagation (-fipa-cp). Enabled at levels -O2, -O3, -Os.",1,4,limited-side-effect,gcc
1652,-fdevirtualize-at-ltrans,Stream extra information needed for aggressive devirtualization when running the link-time optimizer in local transformation mode. This option enables more devirtualization but significantly increases the size of streamed data. For this reason it is disabled by default.,1,6,function-tradeoff,gcc
1653,-fdevirtualize-speculatively,"Attempt to convert calls to virtual functions to speculative direct calls. Based on the analysis of the type inheritance graph, determine for a given call the set of likely targets. If the set is small, preferably of size 1, change the call into a conditional deciding between direct and indirect calls. The speculative calls enable more optimizations, such as inlining. When they seem useless after further optimization, they are converted back into original form.",1,4,limited-side-effect,gcc
1654,-fdiagnostics-column-origin=ORIGIN,"Select the origin for column numbers, i.e. the column number assigned to the first column. The default value of 1 corresponds to traditional GCC behavior and to the GNU style guide. Some utilities may perform better with an origin of 0; any non-negative value may be specified.",0,0,others,gcc
1655,-fdiagnostics-column-unit=UNIT,"Select the units for the column number. This affects traditional diagnostics (in the absence of -fno-show-column), as well as JSON format diagnostics if requested.",0,0,others,gcc
1656,-fdiagnostics-format=FORMAT,Select a different format for printing diagnostics. FORMAT is textor json The default is text,0,0,others,gcc
1657,-fdiagnostics-generate-patch,"Print fix-it hints to stderr in unified diff format, after any diagnostics are printed. For example:",1,6,function-tradeoff,gcc
1658,-fdiagnostics-minimum-margin-width=width,This option controls the minimum width of the left margin printed by -fdiagnostics-show-line-numbers. It defaults to 6.,0,0,others,gcc
1659,-fdiagnostics-parseable-fixits,"Emit fix-it hints in a machine-parseable format, suitable for consumption by IDEs. For each fix-it, a line will be printed after the relevant diagnostic, starting with the string ix-it: For example:",0,0,others,gcc
1660,-fdiagnostics-path-format=KIND,Specify how to print paths of control-flow events for diagnostics that have such a path associated with them.,0,0,others,gcc
1661,-fdiagnostics-plain-output,"This option requests that diagnostic output look as plain as possible, which may be useful when running dejagnu or other utilities that need to parse diagnostics output and prefer that it remain more stable over time. -fdiagnostics-plain-output is currently equivalent to the following options:",1,6,function-tradeoff,gcc
1662,-fdiagnostics-show-location=every-line,Only meaningful in line-wrapping mode. Instructs the diagnostic messages reporter to emit the same source location information (as prefix) for physical lines that result from the process of breaking a message which is too long to fit on a single line.,0,0,others,gcc
1663,-fdiagnostics-show-location=once,"Only meaningful in line-wrapping mode. Instructs the diagnostic messages reporter to emit source location information once; that is, in case the message is too long to fit on a single physical line and has to be wrapped, the source location won't be emitted (as prefix) again, over and over, in subsequent continuation lines. This is the default behavior.",0,0,others,gcc
1664,-fdiagnostics-show-path-depths,This option provides additional information when printing control-flow paths associated with a diagnostic.,0,0,others,gcc
1665,-fdiagnostics-show-template-tree,"In the C++ frontend, when printing diagnostics showing mismatching template types, such as:",1,6,function-tradeoff,gcc
1666,-fdiagnostics-urls[=WHEN],"Use escape sequences to embed URLs in diagnostics. For example, when -fdiagnostics-show-option emits text showing the command-line option controlling a diagnostic, embed a URL for documentation of that option.",0,0,others,gcc
1667,-fdirectives-only,"When preprocessing, handle directives, but do not expand macros.",0,0,others,gcc
1671,-fdisable-tree-pass=range-list,Disable tree pass pass. See -fdisable-rtl for the description of option arguments.,0,0,others,gcc
1672,-fdollars-in-identifiers,Accept in identifiers.,0,0,others,gcc
1674,-fdump-ada-spec[-slim],"For C and C++ source and include files, generate corresponding Ada specs. See Generating Ada Bindings for C and C++ headers in GNAT User Guide, which provides detailed documentation on this feature.",0,0,others,gcc
1675,-fdump-debug,Dump debugging information generated during the debug generation phase.,1,6,function-tradeoff,gcc
1676,-fdump-earlydebug,Dump debugging information generated during the early debug generation phase.,1,6,function-tradeoff,gcc
1677,-fdump-final-insns[=file],"Dump the final internal representation (RTL) to file. If the optional argument is omitted (or if file is .), the name of the dump file is determined by appending .gkd to the dump base name, see -dumpbase.",1,6,function-tradeoff,gcc
1678,-fdump-go-spec=file,"For input files in any language, generate corresponding Go declarations in file. This generates Go const, type, var, and func declarations which may be a useful way to start writing a Go interface to code written in some other language.",0,0,others,gcc
1679,-fdump-ipa-switch-options,"Control the dumping at various stages of inter-procedural analysis language tree to a file. The file name is generated by appending a switch specific suffix to the source file name, and the file is created in the same directory as the output file. The following dumps are possible:",0,0,others,gcc
1680,-fdump-lang,Dump language-specific information. The file name is made by appending .lang to the source file name.,1,6,function-tradeoff,gcc
1682,-fdump-noaddr,"When doing debugging dumps, suppress address output. This makes it more feasible to use diff on debugging dumps for compiler invocations with different compiler binaries and/or different text / bss / data / heap / stack / dso start locations.",0,0,others,gcc
1683,-fdump-passes,Print on stderr the list of optimization passes that are turned on and off by the current command-line options.,1,6,function-tradeoff,gcc
1684,-fdump-rtl-alignments,Dump after branch alignments have been computed.,1,6,function-tradeoff,gcc
1685,-fdump-rtl-all,Produce all the dumps listed above.,0,0,others,gcc
1686,-fdump-rtl-asmcons,Dump after fixing rtl statements that have unsatisfied in/out constraints.,1,6,function-tradeoff,gcc
1687,-fdump-rtl-auto_inc_dec,Dump after auto-inc-dec discovery. This pass is only run on architectures that have auto inc or auto dec instructions.,1,6,function-tradeoff,gcc
1689,-fdump-rtl-bbpart,Dump after partitioning hot and cold basic blocks.,1,6,function-tradeoff,gcc
1690,-fdump-rtl-bbro,Dump after block reordering.,1,6,function-tradeoff,gcc
1691,-fdump-rtl-bypass,Dump after jump bypassing and control flow optimizations.,1,6,function-tradeoff,gcc
1692,-fdump-rtl-ce3,"-fdump-rtl-ce1, -fdump-rtl-ce2, and -fdump-rtl-ce3 enable dumping after the three if conversion passes.",1,6,function-tradeoff,gcc
1693,-fdump-rtl-combine,Dump after the RTL instruction combination pass.,1,6,function-tradeoff,gcc
1694,-fdump-rtl-compgotos,Dump after duplicating the computed gotos.,1,6,function-tradeoff,gcc
1697,-fdump-rtl-dbr,Dump after delayed branch scheduling.,1,6,function-tradeoff,gcc
1699,-fdump-rtl-dce2,-fdump-rtl-DCD1 and -fdump-rtl-DCD2 enable dumping after the two dead store elimination passes.',1,6,function-tradeoff,gcc
1700,-fdump-rtl-dfinish,These dumps are defined but always produce empty files.,1,6,function-tradeoff,gcc
1702,-fdump-rtl-eh_ranges,Dump after conversion of EH handling range regions.,1,6,function-tradeoff,gcc
1703,-fdump-rtl-expand,Dump after RTL generation.,1,6,function-tradeoff,gcc
1704,-fdump-rtl-init-regs,Dump after the initialization of the registers.,1,6,function-tradeoff,gcc
1705,-fdump-rtl-initvals,Dump after the computation of the initial value sets.,1,6,function-tradeoff,gcc
1706,-fdump-rtl-into_cfglayout,Dump after converting to cfglayout mode.,1,6,function-tradeoff,gcc
1707,-fdump-rtl-ira,Dump after iterated register allocation.,1,6,function-tradeoff,gcc
1708,-fdump-rtl-jump,Dump after the second jump optimization.,1,6,function-tradeoff,gcc
1709,-fdump-rtl-mach,"Dump after performing the machine dependent reorganization pass, if that pass exists.",1,6,function-tradeoff,gcc
1710,-fdump-rtl-mode_sw,Dump after removing redundant mode switches.,1,6,function-tradeoff,gcc
1712,-fdump-rtl-pass=filename,Says to make debugging dumps during compilation at times specified by letters. This is used for debugging the RTL-based passes of the compiler.,1,6,function-tradeoff,gcc
1713,-fdump-rtl-peephole2,Dump after the peephole pass.,1,6,function-tradeoff,gcc
1714,-fdump-rtl-postreload,Dump after post-reload optimizations.,1,6,function-tradeoff,gcc
1715,-fdump-rtl-pro_and_epilogue,Dump after generating the function prologues and epilogues.,1,6,function-tradeoff,gcc
1718,-fdump-rtl-seqabstr,Dump after common sequence discovery.,1,6,function-tradeoff,gcc
1719,-fdump-rtl-shorten,Dump after shortening branches.,1,6,function-tradeoff,gcc
1721,-fdump-rtl-sms,Dump after modulo scheduling. This pass is only run on some architectures.,1,6,function-tradeoff,gcc
1722,-fdump-rtl-split5,These options enable dumping after five rounds of instruction splitting.,1,6,function-tradeoff,gcc
1724,-fdump-rtl-unshare,Dump after all rtl has been unshared.,1,6,function-tradeoff,gcc
1725,-fdump-rtl-vartrack,Dump after variable tracking.,1,6,function-tradeoff,gcc
1729,-fdump-tree-switch-options=filename,"Control the dumping at various stages of processing the intermediate language tree to a file. If the -options form is used, options is a list of - separated options which control the details of the dump. Not all options are applicable to all dumps; those that are not meaningful are ignored. The following options are available",0,0,others,gcc
1730,-fdump-unnumbered,"When doing debugging dumps, suppress instruction numbers and address output. This makes it more feasible to use diff on debugging dumps for compiler invocations with different options, in particular with and without -g.",0,0,others,gcc
1731,-fdump-unnumbered-links,"When doing debugging dumps (see -d option above), suppress instruction numbers for the links to the previous and next instructions in a sequence.",0,0,others,gcc
1732,-fearly-inlining,Inline functions marked by always_inline and functions whose body seems smaller than the function call overhead early before doing -fprofile-generate instrumentation and real inlining pass. Doing so makes profiling significantly cheaper and usually inlining faster on programs having large chains of nested wrapper functions.,1,4,limited-side-effect,gcc
1733,-femit-class-debug-always,"Instead of emitting debugging information for a C++ class in only one object file, emit it in all object files using the class. This option should be used only with debuggers that are unable to handle the way GCC normally emits debugging information for classes because using this option increases the size of debugging information by as much as a factor of two.",0,0,others,gcc
1734,-femit-struct-debug-baseonly,Emit debug information for struct-like types only when the base name of the compilation source file matches the base name of file in which the struct is defined.,0,0,others,gcc
1735,-femit-struct-debug-detailed[=spec-list],Specify the struct-like types for which the compiler generates debug information. The intent is to reduce duplicate struct debug information between different object files within the same program.,0,0,others,gcc
1736,-femit-struct-debug-reduced,"Emit debug information for struct-like types only when the base name of the compilation source file matches the base name of file in which the type is defined, unless the struct is a template or defined in a system header.",0,0,others,gcc
1737,-fenable-ipa-pass,"Enable IPA pass pass. pass is the pass name. If the same pass is statically invoked in the compiler multiple times, the pass name should be appended with a sequential number starting from 1.",0,0,others,gcc
1738,-fenable-rtl-pass=range-list,Enable RTL pass pass. See -fdisable-rtl for option argument description and examples.,0,0,others,gcc
1739,-fenable-tree-pass=range-list,Enable tree pass pass. See -fdisable-rtl for the description of option arguments.,1,6,function-tradeoff,gcc
1740,-fexceptions,"Enable exception handling. Generates extra code needed to propagate exceptions. For some targets, this implies GCC generates frame unwind information for all functions, which can produce significant data size overhead, although it does not affect execution. If you do not specify this option, GCC enables it by default for languages like C++ that normally require exception handling, and disables it for languages like C that do not normally require it. However, you may need to enable this option when compiling C code that needs to interoperate properly with exception handlers written in C++. You may also wish to disable this option if you are compiling older C++ programs that don't use exception handling.",0,0,others,gcc
1744,-fextended-identifiers,Accept universal character names and extended characters in identifiers. This option is enabled by default for C99 (and later C standard versions) and C++.,0,0,others,gcc
1745,-fext-numeric-literals (C++ and Objective-C++ only),"Accept imaginary, fixed-point, or machine-defined literal number suffixes as GNU extensions. When this option is turned off these suffixes are treated as C++11 user-defined literal numeric suffixes. This is on by default for all pre-C++11 dialects and all GNU dialects: -std=c++98, -std=gnu++98, -std=gnu++11, -std=gnu++14. This option is off by default for ISO C++11 onwards (-std=c++11, ...).",0,0,others,gcc
1746,-ffast-math,"Sets the options -fno-math-errno, -funsafe-math-optimizations, -ffinite-math-only, -fno-rounding-math, -fno-signaling-nans, -fcx-limited-range and -fexcess-precision=fast.",1,4,limited-side-effect,gcc
1747,-ffat-lto-objects,Fat LTO objects are object files that contain both the intermediate language and the object code. This makes them usable for both LTO linking and normal linking. This option is effective only when compiling with -flto and is ignored at link time.,1,4,limited-side-effect,gcc
1748,-ffile-prefix-map=old=new,"When compiling files residing in directory old, record any references to them in the result of the compilation as if the files resided in directory new instead. Specifying this option is equivalent to specifying all the individual -f*-prefix-map options. This can be used to make reproducible builds that are location independent. See also -fmacro-prefix-map and -fdebug-prefix-map.",0,0,others,gcc
1749,-ffinite-loops,"Assume that a loop with an exit will eventually take the exit and not loop indefinitely. This allows the compiler to remove loops that otherwise have no side-effects, not considering eventual endless looping as such.",1,4,limited-side-effect,gcc
1750,-ffinite-math-only,Allow optimizations for floating-point arithmetic that assume that arguments and results are not NaNs or +-Infs.,1,4,limited-side-effect,gcc
1752,-ffloat-store,"Do not store floating-point variables in registers, and inhibit other options that might change whether a floating-point value is taken from a register or memory.",1,4,limited-side-effect,gcc
1753,-fforward-propagate,"Perform a forward propagation pass on RTL. The pass tries to combine two instructions and checks if the result can be simplified. If loop unrolling is active, two passes are performed and the second is scheduled after loop unrolling.",1,4,limited-side-effect,gcc
1754,-ffreestanding,"Assert that compilation targets a freestanding environment. This implies -fno-builtin. A freestanding environment is one in which the standard library may not exist, and program startup may not necessarily be at main. The most obvious example is an OS kernel. This is equivalent to -fno-hosted.",0,0,others,gcc
1755,-fgcse,Perform a global common subexpression elimination pass. This pass also performs global constant and copy propagation.,1,4,limited-side-effect,gcc
1756,-fgcse-after-reload,"When -fgcse-after-reload is enabled, a redundant load elimination pass is performed after reload. The purpose of this pass is to clean up redundant spilling.",1,4,limited-side-effect,gcc
1757,-fgcse-las,"When -fgcse-las is enabled, the global common subexpression elimination pass eliminates redundant loads that come after stores to the same memory location (both partial and full redundancies).",1,4,limited-side-effect,gcc
1758,-fgcse-lm,"When -fgcse-lm is enabled, global common subexpression elimination attempts to move loads that are only killed by stores into themselves. This allows a loop containing a load/store sequence to be changed to a load outside the loop, and a copy/store within the loop.",1,4,limited-side-effect,gcc
1760,-fgimple,Enable parsing of function definitions marked with __GIMPLE. This is an experimental feature that allows unit testing of GIMPLE passes.,0,0,others,gcc
1761,-fgnu89-inline,The option -fgnu89-inline tells GCC to use the traditional GNU semantics for inline functions when in C99 mode. See An Inline Function is As Fast As a Macro. Using this option is roughly equivalent to adding the gnu_inline function attribute to all inline functions (see Function Attributes).,0,0,others,gcc
1763,-fgnu-tm,"When the option -fgnu-tm is specified, the compiler generates code for the Linux variant of Intel current Transactional Memory ABI specification document (Revision 1.1, May 6 2009). This is an experimental feature whose interface may change in future versions of GCC, as the official specification changes. Please note that not all architectures are supported for this feature.",0,0,others,gcc
1764,-fgraphite-identity,"Enable the identity transformation for graphite. For every SCoP we generate the polyhedral representation and transform it back to gimple. Using -fgraphite-identity we can check the costs or benefits of the GIMPLE -> GRAPHITE -> GIMPLE transformation. Some minimal optimizations are also performed by the code generator isl, like index splitting and dead code elimination in loops.",1,4,limited-side-effect,gcc
1765,-fhoist-adjacent-loads,Speculatively hoist loads from both branches of an if-then-else if the loads are from adjacent locations in the same structure and the target architecture has a conditional move instruction. This flag is enabled by default at -O2 and higher.,1,4,limited-side-effect,gcc
1766,-fhosted,"Assert that compilation targets a hosted environment. This implies -fbuiltin. A hosted environment is one in which the entire standard library is available, and in which main has a return type of int. Examples are nearly everything except a kernel. This is equivalent to -fno-freestanding.",0,0,others,gcc
1767,-fif-conversion,"Attempt to transform conditional jumps into branch-less equivalents. This includes use of conditional moves, min, max, set flags and abs instructions, and some tricks doable by standard arithmetics. The use of conditional execution on chips where it is available is controlled by -fif-conversion2.",1,4,limited-side-effect,gcc
1769,-findirect-inlining,Inline also indirect calls that are discovered to be known at compile time thanks to previous inlining. This option has any effect only when inlining itself is turned on by the -finline-functions or -finline-small-functions options.,1,4,limited-side-effect,gcc
1771,-finline-functions,"Consider all functions for inlining, even if they are not declared inline. The compiler heuristically decides which functions are worth integrating in this way.",1,4,limited-side-effect,gcc
1773,-finline-limit=n,"By default, GCC limits the size of functions that can be inlined. This flag allows coarse control of this limit. n is the size of functions that can be inlined in number of pseudo instructions.",1,6,function-tradeoff,gcc
1774,-finline-small-functions,"Integrate functions into their callers when their body is smaller than expected function call code (so overall size of program gets smaller). The compiler heuristically decides which functions are simple enough to be worth integrating in this way. This inlining applies to all functions, even those not declared inline.",1,4,limited-side-effect,gcc
1775,-finput-charset=charset,"Set the input character set, used for translation from the character set of the input file to the source character set used by GCC. If the locale does not specify, or GCC cannot get this information from the locale, the default is UTF-8. This can be overridden by either the locale or this command-line option. Currently the command-line option takes precedence if there's a conflict. charset can be any encoding supported by the system's iconv library routine.",0,0,others,gcc
1777,"-finstrument-functions-exclude-file-list=file,file,","Set the list of functions that are excluded from instrumentation (see the description of -finstrument-functions). If the file that contains a function definition matches with one of file, then that function is not instrumented. The match is done on substrings: if the file parameter is a substring of the file name, it is considered to be a match.",0,0,others,gcc
1778,"-finstrument-functions-exclude-function-list=sym,sym,","This is similar to -finstrument-functions-exclude-file-list, but this option sets the list of function names to be excluded from instrumentation. The function name to be matched is its user-visible name, such as vector<int> blah(const vector<int> &), not the internal mangled name (e.g., _Z4blahRSt6vectorIiSaIiEE). The match is done on substrings: if the sym parameter is a substring of the function name, it is considered to be a match. For C99 and C++ extended identifiers, the function name must be given in UTF-8, not using universal character names.",0,0,others,gcc
1779,-fipa-bit-cp,"When enabled, perform interprocedural bitwise constant propagation. This flag is enabled by default at -O2 and by -fprofile-use and -fauto-profile. It requires that -fipa-cp is enabled.",1,4,limited-side-effect,gcc
1783,-fipa-modref,Perform interprocedural mod/ref analysis. This optimization analyzes the side effects of functions (memory locations that are modified or referenced) and enables better optimization across the function call boundary. This flag is enabled by default at -O and higher.,1,4,limited-side-effect,gcc
1785,-fipa-pta,Perform interprocedural pointer analysis and interprocedural modification and reference analysis. This option can cause excessive memory and compile-time usage on large compilation units. It is not enabled by default at any optimization level.,1,4,limited-side-effect,gcc
1786,-fipa-pure-const,Discover which functions are pure or constant. Enabled by default at -O and higher.,1,4,limited-side-effect,gcc
1787,-fipa-ra,Use caller save registers for allocation if those registers are not used by any called function. In that case it is not necessary to save and restore them around calls. This is only possible if called functions are part of same compilation unit as current function and they are compiled before it.,1,4,limited-side-effect,gcc
1788,-fipa-reference,Discover which static variables do not escape the compilation unit. Enabled by default at -O and higher.,1,4,limited-side-effect,gcc
1789,-fipa-reference-addressable,"Discover read-only, write-only and non-addressable static variables. Enabled by default at -O and higher.",1,4,limited-side-effect,gcc
1790,-fipa-sra,"Perform interprocedural scalar replacement of aggregates, removal of unused parameters and replacement of parameters passed by reference by parameters passed by value.",1,4,limited-side-effect,gcc
1791,-fipa-stack-alignment,Reduce stack alignment on call sites if possible. Enabled by default.,1,4,limited-side-effect,gcc
1792,-fipa-vrp,"When enabled, perform interprocedural propagation of value ranges. This flag is enabled by default at -O2. It requires that -fipa-cp is enabled.",1,4,limited-side-effect,gcc
1793,-fira-algorithm=algorithm,"Use the specified coloring algorithm for the integrated register allocator. The algorithm argument can be priority which specifies Chow priority coloring, or CB which specifies Chaitin-Briggs coloring. Chaitin-Briggs coloring is not implemented for all architectures, but for those targets that do support it, it is the default because it generates better code.",1,4,limited-side-effect,gcc
1794,-fira-hoist-pressure,"Use IRA to evaluate register pressure in the code hoisting pass for decisions to hoist expressions. This option usually results in smaller code, but it can slow the compiler down.",1,4,limited-side-effect,gcc
1795,-fira-loop-pressure,"Use IRA to evaluate register pressure in loops for decisions to move loop invariants. This option usually results in generation of faster and smaller code on machines with large register files (>= 32 registers), but it can slow the compiler down.",1,4,limited-side-effect,gcc
1796,-fira-region=region,Use specified regions for the integrated register allocator. The region argument should be one of the following:,1,4,limited-side-effect,gcc
1797,-fira-verbose=n,"Control the verbosity of the dump file for the integrated register allocator. The default value is 5. If the value n is greater or equal to 10, the dump output is sent to stderr using the same format as n minus 10.",0,0,others,gcc
1798,-fisolate-erroneous-paths-attribute,"Detect paths that trigger erroneous or undefined behavior due to a null value being used in a way forbidden by a returns_nonnull or nonnull attribute. Isolate those paths from the main control flow and turn the statement with erroneous or undefined behavior into a trap. This is not currently enabled, but may be enabled by -O2 in the future.",1,4,limited-side-effect,gcc
1799,-fisolate-erroneous-paths-dereference,Detect paths that trigger erroneous or undefined behavior due to dereferencing a null pointer. Isolate those paths from the main control flow and turn the statement with erroneous or undefined behavior into a trap. This flag is enabled by default at -O2 and higher and depends on -fdelete-null-pointer-checks also being enabled.,1,4,limited-side-effect,gcc
1800,-fivar-visibility=[public|protected|private|package],Set the default instance variable visibility to the specified option so that instance variables declared outside the scope of any access modifier directives default to the specified visibility.,0,0,others,gcc
1801,-fivopts,"Perform induction variable optimizations (strength reduction, induction variable merging and induction variable elimination) on trees.",1,4,limited-side-effect,gcc
1802,fixit-delete,SGR substring for fix-it hints suggesting text to be deleted.,0,0,others,gcc
1803,fixit-insert,SGR substring for fix-it hints suggesting text to be inserted or replaced.,0,0,others,gcc
1804,-fkeep-inline-functions,"In C, emit static functions that are declared inline into the object file, even if the function has been inlined into all of its callers. This switch does not affect functions using the extern inline extension in GNU C90. In C++, emit any and all inline functions into the object file.",1,4,limited-side-effect,gcc
1805,-fkeep-static-consts,"Emit variables declared static const when optimization isn't turned on, even if the variables aren't referenced.",1,6,function-tradeoff,gcc
1806,-fkeep-static-functions,"Emit static functions into the object file, even if the function is never used.",1,4,limited-side-effect,gcc
1807,-flang-info-include-translate=header,"Inform of include translation events. The first will note accepted include translations, the second will note declined include translations. The header form will inform of include translations relating to that specific header. If header is of the form ""user"" or <system> it will be resolved to a specific user or system header using the include path.",0,0,others,gcc
1808,-flang-info-module-cmi=module,"Inform of Compiled Module Interface pathnames. The first will note all read CMI pathnames. The module form will not reading a specific module's CMI. module may be a named module or a header-unit (the latter indicated by either being a pathname containing directory separators or enclosed in <> or """").",0,0,others,gcc
1809,-flarge-source-files,"Adjust GCC to expect large source files, at the expense of slower compilation and higher memory usage.",1,6,function-tradeoff,gcc
1811,-fleading-underscore,"This option and its counterpart, -fno-leading-underscore, forcibly change the way C symbols are represented in the object file. One use is to help link with legacy assembly code.",0,0,others,gcc
1812,-flimit-function-alignment,"If this option is enabled, the compiler tries to avoid unnecessarily overaligning functions. It attempts to instruct the assembler to align by the amount specified by -falign-functions, but not to skip more bytes than the size of the function.",1,4,limited-side-effect,gcc
1813,-flinker-output=type,"This option controls code generation of the link-time optimizer. By default the linker output is automatically determined by the linker plugin. For debugging the compiler and if incremental linking with a non-LTO object file is desired, it may be useful to control the type manually.",0,0,others,gcc
1814,-flive-patching=level,Control GCC's optimizations to produce output suitable for live-patching.,1,4,limited-side-effect,gcc
1816,-floop-block,"Perform loop nest optimizations. Same as -floop-nest-optimize. To use this code transformation, GCC has to be configured with --with-isl to enable the Graphite loop transformation infrastructure.",1,4,limited-side-effect,gcc
1817,-floop-interchange,"Perform loop interchange outside of graphite. This flag can improve cache performance on loop nest and allow further loop optimizations, like vectorization, to take place. For example, the loop",1,4,limited-side-effect,gcc
1819,-floop-parallelize-all,Use the Graphite data dependence analysis to identify loops that can be parallelized. Parallelize all the loops that can be analyzed to not contain loop carried dependences without checking that it is profitable to parallelize the loops.,1,4,limited-side-effect,gcc
1820,-floop-unroll-and-jam,Apply unroll and jam transformations on feasible loops. In a loop nest this unrolls the outer loop by some factor and fuses the resulting multiple inner loops. This flag is enabled by default at -O3. It is also enabled by -fprofile-use and -fauto-profile.,1,4,limited-side-effect,gcc
1821,-flra-remat,"Enable CFG-sensitive rematerialization in LRA. Instead of loading values of spilled pseudos, LRA tries to rematerialize (recalculate) values if it is profitable.",1,4,limited-side-effect,gcc
1822,-flto[=n],"This option runs the standard link-time optimizer. When invoked with source code, it generates GIMPLE (one of GCC's internal representations) and writes it to special ELF sections in the object file. When the object files are linked together, all the function bodies are read from these ELF sections and instantiated as if they had been part of the same translation unit.",1,4,limited-side-effect,gcc
1824,-flto-partition=alg,"Specify the partitioning algorithm used by the link-time optimizer. The value is either to1to specify a partitioning mirroring the original source files or balancedto specify partitioning into equally sized chunks (whenever possible) or maxto create new partition for every symbol where possible. Specifying noneas an algorithm disables partitioning and streaming completely. The default value is balanced While to1can be used as an workaround for various code ordering issues, the maxpartitioning is intended for internal testing only. The value onespecifies that exactly one partition should be used while the value nonebypasses partitioning and executes the link-time optimization step directly from the WPA phase.",1,4,limited-side-effect,gcc
1825,-flto-report,Prints a report with internal details on the workings of the link-time optimizer. The contents of this report vary from version to version. It is meant to be useful to GCC developers when processing object files in LTO mode (via -flto).,1,6,function-tradeoff,gcc
1826,-flto-report-wpa,"Like -flto-report, but only print for the WPA phase of link-time optimization.",1,6,function-tradeoff,gcc
1827,-fmacro-prefix-map=old=new,"When preprocessing files residing in directory old, expand the __FILE__ and __BASE_FILE__ macros as if the files resided in directory new instead. This can be used to change an absolute path to a relative path by using . for new which can result in more reproducible builds that are location independent. This option also affects __builtin_FILE() during compilation. See also -ffile-prefix-map.",0,0,others,gcc
1828,-fmax-errors=n,"Limits the maximum number of error messages to n, at which point GCC bails out rather than attempting to continue processing the source code. If n is 0 (the default), there is no limit on the number of error messages produced. If -Wfatal-errors is also specified, then -Wfatal-errors takes precedence over this option.",1,6,function-tradeoff,gcc
1829,-fmax-include-depth=depth,Set the maximum depth of the nested #include. The default is 200.,0,0,others,gcc
1831,-fmem-report-wpa,Makes the compiler print some statistics about permanent memory allocation for the WPA phase only.,1,6,function-tradeoff,gcc
1832,-fmerge-all-constants,Attempt to merge identical constants and identical variables.,1,4,limited-side-effect,gcc
1833,-fmerge-constants,Attempt to merge identical constants (string constants and floating-point constants) across compilation units.,1,4,limited-side-effect,gcc
1834,-fmessage-length=n,"Try to format error messages so that they fit on lines of about n characters. If n is zero, then no line-wrapping is done; each error message appears on a single line. This is the default for all front ends.",0,0,others,gcc
1835,-fmodule-header=system,Compile a header file to create an importable header unit.,0,0,others,gcc
1836,-fmodule-implicit-inline,"Member functions defined in their class definitions are not implicitly inline for modular code. This is different to traditional C++ behavior, for good reasons. However, it may result in a difficulty during code porting. This option makes such function definitions implicitly inline. It does however generate an ABI incompatibility, so you must use it everywhere or nowhere. (Such definitions outside of a named module remain implicitly inline, regardless.)",0,0,others,gcc
1837,-fmodule-mapper=file[?ident],"An oracle to query for module name to filename mappings. If unspecified the CXX_MODULE_MAPPER environment variable is used, and if that is unset, an in-process default is provided.",0,0,others,gcc
1838,-fmodule-only,"Only emit the Compiled Module Interface, inhibiting any object file.",0,0,others,gcc
1840,-fmodulo-sched-allow-regmoves,"Perform more aggressive SMS-based modulo scheduling with register moves allowed. By setting this flag certain anti-dependences edges are deleted, which triggers the generation of reg-moves based on the life-range analysis. This option is effective only with -fmodulo-sched enabled.",1,4,limited-side-effect,gcc
1841,-fmove-loop-invariants,"Enables the loop invariant motion pass in the RTL loop optimizer. Enabled at level -O1 and higher, except for -Og.",1,4,limited-side-effect,gcc
1842,-fms-extensions,"Disable Wpedantic warnings about constructs used in MFC, such as implicit int and getting a pointer to member function via non-standard syntax.",0,0,others,gcc
1844,-fnew-ttp-matching,"Enable the P0522 resolution to Core issue 150, template template parameters and default arguments: this allows a template with default template arguments as an argument for a template template parameter with fewer template parameters. This flag is enabled by default for -std=c++17.",0,0,others,gcc
1845,-fnext-runtime,"Generate output compatible with the NeXT runtime. This is the default for NeXT-based systems, including Darwin and Mac OS X. The macro __NEXT_RUNTIME__ is predefined if (and only if) this option is used.",0,0,others,gcc
1846,-fno-access-control,Turn off all access checking. This switch is mainly useful for working around bugs in the access control code.,1,2,security-tradeoff,gcc
1847,-fno-allocation-dce,Do not remove unused C++ allocations in dead code elimination.,1,4,limited-side-effect,gcc
1848,-fno-asm,"Do not recognize asm, inline or typeof as a keyword, so that code can use these words as identifiers. You can use the keywords __asm__, __inline__ and __typeof__ instead. -ansi implies -fno-asm.",0,0,others,gcc
1849,-fno-bit-tests,Do not use bit tests for switch statements even where it would be more efficient than other code generation strategies.,0,0,others,gcc
1850,-fno-branch-count-reg,"Disable the optimization pass that scans for opportunities to use iecrement and branchinstructions on a count register instead of instruction sequences that decrement a register, compare it against zero, and then branch based upon the result. This option is only meaningful on architectures that support such instructions, which include x86, PowerPC, IA-64 and S/390. Note that the -fno-branch-count-reg option doesn't remove the decrement and branch instructions from the generated instruction stream introduced by other optimization passes.",1,4,limited-side-effect,gcc
1851,-fno-builtin-function,"Don't recognize built-in functions that do not begin with __builtin_as prefix. See Other built-in functions provided by GCC, for details of the functions affected, including those which are not built-in functions when -ansi or -std options for strict ISO C conformance are used because they do not have an ISO standard meaning.",0,0,others,gcc
1853,-fno-char8_t,"Enable support for char8_t as adopted for C++20. This includes the addition of a new char8_t fundamental type, changes to the types of UTF-8 string and character literals, new signatures for user-defined literals, associated standard library updates, and new __cpp_char8_t and __cpp_lib_char8_t feature test macros.",0,0,others,gcc
1854,-fno-defer-pop,"For machines that must pop arguments after a function call, always pop the arguments as soon as each function returns. At levels -O1 and higher, -fdefer-pop is the default; this allows the compiler to let arguments accumulate on the stack for several function calls and pop them all at once.",1,4,limited-side-effect,gcc
1856,-fno-diagnostics-show-caret,"By default, each diagnostic emitted includes the original source line and a caret ^indicating the column. This option suppresses this information. The source line is truncated to n characters, if the -fmessage-length=n option is given. When the output is done to the terminal, the width is limited to the width given by the COLUMNS environment variable or, if not set, to the terminal width.",0,0,others,gcc
1857,-fno-diagnostics-show-cwe,"Diagnostic messages can optionally have an associated CWE identifier. GCC itself only provides such metadata for some of the -fanalyzer diagnostics. GCC plugins may also provide diagnostics with such metadata. By default, if this information is present, it will be printed with the diagnostic. This option suppresses the printing of this metadata.",0,0,others,gcc
1858,-fno-diagnostics-show-labels,"By default, when printing source code (via -fdiagnostics-show-caret), diagnostics can label ranges of source code with pertinent information, such as the types of expressions:",0,0,others,gcc
1860,-fno-diagnostics-show-option,"By default, each diagnostic emitted includes text indicating the command-line option that directly controls the diagnostic (if such an option is known to the diagnostic machinery). Specifying the -fno-diagnostics-show-option flag suppresses that behavior.",0,0,others,gcc
1861,-fno-dwarf2-cfi-asm,Emit DWARF unwind info as compiler generated .eh_frame section instead of using GAS .cfi_* directives.,0,0,others,gcc
1863,-fno-elide-type,"By default when the C++ frontend prints diagnostics showing mismatching template types, common parts of the types are printed as to simplify the error message. For example:",0,0,others,gcc
1864,-fno-eliminate-unused-debug-symbols,"By default, no debug information is produced for symbols that are not actually used. Use this option if you want debug information for all symbols.",0,0,others,gcc
1865,-fno-eliminate-unused-debug-types,"Normally, when producing DWARF output, GCC avoids producing debug symbol output for types that are nowhere used in the source file being compiled. Sometimes it is useful to have GCC emit debugging information for all types declared in a compilation unit, regardless of whether or not they are actually used in that compilation unit, for example if, in the debugger, you want to cast a value to a type that is not actually used in your program (but is declared). More often, however, this results in a significant amount of wasted space.",0,0,others,gcc
1867,-fno-extern-tls-init,"The C++11 and OpenMP standards allow thread_local and threadprivate variables to have dynamic (runtime) initialization. To support this, any use of such a variable goes through a wrapper function that performs any necessary initialization. When the use and definition of the variable are in the same translation unit, this overhead can be optimized away, but when the use is in a different translation unit there is significant overhead even if the variable doesn't actually need dynamic initialization. If the programmer can be sure that no use of the variable in a non-defining TU needs to trigger dynamic initialization (either because the variable is statically initialized, or a use of the variable in the defining TU will be executed before any uses in another TU), they can avoid this overhead with the -fno-extern-tls-init option.",0,0,others,gcc
1868,-fno-fp-int-builtin-inexact,"Do not allow the built-in functions ceil, floor, round and trunc, and their float and long double variants, to generate code that raises the nexactfloating-point exception for noninteger arguments. ISO C99 and C11 allow these functions to raise the nexactexception, but ISO/IEC TS 18661-1:2014, the C bindings to IEEE 754-2008, as integrated into ISO C2X, does not allow these functions to do so.",1,4,limited-side-effect,gcc
1869,-fno-function-cse,Do not put function addresses in registers; make each instruction that calls a constant function contain the function address explicitly.,1,4,limited-side-effect,gcc
1870,-fno-gnu-keywords,"Do not recognize typeof as a keyword, so that code can use this word as an identifier. You can use the keyword __typeof__ instead. This option is implied by the strict ISO C++ dialects: -ansi, -std=c++98, -std=c++11, etc.",0,0,others,gcc
1871,-fno-gnu-unique,"On systems with recent GNU assembler and C library, the C++ compiler uses the STB_GNU_UNIQUE binding to make sure that definitions of template static data members and static local variables in inline functions are unique even in the presence of RTLD_LOCAL; this is necessary to avoid problems with a library used by two different RTLD_LOCAL plugins depending on a definition in one of them and therefore disagreeing with the other one about the binding of the symbol. But this causes dlclose to be ignored for affected DSOs; if your program relies on reinitialization of a DSO via dlclose and dlopen, you can use -fno-gnu-unique.",0,0,others,gcc
1872,-fno-guess-branch-probability,Do not guess branch probabilities using heuristics.,1,4,limited-side-effect,gcc
1873,-fno-ident,Ignore the #ident directive.,0,0,others,gcc
1874,-fno-implement-inlines,"To save space, do not emit out-of-line copies of inline functions controlled by #pragma implementation. This causes linker errors if these functions are not inlined everywhere they are called.",0,0,others,gcc
1875,-fno-implicit-inline-templates,"Don't emit code for implicit instantiations of inline templates, either. The default is to handle inlines differently so that compiles with and without optimization need the same set of explicit instantiations.",0,0,others,gcc
1876,-fno-implicit-templates,"Never emit code for non-inline templates that are instantiated implicitly (i.e. by use); only emit code for explicit instantiations. If you use this option, you must take care to structure your code to include all the necessary explicit instantiations to avoid getting undefined symbols at link time. See Template Instantiation, for more information.",0,0,others,gcc
1877,-fno-inline,Do not expand any functions inline apart from those marked with the always_inline attribute. This is the default when not optimizing.,1,4,limited-side-effect,gcc
1879,-fno-ira-share-spill-slots,"Disable sharing of stack slots allocated for pseudo-registers. Each pseudo-register that does not get a hard register gets a separate stack slot, and as a result function stack frames are larger.",1,4,limited-side-effect,gcc
1880,-fno-jump-tables,"Do not use jump tables for switch statements even where it would be more efficient than other code generation strategies. This option is of use in conjunction with -fpic or -fPIC for building code that forms part of a dynamic linker and cannot reference the address of a jump table. On some targets, jump tables do not require a GOT and this option is not needed.",0,0,others,gcc
1882,-fno-lifetime-dse,"In C++ the value of an object is only affected by changes within its lifetime: when the constructor begins, the object has an indeterminate value, and any changes during the lifetime of the object are dead when the object is destroyed. Normally dead store elimination will take advantage of this; if your code relies on the value of the object storage persisting beyond the lifetime of the object, you can use this flag to disable this optimization. To preserve stores before the constructor starts (e.g. because your operator new clears the object storage) but still treat the object as dead after the destructor, you can use -flifetime-dse=1. The default behavior can be explicitly selected with -flifetime-dse=2. -flifetime-dse=0 is equivalent to -fno-lifetime-dse.",1,4,limited-side-effect,gcc
1883,-fno-local-ivars,By default instance variables in Objective-C can be accessed as if they were local variables from within the methods of the class they妾檈 declared in. This can lead to shadowing between instance variables and other variables declared either locally inside a class method or globally with the same name. Specifying the -fno-local-ivars flag disables this behavior thus avoiding variable shadowing issues.,0,0,others,gcc
1884,-fno-math-errno,"Do not set errno after calling math functions that are executed with a single instruction, e.g., sqrt. A program that relies on IEEE exceptions for math error handling may want to use this flag for speed while maintaining IEEE arithmetic compatibility.",1,4,limited-side-effect,gcc
1885,-fno-merge-debug-strings,Direct the linker to not merge together strings in the debugging information that are identical in different object files. Merging is not supported by all assemblers or linkers. Merging decreases the size of the debug information in the output file at the cost of increasing link processing time. Merging is enabled by default.,0,0,others,gcc
1886,-fno-module-lazy,Disable lazy module importing and module mapper creation.,0,0,others,gcc
1887,-fno-modules-ts,"Enable support for C++20 modules (See C++ Modules). The -fno-modules-ts is usually not needed, as that is the default. Even though this is a C++20 feature, it is not currently implicitly enabled by selecting that standard version.",0,0,others,gcc
1888,-fnon-call-exceptions,"Generate code that allows trapping instructions to throw exceptions. Note that this requires platform-specific runtime support that does not exist everywhere. Moreover, it only allows trapping instructions to throw exceptions, i.e. memory references or floating-point instructions. It does not allow exceptions to be thrown from arbitrary signal handlers such as SIGALRM.",0,0,others,gcc
1889,-fno-nil-receivers,Assume that all Objective-C message dispatches ([receiver message:arg]) in this translation unit ensure that the receiver is not nil. This allows for more efficient entry points in the runtime to be used. This option is only available in conjunction with the NeXT runtime and ABI version 0 or 1.,0,0,others,gcc
1890,-fno-nonansi-builtins,"Disable built-in declarations of functions that are not mandated by ANSI/ISO C. These include ffs, alloca, _exit, index, bzero, conjf, and other related functions.",0,0,others,gcc
1891,-fno-operator-names,"Do not treat the operator name keywords and, bitand, bitor, compl, not, or and xor as synonyms as keywords.",0,0,others,gcc
1892,-fno-optional-diags,"Disable diagnostics that the standard says a compiler does not need to issue. Currently, the only such diagnostic issued by G++ is the one for a name having multiple meanings within a class.",0,0,others,gcc
1893,-fno-peephole2,"Disable any machine-specific peephole optimizations. The difference between -fno-peephole and -fno-peephole2 is in how they are implemented in the compiler; some targets use one, some use the other, a few use both.",1,4,limited-side-effect,gcc
1894,-fno-plt,"Do not use the PLT for external function calls in position-independent code. Instead, load the callee address at call sites from the GOT and branch to it. This leads to more efficient code by eliminating PLT stubs and exposing GOT loads to optimizations. On architectures such as 32-bit x86 where PLT stubs expect the GOT pointer in a specific register, this gives more register allocation freedom to the compiler. Lazy binding requires use of the PLT; with -fno-plt all external symbols are resolved at load time.",0,0,others,gcc
1895,-fno-pretty-templates,"When an error message refers to a specialization of a function template, the compiler normally prints the signature of the template followed by the template arguments and any typedefs or typenames in the signature (e.g. void f(T) [with T = int] rather than void f(int)) so that it's clear which template is involved. When an error message refers to a specialization of a class template, the compiler omits any template arguments that match the default template arguments for that template. If either of these behaviors make it harder to understand the error message rather than easier, you can use -fno-pretty-templates to disable them.",0,0,others,gcc
1896,-fno-printf-return-value,"Do not substitute constants for known return value of formatted output functions such as sprintf, snprintf, vsprintf, and vsnprintf (but not printf of fprintf). This transformation allows GCC to optimize or even eliminate branches based on the known return value of these functions called with arguments that are either constant, or whose values are known to be in a range that makes determining the exact return value possible. For example, when -fprintf-return-value is in effect, both the branch and the body of the if statement (but not the call to snprint) can be optimized away when i is a 32-bit or smaller integer because the return value is guaranteed to be at most 8.",1,4,limited-side-effect,gcc
1897,-fno-rtti,"Disable generation of information about every class with virtual functions for use by the C++ run-time type identification features (dynamic_cast and typeid). If you don't use those parts of the language, you can save some space by using this flag. Note that exception handling uses the same information, but G++ generates it as needed. The dynamic_cast operator can still be used for casts that do not require run-time type information, i.e. casts to void * or to unambiguous base classes.",0,0,others,gcc
1898,-fno-sanitize=all,"This option disables all previously enabled sanitizers. -fsanitize=all is not allowed, as some sanitizers cannot be used together.",1,2,security-tradeoff,gcc
1900,-fno-sched-spec,"Disable speculative motion of non-load instructions, which is normally enabled when scheduling before register allocation, i.e. with -fschedule-insns or at -O2 or higher.",1,4,limited-side-effect,gcc
1901,-fno-show-column,"Do not print column numbers in diagnostics. This may be necessary if diagnostics are being scanned by a program that does not understand the column numbers, such as dejagnu.",0,0,others,gcc
1902,-fno-signed-zeros,"Allow optimizations for floating-point arithmetic that ignore the signedness of zero. IEEE arithmetic specifies the behavior of distinct +0.0 and -0.0 values, which then prohibits simplification of expressions such as x+0.0 or 0.0*x (even with -ffinite-math-only). This option implies that the sign of a zero result isn't significant.",1,4,limited-side-effect,gcc
1903,-fno-stack-limit,"Generate code to ensure that the stack does not grow beyond a certain value, either the value of a register or the address of a symbol. If a larger stack is required, a signal is raised at run time. For most targets, the signal is raised before the stack overruns the boundary, so it is possible to catch the signal without taking special precautions.",0,0,others,gcc
1904,-fno-threadsafe-statics,Do not emit the extra code to use the routines specified in the C++ ABI for thread-safe initialization of local statics. You can use this option to reduce code size slightly in code that doesn't need to be thread-safe.,1,4,limited-side-effect,gcc
1905,-fnothrow-opt,"Treat a throw() exception specification as if it were a noexcept specification to reduce or eliminate the text size overhead relative to a function with no exception specification. If the function has local variables of types with non-trivial destructors, the exception specification actually makes the function smaller because the EH cleanups for those variables can be optimized away. The semantic effect is that an exception thrown out of a function with such an exception specification results in a call to terminate rather than unexpected.",0,0,others,gcc
1906,-fno-toplevel-reorder,"Do not reorder top-level functions, variables, and asm statements. Output them in the same order that they appear in the input file. When this option is used, unreferenced static variables are not removed. This option is intended to support existing code that relies on a particular ordering. For new code, it is better to use attributes when possible.",1,4,limited-side-effect,gcc
1907,-fno-trapping-math,"Compile code assuming that floating-point operations cannot generate user-visible traps. These traps include division by zero, overflow, underflow, inexact result and invalid operation. This option requires that -fno-signaling-nans be in effect. Setting this option may allow faster code if one relies on non-stopIEEE arithmetic, for example.",1,4,limited-side-effect,gcc
1908,-fno-unsigned-bitfields,"These options control whether a bit-field is signed or unsigned, when the declaration does not use either signed or unsigned. By default, such a bit-field is signed, because this is consistent: the basic integer types such as int are signed types.",0,0,others,gcc
1909,-fno-use-cxa-get-exception-ptr,"Don't use the __cxa_get_exception_ptr runtime routine. This causes std::uncaught_exception to be incorrect, but is necessary if the runtime routine is not available.",0,0,others,gcc
1910,-fno-weak,"Do not use weak symbol support, even if it is provided by the linker. By default, G++ uses weak symbols if they are available. This option exists only for testing, and should not be used by end-users; it results in inferior code and has no benefits. This option may be removed in a future release of G++.",0,0,others,gcc
1911,-fno-zero-initialized-in-bss,"If the target supports a BSS section, GCC by default puts variables that are initialized to zero into BSS. This can save space in the resulting code.",1,4,limited-side-effect,gcc
1912,-fobjc-abi-version=n,"Use version n of the Objective-C ABI for the selected runtime. This option is currently supported only for the NeXT runtime. In that case, Version 0 is the traditional (32-bit) ABI without support for properties and other Objective-C 2.0 additions. Version 1 is the traditional (32-bit) ABI with support for properties and other Objective-C 2.0 additions. Version 2 is the modern (64-bit) ABI. If nothing is specified, the default is Version 0 on 32-bit target machines, and Version 2 on 64-bit target machines.",0,0,others,gcc
1913,-fobjc-call-cxx-cdtors,"For each Objective-C class, check if any of its instance variables is a C++ object with a non-trivial default constructor. If so, synthesize a special - (id) .cxx_construct instance method which runs non-trivial default constructors on any such instance variables, in order, and then return self. Similarly, check if any instance variable is a C++ object with a non-trivial destructor, and if so, synthesize a special - (void) .cxx_destruct method which runs all such default destructors, in reverse order.",0,0,others,gcc
1914,-fobjc-direct-dispatch,Allow fast jumps to the message dispatcher. On Darwin this is accomplished via the comm page.,0,0,others,gcc
1915,-fobjc-exceptions,"Enable syntactic support for structured exception handling in Objective-C, similar to what is offered by C++. This option is required to use the Objective-C keywords @try, @throw, @catch, @finally and @synchronized. This option is available with both the GNU runtime and the NeXT runtime (but not available in conjunction with the NeXT runtime on Mac OS X 10.2 and earlier).",0,0,others,gcc
1917,-fobjc-nilcheck,"For the NeXT runtime with version 2 of the ABI, check for a nil receiver in method invocations before doing the actual method call. This is the default and can be disabled using -fno-objc-nilcheck. Class methods and super calls are never checked for nil in this way no matter what this flag is set to. Currently this flag does nothing when the GNU runtime, or an older version of the NeXT runtime ABI, is used.",0,0,others,gcc
1918,-fobjc-std=objc1,"Conform to the language syntax of Objective-C 1.0, the language recognized by GCC 4.0. This only affects the Objective-C additions to the C/C++ language; it does not affect conformance to C/C++ standards, which is controlled by the separate C/C++ dialect option flags. When this option is used with the Objective-C or Objective-C++ compiler, any Objective-C syntax that is not recognized by GCC 4.0 is rejected. This is useful if you need to make sure that your Objective-C code can be compiled with older versions of GCC.",0,0,others,gcc
1919,-fomit-frame-pointer,"Omit the frame pointer in functions that don't need one. This avoids the instructions to save, set up and restore the frame pointer; on many targets it also makes an extra register available.",1,4,limited-side-effect,gcc
1920,-fopenacc,"Enable handling of OpenACC directives #pragma acc in C/C++ and !$acc in Fortran. When -fopenacc is specified, the compiler generates accelerated code according to the OpenACC Application Programming Interface v2.6 https://www.openacc.org. This option implies -pthread, and thus is only supported on targets that have support for -pthread.",0,0,others,gcc
1922,-fopenacc-kernels=mode,"Specify mode of OpenACC kernelsconstructs handling. With -fopenacc-kernels=decompose, OpenACC kernelsconstructs are decomposed into parts, a sequence of compute constructs, each then handled individually. This is work in progress. With -fopenacc-kernels=parloops, OpenACC kernelsconstructs are handled by the parloopspass, en bloc. This is the current default.",0,0,others,gcc
1923,-fopenmp,"Enable handling of OpenMP directives #pragma omp in C/C++ and !$omp in Fortran. When -fopenmp is specified, the compiler generates parallel code according to the OpenMP Application Program Interface v4.5 https://www.openmp.org. This option implies -pthread, and thus is only supported on targets that have support for -pthread. -fopenmp implies -fopenmp-simd.",0,0,others,gcc
1924,-fopenmp-simd,Enable handling of OpenMP SIMD directives with #pragma omp in C/C++ and !$omp in Fortran. Other OpenMP directives are ignored.,0,0,others,gcc
1925,-foptimize-sibling-calls,Optimize sibling and tail recursive calls.,1,4,limited-side-effect,gcc
1926,-foptimize-strlen,"Optimize various standard C string functions (e.g. strlen, strchr or strcpy) and their _FORTIFY_SOURCE counterparts into faster alternatives.",1,4,limited-side-effect,gcc
1927,-fopt-info-options=filename,"Controls optimization dumps from various optimization passes. If the optionsform is used, options is a list of separated option keywords to select the dump details and optimizations.",0,0,others,gcc
1928,-fpack-struct[=n],"Without a value specified, pack all structure members together without holes. When a value is specified (which must be a small power of two), pack structure members according to this value, representing the maximum alignment (that is, objects with default alignment requirements larger than this are output potentially unaligned at the next fitting location.",0,0,others,gcc
1929,-fpartial-inlining,Inline parts of functions. This option has any effect only when inlining itself is turned on by the -finline-functions or -finline-small-functions options.,1,4,limited-side-effect,gcc
1930,"-fpatchable-function-entry=N[,M]","Generate N NOPs right at the beginning of each function, with the function entry point before the Mth NOP. If M is omitted, it defaults to 0 so the function entry points to the address just at the first NOP. The NOP instructions reserve extra space which can be used to patch in any desired instrumentation at run time, provided that the code segment is writable. The amount of space is controllable indirectly via the number of NOPs; the NOP instruction used corresponds to the instruction emitted by the internal GCC back-end interface gen_nop. This behavior is target-specific and may also depend on the architecture variant and/or other compilation options.",0,0,others,gcc
1931,-fpcc-struct-return,"Return hortstruct and union values in memory like longer ones, rather than in registers. This convention is less efficient, but it has the advantage of allowing intercallability between GCC-compiled files and files compiled with other compilers, particularly the Portable C Compiler (pcc).",0,0,others,gcc
1932,-fpch-deps,"When using precompiled headers (see Precompiled Headers), this flag causes the dependency-output flags to also list the files from the precompiled header dependencies. If not specified, only the precompiled header are listed and not the files that were used to create it, because those files are not consulted when a precompiled header is used.",0,0,others,gcc
1934,-fpeel-loops,Peels loops for which there is enough information that they do not roll much (from profile feedback or static analysis). It also turns on complete loop peeling (i.e. complete removal of loops with small constant number of iterations).,1,4,limited-side-effect,gcc
1935,-fpermissive,"Downgrade some diagnostics about nonconformant code from errors to warnings. Thus, using -fpermissive allows some nonconforming code to compile.",0,0,others,gcc
1936,-fpermitted-flt-eval-methods=style,"ISO/IEC TS 18661-3 defines new permissible values for FLT_EVAL_METHOD that indicate that operations and constants with a semantic type that is an interchange or extended format should be evaluated to the precision and range of that type. These new values are a superset of those permitted under C99/C11, which does not specify the meaning of other positive values of FLT_EVAL_METHOD. As such, code conforming to C11 may not have been written expecting the possibility of the new values.",0,0,others,gcc
1937,-fpic,"Generate position-independent code (PIC) suitable for use in a shared library, if supported for the target machine. Such code accesses all constant addresses through a global offset table (GOT). The dynamic loader resolves the GOT entries when the program starts (the dynamic loader is not part of GCC; it is part of the operating system). If the GOT size for the linked executable exceeds a machine-specific maximum size, you get an error message from the linker indicating that -fpic does not work; in that case, recompile with -fPIC instead. (These maximums are 8k on the SPARC, 28k on AArch64 and 32k on the m68k and RS/6000. The x86 has no such limit.)",0,0,others,gcc
1938,-fPIC(capital),"If supported for the target machine, emit position-independent code, suitable for dynamic linking and avoiding any limit on the size of the global offset table. This option makes a difference on AArch64, m68k, PowerPC and SPARC.",0,0,others,gcc
1939,-fPIE,"These options are similar to -fpic and -fPIC, but the generated position-independent code can be only linked into executables. Usually these options are used to compile code that will be linked using the -pie GCC option.",0,0,others,gcc
1940,-fplan9-extensions,Accept some non-standard constructs used in Plan 9 code.,0,0,others,gcc
1941,-fplugin=name.so,"Load the plugin code in file name.so, assumed to be a shared object to be dlopen by the compiler. The base name of the shared object file is used to identify the plugin for the purposes of argument parsing (See -fplugin-arg-name-key=value below). Each plugin should define the callback functions specified in the Plugins API.",0,0,others,gcc
1942,-fplugin-arg-name-key=value,Define an argument called key with a value of value for the plugin called name.,0,0,others,gcc
1943,-fpost-ipa-mem-report,Makes the compiler print some statistics about permanent memory allocation before or after interprocedural optimization.,1,6,function-tradeoff,gcc
1944,-fpredictive-commoning,"Perform predictive commoning optimization, i.e., reusing computations (especially memory loads and stores) performed in previous iterations of loops.",1,4,limited-side-effect,gcc
1946,-fpreprocessed,"Indicate to the preprocessor that the input file has already been preprocessed. This suppresses things like macro expansion, trigraph conversion, escaped newline splicing, and processing of most directives. The preprocessor still recognizes and removes comments, so that you can pass a file preprocessed with -C to the compiler without problems. In this mode the integrated preprocessor is little more than a tokenizer for the front ends.",0,0,others,gcc
1947,-fprofile-abs-path,Automatically convert relative source file names to absolute path names in the .gcno files. This allows gcov to find the correct sources in projects where compilations occur with different working directories.,0,0,others,gcc
1948,-fprofile-arcs,"Add code so that program flow arcs are instrumented. During execution the program records how many times each branch and call is executed and how many times it is taken or returns. On targets that support constructors with priority support, profiling properly handles constructors, destructors and C++ constructors (and destructors) of classes which are used as a type of a global variable.",0,0,others,gcc
1949,-fprofile-correction,"Profiles collected using an instrumented binary for multi-threaded programs may be inconsistent due to missed counter updates. When this option is specified, GCC uses heuristics to correct or smooth out such inconsistencies. By default, GCC emits an error message when an inconsistent profile is detected.",1,4,limited-side-effect,gcc
1950,-fprofile-dir=path,"Set the directory to search for the profile data files in to path. This option affects only the profile data generated by -fprofile-generate, -ftest-coverage, -fprofile-arcs and used by -fprofile-use and -fbranch-probabilities and its related options. Both absolute and relative paths can be used. By default, GCC uses the current directory as path, thus the profile data file appears in the same directory as the object file. In order to prevent the file name clashing, if the object file name is not an absolute path, we mangle the absolute path of the sourcename.gcda file and use it as the file name of a .gcda file. See similar option -fprofile-note.",0,0,others,gcc
1951,-fprofile-exclude-files=regex,Instrument only functions from files whose name does not match any of the regular expressions (separated by semi-colons).,0,0,others,gcc
1952,-fprofile-filter-files=regex,Instrument only functions from files whose name matches any of the regular expressions (separated by semi-colons).,0,0,others,gcc
1953,-fprofile-generate=path,Enable options usually used for instrumenting application to produce profile useful for later recompilation with profile feedback based optimization. You must use -fprofile-generate both when compiling and when linking your program.,0,0,others,gcc
1954,-fprofile-info-section=name,"Register the profile information in the specified section instead of using a constructor/destructor. The section name is name if it is specified, otherwise the section name defaults to .gcov_info. A pointer to the profile information generated by -fprofile-arcs or -ftest-coverage is placed in the specified section for each translation unit. This option disables the profile information registration through a constructor and it disables the profile information processing through a destructor. This option is not intended to be used in hosted environments such as GNU/Linux. It targets systems with limited resources which do not support constructors and destructors. The linker could collect the input sections in a continuous memory block and define start and end symbols. The runtime support could dump the profiling information registered in this linker set during program termination to a serial line for example. A GNU linker script example which defines a linker output section follows:",0,0,others,gcc
1955,-fprofile-note=path,"If path is specified, GCC saves .gcno file into path location. If you combine the option with multiple source files, the .gcno file will be overwritten.",0,0,others,gcc
1956,-fprofile-partial-training,"With -fprofile-use all portions of programs not executed during train run are optimized agressively for size rather than speed. In some cases it is not practical to train all possible hot paths in the program. (For example, program may contain functions specific for a given hardware and trianing may not cover all hardware configurations program is run on.) With -fprofile-partial-training profile feedback will be ignored for all functions not executed during the train run leading them to be optimized as if they were compiled without profile feedback. This leads to better performance when train run is not representative but also leads to significantly bigger code.",1,4,limited-side-effect,gcc
1957,-fprofile-prefix-path=path,This option can be used in combination with profile-generate=profile_dir and profile-use=profile_dir to inform GCC where is the base directory of built source tree. By default profile_dir will contain files with mangled absolute paths of all object files in the built project. This is not desirable when directory used to build the instrumented binary differs from the directory used to build the binary optimized with profile feedback because the profile data will not be found during the optimized build. In such setups -fprofile-prefix-path=path with path pointing to the base directory of the build can be used to strip the irrelevant part of the path and keep all file names relative to the main build directory.,0,0,others,gcc
1958,-fprofile-reorder-functions,Function reordering based on profile instrumentation collects first time of execution of a function and orders these functions in ascending order.,1,4,limited-side-effect,gcc
1959,-fprofile-report,Makes the compiler print some statistics about consistency of the (estimated) profile and effect of individual passes.,1,6,function-tradeoff,gcc
1960,-fprofile-reproducible=[multithreaded|parallel-runs|serial],"Control level of reproducibility of profile gathered by -fprofile-generate. This makes it possible to rebuild program with same outcome which is useful, for example, for distribution packages.",0,0,others,gcc
1961,-fprofile-update=method,"Alter the update method for an application instrumented for profile feedback based optimization. The method argument should be one of single atomicor prefer-atomic The first one is useful for single-threaded applications, while the second one prevents profile corruption by emitting thread-safe code.",0,0,others,gcc
1964,-frandom-seed=string,This option provides a seed that GCC uses in place of random numbers in generating certain symbol names that have to be different in every compiled file. It is also used to place unique stamps in coverage data files and the object files that produce them. You can use the -frandom-seed option to produce reproducibly identical object files.,0,0,others,gcc
1965,-freciprocal-math,"Allow the reciprocal of a value to be used instead of dividing by the value if this enables optimizations. For example x / y can be replaced with x * (1/y), which is useful if (1/y) is subject to common subexpression elimination. Note that this loses precision and increases the number of flops operating on the value.",1,4,limited-side-effect,gcc
1966,-frecord-gcc-switches,"This switch causes the command line used to invoke the compiler to be recorded into the object file that is being created. This switch is only implemented on some targets and the exact format of the recording is target and binary file format dependent, but it usually takes the form of a section containing ASCII text. This switch is related to the -fverbose-asm switch, but that switch only records information in the assembler output file as comments, so it never reaches the object file. See also -grecord-gcc-switches for another way of storing compiler options into the object file.",0,0,others,gcc
1967,-free,"Attempt to remove redundant extension instructions. This is especially helpful for the x86-64 architecture, which implicitly zero-extends in 64-bit registers after writing to their lower 32-bit half.",1,4,limited-side-effect,gcc
1968,-freg-struct-return,Return struct and union values in registers when possible. This is more efficient for small structures than -fpcc-struct-return.,0,0,others,gcc
1969,-frename-registers,"Attempt to avoid false dependencies in scheduled code by making use of registers left over after register allocation. This optimization most benefits processors with lots of registers. Depending on the debug information format adopted by the target, however, it can make debugging impossible, since variables no longer stay in a some register",1,4,limited-side-effect,gcc
1970,-freorder-blocks,Reorder basic blocks in the compiled function in order to reduce number of taken branches and improve code locality.,1,4,limited-side-effect,gcc
1973,-freorder-functions,Reorder functions in the object file in order to improve code locality. This is implemented by using special subsections .text.hot for most frequently executed functions and .text.unlikely for unlikely executed functions. Reordering is done by the linker so object file format must support named sections and linker must place them in a reasonable way.,1,4,limited-side-effect,gcc
1974,-freplace-objc-classes,"Emit a special marker instructing ld(1) not to statically link in the resulting object file, and allow dyld(1) to load it in at run time instead. This is used in conjunction with the Fix-and-Continue debugging mode, where the object file in question may be recompiled and dynamically reloaded in the course of program execution, without the need to restart the program itself. Currently, Fix-and-Continue functionality is only available in conjunction with the NeXT runtime on Mac OS X 10.3 and later.",0,0,others,gcc
1976,-frerun-cse-after-loop,Re-run common subexpression elimination after loop optimizations are performed.,1,4,limited-side-effect,gcc
1977,-freschedule-modulo-scheduled-loops,"Modulo scheduling is performed before traditional scheduling. If a loop is modulo scheduled, later scheduling passes may change its schedule. Use this option to control that behavior.",1,4,limited-side-effect,gcc
1978,-frounding-math,"Disable transformations and optimizations that assume default floating-point rounding behavior. This is round-to-zero for all floating point to integer conversions, and round-to-nearest for all other arithmetic truncations. This option should be specified for programs that change the FP rounding mode dynamically, or that may be executed with a non-default rounding mode. This option disables constant folding of floating-point expressions at compile time (which may be affected by rounding mode) and arithmetic transformations that are unsafe in the presence of sign-dependent rounding modes.",1,4,limited-side-effect,gcc
1979,-fsanitize=address,"Enable AddressSanitizer, a fast memory error detector. Memory access instructions are instrumented to detect out-of-bounds and use-after-free bugs. The option enables -fsanitize-address-use-after-scope. See https://github.com/google/sanitizers/wiki/AddressSanitizer for more details. The run-time behavior can be influenced using the ASAN_OPTIONS environment variable. When set to help=1, the available options are shown at startup of the instrumented program. See https://github.com/google/sanitizers/wiki/AddressSanitizerFlags#run-time-flags for a list of supported options. The option cannot be combined with -fsanitize=thread or -fsanitize=hwaddress. Note that the only target -fsanitize=hwaddress is currently supported on is AArch64.",0,0,others,gcc
1980,-fsanitize=alignment,"This option enables checking of alignment of pointers when they are dereferenced, or when a reference is bound to insufficiently aligned target, or when a method or constructor is invoked on insufficiently aligned object.",0,0,others,gcc
1981,-fsanitize=bool,"This option enables instrumentation of loads from bool. If a value other than 0/1 is loaded, a run-time error is issued.",0,0,others,gcc
1982,-fsanitize=bounds,"This option enables instrumentation of array bounds. Various out of bounds accesses are detected. Flexible array members, flexible array member-like arrays, and initializers of variables with static storage are not instrumented.",0,0,others,gcc
1983,-fsanitize=bounds-strict,"This option enables strict instrumentation of array bounds. Most out of bounds accesses are detected, including flexible array members and flexible array member-like arrays. Initializers of variables with static storage are not instrumented.",0,0,others,gcc
1984,-fsanitize=builtin,"This option enables instrumentation of arguments to selected builtin functions. If an invalid value is passed to such arguments, a run-time error is issued. E.g.passing 0 as the argument to __builtin_ctz or __builtin_clz invokes undefined behavior and is diagnosed by this option.",0,0,others,gcc
1985,-fsanitize=enum,"This option enables instrumentation of loads from an enum type. If a value outside the range of values for the enum type is loaded, a run-time error is issued.",0,0,others,gcc
1987,-fsanitize=float-divide-by-zero,"Detect floating-point division by zero. Unlike other similar options, -fsanitize=float-divide-by-zero is not enabled by -fsanitize=undefined, since floating-point division by zero can be a legitimate way of obtaining infinities and NaNs.",0,0,others,gcc
1988,-fsanitize=hwaddress,"Enable Hardware-assisted AddressSanitizer, which uses a hardware ability to ignore the top byte of a pointer to allow the detection of memory errors with a low memory overhead. Memory access instructions are instrumented to detect out-of-bounds and use-after-free bugs. The option enables -fsanitize-address-use-after-scope. See https://clang.llvm.org/docs/HardwareAssistedAddressSanitizerDesign.html for more details. The run-time behavior can be influenced using the HWASAN_OPTIONS environment variable. When set to help=1, the available options are shown at startup of the instrumented program. The option cannot be combined with -fsanitize=thread or -fsanitize=address, and is currently only available on AArch64.",0,0,others,gcc
1989,-fsanitize=integer-divide-by-zero,Detect integer division by zero as well as INT_MIN / -1 division.,0,0,others,gcc
1990,-fsanitize=kernel-address,Enable AddressSanitizer for Linux kernel. See https://github.com/google/kasan for more details.,1,2,security-tradeoff,gcc
1991,-fsanitize=kernel-hwaddress,"Enable Hardware-assisted AddressSanitizer for compilation of the Linux kernel. Similar to -fsanitize=kernel-address but using an alternate instrumentation method, and similar to -fsanitize=hwaddress but with instrumentation differences necessary for compiling the Linux kernel. These differences are to avoid hwasan library initialization calls and to account for the stack pointer having a different value in its top byte.",0,0,others,gcc
1992,-fsanitize=leak,"Enable LeakSanitizer, a memory leak detector. This option only matters for linking of executables and the executable is linked against a library that overrides malloc and other allocator functions. See https://github.com/google/sanitizers/wiki/AddressSanitizerLeakSanitizer for more details. The run-time behavior can be influenced using the LSAN_OPTIONS environment variable. The option cannot be combined with -fsanitize=thread.",0,0,others,gcc
1995,-fsanitize=object-size,This option enables instrumentation of memory references using the __builtin_object_size function. Various out of bounds pointer accesses are detected.,0,0,others,gcc
1996,-fsanitize=pointer-compare,"Instrument comparison operation (<, <=, >, >=) with pointer operands. The option must be combined with either -fsanitize=kernel-address or -fsanitize=address The option cannot be combined with -fsanitize=thread. Note: By default the check is disabled at run time. To enable it, add detect_invalid_pointer_pairs=2 to the environment variable ASAN_OPTIONS. Using detect_invalid_pointer_pairs=1 detects invalid operation only when both pointers are non-null.",0,0,others,gcc
1997,-fsanitize=pointer-overflow,"This option enables instrumentation of pointer arithmetics. If the pointer arithmetics overflows, a run-time error is issued.",0,0,others,gcc
1998,-fsanitize=pointer-subtract,"Instrument subtraction with pointer operands. The option must be combined with either -fsanitize=kernel-address or -fsanitize=address The option cannot be combined with -fsanitize=thread. Note: By default the check is disabled at run time. To enable it, add detect_invalid_pointer_pairs=2 to the environment variable ASAN_OPTIONS. Using detect_invalid_pointer_pairs=1 detects invalid operation only when both pointers are non-null.",0,0,others,gcc
1999,-fsanitize=return,This option enables return statement checking. Programs built with this option turned on will issue an error message when the end of a non-void function is reached without actually returning a value. This option works in C++ only.,1,6,function-tradeoff,gcc
2002,-fsanitize=shift-base,"If the second argument of a shift operation is within range, check that the result of a shift operation is not undefined. Note that what exactly is considered undefined differs slightly between C and C++, as well as between ISO C90 and C99, etc.",1,2,security-tradeoff,gcc
2003,-fsanitize=shift-exponent,This option enables checking that the second argument of a shift operation is not negative and is smaller than the precision of the promoted first argument.,0,0,others,gcc
2005,-fsanitize=thread,"Enable ThreadSanitizer, a fast data race detector. Memory access instructions are instrumented to detect data race bugs. See https://github.com/google/sanitizers/wiki#threadsanitizer for more details. The run-time behavior can be influenced using the TSAN_OPTIONS environment variable; see https://github.com/google/sanitizers/wiki/ThreadSanitizerFlags for a list of supported options. The option cannot be combined with -fsanitize=address, -fsanitize=leak.",0,0,others,gcc
2006,-fsanitize=undefined,"Enable UndefinedBehaviorSanitizer, a fast undefined behavior detector. Various computations are instrumented to detect undefined behavior at runtime. Current suboptions are:",0,0,others,gcc
2007,-fsanitize=unreachable,"With this option, the compiler turns the __builtin_unreachable call into a diagnostics message call instead. When reaching the __builtin_unreachable call, the behavior is undefined.",0,0,others,gcc
2008,-fsanitize=vla-bound,This option instructs the compiler to check that the size of a variable length array is positive.,0,0,others,gcc
2009,-fsanitize=vptr,"This option enables instrumentation of C++ member function calls, member accesses and some conversions between pointers to base and derived classes, to verify the referenced object has the correct dynamic type.",1,2,security-tradeoff,gcc
2010,-fsanitize-address-use-after-scope,Enable sanitization of local variables to detect use-after-scope bugs. The option sets -fstack-reuse to none,0,0,others,gcc
2011,-fsanitize-coverage=trace-cmp,"Enable dataflow guided fuzzing code instrumentation. Inserts a call to __sanitizer_cov_trace_cmp1, __sanitizer_cov_trace_cmp2, __sanitizer_cov_trace_cmp4 or __sanitizer_cov_trace_cmp8 for integral comparison with both operands variable or __sanitizer_cov_trace_const_cmp1, __sanitizer_cov_trace_const_cmp2, __sanitizer_cov_trace_const_cmp4 or __sanitizer_cov_trace_const_cmp8 for integral comparison with one operand constant, __sanitizer_cov_trace_cmpf or __sanitizer_cov_trace_cmpd for float or double comparisons and __sanitizer_cov_trace_switch for switch statements.",0,0,others,gcc
2012,-fsanitize-coverage=trace-pc,Enable coverage-guided fuzzing code instrumentation. Inserts a call to __sanitizer_cov_trace_pc into every basic block.,1,2,security-tradeoff,gcc
2013,"-fsanitize-sections=s1,s2,...",Sanitize global variables in selected user-defined sections. si may contain wildcards.,0,0,others,gcc
2014,-fsanitize-undefined-trap-on-error,"The -fsanitize-undefined-trap-on-error option instructs the compiler to report undefined behavior using __builtin_trap rather than a libubsan library routine. The advantage of this is that the libubsan library is not needed and is not linked in, so this is usable even in freestanding environments.",0,0,others,gcc
2015,-fsave-optimization-record,"Write a SRCFILE.opt-record.json.gz file detailing what optimizations were performed, for those optimizations that support -fopt-info.",0,0,others,gcc
2016,-fsched2-use-superblocks,"When scheduling after register allocation, use superblock scheduling. This allows motion across basic block boundaries, resulting in faster schedules. This option is experimental, as not all machine descriptions used by GCC model the CPU closely enough to avoid unreliable results from the algorithm.",1,4,limited-side-effect,gcc
2018,-fsched-dep-count-heuristic,"Enable the dependent-count heuristic in the scheduler. This heuristic favors the instruction that has more instructions depending on it. This is enabled by default when scheduling is enabled, i.e. with -fschedule-insns or -fschedule-insns2 or at -O2 or higher.",1,4,limited-side-effect,gcc
2019,-fsched-group-heuristic,"Enable the group heuristic in the scheduler. This heuristic favors the instruction that belongs to a schedule group. This is enabled by default when scheduling is enabled, i.e. with -fschedule-insns or -fschedule-insns2 or at -O2 or higher.",1,4,limited-side-effect,gcc
2020,-fsched-last-insn-heuristic,"Enable the last-instruction heuristic in the scheduler. This heuristic favors the instruction that is less dependent on the last instruction scheduled. This is enabled by default when scheduling is enabled, i.e. with -fschedule-insns or -fschedule-insns2 or at -O2 or higher.",1,4,limited-side-effect,gcc
2021,-fsched-pressure,"Enable register pressure sensitive insn scheduling before register allocation. This only makes sense when scheduling before register allocation is enabled, i.e. with -fschedule-insns or at -O2 or higher. Usage of this option can improve the generated code and decrease its size by preventing register pressure increase above the number of available hard registers and subsequent spills in register allocation.",1,4,limited-side-effect,gcc
2022,-fsched-rank-heuristic,"Enable the rank heuristic in the scheduler. This heuristic favors the instruction belonging to a basic block with greater size or frequency. This is enabled by default when scheduling is enabled, i.e. with -fschedule-insns or -fschedule-insns2 or at -O2 or higher.",1,4,limited-side-effect,gcc
2023,-fsched-spec-insn-heuristic,"Enable the speculative instruction heuristic in the scheduler. This heuristic favors speculative instructions with greater dependency weakness. This is enabled by default when scheduling is enabled, i.e. with -fschedule-insns or -fschedule-insns2 or at -O2 or higher.",1,4,limited-side-effect,gcc
2025,-fsched-spec-load-dangerous,"Allow speculative motion of more load instructions. This only makes sense when scheduling before register allocation, i.e. with -fschedule-insns or at -O2 or higher.",1,4,limited-side-effect,gcc
2026,-fsched-stalled-insns=n,"Define how many insns (if any) can be moved prematurely from the queue of stalled insns into the ready list during the second scheduling pass. -fno-sched-stalled-insns means that no insns are moved prematurely, -fsched-stalled-insns=0 means there is no limit on how many queued insns can be moved prematurely. -fsched-stalled-insns without a value is equivalent to -fsched-stalled-insns=1.",1,4,limited-side-effect,gcc
2027,-fsched-stalled-insns-dep=n,"Define how many insn groups (cycles) are examined for a dependency on a stalled insn that is a candidate for premature removal from the queue of stalled insns. This has an effect only during the second scheduling pass, and only if -fsched-stalled-insns is used. -fno-sched-stalled-insns-dep is equivalent to -fsched-stalled-insns-dep=0. -fsched-stalled-insns-dep without a value is equivalent to -fsched-stalled-insns-dep=1.",1,4,limited-side-effect,gcc
2028,-fschedule-fusion,Performs a target dependent pass over the instruction stream to schedule instructions of same type together because target machine can execute them more efficiently if they are adjacent to each other in the instruction flow.,1,4,limited-side-effect,gcc
2030,-fschedule-insns2,"Similar to -fschedule-insns, but requests an additional pass of instruction scheduling after register allocation has been done. This is especially useful on machines with a relatively small number of registers and where memory load instructions take more than one cycle.",1,4,limited-side-effect,gcc
2031,-fsched-verbose=n,"On targets that use instruction scheduling, this option controls the amount of debugging output the scheduler prints to the dump files.",1,6,function-tradeoff,gcc
2032,-fsection-anchors,Try to reduce the number of symbolic address calculations by using shared synchorsymbols to address nearby objects. This transformation can help to reduce the number of GOT entries and GOT accesses on some targets.,1,4,limited-side-effect,gcc
2033,-fselective-scheduling,Schedule instructions using selective scheduling algorithm. Selective scheduling runs instead of the first scheduler pass.,1,4,limited-side-effect,gcc
2034,-fselective-scheduling2,Schedule instructions using selective scheduling algorithm. Selective scheduling runs instead of the second scheduler pass.,1,4,limited-side-effect,gcc
2035,-fsel-sched-pipelining,Enable software pipelining of innermost loops during selective scheduling. This option has no effect unless one of -fselective-scheduling or -fselective-scheduling2 is turned on.,1,4,limited-side-effect,gcc
2036,-fsel-sched-pipelining-outer-loops,"When pipelining loops during selective scheduling, also pipeline outer loops. This option has no effect unless -fsel-sched-pipelining is turned on.",1,4,limited-side-effect,gcc
2037,-fsemantic-interposition,"Some object formats, like ELF, allow interposing of symbols by the dynamic linker. This means that for symbols exported from the DSO, the compiler cannot perform interprocedural propagation, inlining and other optimizations in anticipation that the function or variable in question may change. While this feature is useful, for example, to rewrite memory allocation functions by a debugging implementation, it is expensive in the terms of code quality. With -fno-semantic-interposition the compiler assumes that if interposition happens for functions the overwriting function will have precisely the same semantics (and side effects). Similarly if interposition happens for variables, the constructor of the variable will be the same. The flag has no effect for functions explicitly declared inline (where it is never allowed for interposition to change semantics) and for symbols explicitly declared weak.",1,4,limited-side-effect,gcc
2039,-fshort-wchar,Override the underlying type for wchar_t to be short unsigned int instead of the default for the target. This option is useful for building programs to run under WINE.,0,0,others,gcc
2040,-fshrink-wrap,"Emit function prologues only before parts of the function that need it, rather than at the top of the function. This flag is enabled by default at -O and higher.",1,6,function-tradeoff,gcc
2041,-fshrink-wrap-separate,"Shrink-wrap separate parts of the prologue and epilogue separately, so that those parts are only executed when needed. This option is on by default, but has no effect unless -fshrink-wrap is also turned on and the target supports this.",1,4,limited-side-effect,gcc
2042,-fsignaling-nans,Compile code assuming that IEEE signaling NaNs may generate user-visible traps during floating-point operations. Setting this option disables optimizations that may change the number of exceptions visible with signaling NaNs. This option implies -ftrapping-math.,1,4,limited-side-effect,gcc
2045,-fsingle-precision-constant,Treat floating-point constants as single precision instead of implicitly converting them to double-precision constants.,1,4,limited-side-effect,gcc
2046,-fsized-deallocation,Enable the built-in global declarations,0,0,others,gcc
2047,fsm-maximum-phi-arguments,Maximum number of arguments a PHI may have before the FSM threader will not try to thread through its block.,1,4,limited-side-effect,gcc
2048,fsm-scale-path-blocks,Scale factor to apply to the number of blocks in a threading path when comparing to the number of (scaled) statements.,1,4,limited-side-effect,gcc
2049,fsm-scale-path-stmts,Scale factor to apply to the number of statements in a threading path when comparing to the number of (scaled) blocks.,1,4,limited-side-effect,gcc
2050,-fsplit-ivs-in-unroller,"Enables expression of values of induction variables in later iterations of the unrolled loop using the value in the first iteration. This breaks long dependency chains, thus improving efficiency of the scheduling passes.",1,4,limited-side-effect,gcc
2052,-fsplit-paths,Split paths leading to loop backedges. This can improve dead code elimination and common subexpression elimination. This is enabled by default at -O3 and above.,1,4,limited-side-effect,gcc
2053,-fsplit-stack,"Generate code to automatically split the stack before it overflows. The resulting program has a discontiguous stack which can only overflow if the program is unable to allocate any more memory. This is most useful when running threaded programs, as it is no longer necessary to calculate a good stack size to use for each thread. This is currently only implemented for the x86 targets running GNU/Linux.",1,3,reliability-tradeoff,gcc
2055,-fsplit-wide-types-early,"Fully split wide types early, instead of very late. This option has no effect unless -fsplit-wide-types is turned on.",1,4,limited-side-effect,gcc
2056,-fssa-backprop,"Propagate information about uses of a value up the definition chain in order to simplify the definitions. For example, this pass strips sign operations if the sign of a value never matters. The flag is enabled by default at -O and higher.",1,4,limited-side-effect,gcc
2057,-fssa-phiopt,"Perform pattern matching on SSA PHI nodes to optimize conditional code. This pass is enabled by default at -O1 and higher, except for -Og.",1,4,limited-side-effect,gcc
2058,-fsso-struct=endianness,Set the default scalar storage order of structures and unions to the specified endianness. The accepted values are big-endian little-endianand nativefor the native endianness of the target (the default). This option is not supported for C++.,0,0,others,gcc
2059,-fstack-check,"Generate code to verify that you do not go beyond the boundary of the stack. You should specify this flag if you are running in an environment with multiple threads, but you only rarely need to specify it in a single-threaded environment since stack overflow is automatically detected on nearly all systems if there is only one stack.",0,0,others,gcc
2060,-fstack-clash-protection,"Generate code to prevent stack clash style attacks. When this option is enabled, the compiler will only allocate one page of stack space at a time and each page is accessed immediately after allocation. Thus, it prevents allocations from jumping over any stack guard page provided by the operating system.",0,0,others,gcc
2061,-fstack-protector,"Emit extra code to check for buffer overflows, such as stack smashing attacks. This is done by adding a guard variable to functions with vulnerable objects. This includes functions that call alloca, and functions with buffers larger than or equal to 8 bytes. The guards are initialized when a function is entered and then checked when the function exits. If a guard check fails, an error message is printed and the program exits. Only variables that are actually allocated on the stack are considered, optimized away variables or variables allocated in registers don't count.",1,2,security-tradeoff,gcc
2062,-fstack-protector-all,Like -fstack-protector except that all functions are protected.,0,0,others,gcc
2064,-fstack-protector-strong,"Like -fstack-protector but includes additional functions to be protected those that have local array definitions, or have references to local frame addresses. Only variables that are actually allocated on the stack are considered, optimized away variables or variables allocated in registers don't count.",1,2,security-tradeoff,gcc
2065,-fstack-reuse=reuse-level,"This option controls stack space reuse for user declared local/auto variables and compiler generated temporaries. reuse_level can be all named_vars or none allenables stack reuse for all local variables and temporaries, named_varsenables the reuse only for user defined local variables with names, and nonedisables stack reuse completely. The default value is all The option is needed when the program extends the lifetime of a scoped local variable or a compiler generated temporary beyond the end point defined by the language. When a lifetime of a variable ends, and if the variable lives in memory, the optimizing compiler has the freedom to reuse its stack space with other temporaries or scoped local variables whose live range does not overlap with it. Legacy code extending local lifetime is likely to break with the stack reuse optimization.",1,6,function-tradeoff,gcc
2066,-fstack-usage,"Makes the compiler output stack usage information for the program, on a per-function basis. The filename for the dump is made by appending .su to the auxname. auxname is generated from the name of the output file, if explicitly specified and it is not an executable, otherwise it is the basename of the source file. An entry is made up of three fields:",1,6,function-tradeoff,gcc
2069,-fstore-merging,Perform merging of narrow stores to consecutive memory addresses. This pass merges contiguous stores of immediate values narrower than a word into fewer wider stores to reduce the number of instructions. This is enabled by default at -O2 and higher as well as -Os.,1,4,limited-side-effect,gcc
2070,-fstrict-aliasing,"Allow the compiler to assume the strictest aliasing rules applicable to the language being compiled. For C (and C++), this activates optimizations based on the type of expressions. In particular, an object of one type is assumed never to reside at the same address as an object of a different type, unless the types are almost the same. For example, an unsigned int can alias an int, but not a void* or a double. A character type may alias any other type.",1,4,limited-side-effect,gcc
2071,-fstrict-enums,"Allow the compiler to optimize using the assumption that a value of enumerated type can only be one of the values of the enumeration (as defined in the C++ standard; basically, a value that can be represented in the minimum number of bits needed to represent all the enumerators). This assumption may not be valid if the program uses a cast to convert an arbitrary integer value to the enumerated type.",1,4,limited-side-effect,gcc
2072,-fstrict-overflow,This option implies -fno-wrapv -fno-wrapv-pointer and when negated implies -fwrapv -fwrapv-pointer.,0,0,others,gcc
2074,-fstrong-eval-order,"Evaluate member access, array subscripting, and shift expressions in left-to-right order, and evaluate assignment in right-to-left order, as adopted for C++17. Enabled by default with -std=c++17. -fstrong-eval-order=some enables just the ordering of member access and shift expressions, and is the default without -std=c++17.",0,0,others,gcc
2075,-fsync-libcalls,This option controls whether any out-of-line instance of the __sync family of functions may be used to implement the C++11 __atomic family of functions.,0,0,others,gcc
2076,-fsyntax-only,"Check the code for syntax errors, but don't do anything beyond that.",0,0,others,gcc
2077,-ftabstop=width,"Set the distance between tab stops. This helps the preprocessor report correct column numbers in warnings or errors, even if tabs appear on the line. If the value is less than 1 or greater than 100, the option is ignored. The default is 8.",0,0,others,gcc
2078,-ftemplate-backtrace-limit=n,Set the maximum number of template instantiation notes for a single warning or error to n. The default value is 10.,1,5,workload-specific,gcc
2079,-ftemplate-depth=n,"Set the maximum instantiation depth for template classes to n. A limit on the template instantiation depth is needed to detect endless recursions during template class instantiation. ANSI/ISO C++ conforming programs must not rely on a maximum depth greater than 17 (changed to 1024 in C++11). The default value is 900, as the compiler can run out of stack space before hitting 1024 in some situations.",0,0,others,gcc
2080,-ftest-coverage,Produce a notes file that the gcov code-coverage utility (see gcov's Test Coverage Program) can use to show program coverage. Each source file's note file is called auxname.gcno. Refer to the -fprofile-arcs option above for a description of auxname and instructions on how to generate test coverage data. Coverage data matches the source files more closely if you do not optimize.,0,0,others,gcc
2081,-fthread-jumps,"Perform optimizations that check to see if a jump branches to a location where another comparison subsumed by the first is found. If so, the first branch is redirected to either the destination of the second branch or a point immediately following it, depending on whether the condition is known to be true or false.",1,4,limited-side-effect,gcc
2082,-ftime-report,Makes the compiler print some statistics about the time consumed by each pass when it finishes.,1,6,function-tradeoff,gcc
2084,-ftls-model=model,"Alter the thread-local storage model to be used (see Thread-Local). The model argument should be one of global-dynamic local-dynamic initial-execor local-exec Note that the choice is subject to optimization: the compiler may use a more efficient model for symbols not visible outside of the translation unit, or if -fpic is not given on the command line.",0,0,others,gcc
2085,-ftracer,Perform tail duplication to enlarge superblock size. This transformation simplifies the control flow of the function allowing other optimizations to do a better job.,1,4,limited-side-effect,gcc
2086,-ftrack-macro-expansion[=level],"Track locations of tokens across macro expansions. This allows the compiler to emit diagnostic about the current macro expansion stack when a compilation error occurs in a macro expansion. Using this option makes the preprocessor and the compiler consume more memory. The level parameter can be used to choose the level of precision of token location tracking thus decreasing the memory consumption if necessary. Value of level de-activates this option. Value tracks tokens locations in a degraded mode for the sake of minimal memory overhead. In this mode all tokens resulting from the expansion of an argument of a function-like macro have the same location. Value tracks tokens locations completely. This value is the most memory hungry. When this option is given no argument, the default parameter value is",0,0,others,gcc
2087,-ftrampolines,"For targets that normally need trampolines for nested functions, always generate them instead of using descriptors. Otherwise, for targets that do not need them, like for example HP-PA or IA-64, do nothing.",0,0,others,gcc
2088,-ftrapv,"This option generates traps for signed overflow on addition, subtraction, multiplication operations. The options -ftrapv and -fwrapv override each other, so using -ftrapv -fwrapv on the command-line results in -fwrapv being effective. Note that only active options override, so using -ftrapv -fwrapv -fno-wrapv on the command-line results in -ftrapv being effective.",0,0,others,gcc
2090,-ftree-builtin-call-dce,Perform conditional dead code elimination (DCE) for calls to built-in functions that may set errno but are otherwise free of side effects. This flag is enabled by default at -O2 and higher if -Os is not also specified.,1,4,limited-side-effect,gcc
2091,-ftree-ccp,Perform sparse conditional constant propagation (CCP) on trees. This pass only operates on local scalar variables and is enabled by default at -O and higher.,1,4,limited-side-effect,gcc
2092,-ftree-ch,"Perform loop header copying on trees. This is beneficial since it increases effectiveness of code motion optimizations. It also saves one jump. This flag is enabled by default at -O and higher. It is not enabled for -Os, since it usually increases code size.",1,4,limited-side-effect,gcc
2093,-ftree-coalesce-vars,"While transforming the program out of the SSA representation, attempt to reduce copying by coalescing versions of different user-defined variables, instead of just compiler temporaries. This may severely limit the ability to debug an optimized program compiled with -fno-var-tracking-assignments. In the negated form, this flag prevents SSA coalescing of user variables. This option is enabled by default if optimization is enabled, and it does very little otherwise.",1,4,limited-side-effect,gcc
2094,-ftree-copy-prop,Perform copy propagation on trees. This pass eliminates unnecessary copy operations. This flag is enabled by default at -O and higher.,1,4,limited-side-effect,gcc
2095,-ftree-dce,Perform dead code elimination (DCE) on trees. This flag is enabled by default at -O and higher.,1,4,limited-side-effect,gcc
2097,-ftree-dse,Perform dead store elimination (DSE) on trees. A dead store is a store into a memory location that is later overwritten by another store without any intervening loads. In this case the earlier store can be deleted. This flag is enabled by default at -O and higher.,1,4,limited-side-effect,gcc
2098,-ftree-forwprop,Perform forward propagation on trees. This flag is enabled by default at -O and higher.,1,4,limited-side-effect,gcc
2099,-ftree-fre,"Perform full redundancy elimination (FRE) on trees. The difference between FRE and PRE is that FRE only considers expressions that are computed on all paths leading to the redundant computation. This analysis is faster than PRE, though it exposes fewer redundancies. This flag is enabled by default at -O and higher.",1,4,limited-side-effect,gcc
2101,-ftree-loop-distribution,"Perform loop distribution. This flag can improve cache performance on big loop bodies and allow further loop optimizations, like parallelization or vectorization, to take place. For example, the loop",1,4,limited-side-effect,gcc
2102,-ftree-loop-if-convert,Attempt to transform conditional jumps in the innermost loops to branch-less equivalents. The intent is to remove control-flow from the innermost loops in order to improve the ability of the vectorization pass to handle these loops. This is enabled by default if vectorization is enabled.,1,4,limited-side-effect,gcc
2103,-ftree-loop-im,"Perform loop invariant motion on trees. This pass moves only invariants that are hard to handle at RTL level (function calls, operations that expand to nontrivial sequences of insns). With -funswitch-loops it also moves operands of conditions that are invariant out of the loop, so that we can use just trivial invariantness analysis in loop unswitching. The pass also includes store motion.",1,4,limited-side-effect,gcc
2104,-ftree-loop-ivcanon,Create a canonical counter for number of iterations in loops for which determining number of iterations requires complicated analysis. Later optimizations then may determine the number easily. Useful especially in connection with unrolling.,1,4,limited-side-effect,gcc
2105,-ftree-loop-optimize,Perform loop optimizations on trees. This flag is enabled by default at -O and higher.,1,4,limited-side-effect,gcc
2106,-ftree-loop-vectorize,"Perform loop vectorization on trees. This flag is enabled by default at -O3 and by -ftree-vectorize, -fprofile-use, and -fauto-profile.",1,4,limited-side-effect,gcc
2107,-ftree-parallelize-loops=n,"Parallelize loops, i.e., split their iteration space to run in n threads. This is only possible for loops whose iterations are independent and can be arbitrarily reordered. The optimization is only profitable on multiprocessor machines, for loops that are CPU-intensive, rather than constrained e.g. by memory bandwidth. This option implies -pthread, and thus is only supported on targets that have support for -pthread.",1,1,resource,gcc
2108,-ftree-partial-pre,Make partial redundancy elimination (PRE) more aggressive. This flag is enabled by default at -O3.,1,4,limited-side-effect,gcc
2109,-ftree-phiprop,Perform hoisting of loads from conditional pointers on trees. This pass is enabled by default at -O and higher.,1,4,limited-side-effect,gcc
2110,-ftree-pre,Perform partial redundancy elimination (PRE) on trees. This flag is enabled by default at -O2 and -O3.,1,4,limited-side-effect,gcc
2111,-ftree-pta,"Perform function-local points-to analysis on trees. This flag is enabled by default at -O1 and higher, except for -Og.",1,4,limited-side-effect,gcc
2112,-ftree-reassoc,Perform reassociation on trees. This flag is enabled by default at -O and higher.,1,6,function-tradeoff,gcc
2113,-ftree-scev-cprop,"Perform final value replacement. If a variable is modified in a loop in such a way that its value when exiting the loop can be determined using only its initial value and the number of loop iterations, replace uses of the final value by such a computation, provided it is sufficiently cheap. This reduces data dependencies and may allow further simplifications. Enabled by default at -O and higher.",1,4,limited-side-effect,gcc
2114,-ftree-sink,Perform forward store motion on trees. This flag is enabled by default at -O and higher.,1,4,limited-side-effect,gcc
2115,-ftree-slp-vectorize,"Perform basic block vectorization on trees. This flag is enabled by default at -O3 and by -ftree-vectorize, -fprofile-use, and -fauto-profile.",1,4,limited-side-effect,gcc
2117,-ftree-sra,"Perform scalar replacement of aggregates. This pass replaces structure references with scalars to prevent committing structures to memory too early. This flag is enabled by default at -O1 and higher, except for -Og.",1,4,limited-side-effect,gcc
2118,-ftree-switch-conversion,Perform conversion of simple initializations in a switch to initializations from a scalar array. This flag is enabled by default at -O2 and higher.,1,4,limited-side-effect,gcc
2119,-ftree-tail-merge,"Look for identical code sequences. When found, replace one with a jump to the other. This optimization is known as tail merging or cross jumping. This flag is enabled by default at -O2 and higher. The compilation time in this pass can be limited using max-tail-merge-comparisons parameter and max-tail-merge-iterations parameter.",1,4,limited-side-effect,gcc
2120,-ftree-ter,"Perform temporary expression replacement during the SSA->normal phase. Single use/single def temporaries are replaced at their use location with their defining expression. This results in non-GIMPLE code, but gives the expanders much more complex trees to work on resulting in better RTL generation. This is enabled by default at -O and higher.",1,4,limited-side-effect,gcc
2123,-funconstrained-commons,This option tells the compiler that variables declared in common blocks (e.g. Fortran) may later be overridden with longer trailing arrays. This prevents certain optimizations that depend on knowing the array bounds.,1,4,limited-side-effect,gcc
2125,-funroll-all-loops,"Unroll all loops, even if their number of iterations is uncertain when the loop is entered. This usually makes programs run more slowly. -funroll-all-loops implies the same options as -funroll-loops.",1,4,limited-side-effect,gcc
2126,-funroll-loops,"Unroll loops whose number of iterations can be determined at compile time or upon entry to the loop. -funroll-loops implies -frerun-cse-after-loop, -fweb and -frename-registers. It also turns on complete loop peeling (i.e. complete removal of loops with a small constant number of iterations). This option makes code larger, and may or may not make it run faster.",1,4,limited-side-effect,gcc
2127,-funsafe-math-optimizations,"Allow optimizations for floating-point arithmetic that (a) assume that arguments and results are valid and (b) may violate IEEE or ANSI standards. When used at link time, it may include libraries or startup files that change the default FPU control word or other similar optimizations.",1,4,limited-side-effect,gcc
2128,-funsigned-char,"Let the type char be unsigned, like unsigned char.",0,0,others,gcc
2129,-funswitch-loops,"Move branches with loop invariant conditions out of the loop, with duplicates of the loop on both branches (modified according to result of the condition).",1,4,limited-side-effect,gcc
2130,-funwind-tables,"Similar to -fexceptions, except that it just generates any needed static data, but does not affect the generated code in any other way. You normally do not need to enable this option; instead, a language processor that needs this handling enables it on your behalf.",0,0,others,gcc
2131,-fuse-cxa-atexit,"Register destructors for objects with static storage duration with the __cxa_atexit function rather than the atexit function. This option is required for fully standards-compliant handling of static destructors, but only works if your C library supports __cxa_atexit.",0,0,others,gcc
2132,-fuse-ld=bfd,Use the bfd linker instead of the default linker.,0,0,others,gcc
2133,-fuse-ld=gold,Use the gold linker instead of the default linker.,0,0,others,gcc
2134,-fuse-ld=lld,Use the LLVM lld linker instead of the default linker.,0,0,others,gcc
2135,-fuse-linker-plugin,"Enables the use of a linker plugin during link-time optimization. This option relies on plugin support in the linker, which is available in gold or in GNU ld 2.21 or newer.",1,4,limited-side-effect,gcc
2136,-fvariable-expansion-in-unroller,"With this option, the compiler creates multiple copies of some local variables when unrolling a loop, which can result in superior code.",1,4,limited-side-effect,gcc
2137,-fvar-tracking,Run variable tracking pass. It computes where variables are stored at each position in code. Better debugging information is then generated (if the debugging information format supports this information).,0,0,others,gcc
2138,-fvar-tracking-assignments,"Annotate assignments to user variables early in the compilation and attempt to carry the annotations over throughout the compilation all the way to the end, in an attempt to improve debug information while optimizing. Use of -gdwarf-4 is recommended along with it.",0,0,others,gcc
2139,-fvar-tracking-assignments-toggle,"Toggle -fvar-tracking-assignments, in the same way that -gtoggle toggles -g.",1,6,function-tradeoff,gcc
2140,-fvect-cost-model=model,"Alter the cost model used for vectorization. The model argument should be one of unlimited dynamic cheapor very-cheap With the unlimitedmodel the vectorized code-path is assumed to be profitable while with the dynamicmodel a runtime check guards the vectorized code-path to enable it only for iteration counts that will likely execute faster than when executing the original scalar loop. The cheapmodel disables vectorization of loops where doing so would be cost prohibitive for example due to required runtime checks for data dependence or alignment but otherwise is equal to the dynamicmodel. The very-cheapmodel only allows vectorization if the vector code would entirely replace the scalar code that is being vectorized. For example, if each iteration of a vectorized loop would only be able to handle exactly four iterations of the scalar loop, the very-cheapmodel would only allow vectorization if the scalar iteration count is known to be a multiple of four.",1,4,limited-side-effect,gcc
2142,-fversion-loops-for-strides,"If a loop iterates over an array with a variable stride, create another version of the loop that assumes the stride is always one. For example:",1,4,limited-side-effect,gcc
2143,-fvisibility=[default|internal|hidden|protected],"Set the default ELF image symbol visibility to the specified option ll symbols are marked with this unless overridden within the code. Using this feature can very substantially improve linking and load times of shared object libraries, produce more optimized code, provide near-perfect API export and prevent symbol clashes. It is strongly recommended that you use this in any shared objects you distribute.",0,0,others,gcc
2144,-fvisibility-inlines-hidden,This switch declares that the user does not attempt to compare pointers to inline functions or methods where the addresses of the two functions are taken in different shared objects.,0,0,others,gcc
2145,-fvisibility-ms-compat,This flag attempts to use visibility settings to make GCC C++ linkage model compatible with that of Microsoft Visual Studio.,0,0,others,gcc
2146,-fvpt,"If combined with -fprofile-arcs, this option instructs the compiler to add code to gather information about values of expressions.",1,4,limited-side-effect,gcc
2147,-fvtable-verify=[std|preinit|none],"This option is only available when compiling C++ code. It turns on (or off, if using -fvtable-verify=none) the security feature that verifies at run time, for every virtual call, that the vtable pointer through which the call is made is valid for the type of the object, and has not been corrupted or overwritten. If an invalid vtable pointer is detected at run time, an error is reported and execution of the program is immediately halted.",0,0,others,gcc
2148,-fvtv-counts,"This is a debugging flag. When used in conjunction with -fvtable-verify=std or -fvtable-verify=preinit, this causes the compiler to keep track of the total number of virtual calls it encounters and the number of verifications it inserts. It also counts the number of calls to certain run-time library functions that it inserts and logs this information for each compilation unit. The compiler writes this information to a file named vtv_count_data.log in the directory named by the environment variable VTV_LOGS_DIR if that is defined or the current working directory otherwise. It also counts the size of the vtable pointer sets for each class, and writes this information to vtv_class_set_sizes.log in the same directory.",1,6,function-tradeoff,gcc
2149,-fvtv-debug,"When used in conjunction with -fvtable-verify=std or -fvtable-verify=preinit, causes debug versions of the runtime functions for the vtable verification feature to be called. This flag also causes the compiler to log information about which vtable pointers it finds for each class. This information is written to a file named vtv_set_ptr_data.log in the directory named by the environment variable VTV_LOGS_DIR if that is defined or the current working directory otherwise.",0,0,others,gcc
2150,-fweb,"Constructs webs as commonly used for register allocation purposes and assign each web individual pseudo register. This allows the register allocation pass to operate on pseudos directly, but also strengthens several other optimization passes, such as CSE, loop optimizer and trivial dead code remover. It can, however, make debugging impossible, since variables no longer stay in a some register",1,4,limited-side-effect,gcc
2152,-fwide-exec-charset=charset,"Set the wide execution character set, used for wide string and character constants. The default is UTF-32 or UTF-16, whichever corresponds to the width of wchar_t. As with -fexec-charset, charset can be any encoding supported by the system iconv library routine; however, you will have problems with encodings that do not fit exactly in wchar_t.",0,0,others,gcc
2153,-fworking-directory,"Enable generation of linemarkers in the preprocessor output that let the compiler know the current working directory at the time of preprocessing. When this option is enabled, the preprocessor emits, after the initial linemarker, a second linemarker with the current working directory followed by two slashes. GCC uses this directory, when it's present in the preprocessed input, as the directory emitted as the current working directory in some debugging information formats. This option is implicitly enabled if debugging information is enabled, but this can be inhibited with the negated form -fno-working-directory. If the -P flag is present in the command line, this option has no effect, since no #line directives are emitted whatsoever.",0,0,others,gcc
2156,-fzero-call-used-regs=choice,Zero call-used registers at function return to increase program security by either mitigating Return-Oriented Programming (ROP) attacks or preventing information leakage through registers.,1,4,limited-side-effect,gcc
2157,-fzero-link,"When compiling for the NeXT runtime, the compiler ordinarily replaces calls to objc_getClass("") (when the name of the class is known at compile time) with static class references that get initialized at load time which improves run-time performance. Specifying the -fzero-link flag suppresses this behavior and causes calls to objc_getClass("""""") to be retained. This is useful in Zero-Link debugging mode",0,0,others,gcc
2158,-g,"Produce debugging information in the operating system's native format (stabs, COFF, XCOFF, or DWARF). GDB can work with this debugging information.",1,6,function-tradeoff,gcc
2160,-gas-locview-support,Inform the compiler that the assembler supports view assignment and reset assertion checking in .loc directives.,0,0,others,gcc
2161,gcse-after-reload-critical-fraction,The threshold ratio of critical edges execution count that permit performing redundancy elimination after reload.,1,5,workload-specific,gcc
2163,gcse-cost-distance-ratio,"Scaling factor in calculation of maximum distance an expression can be moved by GCSE optimizations. This is currently supported only in the code hoisting pass. The bigger the ratio, the more aggressive code hoisting is with simple expressions, i.e., the expressions that have cost less than gcse-unrestricted-cost. Specifying 0 disables hoisting of simple expressions.",1,4,limited-side-effect,gcc
2164,gcse-unrestricted-cost,"Cost, roughly measured as the cost of a single typical machine instruction, at which GCSE optimizations do not constrain the distance an expression can travel. This is currently supported only in the code hoisting pass. The lesser the cost, the more aggressive code hoisting is. Specifying 0 allows all expressions to travel unrestricted distances.",1,4,limited-side-effect,gcc
2165,-gdescribe-dies,"Add description attributes to some DWARF DIEs that have no name attribute, such as artificial variables, external references and call site parameter DIEs.",0,0,others,gcc
2166,-gdwarf64,"If DWARF debugging information is enabled, the -gdwarf32 selects the 32-bit DWARF format and the -gdwarf64 selects the 64-bit DWARF format. The default is target specific, on most targets it is -gdwarf32 though. The 32-bit DWARF format is smaller, but can't support more than 2GiB of debug information in any of the DWARF debug information sections. The 64-bit DWARF format allows larger debug information and might not be well supported by all consumers yet.",0,0,others,gcc
2167,-gdwarf-version,"Produce debugging information in DWARF format (if that is supported). The value of version may be either 2, 3, 4 or 5; the default version for most targets is 5 (with the exception of VxWorks, TPF and Darwin/Mac OS X, which default to version 2, and AIX, which defaults to version 4).",0,0,others,gcc
2168,-gen-decls,Dump interface declarations for all classes seen in the source file to a file named sourcename.decl.,1,6,function-tradeoff,gcc
2169,ggc-min-expand,GCC uses a garbage collector to manage its own memory allocation. This parameter specifies the minimum percentage by which the garbage collector heap should be allowed to expand between collections. Tuning this may improve compilation speed; it has no effect on code generation.,1,4,limited-side-effect,gcc
2172,-ggnu-pubnames,Generate .debug_pubnames and .debug_pubtypes sections in a format suitable for conversion into a GDB index. This option is only useful with a linker that can produce GDB index version 7.,0,0,others,gcc
2173,gimple-fe-computed-hot-bb-threshold,The number of executions of a basic block which is considered hot. The parameter is used only in GIMPLE FE.,1,4,limited-side-effect,gcc
2174,-gno-as-loc-support,"Force GCC to generate DWARF2+ line number tables internally, if DWARF2+ line number tables are to be generated.",0,0,others,gcc
2175,-gno-as-locview-support,"Force GCC to assign view numbers internally, if -gvariable-location-views are explicitly requested.",0,0,others,gcc
2176,-gno-column-info,"Emit location column information into DWARF debugging information, rather than just file and line. This option is enabled by default.",0,0,others,gcc
2177,-gno-inline-points,"Generate extended debug information for inlined functions. Location view tracking markers are inserted at inlined entry points, so that address and view numbers can be computed and output in debug information. This can be enabled independently of location views, in which case the view numbers won't be output, but it can only be enabled along with statement frontiers, and it is only enabled by default if location views are enabled.",0,0,others,gcc
2178,-gno-internal-reset-location-views,"Attempt to determine location views that can be omitted from location view lists. This requires the compiler to have very accurate insn length estimates, which isn't always the case, and it may cause incorrect view lists to be generated silently when using an assembler that does not support location view lists. The GNU assembler will flag any such error as a view number mismatch. This is only enabled on ports that define a reliable estimation function.",0,0,others,gcc
2179,-gno-record-gcc-switches,This switch causes the command-line options used to invoke the compiler that may affect code generation to be appended to the DW_AT_producer attribute in DWARF debugging information. The options are concatenated with spaces separating them from each other and from the compiler version. It is enabled by default. See also -frecord-gcc-switches for another way of storing compiler options into the object file.,0,0,others,gcc
2181,-gno-strict-dwarf,Allow using extensions of later DWARF standard version than selected with -gdwarf-version.,0,0,others,gcc
2182,-gno-variable-location-views,"Augment variable location lists with progressive view numbers implied from the line number table. This enables debug information consumers to inspect state at certain points of the program, even if no instructions associated with the corresponding source locations are present at that point. If the assembler lacks support for view numbers in line number tables, this will cause the compiler to emit the line number table, which generally makes them somewhat less compact. The augmented line number tables and location lists are fully backward-compatible, so they can be consumed by debug information consumers that are not aware of these augmentations, but they won't derive any benefit from them either.",0,0,others,gcc
2183,gnu++03,GNU dialect of -std=c++98.,0,0,others,gcc
2184,gnu++0x,GNU dialect of -std=c++11. The name gnu++0x is deprecated.,0,0,others,gcc
2185,gnu++1y,GNU dialect of -std=c++14. The name gnu++1y is deprecated.,0,0,others,gcc
2186,gnu++1z,GNU dialect of -std=c++17. This is the default for C++ code. The name gnu++1z is deprecated.,0,0,others,gcc
2187,gnu++23,"GNU dialect of -std=c++2b. Support is highly experimental, and will almost certainly change in incompatible ways in future releases.",0,0,others,gcc
2188,gnu++2a,"GNU dialect of -std=c++20. Support is experimental, and could change in incompatible ways in future releases. The name gnu++2a is deprecated.",0,0,others,gcc
2189,gnu18,GNU dialect of ISO C17. This is the default for C code.,0,0,others,gcc
2190,gnu1x,GNU dialect of ISO C11. The name gnu1x is deprecated.,0,0,others,gcc
2191,gnu2x,"The next version of the ISO C standard, still under development, plus GNU extensions. The support for this version is experimental and incomplete.",0,0,others,gcc
2192,gnu89,GNU dialect of ISO C90 (including some C99 features).,0,0,others,gcc
2193,gnu9x,GNU dialect of ISO C99. The name gnu9x is deprecated.,0,0,others,gcc
2194,-gpubnames,Generate DWARF .debug_pubnames and .debug_pubtypes sections.,0,0,others,gcc
2195,graphite-allow-codegen-errors,Whether codegen errors should be ICEs when -fchecking.,1,4,limited-side-effect,gcc
2196,graphite-max-arrays-per-scop,Maximum number of arrays per scop.,1,5,workload-specific,gcc
2197,graphite-max-nb-scop-params,"To avoid exponential effects in the Graphite loop transforms, the number of parameters in a Static Control Part (SCoP) is bounded. A value of zero can be used to lift the bound. A variable whose value is unknown at compilation time and defined outside a SCoP is a parameter of the SCoP.",1,5,workload-specific,gcc
2198,-gsplit-dwarf,"If DWARF debugging information is enabled, separate as much debugging information as possible into a separate output file with the extension .dwo. This option allows the build system to avoid linking files with debug information. To be useful, this option requires a debugger capable of reading .dwo files.",0,0,others,gcc
2200,-gstabs+,"Produce debugging information in stabs format (if that is supported), using GNU extensions understood only by the GNU debugger (GDB). The use of these extensions is likely to make other debuggers crash or refuse to read the program.",0,0,others,gcc
2202,-gtoggle,"Turn off generation of debug info, if leaving out this option generates it, or turn it on at level 2 otherwise. The position of this argument in the command line does not matter; it takes effect after all other options are processed, and it does so only once, no matter how many times it is given. This is mainly intended to be used with -fcompare-debug.",1,6,function-tradeoff,gcc
2203,-gvms,Produce debugging information in Alpha/VMS debug format (if that is supported). This is the format used by DEBUG on Alpha/VMS systems.,0,0,others,gcc
2204,-gvmslevel,Request debugging information and also use level to specify how much information. The default level is 2.,1,6,function-tradeoff,gcc
2205,-gxcoff,Produce debugging information in XCOFF format (if that is supported). This is the format used by the DBX debugger on IBM RS/6000 systems.,0,0,others,gcc
2206,-gxcoff+,"Produce debugging information in XCOFF format (if that is supported), using GNU extensions understood only by the GNU debugger (GDB). The use of these extensions is likely to make other debuggers crash or refuse to read the program, and may cause assemblers other than the GNU assembler (GAS) to fail with an error.",0,0,others,gcc
2208,-H,"Print the name of each header file used, in addition to other normal activities. Each name is indented to show how deep in the includestack it is. Precompiled header files are also printed, even if they are found to be invalid; an invalid precompiled header file is printed with ..xand a valid one with ..!.",1,6,function-tradeoff,gcc
2209,hash-table-verification-limit,The number of elements for which hash table verification is done for each searched element.,1,4,limited-side-effect,gcc
2210,--help,Print (on the standard output) a description of the command-line options understood by the compiler that fit into all specified classes and qualifiers. These are the supported classes:,1,6,function-tradeoff,gcc
2211,hot-bb-count-fraction,"The denominator n of fraction 1/n of the maximal execution count of a basic block in the entire program that a basic block needs to at least have in order to be considered hot. The default is 10000, which means that a basic block is considered hot if its execution count is greater than 1/10000 of the maximal execution count. 0 means that it is never considered hot. Used in non-LTO mode.",1,4,limited-side-effect,gcc
2212,hot-bb-count-ws-permille,"The number of most executed permilles, ranging from 0 to 1000, of the profiled execution of the entire program to which the execution count of a basic block must be part of in order to be considered hot. The default is 990, which means that a basic block is considered hot if its execution count contributes to the upper 990 permilles, or 99.0%, of the profiled execution of the entire program. 0 means that it is never considered hot. Used in LTO mode.",1,4,limited-side-effect,gcc
2213,hot-bb-frequency-fraction,"The denominator n of fraction 1/n of the execution frequency of the entry block of a function that a basic block of this function needs to at least have in order to be considered hot. The default is 1000, which means that a basic block is considered hot in a function if it is executed more frequently than 1/1000 of the frequency of the entry block of the function. 0 means that it is never considered hot.",1,4,limited-side-effect,gcc
2215,hwasan-instrument-mem-intrinsics,Enable hwasan instrumentation of builtin functions. Instrumentation of these builtin functions is enabled by default for both -fsanitize=hwaddress and -fsanitize=kernel-hwaddress. To disable instrumentation of builtin functions use --param hwasan-instrument-mem-intrinsics=0.,1,4,limited-side-effect,gcc
2217,hwasan-instrument-stack,"Enable hwasan instrumentation of statically sized stack-allocated variables. This kind of instrumentation is enabled by default when using -fsanitize=hwaddress and disabled by default when using -fsanitize=kernel-hwaddress. To disable stack instrumentation use --param hwasan-instrument-stack=0, and to enable it use --param hwasan-instrument-stack=1.",1,4,limited-side-effect,gcc
2218,hwasan-instrument-writes,Enable hwasan checks on memory writes. Instrumentation of writes is enabled by default for both -fsanitize=hwaddress and -fsanitize=kernel-hwaddress. To disable checking memory writes use --param hwasan-instrument-writes=0.,1,4,limited-side-effect,gcc
2220,-I-,Split the include path. This option has been deprecated. Please use -iquote instead for -I directories before the -I- and remove the -I- option.,0,0,others,gcc
2221,-idirafter dir,"Add the directory dir to the list of directories to be searched for header files during preprocessing. If dir begins with = or $SYSROOT, then the = or $SYSROOT is replaced by the sysroot prefix; see --sysroot and -isysroot.",0,0,others,gcc
2222,-imacros file,"Exactly like -include, except that any output produced by scanning file is thrown away. Macros it defines remain defined. This allows you to acquire all the macros from a header without also processing its declarations.",0,0,others,gcc
2223,-imultilib dir,Use dir as a subdirectory of the directory containing target-specific C++ headers.,0,0,others,gcc
2224,-include file,"Process file as if #include ""file"" appeared as the first line of the primary source file. However, the first directory searched for file is the preprocessor's working directory instead of the directory containing the main source file. If not found there, it is searched for in the remainder of the #include """" search chain as normal.",0,0,others,gcc
2225,inline,Enable dumps from all inlining optimizations.,1,6,function-tradeoff,gcc
2226,inline-clone,"Only enable inlining and cloning optimizations, which includes inlining, cloning, interprocedural scalar replacement of aggregates and partial inlining. As a result, when patching a function, all its callers and its clonescallers are impacted, therefore need to be patched as well.",1,4,limited-side-effect,gcc
2228,inline-min-speedup,"When estimated performance improvement of caller + callee runtime exceeds this threshold (in percent), the function can be inlined regardless of the limit on --param max-inline-insns-single and --param max-inline-insns-auto.",1,5,workload-specific,gcc
2229,inline-only-static,"Only enable inlining of static functions. As a result, when patching a static function, all its callers are impacted and so need to be patched as well.",1,4,limited-side-effect,gcc
2231,integer-share-limit,"Small integer constants can use a shared data structure, reducing the compiler's memory usage and increasing its speed. This sets the maximum value of a shared integer constant.",1,4,limited-side-effect,gcc
2232,internals,"By default, only high-level messages are emitted. This option enables additional, more detailed, messages, which are likely to only be of interest to GCC developers.",1,6,function-tradeoff,gcc
2233,ipa,Enable dumps from all interprocedural optimizations.,1,6,function-tradeoff,gcc
2234,ipa-cp-eval-threshold,IPA-CP calculates its own score of cloning profitability heuristics and performs those cloning opportunities with scores that exceed ipa-cp-eval-threshold.,1,5,workload-specific,gcc
2235,ipa-cp-large-unit-insns,The size of translation unit that IPA-CP pass considers large.,1,4,limited-side-effect,gcc
2238,ipa-cp-min-recursive-probability,Recursive cloning only when the probability of call being executed exceeds the parameter.,1,4,limited-side-effect,gcc
2239,ipa-cp-recursion-penalty,Percentage penalty the recursive functions will receive when they are evaluated for cloning.,1,4,limited-side-effect,gcc
2240,ipa-cp-single-call-penalty,Percentage penalty functions containing a single call to another function will receive when they are evaluated for cloning.,1,4,limited-side-effect,gcc
2241,ipa-cp-unit-growth,"Specifies maximal overall growth of the compilation unit caused by interprocedural constant propagation. For example, parameter value 10 limits unit growth to 1.1 times the original size.",0,0,others,gcc
2242,ipa-cp-value-list-size,IPA-CP attempts to track all possible values and types passed to a function's parameter in order to propagate them and perform devirtualization. ipa-cp-value-list-size is the maximum number of values and types it stores per one formal parameter of a function.,1,4,limited-side-effect,gcc
2243,ipa-jump-function-lookups,Specifies number of statements visited during jump function offset discovery.,1,4,limited-side-effect,gcc
2245,ipa-max-agg-items,IPA-CP is also capable to propagate a number of scalar values passed in an aggregate. ipa-max-agg-items controls the maximum number of such values per one parameter.,1,4,limited-side-effect,gcc
2246,ipa-max-loop-predicates,The maximum number of different predicates IPA will use to describe when loops in a function have known properties.,1,4,limited-side-effect,gcc
2247,ipa-max-param-expr-ops,"IPA-CP will analyze conditional statement that references some function parameter to estimate benefit for cloning upon certain constant value. But if number of operations in a parameter expression exceeds ipa-max-param-expr-ops, the expression is treated as complicated one, and is not handled by IPA analysis.",1,4,limited-side-effect,gcc
2248,ipa-max-switch-predicate-bounds,"Maximal number of boundary endpoints of case ranges of switch statement. For switch exceeding this limit, IPA-CP will not construct cloning cost predicate, which is used to estimate cloning benefit, for default case of the switch statement.",1,4,limited-side-effect,gcc
2249,ipa-sra-max-replacements,"Maximum pieces of an aggregate that IPA-SRA tracks. As a consequence, it is also the maximum number of replacements of a formal parameter.",1,5,workload-specific,gcc
2251,-iplugindir=dir,"Set the directory to search for plugins that are passed by -fplugin=name instead of -fplugin=path/name.so. This option is not meant to be used by the user, but only passed by the driver.",0,0,others,gcc
2252,-iprefix prefix,"Specify prefix as the prefix for subsequent -iwithprefix options. If the prefix represents a directory, you should include the final /.",0,0,others,gcc
2253,ira-loop-reserved-regs,IRA can be used to evaluate more accurate register pressure in loops for decisions to move loop invariants (see -O3). The number of available registers reserved for some other purposes is given by this parameter. Default of the parameter is the best found from numerous experiments.,1,4,limited-side-effect,gcc
2254,ira-max-conflict-table-size,"Although IRA uses a sophisticated algorithm to compress the conflict table, the table can still require excessive amounts of memory for huge functions. If the conflict table for a function could be more than the size in MB given by this parameter, the register allocator instead uses a faster, simpler, and lower-quality algorithm that does not require building a pseudo-register conflict table.",1,4,limited-side-effect,gcc
2255,ira-max-loops-num,"IRA uses regional register allocation by default. If a function contains more loops than the number given by this parameter, only at most the given number of the most frequently-executed loops form regions for regional register allocation.",1,5,workload-specific,gcc
2256,iso9899:1990,Support all ISO C90 programs (certain GNU extensions that conflict with ISO C90 are disabled). Same as -ansi for C code.,0,0,others,gcc
2257,iso9899:199409,ISO C90 as modified in amendment 1.,0,0,others,gcc
2258,iso9899:199x,"ISO C99. This standard is substantially completely supported, modulo bugs and floating-point issues (mainly but not entirely relating to optional C99 features from Annexes F and G). See http://gcc.gnu.org/c99status.html for more information. The names c9xand iso9899:199xare deprecated.",0,0,others,gcc
2259,iso9899:2011,"ISO C11, the 2011 revision of the ISO C standard. This standard is substantially completely supported, modulo bugs, floating-point issues (mainly but not entirely relating to optional C11 features from Annexes F and G) and the optional Annexes K (Bounds-checking interfaces) and L (Analyzability). The name c1xis deprecated.",0,0,others,gcc
2260,iso9899:2018,"ISO C17, the 2017 revision of the ISO C standard (published in 2018). This standard is same as C11 except for corrections of defects (all of which are also applied with -std=c11) and a new value of __STDC_VERSION__, and so is supported to the same extent as C11.",0,0,others,gcc
2261,-isysroot dir,"This option is like the --sysroot option, but applies only to header files (except for Darwin targets, where it applies to both header files and libraries). See the --sysroot option for more information.",0,0,others,gcc
2262,iv-always-prune-cand-set-bound,"If the number of candidates in the set is smaller than this value, always try to remove unnecessary ivs from the set when adding a new one.",1,5,workload-specific,gcc
2263,iv-consider-all-candidates-bound,"Bound on number of candidates for induction variables, below which all candidates are considered for each use in induction variable optimizations. If there are more candidates than this, only the most relevant ones are considered to avoid quadratic time complexity.",1,4,limited-side-effect,gcc
2264,iv-max-considered-uses,The induction variable optimizations give up on loops that contain more induction variable uses.,1,4,limited-side-effect,gcc
2265,-iwithprefixbefore dir,"Append dir to the prefix specified previously with -iprefix, and add the resulting directory to the include search path. -iwithprefixbefore puts it in the same place -I would; -iwithprefix puts it where -idirafter would.",0,0,others,gcc
2267,jump-table-max-growth-ratio-for-size,The maximum code size growth ratio when expanding into a jump table (in percent). The parameter is used when optimizing for size.,1,4,limited-side-effect,gcc
2268,jump-table-max-growth-ratio-for-speed,The maximum code size growth ratio when expanding into a jump table (in percent). The parameter is used when optimizing for speed.,1,4,limited-side-effect,gcc
2269,-l library,Search the library named library when linking. (The second alternative with the library as a separate argument is only for POSIX compliance and is not recommended.),0,0,others,gcc
2271,l1-cache-size,"The size of L1 data cache, in kilobytes.",1,1,resource,gcc
2273,language,"Display the options supported for language, where language is the name of one of the languages supported in this version of GCC. If an option is supported by all languages, one needs to select common class.",0,0,others,gcc
2275,large-function-insns,"The limit specifying really large functions. For functions larger than this limit after inlining, inlining is constrained by --param large-function-growth. This parameter is useful primarily to avoid extreme compilation time caused by non-linear algorithms used by the back end.",1,4,limited-side-effect,gcc
2276,large-stack-frame,The limit specifying large stack frames. While inlining the algorithm is trying to not grow past this limit too much.,1,4,limited-side-effect,gcc
2277,large-stack-frame-growth,"Specifies maximal growth of large stack frames caused by inlining in percents. For example, parameter value 1000 limits large stack frame growth to 11 times the original size.",1,4,limited-side-effect,gcc
2278,large-unit-insns,"The limit specifying large translation unit. Growth caused by inlining of units larger than this limit is limited by --param inline-unit-growth. For small units this might be too tight. For example, consider a unit consisting of function A that is inline and B that just calls A three times. If B is small relative to A, the growth of unit is 300\% and yet such inlining is very sane. For very large units consisting of small inlineable functions, however, the overall unit growth limit is needed to avoid exponential explosion of code size. Thus for smaller units, the size is increased to --param large-unit-insns before applying --param inline-unit-growth.",1,4,limited-side-effect,gcc
2280,-Ldir,Add directory dir to the list of directories to be searched for -l.,0,0,others,gcc
2281,lim-expensive,The minimum cost of an expensive expression in the loop invariant motion.,1,4,limited-side-effect,gcc
2283,-lobjc,You need this special case of the -l option in order to link an Objective-C or Objective-C++ program.,0,0,others,gcc
2284,locus,SGR substring for location information,0,0,others,gcc
2285,loop,Enable dumps from all loop optimizations.,1,6,function-tradeoff,gcc
2286,loop-block-tile-size,"Loop blocking or strip mining transforms, enabled with -floop-block or -floop-strip-mine, strip mine each loop in the loop nest by a given number of iterations. The strip length can be changed using the loop-block-tile-size parameter.",1,4,limited-side-effect,gcc
2287,loop-interchange-max-num-stmts,The maximum number of stmts in a loop to be interchanged.,1,5,workload-specific,gcc
2288,loop-interchange-stride-ratio,The minimum ratio between stride of two loops for interchange to be profitable.,1,5,workload-specific,gcc
2290,loop-max-datarefs-for-datadeps,Building data dependencies is expensive for very large loops. This parameter limits the number of data references in loops that are considered for data dependence analysis. These large loops are no handled by the optimizations using loop data dependencies.,1,5,workload-specific,gcc
2291,loop-versioning-max-inner-insns,The maximum number of instructions that an inner loop can have before the loop versioning pass considers it too big to copy.,1,5,workload-specific,gcc
2292,loop-versioning-max-outer-insns,"The maximum number of instructions that an outer loop can have before the loop versioning pass considers it too big to copy, discounting any instructions in inner loops that directly benefit from versioning.",1,5,workload-specific,gcc
2294,lra-max-considered-reload-pseudos,The max number of reload pseudos which are considered during spilling a non-reload pseudo.,1,4,limited-side-effect,gcc
2296,lto-max-streaming-parallelism,Maximal number of parallel processes used for LTO streaming.,1,4,limited-side-effect,gcc
2297,lto-min-partition,Size of minimal partition for WHOPR (in estimated instructions). This prevents expenses of splitting very small programs into too many partitions.,1,4,limited-side-effect,gcc
2298,lto-partitions,Specify desired number of partitions produced during WHOPR compilation. The number of partitions should exceed the number of CPUs used for compilation.,1,4,limited-side-effect,gcc
2299,-M,"Instead of outputting the result of preprocessing, output a rule suitable for make describing the dependencies of the main source file. The preprocessor outputs one make rule containing the object file name for that source file, a colon, and the names of all the included files, including those coming from -include or -imacros command-line options.",0,0,others,gcc
2300,max-average-unrolled-insns,"The maximum number of instructions biased by probabilities of their execution that a loop may have to be unrolled. If a loop is unrolled, this parameter also determines how many times the loop code is unrolled.",1,4,limited-side-effect,gcc
2301,max-combine-insns,The maximum number of instructions the RTL combiner tries to combine.,1,4,limited-side-effect,gcc
2303,max-completely-peel-loop-nest-depth,The maximum depth of a loop nest suitable for complete peeling.,1,5,workload-specific,gcc
2304,max-completely-peel-times,The maximum number of iterations of a loop to be suitable for complete peeling.,1,5,workload-specific,gcc
2305,max-crossjump-edges,"The maximum number of incoming edges to consider for cross-jumping. The algorithm used by -fcrossjumping is O(N^2) in the number of edges incoming to each block. Increasing values mean more aggressive optimization, making the compilation time increase with probably small improvement in executable size.",1,4,limited-side-effect,gcc
2306,max-cse-insns,The maximum number of instructions CSE processes before flushing.,1,4,limited-side-effect,gcc
2307,max-cselib-memory-locations,"The maximum number of memory locations cselib should take into account. Increasing values mean more aggressive optimization, making the compilation time increase with probably slightly better performance.",1,4,limited-side-effect,gcc
2308,max-cse-path-length,The maximum number of basic blocks on path that CSE considers.,1,4,limited-side-effect,gcc
2309,max-debug-marker-count,"Sets a threshold on the number of debug markers (e.g. begin stmt markers) to avoid complexity explosion at inlining or expanding to RTL. If a function has more such gimple stmts than the set limit, such stmts will be dropped from the inlined copy of a function, and from its RTL expansion.",1,5,workload-specific,gcc
2311,max-delay-slot-live-search,"When trying to fill delay slots, the maximum number of instructions to consider when searching for a block with valid live register information. Increasing this arbitrarily chosen value means more aggressive optimization, increasing the compilation time. This parameter should be removed when the delay slot code is rewritten to maintain the control-flow graph.",1,5,workload-specific,gcc
2314,max-fields-for-field-sensitive,Maximum number of fields in a structure treated in a field sensitive manner during pointer analysis.,1,5,workload-specific,gcc
2315,max-find-base-term-values,Maximum number of VALUEs handled during a single find_base_term call.,1,5,workload-specific,gcc
2316,max-fsm-thread-length,Maximum number of basic blocks on a finite state automaton jump thread path.,1,4,limited-side-effect,gcc
2319,max-gcse-insertion-ratio,"If the ratio of expression insertions to deletions is larger than this value for any expression, then RTL PRE inserts or removes the expression and thus leaves partially redundant computations in the instruction stream.",1,4,limited-side-effect,gcc
2321,max-goto-duplication-insns,"The maximum number of instructions to duplicate to a block that jumps to a computed goto. To avoid O(N^2) behavior in a number of passes, GCC factors computed gotos early in the compilation process, and unfactors them as late as possible. Only computed jumps at the end of a basic blocks with no more than max-goto-duplication-insns are unfactored.",1,4,limited-side-effect,gcc
2322,max-grow-copy-bb-insns,The maximum code size expansion factor when copying basic blocks instead of jumping. The expansion is relative to a jump instruction.,1,5,workload-specific,gcc
2324,max-inline-insns-auto,"When you use -finline-functions (included in -O3), a lot of functions that would otherwise not be considered for inlining by the compiler are investigated. To those functions, a different (more restrictive) limit compared to functions declared inline can be applied (--param max-inline-insns-auto).",1,4,limited-side-effect,gcc
2325,max-inline-insns-recursive-auto,The maximum number of instructions non-inline function can grow to via recursive inlining.,1,4,limited-side-effect,gcc
2326,max-inline-insns-single,Several parameters control the tree inliner used in GCC. This number sets the maximum number of instructions (counted in GCC internal representation) in a single function that the tree inliner considers for inlining. This only affects functions declared inline and methods implemented in a class declaration (C++).,1,4,limited-side-effect,gcc
2327,max-inline-insns-size,This is bound applied to calls which are optimized for size. Small growth may be desirable to anticipate optimization oppurtunities exposed by inlining.,1,4,limited-side-effect,gcc
2328,max-inline-insns-small,This is bound applied to calls which are considered relevant with -finline-small-functions.,1,4,limited-side-effect,gcc
2329,max-inline-recursive-depth-auto,Specifies the maximum recursion depth used for recursive inlining.,1,3,reliability-tradeoff,gcc
2330,max-isl-operations,"Maximum number of isl operations, 0 means unlimited.",1,5,workload-specific,gcc
2331,max-iterations-computation-cost,Bound on the cost of an expression to compute the number of iterations.,1,5,workload-specific,gcc
2332,max-iterations-to-track,The maximum number of iterations of a loop the brute-force algorithm for analysis of the number of iterations of the loop tries to evaluate.,1,5,workload-specific,gcc
2333,max-jump-thread-duplication-stmts,Maximum number of statements allowed in a block that needs to be duplicated when threading jumps.,1,5,workload-specific,gcc
2334,max-last-value-rtl,The maximum size measured as number of RTLs that can be recorded in an expression in combiner for a pseudo register as last known value of that register.,1,4,limited-side-effect,gcc
2335,max-loop-header-insns,The maximum number of insns in loop header duplicated by the copy loop headers pass.,1,4,limited-side-effect,gcc
2338,max-peel-branches,The maximum number of branches on the hot path through the peeled sequence.,1,5,workload-specific,gcc
2339,max-peeled-insns,"The maximum number of instructions that a loop may have to be peeled. If a loop is peeled, this parameter also determines how many times the loop code is peeled.",1,4,limited-side-effect,gcc
2340,max-peel-times,The maximum number of peelings of a single loop.,1,5,workload-specific,gcc
2341,max-pending-list-length,The maximum number of pending dependencies scheduling allows before flushing the current state and starting over. Large functions with few branches or calls can create excessively large lists which needlessly consume memory and resources.,1,4,limited-side-effect,gcc
2342,max-pipeline-region-blocks,The maximum number of blocks in a region to be considered for pipelining in the selective scheduler.,1,5,workload-specific,gcc
2343,max-pipeline-region-insns,The maximum number of insns in a region to be considered for pipelining in the selective scheduler.,1,5,workload-specific,gcc
2344,max-pow-sqrt-depth,Maximum depth of sqrt chains to use when synthesizing exponentiation by a real constant.,1,5,workload-specific,gcc
2345,max-predicted-iterations,"The maximum number of loop iterations we predict statically. This is useful in cases where a function contains a single loop with known bound and another loop with unknown bound. The known number of iterations is predicted correctly, while the unknown number of iterations average to roughly 10. This means that the loop without bounds appears artificially cold relative to the other one.",1,5,workload-specific,gcc
2346,max-reload-search-insns,"The maximum number of instruction reload should look backward for equivalent register. Increasing values mean more aggressive optimization, making the compilation time increase with probably slightly better performance.",1,5,workload-specific,gcc
2348,max-rtl-if-conversion-unpredictable-cost,Maximum permissible cost for the sequence that would be generated by the RTL if-conversion pass for a branch that is considered unpredictable.,1,5,workload-specific,gcc
2349,max-sched-extend-regions-iters,The maximum number of iterations through CFG to extend regions. A value of 0 disables region extensions.,1,5,workload-specific,gcc
2350,max-sched-insn-conflict-delay,The maximum conflict delay for an insn to be considered for speculative motion.,1,5,workload-specific,gcc
2351,max-sched-ready-insns,"The maximum number of instructions ready to be issued the scheduler should consider at any given time during the first scheduling pass. Increasing values mean more thorough searches, making the compilation time increase with probably little benefit.",1,5,workload-specific,gcc
2352,max-sched-region-blocks,The maximum number of blocks in a region to be considered for interblock scheduling.,1,5,workload-specific,gcc
2353,max-sched-region-insns,The maximum number of insns in a region to be considered for interblock scheduling.,1,5,workload-specific,gcc
2354,max-slsr-cand-scan,Set the maximum number of existing candidates that are considered when seeking a basis for a new straight-line strength reduction candidate.,1,5,workload-specific,gcc
2355,max-speculative-devirt-maydefs,The maximum number of may-defs we analyze when looking for a must-def specifying the dynamic type of an object that invokes a virtual call we may be able to devirtualize speculatively.,1,4,limited-side-effect,gcc
2356,max-ssa-name-query-depth,Maximum depth of recursion when querying properties of SSA names in things like fold routines. One level of recursion corresponds to following a use-def chain.,1,4,limited-side-effect,gcc
2358,max-stores-to-merge,The maximum number of stores to attempt to merge into wider stores in the store merging pass.,1,5,workload-specific,gcc
2359,max-stores-to-sink,The maximum number of conditional store pairs that can be sunk. Set to 0 if either vectorization (-ftree-vectorize) or if-conversion (-ftree-loop-if-convert) is disabled.,1,5,workload-specific,gcc
2360,max-stores-to-track,The maximum number of stores to track at the same time in the attemt to to merge them into wider stores in the store merging pass.,1,5,workload-specific,gcc
2361,max-tail-merge-comparisons,The maximum amount of similar bbs to compare a bb with. This is used to avoid quadratic behavior in tree tail merging.,1,4,limited-side-effect,gcc
2362,max-tail-merge-iterations,The maximum amount of iterations of the pass over the function. This is used to limit compilation time in tree tail merging.,1,5,workload-specific,gcc
2363,max-tracked-strlens,Maximum number of strings for which strlen optimization pass will track string lengths.,1,4,limited-side-effect,gcc
2364,max-tree-if-conversion-phi-args,Maximum number of arguments in a PHI supported by TREE if conversion unless the loop is marked with simd pragma.,1,4,limited-side-effect,gcc
2365,max-unrolled-insns,"The maximum number of instructions that a loop may have to be unrolled. If a loop is unrolled, this parameter also determines how many times the loop code is unrolled.",1,5,workload-specific,gcc
2366,max-unroll-times,The maximum number of unrollings of a single loop.,1,5,workload-specific,gcc
2367,max-unswitch-insns,The maximum number of insns of an unswitched loop.,1,5,workload-specific,gcc
2368,max-unswitch-level,The maximum number of branches unswitched in a single loop.,1,5,workload-specific,gcc
2369,max-variable-expansions-in-unroller,"If -fvariable-expansion-in-unroller is used, the maximum number of times that an individual variable will be expanded during loop unrolling.",1,4,limited-side-effect,gcc
2370,max-vartrack-expr-depth,"Sets a maximum number of recursion levels when attempting to map variable names or debug temporaries to value expressions. This trades compilation time for more complete debug information. If this is set too low, value expressions that are available and could be represented in debug information may end up not being used; setting this higher may enable the compiler to find more complex debug expressions, but compile time and memory use may grow.",1,4,limited-side-effect,gcc
2371,max-vartrack-reverse-op-size,Max. size of loc list for which reverse ops should be added.,1,4,limited-side-effect,gcc
2372,max-vartrack-size,"Sets a maximum number of hash table slots to use during variable tracking dataflow analysis of any function. If this limit is exceeded with variable tracking at assignments enabled, analysis for that function is retried without it, after removing all debug insns from the function. If the limit is exceeded even without debug insns, var tracking analysis is completely disabled for the function. Setting the parameter to zero makes it unlimited.",1,4,limited-side-effect,gcc
2373,max-vrp-switch-assertions,The maximum number of assertions to add along the default edge of a switch statement during VRP.,1,5,workload-specific,gcc
2374,-MD,"-MD is equivalent to -M -MF file, except that -E is not implied. The driver determines file based on whether an -o option is given. If it is, the driver uses its argument but with a suffix of .d, otherwise it takes the name of the input file, removes any directory components and suffix, and applies a .d suffix.",0,0,others,gcc
2375,-MF file,"When used with -M or -MM, specifies a file to write the dependencies to. If no -MF switch is given the preprocessor sends the rules to the same place it would send preprocessed output.",0,0,others,gcc
2376,-MG,"In conjunction with an option such as -M requesting dependency generation, -MG assumes missing header files are generated files and adds them to the dependency list without raising an error. The dependency filename is taken directly from the #include directive without prepending any path. -MG also suppresses preprocessed output, as a missing header file renders this useless.",0,0,others,gcc
2378,min-inline-recursive-probability,Recursive inlining is profitable only for function having deep recursion in average and can hurt for function having little recursion depth by increasing the prologue size or complexity of function body to other optimizers.,1,4,limited-side-effect,gcc
2379,min-insn-to-prefetch-ratio,The minimum ratio between the number of instructions and the number of prefetches to enable prefetching in a loop.,1,5,workload-specific,gcc
2380,min-loop-cond-split-prob,"When FDO profile information is available, min-loop-cond-split-prob specifies minimum threshold for probability of semi-invariant condition statement to trigger loop split.",1,5,workload-specific,gcc
2381,min-nondebug-insn-uid,"Use uids starting at this parameter for nondebug insns. The range below the parameter is reserved exclusively for debug insns created by -fvar-tracking-assignments, but debug insns may get (non-overlapping) uids above it if the reserved range is exhausted.",1,4,limited-side-effect,gcc
2382,min-size-for-stack-sharing,The minimum size of variables taking part in stack slot sharing when not optimizing.,1,4,limited-side-effect,gcc
2383,min-spec-prob,The minimum probability (in percents) of reaching a source block for interblock speculative scheduling.,1,5,workload-specific,gcc
2384,min-vect-loop-bound,The minimum number of iterations under which loops are not vectorized when -ftree-vectorize is used. The number of iterations after vectorization needs to be greater than the value specified by this option to allow vectorization.,1,5,workload-specific,gcc
2386,-MMD,"Like -MD except mention only user header files, not system header files.",0,0,others,gcc
2387,-Mno-modules,Disable dependency generation for compiled module interfaces.,0,0,others,gcc
2388,modref-max-accesses,"Specifies the maximal number of base pointers, references and accesses stored for a single function by mod/ref analysis.",1,4,limited-side-effect,gcc
2389,modref-max-depth,Specifies the maximum depth of DFS walk used by modref escape analysis. Setting to 0 disables the analysis completely.,1,4,limited-side-effect,gcc
2390,modref-max-escape-points,Specifies the maximum number of escape points tracked by modref per SSA-name.,1,4,limited-side-effect,gcc
2392,module,"Dump module information. Options lineno (locations), graph (reachability), blocks (clusters), uid (serialization), alias (mergeable), asmname (Elrond), eh (mapper) & vops (macros) may provide additional information. This option is applicable to C++ only.",1,6,function-tradeoff,gcc
2393,-MP,"This option instructs CPP to add a phony target for each dependency other than the main file, causing each to depend on nothing. These dummy rules work around errors make gives if you remove header files without updating the Makefile to match.",0,0,others,gcc
2394,-MQ target,"Same as -MT, but it quotes any characters which are special to Make. -MQ'$(objpfx)foo.o' gives",0,0,others,gcc
2395,-MT target,"Change the target of the rule emitted by dependency generation. By default CPP takes the name of the main input file, deletes any directory components and any file suffix such as c and appends the platform usual object suffix. The result is the target.",0,0,others,gcc
2396,-no-canonical-prefixes,"Do not expand any symbolic links, resolve references to /../ or /./, or make the path absolute when generating a relative prefix.",0,0,others,gcc
2397,-nodefaultlibs,"Do not use the standard system libraries when linking. Only the libraries you specify are passed to the linker, and options specifying linkage of the system libraries, such as -static-libgcc or -shared-libgcc, are ignored. The standard startup files are used normally, unless -nostartfiles is used.",0,0,others,gcc
2398,-no-integrated-cpp,"Perform preprocessing as a separate pass before compilation. By default, GCC performs preprocessing as an integrated part of input tokenization and parsing. If this option is provided, the appropriate language front end (cc1, cc1plus, or cc1obj for C, C++, and Objective-C, respectively) is instead invoked twice, once for preprocessing only and once for actual compilation of the preprocessed input. This option may be useful in conjunction with the -B or -wrapper options to specify an alternate preprocessor or perform additional processing of the program source between normal preprocessing and compilation.",0,0,others,gcc
2399,-nolibc,"Do not use the C library or system libraries tightly coupled with it when linking. Still link with the startup files, libgcc or toolchain provided language support libraries such as libgnat, libgfortran or libstdc++ unless options preventing their inclusion are used as well. This typically removes -lc from the link command line, as well as system libraries that normally go with it and become meaningless when absence of a C library is assumed, for example -lpthread or -lm in some configurations. This is intended for bare-board targets when there is indeed no C library available.",0,0,others,gcc
2400,-no-pie,Don't produce a dynamically linked position independent executable.,0,0,others,gcc
2401,-nostartfiles,"Do not use the standard system startup files when linking. The standard system libraries are used normally, unless -nostdlib, -nolibc, or -nodefaultlibs is used.",0,0,others,gcc
2402,-nostdinc,"Do not search the standard system directories for header files. Only the directories explicitly specified with -I, -iquote, -isystem, and/or -idirafter options (and the directory of the current file, if appropriate) are searched.",0,0,others,gcc
2405,--no-sysroot-suffix,"For some targets, a suffix is added to the root directory specified with --sysroot, depending on the other options used, so that headers may for example be found in dir/suffix/usr/include instead of dir/usr/include. This option disables the addition of such a suffix.",0,0,others,gcc
2406,-o file,"Place the primary output in file file. This applies to whatever sort of output is being produced, whether it be an executable file, an object file, an assembler file or preprocessed C code.",0,0,others,gcc
2408,-O1,"Optimize. Optimizing compilation takes somewhat more time, and a lot more memory for a large function.",1,4,limited-side-effect,gcc
2409,-O2,"Optimize even more. GCC performs nearly all supported optimizations that do not involve a space-speed tradeoff. As compared to -O, this option increases both compilation time and the performance of the generated code.",1,4,limited-side-effect,gcc
2410,-O3,Optimize yet more. -O3 turns on all optimizations specified by -O2 and also turns on the following optimization flags:,1,4,limited-side-effect,gcc
2411,object-file-name,"A file name that does not end in a special recognized suffix is considered to name an object file or library. (Object files are distinguished from libraries by the linker according to the file contents.) If linking is done, these object files are used as input to the linker.",0,0,others,gcc
2412,-Ofast,"Disregard strict standards compliance. -Ofast enables all -O3 optimizations. It also enables optimizations that are not valid for all standard-compliant programs. It turns on -ffast-math, -fallow-store-data-races and the Fortran-specific -fstack-arrays, unless -fmax-stack-var-size is specified, and -fno-protect-parens.",1,4,limited-side-effect,gcc
2413,-Og,"Optimize debugging experience. -Og should be the optimization level of choice for the standard edit-compile-debug cycle, offering a reasonable level of optimization while maintaining fast compilation and a good debugging experience. It is a better choice than -O0 for producing debuggable code because some compiler passes that collect debug information are disabled at -O0.",1,4,limited-side-effect,gcc
2414,-Os,Optimize for size. -Os enables all -O2 optimizations except those that often increase code size:,1,4,limited-side-effect,gcc
2415,-P,"Inhibit generation of linemarkers in the output from the preprocessor. This might be useful when running the preprocessor on something that is not C code, and will be sent to a program which might be confused by the linemarkers.",0,0,others,gcc
2416,--param name=value,"In some places, GCC uses various constants to control the amount of optimization that is done. For example, GCC does not inline functions that contain more than a certain number of instructions. You can control some of these constants on the command line using the --param option.",1,4,limited-side-effect,gcc
2417,params,Display the values recognized by the --param option.,1,6,function-tradeoff,gcc
2418,parloops-chunk-size,Chunk size of omp schedule for loops parallelized by parloops.,1,4,limited-side-effect,gcc
2419,parloops-min-per-thread,The minimum number of iterations per thread of an innermost parallelized loop for which the parallelized variant is preferred over the single threaded one. Note that for a parallelized loop nest the minimum number of iterations of the outermost loop per thread is two.,1,4,limited-side-effect,gcc
2420,parloops-schedule,"Schedule type of omp schedule for loops parallelized by parloops (static, dynamic, guided, auto, runtime).",1,4,limited-side-effect,gcc
2421,partial-inlining-entry-probability,Maximum probability of the entry BB of split region (in percent relative to entry BB of the function) to make partial inlining happen.,1,4,limited-side-effect,gcc
2422,-pass-exit-codes,"Normally the gcc program exits with the code of 1 if any phase of the compiler returns a non-success return code. If you specify -pass-exit-codes, the gcc program instead returns with the numerically highest error produced by any phase returning an error indication. The C, C++, and Fortran front ends return 4 if an internal compiler error is encountered.",0,0,others,gcc
2423,path=,"SGR substring for colorizing paths of control-flow events as printed via -fdiagnostics-path-format=, such as the identifiers of individual events and lines indicating interprocedural calls and returns.",0,0,others,gcc
2424,-pedantic,"Issue all the warnings demanded by strict ISO C and ISO C++; reject all programs that use forbidden extensions, and some other programs that do not follow ISO C and ISO C++. For ISO C, follows the version of the ISO C standard specified by any -std option used.",0,0,others,gcc
2425,-pedantic-errors,"Give an error whenever the base standard (see -Wpedantic) requires a diagnostic, in some cases where there is undefined behavior at compile-time and in some other cases that do not prevent compilation of programs that are valid according to the standard. This is not equivalent to -Werror=pedantic, since there are errors enabled by this option and not enabled by the latter and vice versa.",0,0,others,gcc
2426,-pg,"Generate extra code to write profile information suitable for the analysis program prof (for -p) or gprof (for -pg). You must use this option when compiling the source files you want data about, and you must also use it when linking.",0,0,others,gcc
2427,-pie,"Produce a dynamically linked position independent executable on targets that support it. For predictable results, you must also specify the same set of options used for compilation (-fpie, -fPIE, or model suboptions) when you specify this linker option.",0,0,others,gcc
2428,-pipe,Use pipes rather than temporary files for communication between the various stages of compilation. This fails to work on some systems where the assembler is unable to read from a pipe; but the GNU assembler has no trouble.,0,0,others,gcc
2429,predictable-branch-outcome,"When branch is predicted to be taken with probability lower than this threshold (in percent), then it is considered well predictable.",1,5,workload-specific,gcc
2431,prefetch-latency,Estimate on average number of instructions that are executed before prefetch finishes. The distance prefetched ahead is proportional to this constant. Increasing this number may also lead to less streams being prefetched (see simultaneous-prefetches).,1,4,limited-side-effect,gcc
2432,prefetch-minimum-stride,"Minimum constant stride, in bytes, to start using prefetch hints for. If the stride is less than this threshold, prefetch hints will not be issued.",1,5,workload-specific,gcc
2433,prefetch-min-insn-to-mem-ratio,The minimum ratio between the number of instructions and the number of memory references to enable prefetching in a loop.,1,4,limited-side-effect,gcc
2434,-print-file-name=library,"Print the full absolute name of the library file library that would be used when linking and don't do anything else. With this option, GCC does not compile or link anything; it just prints the file name.",1,6,function-tradeoff,gcc
2435,-print-libgcc-file-name,Same as -print-file-name=libgcc.a.,1,6,function-tradeoff,gcc
2436,-print-multiarch,"Print the path to OS libraries for the selected multiarch, relative to some lib subdirectory.",1,6,function-tradeoff,gcc
2437,-print-multi-directory,Print the directory name corresponding to the multilib selected by any other switches present in the command line. This directory is supposed to exist in GCC_EXEC_PREFIX.,1,6,function-tradeoff,gcc
2439,-print-multi-os-directory,"Print the path to OS libraries for the selected multilib, relative to some lib subdirectory. If OS libraries are present in the lib subdirectory and no multilibs are used, this is usually just ., if OS libraries are present in libsuffix sibling directories this prints e.g. ../lib64, ../lib or ../lib32, or if OS libraries are present in lib/subdir subdirectories it prints e.g. amd64, sparcv9 or ev6.",1,6,function-tradeoff,gcc
2440,-print-objc-runtime-info,"Generate C header describing the largest structure that is passed by value, if any.",0,0,others,gcc
2441,-print-prog-name=program,"Like -print-file-name, but searches for a program such as cpp.",1,6,function-tradeoff,gcc
2442,-print-search-dirs,Print the name of the configured installation directory and a list of program and library directories gcc searches and don't do anything else.,1,6,function-tradeoff,gcc
2443,-print-sysroot,"Print the target sysroot directory that is used during compilation. This is the target sysroot specified either at configure time or using the --sysroot option, possibly with an extra suffix that depends on compilation options. If no target sysroot is specified, the option prints nothing.",1,6,function-tradeoff,gcc
2444,-print-sysroot-headers-suffix,"Print the suffix added to the target sysroot when searching for headers, or give an error if the compiler is not configured with such a suffix and don't do anything else.",1,6,function-tradeoff,gcc
2445,profile-func-internal-id,"A parameter to control whether to use function internal id in profile database lookup. If the value is 0, the compiler uses an id that is based on function assembler name and filename, which makes old profile data more tolerant to source changes such as function reordering etc.",1,4,limited-side-effect,gcc
2447,-Q,"Makes the compiler print out each function name as it is compiled, and print some statistics about each pass when it finishes.",1,6,function-tradeoff,gcc
2448,quote=,SGR substring for information printed within quotes.,0,0,others,gcc
2449,-r,Produce a relocatable object as output. This is also known as partial linking.,1,6,function-tradeoff,gcc
2450,range1=,SGR substring for first additional range.,0,0,others,gcc
2451,range2=,SGR substring for second additional range.,0,0,others,gcc
2453,-remap,"Enable special code to work around file systems which only permit very short file names, such as MS-DOS.",0,0,others,gcc
2454,rpo-vn-max-loop-depth,Maximum loop depth that is value-numbered optimistically. When the limit hits the innermost rpo-vn-max-loop-depth loops and the outermost loop in the loop nest are value-numbered optimistically and the remaining ones not.,1,5,workload-specific,gcc
2456,-S(capital),Stop after the stage of compilation proper; do not assemble. The output is in the form of an assembler code file for each non-assembler input file specified.,0,0,others,gcc
2457,-save-temps,"Store the usual temporary intermediate files permanently; name them as auxiliary output files, as specified described under -dumpbase and -dumpdir.",0,0,others,gcc
2458,-save-temps=cwd,Equivalent to -save-temps -dumpdir ./.,0,0,others,gcc
2459,-save-temps=obj,"Equivalent to -save-temps -dumpdir outdir/, where outdir/ is the directory of the output file specified after the -o option, including any directory separators. If the -o option is not used, the -save-temps=obj switch behaves like -save-temps=cwd.",0,0,others,gcc
2460,sccvn-max-alias-queries-per-access,Maximum number of alias-oracle queries we perform when looking for redundancies for loads and stores. If this limit is hit the search is aborted and the load or store is not considered redundant. The number of queries is algorithmically limited to the number of stores on all paths from the load to the function entry.,1,4,limited-side-effect,gcc
2461,scev,Enable showing scalar evolution analysis details.,1,6,function-tradeoff,gcc
2462,scev-max-expr-complexity,Bound on the complexity of the expressions in the scalar evolutions analyzer. Complex expressions slow the analyzer.,1,5,workload-specific,gcc
2464,sched-autopref-queue-depth,Hardware autoprefetcher scheduler model control flag. Number of lookahead cycles the model looks into; at only enable instruction sorting heuristic.,1,4,limited-side-effect,gcc
2465,sched-mem-true-dep-cost,Minimal distance (in CPU cycles) between store and load targeting same memory locations.,1,4,limited-side-effect,gcc
2466,sched-pressure-algorithm,Choose between the two available implementations of -fsched-pressure. Algorithm 1 is the original implementation and is the more likely to prevent instructions from being reordered. Algorithm 2 was designed to be a compromise between the relatively conservative approach taken by algorithm 1 and the rather aggressive approach taken by the default scheduler. It relies more heavily on having a regular register file and accurate register pressure classes. See haifa-sched.c in the GCC sources for more details.,1,4,limited-side-effect,gcc
2467,sched-spec-prob-cutoff,"The minimal probability of speculation success (in percents), so that speculative insns are scheduled.",1,5,workload-specific,gcc
2468,sched-state-edge-prob-cutoff,The minimum probability an edge must have for the scheduler to save its state across it.,1,5,workload-specific,gcc
2471,selsched-max-sched-times,The maximum number of times that an instruction is scheduled during selective scheduling. This is the limit on the number of iterations through which the instruction may be pipelined.,1,4,limited-side-effect,gcc
2472,separate,"Display options taking an argument that appears as a separate word following the original option, such as: -o output-file.",0,0,others,gcc
2473,-shared,"On systems that support dynamic linking, this overrides -pie and prevents linking with the shared libraries. On other systems, this option has no effect.",0,0,others,gcc
2474,-shared-libgcc,"Produce a shared object which can then be linked with other objects to form an executable. Not all systems support this option. For predictable results, you must also specify the same set of options used for compilation (-fpic, -fPIC, or model suboptions) when you specify this linker option.1",0,0,others,gcc
2476,sink-frequency-threshold,The maximum relative execution frequency (in percents) of the target block relative to a statement original block to allow statement sinking of a statement. Larger numbers result in more aggressive statement sinking. A small positive adjustment is applied for statements with memory operands as those are even more profitable so sink.,1,1,resource,gcc
2477,sms-dfa-history,The number of cycles the swing modulo scheduler considers when checking conflicts using DFA.,1,4,limited-side-effect,gcc
2478,sms-loop-average-count-threshold,A threshold on the average loop count considered by the swing modulo scheduler.,1,5,workload-specific,gcc
2481,-specs=file,"Process file after the compiler reads in the standard specs file, in order to override the defaults which the gcc driver program uses when determining what switches to pass to cc1, cc1plus, as, ld, etc. More than one -specs=file can be specified on the command line, and they are processed in order, from left to right. See Spec Files, for information about the format of the file.",0,0,others,gcc
2482,sra-max-propagations,"The maximum number of artificial accesses that Scalar Replacement of Aggregates (SRA) will track, per one local variable, in order to facilitate copy propagation.",1,4,limited-side-effect,gcc
2483,sra-max-scalarization-size-Osize,"Maximum size, in storage units, of an aggregate which should be considered for scalarization when compiling for size.",1,5,workload-specific,gcc
2484,ssa-name-def-chain-limit,The maximum number of SSA_NAME assignments to follow in determining a property of a variable such as its value. This limits the number of iterations or recursive calls GCC performs when optimizing certain statements or when determining their validity prior to issuing diagnostics.,1,5,workload-specific,gcc
2485,ssp-buffer-size,The minimum size of buffers (i.e. arrays) that receive stack smashing protection when -fstack-protection is used.,1,4,limited-side-effect,gcc
2486,stack-clash-protection-guard-size,"Specify the size of the operating system provided stack guard as 2 raised to num bytes. Higher values may reduce the number of explicit probes, but a value larger than the operating system provided guard will leave code vulnerable to stack clash style attacks.",1,4,limited-side-effect,gcc
2488,-static-libasan,"When the -fsanitize=address option is used to link a program, the GCC driver automatically links against libasan. If libasan is available as a shared library, and the -static option is not used, then this links against the shared version of libasan. The -static-libasan option directs the GCC driver to link libasan statically, without necessarily linking other libraries statically.",0,0,others,gcc
2489,-static-libgcc,"On systems that provide libgcc as a shared library, these options force the use of either the shared or static version, respectively. If no shared version of libgcc was built when the compiler was configured, these options have no effect.",0,0,others,gcc
2490,-static-liblsan,"When the -fsanitize=leak option is used to link a program, the GCC driver automatically links against liblsan. If liblsan is available as a shared library, and the -static option is not used, then this links against the shared version of liblsan. The -static-liblsan option directs the GCC driver to link liblsan statically, without necessarily linking other libraries statically.",0,0,others,gcc
2491,-static-libstdc++,"When the g++ program is used to link a C++ program, it normally automatically links against libstdc++. If libstdc++ is available as a shared library, and the -static option is not used, then this links against the shared version of libstdc++. That is normally fine. However, it is sometimes useful to freeze the version of libstdc++ used by the program without going all the way to a fully static link. The -static-libstdc++ option directs the g++ driver to link libstdc++ statically, without necessarily linking other libraries statically.",0,0,others,gcc
2492,-static-libtsan,"When the -fsanitize=thread option is used to link a program, the GCC driver automatically links against libtsan. If libtsan is available as a shared library, and the -static option is not used, then this links against the shared version of libtsan. The -static-libtsan option directs the GCC driver to link libtsan statically, without necessarily linking other libraries statically.",0,0,others,gcc
2493,-static-libubsan,"When the -fsanitize=undefined option is used to link a program, the GCC driver automatically links against libubsan. If libubsan is available as a shared library, and the -static option is not used, then this links against the shared version of libubsan. The -static-libubsan option directs the GCC driver to link libubsan statically, without necessarily linking other libraries statically.",0,0,others,gcc
2494,-static-pie,"Produce a static position independent executable on targets that support it. A static position independent executable is similar to a static executable, but can be loaded at any address without a dynamic linker. For predictable results, you must also specify the same set of options used for compilation (-fpie, -fPIE, or model suboptions) when you specify this linker option.",0,0,others,gcc
2495,stats,Enable dumping various statistics about the pass (not honored by every dump option).,1,6,function-tradeoff,gcc
2499,store-merging-max-size,Maximum size of a single store merging region in bytes.,1,1,resource,gcc
2500,switch-conversion-max-branch-ratio,Switch initialization conversion refuses to create arrays that are bigger than switch-conversion-max-branch-ratio times the number of branches in the switch.,1,4,limited-side-effect,gcc
2502,--sysroot=dir,"Use dir as the logical root directory for headers and libraries. For example, if the compiler normally searches for headers in /usr/include and libraries in /usr/lib, it instead searches dir/usr/include and dir/usr/lib.",0,0,others,gcc
2503,-T script,"Use script as the linker script. This option is supported by most systems using the GNU linker. On some targets, such as bare-board targets without an operating system, the -T option may be required when linking to avoid references to undefined symbols.",0,0,others,gcc
2505,--target-help,Print (on the standard output) a description of target-specific command-line options for each tool. For some targets extra target-specific information may also be printed.,1,6,function-tradeoff,gcc
2506,-time[=file],"Report the CPU time taken by each subprocess in the compilation sequence. For C source files, this is the compiler proper and assembler (plus the linker if linking is done).",0,0,others,gcc
2508,tracer-dynamic-coverage-feedback,"The percentage of function, weighted by execution frequency, that must be covered by trace formation. Used when profile feedback is available.",1,4,limited-side-effect,gcc
2509,tracer-max-code-growth,"Stop tail duplication once code growth has reached given percentage. This is a rather artificial limit, as most of the duplicates are eliminated later in cross jumping, so it may be set to much higher values than is the desired code growth.",1,4,limited-side-effect,gcc
2511,tracer-min-branch-ratio,Stop reverse growth when the reverse probability of best edge is less than this threshold (in percent).,1,5,workload-specific,gcc
2512,-traditional-cpp,"Try to imitate the behavior of pre-standard C preprocessors, as opposed to ISO C preprocessors. See the GNU CPP manual for details.",0,0,others,gcc
2513,tree-reassoc-width,Set the maximum number of instructions executed in parallel in reassociated tree. This parameter overrides target dependent heuristics used by default if has non zero value.,1,5,workload-specific,gcc
2514,-trigraphs,"Support ISO C trigraphs. These are three-character sequences, all starting with ? that are defined by ISO C to stand for single characters. For example, ?/stands for \ so ??/n'is a character constant for a newline.",0,0,others,gcc
2515,tsan-distinguish-volatile,Emit special instrumentation for accesses to volatiles.,1,6,function-tradeoff,gcc
2516,tsan-instrument-func-entry-exit,Emit instrumentation calls to __tsan_func_entry() and __tsan_func_exit().,1,6,function-tradeoff,gcc
2517,type-diff=,SGR substring for highlighting mismatching types within template arguments in the C++ frontend.,0,0,others,gcc
2518,-U name,"Cancel any previous definition of name, either built in or provided with a -D option.",0,0,others,gcc
2519,-u symbol,"Pretend the symbol symbol is undefined, to force linking of library modules to define it. You can use -u multiple times with different symbols to force loading of additional library modules.",0,0,others,gcc
2520,uid,Enable showing the unique ID (DECL_UID) for each variable.,0,0,others,gcc
2521,-undef,Do not predefine any system-specific or GCC-specific macros. The standard predefined macros remain defined.,0,0,others,gcc
2522,undocumented,Display only those options that are undocumented.,1,6,function-tradeoff,gcc
2523,uninit-control-dep-attempts,Maximum number of nested calls to search for control dependencies during uninitialized variable analysis.,1,5,workload-specific,gcc
2525,uninlined-function-time,Extra time accounted by inliner for function overhead such as time needed to execute function prologue and epilogue,1,4,limited-side-effect,gcc
2527,unlikely-bb-count-fraction,"The denominator n of fraction 1/n of the number of profiled runs of the entire program below which the execution count of a basic block must be in order for the basic block to be considered unlikely executed. The default is 20, which means that a basic block is considered unlikely executed if it is executed in fewer than 1/20, or 5%, of the runs of the program. 0 means that it is always considered unlikely executed.",1,4,limited-side-effect,gcc
2528,unroll-jam-max-unroll,The maximum number of times the outer loop should be unrolled by the unroll-and-jam transformation.,1,5,workload-specific,gcc
2529,unroll-jam-min-percent,The minimum percentage of memory references that must be optimized away for the unroll-and-jam transformation to be considered profitable.,1,5,workload-specific,gcc
2530,use-after-scope-direct-emission-threshold,"If the size of a local variable in bytes is smaller or equal to this number, directly poison (or unpoison) shadow memory instead of using run-time callbacks.",1,4,limited-side-effect,gcc
2531,use-canonical-types,"Whether the compiler should use the anonicaltype system. Should always be 1, which uses a more efficient internal mechanism for comparing types in C++ and Objective-C++. However, if bugs in the canonical type system are causing compilation failures, set this value to 0 to disable canonical types.",1,4,limited-side-effect,gcc
2532,-v,Print (on standard error output) the commands executed to run the stages of compilation. Also print the version number of the compiler driver program and of the preprocessor and the compiler proper.,1,6,function-tradeoff,gcc
2533,vec,Enable dumps from all vectorization optimizations.,1,6,function-tradeoff,gcc
2534,vect-epilogues-nomask,Enable loop epilogue vectorization using smaller vector size.,1,4,limited-side-effect,gcc
2535,vect-max-peeling-for-alignment,The maximum number of loop peels to enhance access alignment for vectorizer. Value -1 means no limit.,1,4,limited-side-effect,gcc
2536,vect-max-version-for-alias-checks,The maximum number of run-time checks that can be performed when doing loop versioning for alias in the vectorizer.,1,5,workload-specific,gcc
2537,vect-max-version-for-alignment-checks,The maximum number of run-time checks that can be performed when doing loop versioning for alignment in the vectorizer.,1,5,workload-specific,gcc
2539,verbose,Enable showing the tree dump for each statement.,1,6,function-tradeoff,gcc
2540,--version,Display the version number and copyrights of the invoked GCC.,0,0,others,gcc
2541,vops,Enable showing virtual operands for every statement.,0,0,others,gcc
2542,-w,Inhibit all warning messages.,1,6,function-tradeoff,gcc
2543,"-Wa,option","Pass option as an option to the assembler. If option contains commas, it is split into multiple options at the commas.",0,0,others,gcc
2544,"-Wabi (C, Objective-C, C++ and Objective-C++ only)",Warn about code affected by ABI changes. This includes code that may not be compatible with the vendor-neutral C++ ABI as well as the psABI for the particular target.,1,6,function-tradeoff,gcc
2545,-Wabi-tag (C++ and Objective-C++ only),Warn when a type with an ABI tag is used in a context that does not have that ABI tag. See C++ Attributes for more information about ABI tags.,1,6,function-tradeoff,gcc
2546,-Wabsolute-value (C and Objective-C only),"Warn for calls to standard functions that compute the absolute value of an argument when a more appropriate standard function is available. For example, calling abs(3.14) triggers the warning because the appropriate function to call to compute the absolute value of a double argument is fabs. The option also triggers warnings when the argument in a call to such a function has an unsigned type. This warning can be suppressed with an explicit type cast and it is also enabled by -Wextra.",1,6,function-tradeoff,gcc
2547,-Waddress,"Warn about suspicious uses of memory addresses. These include using the address of a function in a conditional expression, such as void func(void); if (func), and comparisons against the memory address of a string literal, such as if (x == ""abc""). Such uses typically indicate a programmer error: the address of a function always evaluates to true, so their use in a conditional usually indicate that the programmer forgot the parentheses in a function call; and comparisons against string literals result in unspecified behavior and are not portable in C, so they usually indicate that the programmer intended to use strcmp. This warning is enabled by -Wall.",1,6,function-tradeoff,gcc
2548,-Waggregate-return,"Warn if any functions that return structures or unions are defined or called. (In languages where you can return an array, this also elicits a warning.)",0,0,others,gcc
2549,-Waligned-new,Warn about a new-expression of a type that requires greater alignment than the alignof(std::max_align_t) but uses an allocation function without an explicit alignment parameter. This option is enabled by -Wall.,1,6,function-tradeoff,gcc
2552,-Walloca-larger-than=byte-size,"This option warns on calls to alloca with an integer argument whose value is either zero, or that is not bounded by a controlling predicate that limits its value to at most byte-size. It also warns for calls to alloca where the bound value is unknown. Arguments of non-integer types are considered unbounded even if they appear to be constrained to the expected range.",0,0,others,gcc
2553,-Walloc-size-larger-than=byte-size,"Warn about calls to functions decorated with attribute alloc_size that attempt to allocate objects larger than the specified number of bytes, or where the result of the size computation in an integer type with infinite precision would exceed the value of PTRDIFF_MAXon the target. -Walloc-size-larger-than=PTRDIFF_MAXis enabled by default. Warnings controlled by the option can be disabled either by specifying byte-size of SIZE_MAXor more or by -Wno-alloc-size-larger-than. See Function Attributes.",1,6,function-tradeoff,gcc
2554,-Walloc-zero,"Warn about calls to allocation functions decorated with attribute alloc_size that specify zero bytes, including those to the built-in forms of the functions aligned_alloc, alloca, calloc, malloc, and realloc. Because the behavior of these functions when called with a zero size differs among implementations (and in the case of realloc has been deprecated) relying on it may result in subtle portability bugs and should be avoided.",1,6,function-tradeoff,gcc
2555,-Warith-conversion,"Do warn about implicit conversions from arithmetic operations even when conversion of the operands to the same type cannot change their values. This affects warnings from -Wconversion, -Wfloat-conversion, and -Wsign-conversion.",1,6,function-tradeoff,gcc
2557,-Warray-bounds=2,This warning level also warns about out of bounds access for arrays at the end of a struct and for arrays accessed through pointers. This warning level may give a larger number of false positives and is deactivated by default.,0,0,others,gcc
2558,-Warray-bounds=n,This option is only active when -ftree-vrp is active (default for -O2 and above). It warns about subscripts to arrays that are always out of bounds. This warning is enabled by -Wall.,0,0,others,gcc
2560,-Wassign-intercept (Objective-C and Objective-C++ only),Warn whenever an Objective-C assignment is being intercepted by the garbage collector.,1,6,function-tradeoff,gcc
2561,-Wattribute-alias=1,The default warning level of the -Wattribute-alias option diagnoses incompatibilities between the type of the alias declaration and that of its target. Such incompatibilities are typically indicative of bugs.,0,0,others,gcc
2562,-Wattribute-alias=2,"At this level -Wattribute-alias also diagnoses cases where the attributes of the alias declaration are more restrictive than the attributes applied to its target. These mismatches can potentially result in incorrect code generation. In other cases they may be benign and could be resolved simply by adding the missing attribute to the target. For comparison, see the -Wmissing-attributes option, which controls diagnostics when the alias declaration is less restrictive than the target, rather than more restrictive.",0,0,others,gcc
2565,-Wbool-operation,"Warn about suspicious operations on expressions of a boolean type. For instance, bitwise negation of a boolean is very likely a bug in the program. For C, this warning also warns about incrementing or decrementing a boolean, which rarely makes sense. (In C++, decrementing a boolean is always invalid. Incrementing a boolean is invalid in C++17, and deprecated otherwise.)",0,0,others,gcc
2566,-Wc++11-compat (C++ and Objective-C++ only),"Warn about C++ constructs whose meaning differs between ISO C++ 1998 and ISO C++ 2011, e.g., identifiers in ISO C++ 1998 that are keywords in ISO C++ 2011. This warning turns on -Wnarrowing and is enabled by -Wall.",1,6,function-tradeoff,gcc
2568,-Wc++17-compat (C++ and Objective-C++ only),Warn about C++ constructs whose meaning differs between ISO C++ 2014 and ISO C++ 2017. This warning is enabled by -Wall.,1,6,function-tradeoff,gcc
2569,-Wc++20-compat (C++ and Objective-C++ only),Warn about C++ constructs whose meaning differs between ISO C++ 2017 and ISO C++ 2020. This warning is enabled by -Wall.,1,6,function-tradeoff,gcc
2570,-Wc++-compat (C and Objective-C only),"Warn about ISO C constructs that are outside of the common subset of ISO C and ISO C++, e.g. request for implicit conversion from void * to a pointer to non-void type.",1,6,function-tradeoff,gcc
2571,-Wc11-c2x-compat (C and Objective-C only),"Warn about features not present in ISO C11, but present in ISO C2X. For instance, warn about omitting the string in _Static_assert, use of [[]]syntax for attributes, use of decimal floating-point types, and so on. This option is independent of the standards mode. Warnings are disabled in the expression that follows __extension__.",1,6,function-tradeoff,gcc
2573,-Wc99-c11-compat (C and Objective-C only),"Warn about features not present in ISO C99, but present in ISO C11. For instance, warn about use of anonymous structures and unions, _Atomic type qualifier, _Thread_local storage-class specifier, _Alignas specifier, Alignof operator, _Generic keyword, and so on. This option is independent of the standards mode. Warnings are disabled in the expression that follows __extension__.",1,6,function-tradeoff,gcc
2574,-Wcast-align,"Warn whenever a pointer is cast such that the required alignment of the target is increased. For example, warn if a char * is cast to an int * on machines where integers can only be accessed at two- or four-byte boundaries.",0,0,others,gcc
2575,-Wcast-align=strict,"Warn whenever a pointer is cast such that the required alignment of the target is increased. For example, warn if a char * is cast to an int * regardless of the target machine.",1,6,function-tradeoff,gcc
2577,-Wcast-qual,"Warn whenever a pointer is cast so as to remove a type qualifier from the target type. For example, warn if a const char * is cast to an ordinary char *.",1,6,function-tradeoff,gcc
2578,-Wcatch-value=n (C++ and Objective-C++ only),Warn about catch handlers that do not catch via reference. With -Wcatch-value=1 (or -Wcatch-value for short) warn about polymorphic class types that are caught by value. With -Wcatch-value=2 warn about all class types that are caught by value. With -Wcatch-value=3 warn about all types that are not caught by reference. -Wcatch-value is enabled by -Wall.,1,6,function-tradeoff,gcc
2579,-Wchar-subscripts,"Warn if an array subscript has type char. This is a common cause of error, as programmers often forget that this type is signed on some machines. This warning is enabled by -Wall.",1,6,function-tradeoff,gcc
2581,-Wclobbered,Warn for variables that might be changed by longjmp or vfork. This warning is also enabled by -Wextra.,1,6,function-tradeoff,gcc
2582,-Wcomma-subscript (C++ and Objective-C++ only),"Warn about uses of a comma expression within a subscripting expression. This usage was deprecated in C++20. However, a comma expression wrapped in ( ) is not deprecated. Example:",0,0,others,gcc
2583,-Wcomments,"Warn whenever a comment-start sequence *appears in a *comment, or whenever a backslash-newline appears in a /comment. This warning is enabled by -Wall.",1,6,function-tradeoff,gcc
2586,-Wctad-maybe-unsupported (C++ and Objective-C++ only),"Warn when performing class template argument deduction (CTAD) on a type with no explicitly written deduction guides. This warning will point out cases where CTAD succeeded only because the compiler synthesized the implicit deduction guides, which might not be what the programmer intended. Certain style guides allow CTAD only on types that specifically ""opt-in""; i.e., on types that are designed to support CTAD. This warning can be suppressed with the following pattern:",0,0,others,gcc
2587,-Wctor-dtor-privacy (C++ and Objective-C++ only),"Warn when a class seems unusable because all the constructors or destructors in that class are private, and it has neither friends nor public static member functions. Also warn if there are no non-private methods, and there's at least one private member function that isn't a constructor or destructor.",1,6,function-tradeoff,gcc
2588,-Wdangling-else,Warn about constructions where there may be confusion to which if statement an else branch belongs. Here is an example of such a case:,0,0,others,gcc
2589,-Wdate-time,"Warn when macros __TIME__, __DATE__ or __TIMESTAMP__ are encountered as they might prevent bit-wise-identical reproducible compilations.",1,6,function-tradeoff,gcc
2591,-Wdelete-non-virtual-dtor (C++ and Objective-C++ only),Warn when delete is used to destroy an instance of a class that has virtual functions and non-virtual destructor. It is unsafe to delete an instance of a derived class through a pointer to a base class if the base class does not have a virtual destructor. This warning is enabled by -Wall.,1,6,function-tradeoff,gcc
2592,-Wdeprecated-copy (C++ and Objective-C++ only),"Warn that the implicit declaration of a copy constructor or copy assignment operator is deprecated if the class has a user-provided copy constructor or copy assignment operator, in C++11 and up. This warning is enabled by -Wextra. With -Wdeprecated-copy-dtor, also deprecate if the class has a user-provided destructor.",0,0,others,gcc
2593,-Wdisabled-optimization,"Warn if a requested optimization pass is disabled. This warning does not generally indicate that there is anything wrong with your code; it merely indicates that GCC optimizers are unable to handle the code effectively. Often, the problem is that your code is too big or too complex; GCC refuses to optimize programs when the optimization itself is likely to take inordinate amounts of time.",1,6,function-tradeoff,gcc
2595,-Wduplicated-branches,Warn when an if-else has identical branches. This warning detects cases like,1,6,function-tradeoff,gcc
2596,-Wduplicated-cond,"Warn about duplicated conditions in an if-else-if chain. For instance, warn for the following code:",1,6,function-tradeoff,gcc
2598,-Weffc++ (C++ and Objective-C++ only),Warn about violations of the following style guidelines from Scott MeyersEffective C++ series of books:,1,6,function-tradeoff,gcc
2599,-Wempty-body,"Warn if an empty body occurs in an if, else or do while statement. This warning is also enabled by -Wextra.",1,6,function-tradeoff,gcc
2600,-Wenum-compare,Warn about a comparison between values of different enumerated types. In C++ enumerated type mismatches in conditional expressions are also diagnosed and the warning is enabled by default. In C this warning is enabled by -Wall.,1,6,function-tradeoff,gcc
2601,-Wenum-conversion,Warn when a value of enumerated type is implicitly converted to a different enumerated type. This warning is enabled by -Wextra in C.,1,6,function-tradeoff,gcc
2602,-Werror,Make all warnings into errors.,0,0,others,gcc
2603,-Werror=,"Make the specified warning into an error. The specifier for a warning is appended; for example -Werror=switch turns the warnings controlled by -Wswitch into errors. This switch takes a negative form, to be used to negate -Werror for specific warnings; for example -Wno-error=switch makes -Wswitch warnings not be errors, even when -Werror is in effect.",0,0,others,gcc
2604,-Wexpansion-to-defined,Warn whenever defined is encountered in the expansion of a macro (including the case where the macro is expanded by an #if directive). Such usage is not portable. This warning is also enabled by -Wpedantic and -Wextra.,0,0,others,gcc
2605,-Wextra,"This enables some extra warning flags that are not enabled by -Wall. (This option used to be called -W. The older name is still supported, but the newer name is more descriptive.)",0,0,others,gcc
2606,"-Wextra-semi (C++, Objective-C++ only)",Warn about redundant semicolons after in-class function definitions.,1,6,function-tradeoff,gcc
2607,-Wfatal-errors,This option causes the compiler to abort compilation on the first error occurred rather than trying to keep going and printing further error messages.,0,0,others,gcc
2609,-Wfloat-equal,Warn if floating-point values are used in equality comparisons.,1,6,function-tradeoff,gcc
2610,-Wformat,"Option -Wformat is equivalent to -Wformat=1, and -Wno-format is equivalent to -Wformat=0. Since -Wformat also checks for null format arguments for several functions, -Wformat also implies -Wnonnull. Some aspects of this level of format checking can be disabled by the options: -Wno-format-contains-nul, -Wno-format-extra-args, and -Wno-format-zero-length. -Wformat is enabled by -Wall.",0,0,others,gcc
2611,-Wformat=2,Enable -Wformat plus additional format checks. Currently equivalent to -Wformat -Wformat-nonliteral -Wformat-security -Wformat-y2k.,0,0,others,gcc
2612,-Wformat=n,"Check calls to printf and scanf, etc., to make sure that the arguments supplied have types appropriate to the format string specified, and that the conversions specified in the format string make sense. This includes standard functions, and others specified by format attributes (see Function Attributes), in the printf, scanf, strftime and strfmon (an X/Open extension, not in the C standard) families (or other target-specific families). Which functions are checked without format attributes having been specified depends on the standard version selected, and such checks of functions without the attribute specified are disabled by -ffreestanding or -fno-builtin.",1,6,function-tradeoff,gcc
2613,-Wformat-nonliteral,"If -Wformat is specified, also warn about uses of format functions that represent possible security problems. At present, this warns about calls to printf and scanf functions where the format string is not a string literal and there are no format arguments, as in printf (foo);. This may be a security hole if the format string came from untrusted input and contains n (This is currently a subset of what -Wformat-nonliteral warns about, but in future warnings may be added to -Wformat-security that are not included in -Wformat-nonliteral.)",1,6,function-tradeoff,gcc
2614,-Wformat-overflow,"Level 1 of -Wformat-overflow enabled by -Wformat employs a conservative approach that warns only about calls that most likely overflow the buffer. At this level, numeric arguments to format directives with unknown values are assumed to have the value of one, and strings of unknown length to be empty. Numeric arguments that are known to be bounded to a subrange of their type, or string arguments whose output is bounded either by their directive precision or by a finite set of string literals, are assumed to take on the value within the range that results in the most bytes on output. For example, the call to sprintf below is diagnosed because even with both a and b equal to zero, the terminating NUL character ('\0') appended by the function to the destination buffer will be written past its end. Increasing the size of the buffer by a single byte is sufficient to avoid the warning, though it may not be sufficient to avoid the overflow.",1,3,reliability-tradeoff,gcc
2616,-Wformat-truncation,"If -Wformat is specified, also warn about strftime formats that may yield only a two-digit year.",0,0,others,gcc
2618,-Wframe-address,Warn when the __builtin_frame_addressor __builtin_return_addressis called with an argument greater than 0. Such calls may return indeterminate values or crash the program. The warning is included in -Wall.,1,6,function-tradeoff,gcc
2620,-Wignored-qualifiers (C and C++ only),"This option controls warnings when an attribute is ignored. This is different from the -Wattributes option in that it warns whenever the compiler decides to drop an attribute, not that the attribute is either unknown, used in a wrong place, etc. This warning is enabled by default.",0,0,others,gcc
2621,-Wimplicit (C and Objective-C only),-Wimplicit-fallthrough is the same as -Wimplicit-fallthrough=3 and -Wno-implicit-fallthrough is the same as -Wimplicit-fallthrough=0.,0,0,others,gcc
2622,-Wimplicit-fallthrough,Warn when a switch case falls through. For example:,1,6,function-tradeoff,gcc
2625,-Winline,"Warn if a function that is declared as inline cannot be inlined. Even with this option, the compiler does not warn about failures to inline functions declared in system headers.",1,6,function-tradeoff,gcc
2626,-Wint-in-bool-context,"Warn for suspicious use of integer values where boolean values are expected, such as conditional expressions (?:) using non-boolean integer constants in boolean context, like if (a <= b ? 2 : 3). Or left shifting of signed integers in boolean context, like for (a = 0; 1 << a; a++);. Likewise for all kinds of multiplications regardless of the data type. This warning is enabled by -Wall.",0,0,others,gcc
2628,-Winvalid-pch,Warn if a precompiled header (see Precompiled Headers) is found in the search path but cannot be used.,0,0,others,gcc
2630,"-Wl,option","Pass option as an option to the linker. If option contains commas, it is split into multiple options at the commas. You can use this syntax to pass an argument to the option. For example, -Wl,-Map,output.map passes -Map output.map to the linker. When using the GNU linker, you can also get the same effect with -Wl,-Map=output.map.",0,0,others,gcc
2632,-Wlogical-not-parentheses,Warn about logical not used on the left hand side operand of a comparison. This option does not warn if the right operand is considered to be a boolean expression. Its purpose is to detect suspicious code like the following:,1,6,function-tradeoff,gcc
2633,-Wlogical-op,Warn about suspicious uses of logical operators in expressions. This includes using logical operators in contexts where a bit-wise operator is likely to be expected. Also warns when the operands of a logical operator are the same:,0,0,others,gcc
2634,-Wlong-long,"Warn if long long type is used. This is enabled by either -Wpedantic or -Wtraditional in ISO C90 and C++98 modes. To inhibit the warning messages, use -Wno-long-long.",1,6,function-tradeoff,gcc
2635,-Wmain,"Warn when the indentation of the code does not reflect the block structure. Specifically, a warning is issued for if, else, while, and for clauses with a guarded statement that does not use braces, followed by an unguarded statement with the same indentation.",1,6,function-tradeoff,gcc
2636,-Wmaybe-uninitialized,"For an object with automatic or allocated storage duration, if there exists a path from the function entry to a use of the object that is initialized, but there exist some other paths for which the object is not initialized, the compiler emits a warning if it cannot prove the uninitialized paths are not executed at run time.",1,5,workload-specific,gcc
2637,-Wmemset-elt-size,"Warn for suspicious calls to the memset built-in function, if the first argument references an array, and the third argument is a number equal to the number of elements, but not equal to the size of the array in memory. This indicates that the user has omitted a multiplication by the element size. This warning is enabled by -Wall.",1,6,function-tradeoff,gcc
2638,-Wmemset-transposed-args,"Warn for suspicious calls to the memset built-in function where the second argument is not zero and the third argument is zero. For example, the call memset (buf, sizeof buf, 0) is diagnosed because memset (buf, 0, sizeof buf) was meant instead. The diagnostic is only emitted if the third argument is a literal zero. Otherwise, if it is an expression that is folded to zero, or a cast of zero to some type, it is far less likely that the arguments have been mistakenly transposed and no warning is emitted. This warning is enabled by -Wall.",1,6,function-tradeoff,gcc
2639,-Wmisleading-indentation (C and C++ only),"Warn when a declaration of a function is missing one or more attributes that a related function is declared with and whose absence may adversely affect the correctness or efficiency of generated code. For example, the warning is issued for declarations of aliases that use attributes to specify less restrictive requirements than those of their targets. This typically represents a potential optimization opportunity. By contrast, the -Wattribute-alias=2 option controls warnings issued when the alias is more restrictive than the target, which could lead to incorrect code generation. Attributes considered include alloc_align, alloc_size, cold, const, hot, leaf, malloc, nonnull, noreturn, nothrow, pure, returns_nonnull, and returns_twice.",1,6,function-tradeoff,gcc
2640,-Wmismatched-tags (C++ and Objective-C++ only),"Warn for declarations of structs, classes, and class templates and their specializations with a class-key that does not match either the definition or the first declaration if no definition is provided.",1,6,function-tradeoff,gcc
2641,-Wmissing-attributes,"Warn if an aggregate or union initializer is not fully bracketed. In the following example, the initializer for a is not fully bracketed, but that for b is fully bracketed.",1,6,function-tradeoff,gcc
2642,-Wmissing-braces,Warn if a user-supplied include directory does not exist.,0,0,others,gcc
2643,-Wmissing-declarations,"Warn if a global function is defined without a previous declaration. Do so even if the definition itself provides a prototype. Use this option to detect global functions that are not declared in header files. In C, no warnings are issued for functions with previous non-prototype declarations; use -Wmissing-prototypes to detect missing prototypes. In C++, no warnings are issued for function templates, or for inline functions, or for functions in anonymous namespaces.",1,6,function-tradeoff,gcc
2645,-Wmissing-format-attribute,"Warn about function pointers that might be candidates for format attributes. Note these are only possible candidates, not absolute ones. GCC guesses that function pointers with format attributes that are used in assignment, initialization, parameter passing or return statements should have a corresponding format attribute in the resulting type. I.e. the left-hand side of the assignment or initialization, the type of the parameter variable, or the return type of the containing function respectively should also have a format attribute to avoid the warning.",1,6,function-tradeoff,gcc
2646,"-Wmissing-include-dirs (C, C++, Objective-C and Objective-C++ only)","This option controls warnings if feedback profiles are missing when using the -fprofile-use option. This option diagnoses those cases where a new function or a new file is added between compiling with -fprofile-generate and with -fprofile-use, without regenerating the profiles. In these cases, the profile feedback data files do not contain any profile feedback information for the newly added function or file respectively. Also, in the case when profile count data (.gcda) files are removed, GCC cannot use any profile feedback information. In all these cases, warnings are issued to inform you that a profile generation step is due. Ignoring the warning can result in poorly optimized code. -Wno-missing-profile can be used to disable the warning, but this is not recommended and should be done only when non-existent profile data is justified.",0,0,others,gcc
2647,-Wmissing-parameter-type (C and Objective-C only),A function parameter is declared without a type specifier in K&R-style functions:,0,0,others,gcc
2648,-Wmissing-prototypes (C and Objective-C only),Warn if a global function is defined without a previous prototype declaration. This warning is issued even if the definition itself provides a prototype. Use this option to detect global functions that do not have a matching prototype declaration in a header file. This option is not valid for C++ because all function declarations provide prototypes and a non-matching declaration declares an overload rather than conflict with an earlier declaration. Use -Wmissing-declarations to detect missing declarations in C++.,1,6,function-tradeoff,gcc
2649,-Wmultiple-inheritance (C++ and Objective-C++ only),"Warn when a class is defined with multiple direct base classes. Some coding rules disallow multiple inheritance, and this may be used to enforce that rule. The warning is inactive inside a system header file, such as the STL, so one can still use the STL. One may also define classes that indirectly use multiple inheritance.",0,0,others,gcc
2650,-Wmultistatement-macros,"Warn if parentheses are omitted in certain contexts, such as when there is an assignment in a context where a truth value is expected, or when operators are nested whose precedence people often get confused about.",0,0,others,gcc
2651,-Wnamespaces,"Warn when a namespace definition is opened. Some coding rules disallow namespaces, and this may be used to enforce that rule. The warning is inactive inside a system header file, such as the STL, so one can still use the STL. One may also use using directives and qualified names.",0,0,others,gcc
2652,-Wnested-externs (C and Objective-C only),Warn if an extern declaration is encountered within a function.,1,6,function-tradeoff,gcc
2653,-Wno-address-of-packed-member,"Do not warn when the address of packed member of struct or union is taken, which usually results in an unaligned pointer value. This is enabled by default.",0,0,others,gcc
2654,-Wno-aggressive-loop-optimizations,Warn if in a loop with constant number of iterations the compiler detects undefined behavior in some statement during one or more of the iterations.,1,6,function-tradeoff,gcc
2655,-Wno-alloca-larger-than,Disable -Walloca-larger-than= warnings. The option is equivalent to -Walloca-larger-than=SIZE_MAXor larger.,0,0,others,gcc
2656,-Wno-alloc-size-larger-than,Disable -Walloc-size-larger-than= warnings. The option is equivalent to -Walloc-size-larger-than=SIZE_MAXor larger.,0,0,others,gcc
2657,-Wno-attribute-alias,Warn about declarations using the alias and similar attributes whose target is incompatible with the type of the alias. See Declaring Attributes of Functions.,1,6,function-tradeoff,gcc
2658,-Wno-attributes,"Do not warn if an unexpected __attribute__ is used, such as unrecognized attributes, function attributes applied to variables, etc. This does not stop errors for incorrect use of supported attributes.",1,6,function-tradeoff,gcc
2659,-Wno-attribute-warning,"Do not warn about usage of functions (see Function Attributes) declared with warning attribute. By default, this warning is enabled. -Wno-attribute-warning can be used to disable the warning or -Wno-error=attribute-warning can be used to disable the error when compiled with -Werror flag.",1,6,function-tradeoff,gcc
2660,-Wno-builtin-declaration-mismatch,"Warn if a built-in function is declared with an incompatible signature or as a non-function, or when a built-in function declared with a type that does not include a prototype is called with arguments whose promoted types do not match those expected by the function. When -Wextra is specified, also warn when a built-in function that takes arguments is declared without a prototype. The -Wbuiltin-declaration-mismatch warning is enabled by default. To avoid the warning include the appropriate header to bring the prototypes of built-in functions into scope.",1,6,function-tradeoff,gcc
2661,-Wno-builtin-macro-redefined,"Do not warn if certain built-in macros are redefined. This suppresses warnings for redefinition of __TIMESTAMP__, __TIME__, __DATE__, __FILE__, and __BASE_FILE__.",1,6,function-tradeoff,gcc
2662,-Wno-class-conversion (C++ and Objective-C++ only),"Do not warn when a conversion function converts an object to the same type, to a base class of that type, or to void; such a conversion function will never be called.",1,6,function-tradeoff,gcc
2663,-Wno-conversion-null (C++ and Objective-C++ only),Do not warn for conversions between NULL and non-pointer types. -Wconversion-null is enabled by default.,1,6,function-tradeoff,gcc
2664,-Wno-coverage-mismatch,"Warn if feedback profiles do not match when using the -fprofile-use option. If a source file is changed between compiling with -fprofile-generate and with -fprofile-use, the files with the profile feedback can fail to match the source file and GCC cannot use the profile feedback information. By default, this warning is enabled and is treated as an error. -Wno-coverage-mismatch can be used to disable the warning or -Wno-error=coverage-mismatch can be used to disable the error. Disabling the error for this warning can result in poorly optimized code and is useful only in the case of very minor changes such as bug fixes to an existing code-base. Completely disabling the warning is not recommended.",1,6,function-tradeoff,gcc
2665,-Wno-cpp,"(C, Objective-C, C++, Objective-C++ and Fortran only) Suppress warning messages emitted by #warning directives.",0,0,others,gcc
2667,-Wno-deprecated,Do not warn about usage of deprecated features. See Deprecated Features.,0,0,others,gcc
2668,-Wno-deprecated-declarations,"Do not warn about uses of functions (see Function Attributes), variables (see Variable Attributes), and types (see Type Attributes) marked as deprecated by using the deprecated attribute.",1,6,function-tradeoff,gcc
2669,-Wno-deprecated-enum-enum-conversion (C++ and Objective-C++ only),Disable the warning about the case when the usual arithmetic conversions are applied on operands where one is of enumeration type and the other is of a different enumeration type. This conversion was deprecated in C++20. For example:,0,0,others,gcc
2670,-Wno-deprecated-enum-float-conversion (C++ and Objective-C++ only),Disable the warning about the case when the usual arithmetic conversions are applied on operands where one is of enumeration type and the other is of a floating-point type. This conversion was deprecated in C++20. For example:,0,0,others,gcc
2671,-Wno-designated-init (C and Objective-C only),Suppress warnings when a positional initializer is used to initialize a structure that has been marked with the designated_init attribute.,0,0,others,gcc
2672,-Wno-discarded-array-qualifiers (C and Objective-C only),"Do not warn if type qualifiers on arrays which are pointer targets are being discarded. Typically, the compiler warns if a const int (*)[] variable is passed to a function that takes a int (*)[] parameter. This option can be used to suppress such a warning.",1,6,function-tradeoff,gcc
2673,-Wno-discarded-qualifiers (C and Objective-C only),"Do not warn if type qualifiers on pointers are being discarded. Typically, the compiler warns if a const char * variable is passed to a function that takes a char * parameter. This option can be used to suppress such a warning.",1,6,function-tradeoff,gcc
2674,-Wno-div-by-zero,"Do not warn about compile-time integer division by zero. Floating-point division by zero is not warned about, as it can be a legitimate way of obtaining infinities and NaNs.",0,0,others,gcc
2675,-Wno-endif-labels,Do not warn about stray tokens after #else and #endif.,1,6,function-tradeoff,gcc
2676,-Wnoexcept (C++ and Objective-C++ only),Warn when a noexcept-expression evaluates to false because of a call to a function that does not have a non-throwing exception specification (i.e. throw() or noexcept) but is known by the compiler to never throw an exception.,1,6,function-tradeoff,gcc
2677,-Wno-exceptions (C++ and Objective-C++ only),"Disable the warning about the case when an exception handler is shadowed by another handler, which can point out a wrong ordering of exception handlers.",1,6,function-tradeoff,gcc
2678,-Wnoexcept-type (C++ and Objective-C++ only),Warn if the C++17 feature making noexcept part of a function type changes the mangled name of a symbol relative to C++14. Enabled by -Wabi and -Wc++17-compat.,1,6,function-tradeoff,gcc
2679,-Wno-format-contains-nul,"If -Wformat is specified, do not warn about format strings that contain NUL bytes.",1,6,function-tradeoff,gcc
2681,-Wno-format-zero-length,"If -Wformat is specified, also warn if the format string is not a string literal and so cannot be checked, unless the format function takes its format arguments as a va_list.",1,6,function-tradeoff,gcc
2682,-Wno-frame-larger-than,Disable -Wframe-larger-than= warnings. The option is equivalent to -Wframe-larger-than=SIZE_MAXor larger.,0,0,others,gcc
2684,"-Wno-if-not-aligned (C, C++, Objective-C and Objective-C++ only)","Warn if the return type of a function has a type qualifier such as const. For ISO C such a type qualifier has no effect, since the value returned by a function is not an lvalue. For C++, the warning is only emitted for scalar types or void. ISO C prohibits qualified void return types on function definitions, so such return types always receive a warning even without this option.",1,6,function-tradeoff,gcc
2685,-Wno-ignored-attributes (C and C++ only),"Warn if the type of main is suspicious. main should be a function with external linkage, returning int, taking either zero arguments, two, or three arguments of appropriate types. This warning is enabled by default in C++ and is enabled by either -Wall or -Wpedantic.",1,6,function-tradeoff,gcc
2686,-Wno-implicit-function-declaration (C and Objective-C only),Same as -Wimplicit-int and -Wimplicit-function-declaration. This warning is enabled by -Wall.,0,0,others,gcc
2687,-Wno-implicit-int (C and Objective-C only),"This option controls warnings when a function is used before being declared. This warning is enabled by default in C99 and later dialects of C, and also by -Wall. The warning is made into an error by -pedantic-errors.",0,0,others,gcc
2688,"-Wno-inaccessible-base (C++, Objective-C++ only)",This option controls warnings when a base class is inaccessible in a class derived from it due to ambiguity. The warning is enabled by default. Note that the warning for ambiguous virtual bases is enabled by the -Wextra option.,1,6,function-tradeoff,gcc
2689,-Wno-incompatible-pointer-types (C and Objective-C only),"Do not warn when there is a conversion between pointers that have incompatible types. This warning is for cases not covered by -Wno-pointer-sign, which warns for pointer argument passing or assignment with different signedness.",1,6,function-tradeoff,gcc
2691,-Wno-init-list-lifetime (C++ and Objective-C++ only),"Do not warn about uses of std::initializer_list that are likely to result in dangling pointers. Since the underlying array for an initializer_list is handled like a normal C++ temporary object, it is easy to inadvertently keep a pointer to the array past the end of the array's lifetime. For example:",1,6,function-tradeoff,gcc
2692,-Wno-int-conversion (C and Objective-C only),Do not warn about incompatible integer to pointer and pointer to integer conversions. This warning is about implicit conversions; for explicit conversions the warnings -Wno-int-to-pointer-cast and -Wno-pointer-to-int-cast may be used.,1,6,function-tradeoff,gcc
2693,-Wno-int-to-pointer-cast,"Suppress warnings from casts to pointer type of an integer of a different size. In C++, casting to a pointer type of smaller size is an error. Wint-to-pointer-cast is enabled by default.",0,0,others,gcc
2694,-Wno-invalid-memory-model,"This option controls warnings for invocations of __atomic Builtins, __sync Builtins, and the C11 atomic generic functions with a memory consistency argument that is either invalid for the operation or outside the range of values of the memory_order enumeration. For example, since the __atomic_store and __atomic_store_n built-ins are only defined for the relaxed, release, and sequentially consistent memory orders the following code is diagnosed:",0,0,others,gcc
2695,-Wno-invalid-offsetof (C++ and Objective-C++ only),"Suppress warnings from applying the offsetof macro to a non-POD type. According to the 2014 ISO C++ standard, applying offsetof to a non-standard-layout type is undefined. In existing C++ implementations, however, offsetof typically gives meaningful results. This flag is for users who are aware that they are writing nonportable code and who have deliberately chosen to ignore the warning about it.",1,4,limited-side-effect,gcc
2697,-Wno-literal-suffix (C++ and Objective-C++ only),"Do not warn when a string or character literal is followed by a ud-suffix which does not begin with an underscore. As a conforming extension, GCC treats such suffixes as separate preprocessing tokens in order to maintain backwards compatibility with code that uses formatting macros from <inttypes.h>. For example:",1,6,function-tradeoff,gcc
2699,-Wno-mismatched-dealloc,"Warn about unsafe multiple statement macros that appear to be guarded by a clause such as if, else, for, switch, or while, in which only the first statement is actually guarded after the macro is expanded.",1,6,function-tradeoff,gcc
2700,-Wno-mismatched-new-delete (C++ and Objective-C++ only),"Warn for mismatches between calls to operator new or operator delete and the corresponding call to the allocation or deallocation function. This includes invocations of C++ operator delete with pointers returned from either mismatched forms of operator new, or from other functions that allocate objects for which the operator delete isn't a suitable deallocator, as well as calls to other deallocation functions with pointers returned from operator new for which the deallocation function isn't suitable.",1,6,function-tradeoff,gcc
2701,-Wno-missing-profile,"Warn for calls to deallocation functions with pointer arguments returned from from allocations functions for which the former isn't a suitable deallocator. A pair of functions can be associated as matching allocators and deallocators by use of attribute malloc. Unless disabled by the -fno-builtin option the standard functions calloc, malloc, realloc, and free, as well as the corresponding forms of C++ operator new and operator delete are implicitly associated as matching allocators and deallocators. In the following example mydealloc is the deallocator for pointers returned from myalloc.",1,6,function-tradeoff,gcc
2703,-Wno-narrowing (C++ and Objective-C++ only),"For C++11 and later standards, narrowing conversions are diagnosed by default, as required by the standard. A narrowing conversion from a constant produces an error, and a narrowing conversion from a non-constant produces a warning, but -Wno-narrowing suppresses the diagnostic. Note that this does not affect the meaning of well-formed code; narrowing conversions are still considered ill-formed in SFINAE contexts.",0,0,others,gcc
2705,-Wnonnull-compare,"Warn if the compiler detects paths that trigger erroneous or undefined behavior due to dereferencing a null pointer. This option is only active when -fdelete-null-pointer-checks is active, which is enabled by optimizations in most targets. The precision of the warnings depends on the optimization options used.",1,6,function-tradeoff,gcc
2706,-Wno-non-template-friend (C++ and Objective-C++ only),"Disable warnings when non-template friend functions are declared within a template. In very old versions of GCC that predate implementation of the ISO standard, declarations such as friend int foo(int) where the name of the friend is an unqualified-id, could be interpreted as a particular specialization of a template function; the warning exists to diagnose compatibility problems, and is enabled by default.",1,6,function-tradeoff,gcc
2707,-Wnon-virtual-dtor (C++ and Objective-C++ only),"Warn when a class has virtual functions and an accessible non-virtual destructor itself or in an accessible polymorphic base class, in which case it is possible but unsafe to delete an instance of a derived class through a pointer to the class itself or base class. This warning is automatically enabled if -Weffc++ is specified.",1,6,function-tradeoff,gcc
2709,-Wno-overflow,Do not warn about compile-time overflow in constant expressions.,0,0,others,gcc
2712,-Wno-pedantic-ms-format (MinGW targets only),"When used in combination with -Wformat and -pedantic without GNU extensions, this option disables the warnings about non-ISO printf / scanf format width specifiers I32, I64, and I used on Windows targets, which depend on the MS runtime.",0,0,others,gcc
2713,-Wno-pessimizing-move (C++ and Objective-C++ only),"This warning warns when a call to std::move prevents copy elision. A typical scenario when copy elision can occur is when returning in a function with a class return type, when the expression being returned is the name of a non-volatile automatic object, and is not a function parameter, and has the same type as the function return type.",1,6,function-tradeoff,gcc
2714,-Wno-pmf-conversions (C++ and Objective-C++ only),Disable the diagnostic for converting a bound pointer to member function to a plain pointer.,1,6,function-tradeoff,gcc
2716,-Wno-pointer-to-int-cast (C and Objective-C only),Suppress warnings from casts from a pointer to an integer type of a different size.,0,0,others,gcc
2717,-Wno-pragmas,"Do not warn about misuses of pragmas, such as incorrect parameters, invalid syntax, or conflicts between pragmas. See also -Wunknown-pragmas.",1,6,function-tradeoff,gcc
2719,-Wno-property-assign-default (Objective-C and Objective-C++ only),Do not warn if a property for an Objective-C object has no assign semantics specified.,1,6,function-tradeoff,gcc
2720,-Wno-protocol (Objective-C and Objective-C++ only),"If a class is declared to implement a protocol, a warning is issued for every method in the protocol that is not implemented by the class. The default behavior is to issue a warning for every method not explicitly implemented in the class, even if a method implementation is inherited from the superclass. If you use the -Wno-protocol option, then methods inherited from the superclass are considered to be implemented, and no warning is issued for them.",1,2,security-tradeoff,gcc
2721,-Wno-redundant-move (C++ and Objective-C++ only),"This warning warns about redundant calls to std::move; that is, when a move operation would have been performed even without the std::move call. This happens because the compiler is forced to treat the object as if it were an rvalue in certain situations such as returning a local variable, where copy elision isn妾?applicable. Consider:",0,0,others,gcc
2722,-Wno-return-local-addr,Warn whenever a function is defined with a return type that defaults to int. Also warn about any return statement with no return value in a function whose return type is not void (falling off the end of the function body is considered returning without a value).,1,6,function-tradeoff,gcc
2723,-Wnormalized=[none|id|nfc|nfkc],"In ISO C and ISO C++, two identifiers are different if they are different sequences of characters. However, sometimes when characters outside the basic ASCII character set are used, you can have two different character sequences that look the same. To avoid confusion, the ISO 10646 standard sets out some normalization rules which when applied ensure that two sequences that look the same are turned into the same sequence. GCC can warn you if you are using identifiers that have not been normalized; this option controls that warning.",1,6,function-tradeoff,gcc
2724,-Wno-scalar-storage-order,Do not warn on suspicious constructs involving reverse scalar storage order.,1,6,function-tradeoff,gcc
2726,-Wno-shift-count-negative,Controls warnings if a shift count is greater than or equal to the bit width of the type. This warning is enabled by default.,0,0,others,gcc
2727,-Wno-shift-count-overflow,Warn if left shifting a negative value. This warning is enabled by -Wextra in C99 and C++11 modes (and newer).,1,6,function-tradeoff,gcc
2728,-Wno-shift-overflow,These options control warnings about left shift overflows.,0,0,others,gcc
2729,-Wno-sizeof-array-argument,Do not warn when the sizeof operator is applied to a parameter that is declared as an array in a function definition. This warning is enabled by default for C and C++ programs.,1,6,function-tradeoff,gcc
2730,-Wno-stack-usage,Disable -Wstack-usage= warnings. The option is equivalent to -Wstack-usage=SIZE_MAXor larger.,0,0,others,gcc
2731,-Wno-stringop-overread,"Warn for calls to string manipulation functions such as memchr, or strcpy that are determined to read past the end of the source sequence.",1,6,function-tradeoff,gcc
2732,-Wno-stringop-truncation,"Do not warn for calls to bounded string manipulation functions such as strncat, strncpy, and stpncpy that may either truncate the copied string or leave the destination unchanged.",1,6,function-tradeoff,gcc
2733,-Wno-subobject-linkage (C++ and Objective-C++ only),"Do not warn if a class type has a base or a field whose type uses the anonymous namespace or depends on a type with no linkage. If a type A depends on a type B with no or internal linkage, defining it in multiple translation units would be an ODR violation because the meaning of B is different in each translation unit. If A only appears in a single translation unit, the best way to silence the warning is to give it internal linkage by putting it in an anonymous namespace as well. The compiler doesn妾?give this warning for types defined in the main .C file, as those are unlikely to have multiple definitions. -Wsubobject-linkage is enabled by default.",1,6,function-tradeoff,gcc
2734,-Wno-switch-bool,Do not warn when a switch statement has an index of boolean type and the case values are outside the range of a boolean type. It is possible to suppress this warning by casting the controlling expression to a type other than bool. For example:,1,6,function-tradeoff,gcc
2736,-Wno-switch-unreachable,"Do not warn when a switch statement contains statements between the controlling expression and the first case label, which will never be executed. For example:",1,6,function-tradeoff,gcc
2737,-Wno-terminate (C++ and Objective-C++ only),Disable the warning about a throw-expression that will immediately result in a call to terminate.,1,6,function-tradeoff,gcc
2738,-Wno-unused-result,Do not warn if a caller of a function marked with attribute warn_unused_result (see Function Attributes) does not use its return value. The default is -Wunused-result.,1,6,function-tradeoff,gcc
2739,-Wno-varargs,Do not warn upon questionable usage of the macros used to handle variable arguments like va_start. These warnings are enabled by default.,1,6,function-tradeoff,gcc
2740,-Wno-vexing-parse (C++ and Objective-C++ only),"Warn about the most vexing parse syntactic ambiguity. This warns about the cases when a declaration looks like a variable definition, but the C++ language requires it to be interpreted as a function declaration. For instance:",1,6,function-tradeoff,gcc
2741,-Wno-virtual-move-assign,"Suppress warnings about inheriting from a virtual base with a non-trivial C++11 move assignment operator. This is dangerous because if the virtual base is reachable along more than one path, it is moved multiple times, which can mean both objects end up in the moved-from state. If the move assignment operator is written to avoid moving from a moved-from object, this warning can be disabled.",1,6,function-tradeoff,gcc
2742,-Wno-vla-larger-than,Disable -Wvla-larger-than= warnings. The option is equivalent to -Wvla-larger-than=SIZE_MAXor larger.,0,0,others,gcc
2743,-Wnull-dereference,Warn about uninitialized variables that are initialized with themselves. Note this option can only be used with the -Wuninitialized option.,1,6,function-tradeoff,gcc
2744,-Wobjc-root-class (Objective-C and Objective-C++ only),"Warn if a class interface lacks a superclass. Most classes will inherit from NSObject (or Object) for example. When declaring classes intended to be root classes, the warning can be suppressed by marking their interfaces with __attribute__((objc_root_class)).",0,0,others,gcc
2745,-Wold-style-cast (C++ and Objective-C++ only),"Warn if an old-style (C-style) cast to a non-void type is used within a C++ program. The new-style casts (dynamic_cast, static_cast, reinterpret_cast, and const_cast) are less vulnerable to unintended effects and much easier to search for.",1,6,function-tradeoff,gcc
2747,-Wold-style-definition (C and Objective-C only),"Warn if an old-style function definition is used. A warning is given even if there is a previous prototype. A definition using )is not considered an old-style definition in C2X mode, because it is equivalent to void)in that case, but is considered an old-style definition for older standards.",1,6,function-tradeoff,gcc
2749,-Woverlength-strings,"Warn about string constants that are longer than the minimum maximumlength specified in the C standard. Modern compilers generally allow string constants that are much longer than the standard's minimum limit, but very portable programs should avoid using longer strings.",1,6,function-tradeoff,gcc
2750,-Woverloaded-virtual (C++ and Objective-C++ only),"Warn when a function declaration hides virtual functions from a base class. For example, in:",1,6,function-tradeoff,gcc
2751,-Woverride-init (C and Objective-C only),Warn if an initialized field without side effects is overridden when using designated initializers (see Designated Initializers).,1,6,function-tradeoff,gcc
2752,"-Wp,option","You can use -Wp,option to bypass the compiler driver and pass option directly through to the preprocessor. If option contains commas, it is split into multiple options at the commas. However, many options are modified, translated or interpreted by the compiler driver before being passed to the preprocessor, and -Wp forcibly bypasses this phase. The preprocessor's direct interface is undocumented and subject to change, so whenever possible you should avoid using -Wp and let the driver handle the options instead.",0,0,others,gcc
2753,-Wpacked,"Warn if a structure is given the packed attribute, but the packed attribute has no effect on the layout or size of the structure. Such structures may be mis-aligned for little benefit. For instance, in this code, the variable f.x in struct bar is misaligned even though struct bar does not itself have the packed attribute:",1,6,function-tradeoff,gcc
2754,"-Wpacked-not-aligned (C, C++, Objective-C and Objective-C++ only)","Warn if a structure field with explicitly specified alignment in a packed struct or union is misaligned. For example, a warning will be issued on struct S, like, warning: alignment 1 of 'struct S' is less than 8, in this code:",1,6,function-tradeoff,gcc
2755,-Wpadded,"Warn if padding is included in a structure, either to align an element of the structure or to align the whole structure. Sometimes when this happens it is possible to rearrange the fields of the structure to reduce the padding and so make the structure smaller.",1,6,function-tradeoff,gcc
2756,-Wparentheses,Warn about code that may have undefined semantics because of violations of sequence point rules in the C and C++ standards.,1,6,function-tradeoff,gcc
2757,-Wplacement-new=1,"This is the default warning level of -Wplacement-new. At this level the warning is not issued for some strictly undefined constructs that GCC allows as extensions for compatibility with legacy code. For example, the following new expression is not diagnosed at this level even though it has undefined behavior according to the C++ standard because it writes past the end of the one-element array.",1,6,function-tradeoff,gcc
2758,-Wplacement-new=2,"At this level, in addition to diagnosing all the same constructs as at level 1, a diagnostic is also issued for placement new expressions that construct an object in the last member of structure whose type is an array of a single element and whose size is less than the size of the object being constructed. While the previous example would be diagnosed, the following construct makes use of the flexible member array extension to avoid the warning at level 2.",1,6,function-tradeoff,gcc
2759,-Wplacement-new=n,"Warn about placement new expressions with undefined behavior, such as constructing an object in a buffer that is smaller than the type of the object. For example, the placement new expression below is diagnosed because it attempts to construct an array of 64 integers in a buffer only 64 bytes large.",1,6,function-tradeoff,gcc
2760,-Wpointer-arith,"Warn about anything that depends on the size ofa function type or of void. GNU C assigns these types a size of 1, for convenience in calculations with void * pointers and pointers to functions. In C++, warn also when an arithmetic operation involves NULL. This warning is also enabled by -Wpedantic.",1,6,function-tradeoff,gcc
2761,-Wpointer-sign (C and Objective-C only),"Warn for pointer argument passing or assignment with different signedness. This option is only supported for C and Objective-C. It is implied by -Wall and by -Wpedantic, which can be disabled with -Wno-pointer-sign.",1,6,function-tradeoff,gcc
2762,-Wrange-loop-construct (C++ and Objective-C++ only),"This warning warns when a C++ range-based for-loop is creating an unnecessary copy. This can happen when the range declaration is not a reference, but probably should be. For example:",1,6,function-tradeoff,gcc
2763,-wrapper,Invoke all subcommands under a wrapper program. The name of the wrapper program and its parameters are passed as a comma separated list.,0,0,others,gcc
2764,-Wredundant-decls,"Warn if anything is declared more than once in the same scope, even in cases where multiple declaration is valid and changes nothing.",0,0,others,gcc
2765,-Wredundant-tags (C++ and Objective-C++ only),Warn about redundant class-key and enum-key in references to class types and enumerated types in contexts where the key can be eliminated without causing an ambiguity. For example:,0,0,others,gcc
2766,-Wregister (C++ and Objective-C++ only),"Warn on uses of the register storage class specifier, except when it is part of the GNU Explicit Register Variables extension. The use of the register keyword as storage class specifier has been deprecated in C++11 and removed in C++17. Enabled by default with -std=c++17.",0,0,others,gcc
2767,-Wreorder (C++ and Objective-C++ only),Warn when the order of member initializers given in the code does not match the order in which they must be executed. For instance:,1,6,function-tradeoff,gcc
2768,-Wrestrict,"Warn when an object referenced by a restrict-qualified parameter (or, in C++, a __restrict-qualified parameter) is aliased by another argument, or when copies between such objects overlap. For example, the call to the strcpy function below attempts to truncate the string by replacing its initial characters with the last four. However, because the call writes the terminating NUL into a[4], the copies overlap and the call is diagnosed.",1,6,function-tradeoff,gcc
2769,-Wreturn-type,Controls warnings if a shift count is negative. This warning is enabled by default.,0,0,others,gcc
2770,-Wselector (Objective-C and Objective-C++ only),"Warn if multiple methods of different types for the same selector are found during compilation. The check is performed on the list of methods in the final stage of compilation. Additionally, a check is performed for each selector appearing in a @selector( expression, and a corresponding method for that selector has been found during compilation. Because these checks scan the method table only at the end of compilation, these warnings are not produced if the final stage of compilation is not reached, for example because an error is found during compilation, or because the -fsyntax-only option is being used.",1,6,function-tradeoff,gcc
2775,-Wshadow=local,Warn when a local variable shadows another local variable or parameter.,1,6,function-tradeoff,gcc
2776,-Wshift-overflow=1,"This is the warning level of -Wshift-overflow and is enabled by default in C99 and C++11 modes (and newer). This warning level does not warn about left-shifting 1 into the sign bit. (However, in C, such an overflow is still rejected in contexts where an integer constant expression is required.) No warning is emitted in C++20 mode (and newer), as signed left shifts always wrap.",0,0,others,gcc
2778,-Wsign-compare,"Warn when a comparison between signed and unsigned values could produce an incorrect result when the signed value is converted to unsigned. In C++, this warning is also enabled by -Wall. In C, it is also enabled by -Wextra.",1,6,function-tradeoff,gcc
2779,-Wsign-conversion,"Warn for implicit conversions that may change the sign of an integer value, like assigning a signed integer expression to an unsigned integer variable. An explicit cast silences the warning. In C, this option is enabled also by -Wconversion.",1,6,function-tradeoff,gcc
2780,-Wsign-promo (C++ and Objective-C++ only),"Warn when overload resolution chooses a promotion from unsigned or enumerated type to a signed type, over a conversion to an unsigned type of the same size. Previous versions of G++ tried to preserve unsignedness, but the standard mandates the current behavior.",1,6,function-tradeoff,gcc
2781,-Wsized-deallocation (C++ and Objective-C++ only),Warn about a definition of an unsized deallocation function,1,6,function-tradeoff,gcc
2782,-Wsizeof-array-div,"Warn about divisions of two sizeof operators when the first one is applied to an array and the divisor does not equal the size of the array element. In such a case, the computation will not yield the number of elements in the array, which is likely what the user intended. This warning warns e.g. about",1,6,function-tradeoff,gcc
2783,-Wsizeof-pointer-div,"Warn for suspicious divisions of two sizeof expressions that divide the pointer size by the element size, which is the usual way to compute the array size but won't work out correctly with pointers. This warning warns e.g. about sizeof (ptr) / sizeof (ptr[0]) if ptr is not an array, but a pointer. This warning is enabled by -Wall.",1,6,function-tradeoff,gcc
2784,-Wsizeof-pointer-memaccess,"Warn for suspicious length parameters to certain string and memory built-in functions if the argument uses sizeof. This warning triggers for example for memset (ptr, 0, sizeof (ptr)); if ptr is not an array, but a pointer, and suggests a possible fix, or about memcpy (&foo, ptr, sizeof (&foo));. -Wsizeof-pointer-memaccess also warns about calls to bounded string copy functions like strncat or strncpy that specify as the bound a sizeof expression of the source array. For example, in the following function the call to strncat specifies the size of the source string as the bound. That is almost certainly a mistake and so the call is diagnosed.",1,6,function-tradeoff,gcc
2785,-Wstack-protector,This option is only active when -fstack-protector is active. It warns about functions that are not protected against stack smashing.,0,0,others,gcc
2786,-Wstack-usage=byte-size,"Warn if the stack usage of a function might exceed byte-size. The computation done to determine the stack usage is conservative. Any space allocated via alloca, variable-length arrays, or related constructs is included by the compiler when determining whether or not to issue a warning.",1,6,function-tradeoff,gcc
2787,-Wstrict-aliasing,"This option is only active when -fstrict-aliasing is active. It warns about code that might break the strict aliasing rules that the compiler is using for optimization. The warning does not catch all cases, but does attempt to catch the more common pitfalls. It is included in -Wall. It is equivalent to -Wstrict-aliasing=3",0,0,others,gcc
2788,-Wstrict-aliasing=n,"This option is only active when -fstrict-aliasing is active. It warns about code that might break the strict aliasing rules that the compiler is using for optimization. Higher levels correspond to higher accuracy (fewer false positives). Higher levels also correspond to more effort, similar to the way -O works. -Wstrict-aliasing is equivalent to -Wstrict-aliasing=3.",0,0,others,gcc
2790,-Wstrict-overflow=1,"Warn about cases that are both questionable and easy to avoid. For example the compiler simplifies x + 1 > x to 1. This level of -Wstrict-overflow is enabled by -Wall; higher levels are not, and must be explicitly requested.",1,6,function-tradeoff,gcc
2791,-Wstrict-overflow=2,"Also warn about other cases where a comparison is simplified to a constant. For example: abs (x) >= 0. This can only be simplified when signed integer overflow is undefined, because abs (INT_MIN) overflows to INT_MIN, which is less than zero. -Wstrict-overflow (with no level) is the same as -Wstrict-overflow=2.",0,0,others,gcc
2792,-Wstrict-overflow=3,Also warn about other cases where a comparison is simplified. For example: x + 1 > 1 is simplified to x > 0.,0,0,others,gcc
2793,-Wstrict-overflow=4,Also warn about other simplifications not covered by the above cases. For example: (x * 10) / 5 is simplified to x * 2.,1,6,function-tradeoff,gcc
2794,-Wstrict-overflow=5,"Also warn about cases where the compiler reduces the magnitude of a constant involved in a comparison. For example: x + 2 > y is simplified to x + 1 >= y. This is reported only at the highest warning level because this simplification applies to many comparisons, so this warning level gives a very large number of false positives.",1,6,function-tradeoff,gcc
2795,-Wstrict-overflow=n,This option is only active when signed overflow is undefined. It warns about cases where the compiler optimizes based on the assumption that signed overflow does not occur. Note that it does not warn about all cases where the code might overflow: it only warns about cases where the compiler implements some optimization. Thus this warning depends on the optimization level.,1,6,function-tradeoff,gcc
2796,-Wstrict-prototypes (C and Objective-C only),Warn if a function is declared or defined without specifying the argument types. (An old-style function definition is permitted without a warning if preceded by a declaration that specifies the argument types.),1,6,function-tradeoff,gcc
2797,-Wstrict-selector-match (Objective-C and Objective-C++ only),"Warn if multiple methods with differing argument and/or return types are found for a given selector when attempting to send a message using this selector to a receiver of type id or Class. When this flag is off (which is the default behavior), the compiler omits such warnings if any differences found are confined to types that share the same size and alignment.",1,6,function-tradeoff,gcc
2798,-Wstring-compare,"Warn for calls to strcmp and strncmp whose result is determined to be either zero or non-zero in tests for such equality owing to the length of one argument being greater than the size of the array the other argument is stored in (or the bound in the case of strncmp). Such calls could be mistakes. For example, the call to strcmp below is diagnosed because its result is necessarily non-zero irrespective of the contents of the array a.",1,6,function-tradeoff,gcc
2799,-Wstringop-overflow=1,"The -Wstringop-overflow=1 option uses type-zero Object Size Checking to determine the sizes of destination objects. At this setting the option does not warn for writes past the end of subobjects of larger objects accessed by pointers unless the size of the largest surrounding object is known. When the destination may be one of several objects it is assumed to be the largest one of them. On Linux systems, when optimization is enabled at this setting the option warns for the same code as when the _FORTIFY_SOURCE macro is defined to a non-zero value.",1,6,function-tradeoff,gcc
2800,-Wstringop-overflow=2,"The -Wstringop-overflow=2 option uses type-one Object Size Checking to determine the sizes of destination objects. At this setting the option warns about overflows when writing to members of the largest complete objects whose exact size is known. However, it does not warn for excessive writes to the same members of unknown objects referenced by pointers since they may point to arrays containing unknown numbers of elements. This is the default setting of the option.",1,6,function-tradeoff,gcc
2801,-Wstringop-overflow=3,The -Wstringop-overflow=3 option uses type-two Object Size Checking to determine the sizes of destination objects. At this setting the option warns about overflowing the smallest object or data member. This is the most restrictive setting of the option that may result in warnings for safe code.,0,0,others,gcc
2802,-Wstringop-overflow=4,"The -Wstringop-overflow=4 option uses type-three Object Size Checking to determine the sizes of destination objects. At this setting the option warns about overflowing any data members, and when the destination is one of several objects it uses the size of the largest of them to decide whether to issue a warning. Similarly to -Wstringop-overflow=3 this setting of the option may result in warnings for benign code.",0,0,others,gcc
2803,-Wstringop-overflow=type,"Warn for calls to string manipulation functions such as memcpy and strcpy that are determined to overflow the destination buffer. The optional argument is one greater than the type of Object Size Checking to perform to determine the size of the destination. See Object Size Checking. The argument is meaningful only for functions that operate on character arrays but not for raw memory functions like memcpy which always make use of Object Size type-0. The option also warns for calls that specify a size in excess of the largest possible object or at most SIZE_MAX / 2 bytes. The option produces the best results with optimization enabled but can detect a small subset of simple buffer overflows even without optimization in calls to the GCC built-in functions like __builtin_memcpy that correspond to the standard functions. In any case, the option warns about just a subset of buffer overflows detected by the corresponding overflow checking built-ins. For example, the option issues a warning for the strcpy call below because it copies at least 5 characters (the string ""blue"" including the terminating NUL) into the buffer of size 4.",1,6,function-tradeoff,gcc
2804,-Wsuggest-attribute=[pure|const|noreturn|format|cold|malloc],Warn for cases where adding an attribute may be beneficial. The attributes currently supported are listed below.,0,0,others,gcc
2805,-Wsuggest-attribute=cold,Warn about functions that might be candidates for cold attribute. This is based on static detection and generally only warns about functions which always leads to a call to another cold function such as wrappers of C++ throw or fatal error reporting functions leading to abort.,1,6,function-tradeoff,gcc
2806,-Wsuggest-attribute=malloc,"Warn about functions that might be candidates for attributes pure, const or noreturn or malloc. The compiler only warns for functions visible in other compilation units or (in the case of pure and const) if it cannot prove that the function returns normally. A function returns normally if it doesn't contain an infinite loop or return abnormally by throwing, calling abort or trapping. This analysis requires option -fipa-pure-const, which is enabled by default at -O and higher. Higher optimization levels improve the accuracy of the analysis.",1,6,function-tradeoff,gcc
2808,-Wsuggest-final-types,"Warn about types with virtual methods where code quality would be improved if the type were declared with the C++11 final specifier, or, if possible, declared in an anonymous namespace. This allows GCC to more aggressively devirtualize the polymorphic calls. This warning is more effective with link-time optimization, where the information about the class hierarchy graph is more complete.",1,6,function-tradeoff,gcc
2809,-Wsuggest-override,Warn about overriding virtual functions that are not marked with the override keyword.,1,6,function-tradeoff,gcc
2810,-Wswitch,Warn whenever a switch statement has an index of enumerated type and lacks a case for one or more of the named codes of that enumeration. (The presence of a default label prevents this warning.) case labels outside the enumeration range also provoke warnings when this option is used (even if there is a default label). This warning is enabled by -Wall.,1,6,function-tradeoff,gcc
2811,-Wswitch-default,Warn whenever a switch statement does not have a default case.,1,6,function-tradeoff,gcc
2812,-Wswitch-enum,Warn whenever a switch statement has an index of enumerated type and lacks a case for one or more of the named codes of that enumeration. case labels outside the enumeration range also provoke warnings when this option is used. The only difference between -Wswitch and this option is that this option gives a warning about an omitted enumeration code even if there is a default label.,1,6,function-tradeoff,gcc
2813,-Wsync-nand (C and C++ only),Warn when __sync_fetch_and_nand and __sync_nand_and_fetch built-in functions are used. These functions changed semantics in GCC 4.4.,1,6,function-tradeoff,gcc
2814,-Wsystem-headers,"Print warning messages for constructs found in system header files. Warnings from system headers are normally suppressed, on the assumption that they usually do not indicate real problems and would only make the compiler output harder to read. Using this command-line option tells GCC to emit warnings from system headers as if they occurred in user code. However, note that using -Wall in conjunction with this option does not warn about unknown pragmas in system headers or that, -Wunknown-pragmas must also be used.",1,6,function-tradeoff,gcc
2815,-Wtautological-compare,Warn if a self-comparison always evaluates to true or false. This warning detects various mistakes such as:,1,6,function-tradeoff,gcc
2816,-Wtemplates (C++ and Objective-C++ only),"Warn when a primary template declaration is encountered. Some coding rules disallow templates, and this may be used to enforce that rule. The warning is inactive inside a system header file, such as the STL, so one can still use the STL. One may also instantiate or specialize templates.",0,0,others,gcc
2818,-Wtraditional-conversion (C and Objective-C only),"Warn if a prototype causes a type conversion that is different from what would happen to the same argument in the absence of a prototype. This includes conversions of fixed point to floating and vice versa, and conversions changing the width or signedness of a fixed-point argument except when the same as the default promotion.",1,6,function-tradeoff,gcc
2819,-Wtrampolines,"Warn about trampolines generated for pointers to nested functions. A trampoline is a small piece of data or code that is created at run time on the stack when the address of a nested function is taken, and is used to call the nested function indirectly. For some targets, it is made up of data only and thus requires no special treatment. But, for most targets, it is made up of code and thus requires the stack to be made executable in order for the program to work properly.",1,6,function-tradeoff,gcc
2820,-Wtrigraphs,"Warn if any trigraphs are encountered that might change the meaning of the program. Trigraphs within comments are not warned about, except those that would form escaped newlines.",1,6,function-tradeoff,gcc
2821,-Wtsan,Warn about unsupported features in ThreadSanitizer.,1,6,function-tradeoff,gcc
2822,-Wtype-limits,"Warn if a comparison is always true or always false due to the limited range of the data type, but do not warn for constant expressions. For example, warn if an unsigned variable is compared against zero with < or >=. This warning is also enabled by -Wextra.",1,6,function-tradeoff,gcc
2823,-Wundeclared-selector (Objective-C and Objective-C++ only),"Warn if a @selector( expression referring to an undeclared selector is found. A selector is considered undeclared if no method with that name has been declared before the @selector( expression, either explicitly in an @interface or @protocol declaration, or implicitly in an @implementation section. This option always performs its checks as soon as a @selector( expression is found, while -Wselector only performs its checks in the final stage of compilation. This also enforces the coding style convention that methods and selectors must be declared before being used.",0,0,others,gcc
2824,-Wundef,Warn if an undefined identifier is evaluated in an #if directive. Such identifiers are replaced with zero.,1,6,function-tradeoff,gcc
2825,-Wunknown-pragmas,"Warn when a #pragma directive is encountered that is not understood by GCC. If this command-line option is used, warnings are even issued for unknown pragmas in system header files. This is not the case if the warnings are only enabled by the -Wall command-line option.",1,6,function-tradeoff,gcc
2828,-Wunused,All the above -Wunused options combined.,0,0,others,gcc
2830,-Wunused-but-set-variable,"Warn whenever a local variable is assigned to, but otherwise unused (aside from its declaration). This warning is enabled by -Wall.",0,0,others,gcc
2831,-Wunused-const-variable,"This is the warning level that is enabled by -Wunused-variable for C. It warns only about unused static const variables defined in the main compilation unit, but not about static const variables declared in any header included.",0,0,others,gcc
2832,-Wunused-function,Warn whenever a static function is declared but not defined or a non-inline static function is unused. This warning is enabled by -Wall.,1,6,function-tradeoff,gcc
2833,-Wunused-label,Warn whenever a label is declared but not used. This warning is enabled by -Wall.,1,6,function-tradeoff,gcc
2835,-Wunused-macros,Warn about macros defined in the main file that are unused. A macro is used if it is expanded or tested for existence at least once. The preprocessor also warns if the macro has not been used at the time it is redefined or undefined.,0,0,others,gcc
2836,-Wunused-parameter,Warn whenever a function parameter is unused aside from its declaration.,1,6,function-tradeoff,gcc
2837,-Wunused-value,"Warn whenever a statement computes a result that is explicitly not used. To suppress this warning cast the unused expression to void. This includes an expression-statement or the left-hand side of a comma expression that contains no side effects. For example, an expression such as x[i,j] causes a warning, while x[(void)i,j] does not.",0,0,others,gcc
2838,-Wunused-variable,"Warn whenever a local or static variable is unused aside from its declaration. This option implies -Wunused-const-variable=1 for C, but not for C++. This warning is enabled by -Wall.",0,0,others,gcc
2839,-Wuseless-cast (C++ and Objective-C++ only),Warn when an expression is casted to its own type.,1,6,function-tradeoff,gcc
2840,-Wvariadic-macros,"Warn if variadic macros are used in ISO C90 mode, or if the GNU alternate syntax is used in ISO C99 mode. This is enabled by either -Wpedantic or -Wtraditional. To inhibit the warning messages, use -Wno-variadic-macros.",1,6,function-tradeoff,gcc
2841,-Wvector-operation-performance,"Warn if vector operation is not implemented via SIMD capabilities of the architecture. Mainly useful for the performance tuning. Vector operation can be implemented piecewise, which means that the scalar operation is performed on every vector element; in parallel, which means that the vector operation is implemented using scalars of wider type, which normally is more performance efficient; and as a single scalar, which means that vector fits into a scalar type.",1,6,function-tradeoff,gcc
2842,-Wvirtual-inheritance,"Warn when a class is defined with a virtual direct base class. Some coding rules disallow multiple inheritance, and this may be used to enforce that rule. The warning is inactive inside a system header file, such as the STL, so one can still use the STL. One may also define classes that indirectly use virtual inheritance.",0,0,others,gcc
2843,-Wvla,Warn if a variable-length array is used in the code. -Wno-vla prevents the -Wpedantic warning of the variable-length array.,1,6,function-tradeoff,gcc
2844,-Wvla-larger-than=byte-size,"If this option is used, the compiler warns for declarations of variable-length arrays whose size is either unbounded, or bounded by an argument that allows the array size to exceed byte-size bytes. This is similar to how -Walloca-larger-than=byte-size works, but with variable-length arrays.",0,0,others,gcc
2847,-Wvolatile-register-var,Warn if a register variable is declared volatile. The volatile modifier does not inhibit all optimizations that may eliminate reads and/or writes to register variables. This warning is enabled by -Wall.,1,6,function-tradeoff,gcc
2848,-Wwrite-strings,"When compiling C, give string constants the type const char[length] so that copying the address of one into a non-const char * pointer produces a warning. These warnings help you find at compile time code that can try to write into a string constant, but only if you have been very careful about using const in declarations and prototypes. Otherwise, it is just a nuisance. This is why we did not make -Wall request these warnings.",0,0,others,gcc
2849,-Wzero-as-null-pointer-constant (C++ and Objective-C++ only),Warn when a literal is used as null pointer constant. This can be useful to facilitate the conversion to nullptr in C++11.,1,6,function-tradeoff,gcc
2850,-Wzero-length-bounds,Warn about accesses to elements of zero-length array members that might overlap other members of the same object. Declaring interior zero-length arrays is discouraged because accesses to them are undefined. See See Zero Length.,1,6,function-tradeoff,gcc
2851,-x language,Specify explicitly the language for the following input files (rather than letting the compiler choose a default based on the file name suffix). This option applies to all following input files until the next -x option. Possible values for language are:,0,0,others,gcc
2852,-x none,"Turn off any specification of a language, so that subsequent files are handled according to their file name suffixes (as they are if -x has not been used at all).",0,0,others,gcc
2853,-Xassembler option,Pass option as an option to the assembler. You can use this to supply system-specific assembler options that GCC does not recognize.,0,0,others,gcc
2855,-Xpreprocessor option,Pass option as an option to the preprocessor. You can use this to supply system-specific preprocessor options that GCC does not recognize.,0,0,others,gcc
2856,datanode.https.port,HTTPS port for DataNode.,0,0,others,hdfs
2857,dfs.balancer.address,The hostname used for a keytab based Kerberos login. Keytab based login can be enabled with dfs.balancer.keytab.enabled.,0,0,others,hdfs
2858,dfs.balancer.block-move.timeout,"Maximum amount of time in milliseconds for a block to move. If this is set greater than 0, Balancer will stop waiting for a block move completion after this time. In typical clusters, a 3 to 5 minute timeout is reasonable. If timeout happens to a large proportion of block moves, this needs to be increased. It could also be that too much work is dispatched and many nodes are constantly exceeding the bandwidth limit as a result. In that case, other balancer parameters might need to be adjusted. It is disabled (0) by default.",0,0,others,hdfs
2859,dfs.balancer.dispatcherThreads,Size of the thread pool for the HDFS balancer block mover. dispatchExecutor,1,1,resource,hdfs
2860,dfs.balancer.getBlocks.min-block-size,Minimum block threshold size in bytes to ignore when fetching a source's block list.,0,0,others,hdfs
2863,dfs.balancer.keytab.enabled,Set to true to enable login using a keytab for Kerberized Hadoop.,0,0,others,hdfs
2864,dfs.balancer.keytab.file,The keytab file used by the Balancer to login as its service principal. The principal name is configured with dfs.balancer.kerberos.principal. Keytab based login can be enabled with dfs.balancer.keytab.enabled.,0,0,others,hdfs
2866,dfs.balancer.max-no-move-interval,"If this specified amount of time has elapsed and no block has been moved out of a source DataNode, on more effort will be made to move blocks out of this DataNode in the current Balancer iteration.",0,0,others,hdfs
2867,dfs.balancer.max-size-to-move,Maximum number of bytes that can be moved by the balancer in a single thread.,1,5,workload-specific,hdfs
2868,dfs.balancer.movedWinWidth,Window of time in ms for the HDFS balancer tracking blocks and its locations.,0,0,others,hdfs
2869,dfs.balancer.moverThreads,Thread pool size for executing block moves. moverThreadAllocator,1,1,resource,hdfs
2870,dfs.balancer.service.interval,The schedule interval of balancer when running as a long service.,0,0,others,hdfs
2872,dfs.batched.ls.limit,"Limit the number of paths that can be listed in a single batched listing call. printed by ls. If less or equal to zero, at most DFS_LIST_LIMIT_DEFAULT (= 1000) will be printed.",0,0,others,hdfs
2873,dfs.block.access.key.update.interval,Interval in minutes at which namenode updates its access keys.,0,0,others,hdfs
2875,dfs.block.access.token.lifetime,The lifetime of access tokens in minutes.,0,0,others,hdfs
2876,dfs.block.access.token.protobuf.enable,"If ""true"", block tokens are written using Protocol Buffers. If ""false"", block tokens are written using Legacy format.",1,4,limited-side-effect,hdfs
2877,dfs.block.invalidate.limit,"The maximum number of invalidate blocks sent by namenode to a datanode per heartbeat deletion command. This property works with ""dfs.namenode.invalidate.work.pct.per.iteration"" to throttle block deletions.",1,5,workload-specific,hdfs
2878,dfs.block.local-path-access.user,Comma separated list of the users allowed to open block files on legacy short-circuit local read.,0,0,others,hdfs
2879,dfs.block.misreplication.processing.limit,Maximum number of blocks to process for initializing replication queues.,0,0,others,hdfs
2883,dfs.blockreport.incremental.intervalMsec,"If set to a positive integer, the value in ms to wait between sending incremental block reports from the Datanode to the Namenode.",0,0,others,hdfs
2885,dfs.blockreport.intervalMsec,Determines block reporting interval in milliseconds.,0,0,others,hdfs
2886,dfs.blockreport.split.threshold,If the number of blocks on the DataNode is below this threshold then it will send block reports for all Storage Directories in a single message. If the number of blocks exceeds this threshold then the DataNode will send block reports for each Storage Directory in separate messages. Set to zero to always split.,1,5,workload-specific,hdfs
2887,dfs.blocksize,"The default block size for new files, in bytes. You can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.), Or provide complete size in bytes (such as 134217728 for 128 MB).",0,0,others,hdfs
2888,dfs.bytes-per-checksum,The number of bytes per checksum. Must not be larger than dfs.stream-buffer-size,0,0,others,hdfs
2889,dfs.cachereport.intervalMsec,"Determines cache reporting interval in milliseconds. After this amount of time, the DataNode sends a full report of its cache state to the NameNode. The NameNode uses the cache report to update its map of cached blocks to DataNode locations. This configuration has no effect if in-memory caching has been disabled by setting dfs.datanode.max.locked.memory to 0 (which is the default). If the native libraries are not available to the DataNode, this configuration has no effect.",1,6,function-tradeoff,hdfs
2890,dfs.checksum.combine.mode,"Defines how lower-level chunk/block checksums are combined into file-level checksums; the original MD5MD5CRC mode is not comparable between files with different block layouts, while modes like COMPOSITE_CRC are comparable independently of block layout.",0,0,others,hdfs
2892,dfs.client.block.reader.remote.buffer.size,"The output stream buffer size of a DFSClient remote read. The buffer default value is 512B. The buffer includes only some request parameters that are: block, blockToken, clientName, startOffset, len, verifyChecksum, cachingStrategy.",1,1,resource,hdfs
2893,dfs.client.block.write.locateFollowingBlock.initial.delay.ms,"The initial delay (unit is ms) for locateFollowingBlock, the delay time will increase exponentially(double) for each retry until dfs.client.block.write.locateFollowingBlock.max.delay.ms is reached, after that the delay for each retry will be dfs.client.block.write.locateFollowingBlock.max.delay.ms.",0,0,others,hdfs
2894,dfs.client.block.write.locateFollowingBlock.max.delay.ms,The maximum delay (unit is ms) before retrying locateFollowingBlock.,0,0,others,hdfs
2895,dfs.client.block.write.locateFollowingBlock.retries,Number of retries to use when finding the next block during HDFS writes.,0,0,others,hdfs
2896,dfs.client.block.write.replace-datanode-on-failure.best-effort,"This property is used only if the value of dfs.client.block.write.replace-datanode-on-failure.enable is true. Best effort means that the client will try to replace a failed datanode in write pipeline (provided that the policy is satisfied), however, it continues the write operation in case that the datanode replacement also fails. Suppose the datanode replacement fails. false: An exception should be thrown so that the write will fail. true : The write should be resumed with the remaining datandoes. Note that setting this property to true allows writing to a pipeline with a smaller number of datanodes. As a result, it increases the probability of data loss.",1,3,reliability-tradeoff,hdfs
2897,dfs.client.block.write.replace-datanode-on-failure.enable,"If there is a datanode/network failure in the write pipeline, DFSClient will try to remove the failed datanode from the pipeline and then continue writing with the remaining datanodes. As a result, the number of datanodes in the pipeline is decreased. The feature is to add new datanodes to the pipeline. This is a site-wide property to enable/disable the feature. When the cluster size is extremely small, e.g. 3 nodes or less, cluster administrators may want to set the policy to NEVER in the default configuration file or disable this feature. Otherwise, users may experience an unusually high rate of pipeline failures since it is impossible to find new datanodes for replacement. See also dfs.client.block.write.replace-datanode-on-failure.policy",1,3,reliability-tradeoff,hdfs
2898,dfs.client.block.write.replace-datanode-on-failure.min-replication,"The minimum number of replications that are needed to not to fail the write pipeline if new datanodes can not be found to replace failed datanodes (could be due to network failure) in the write pipeline. If the number of the remaining datanodes in the write pipeline is greater than or equal to this property value, continue writing to the remaining nodes. Otherwise throw exception. If this is set to 0, an exception will be thrown, when a replacement can not be found. See also dfs.client.block.write.replace-datanode-on-failure.policy",1,3,reliability-tradeoff,hdfs
2899,dfs.client.block.write.replace-datanode-on-failure.policy,This property is used only if the value of dfs.client.block.write.replace-datanode-on-failure.enable is true. ALWAYS: always add a new datanode when an existing datanode is removed. NEVER: never add a new datanode. DEFAULT: Let r be the replication number. Let n be the number of existing datanodes. Add a new datanode only if r is greater than or equal to 3 and either (1) floor(r/2) is greater than or equal to n; or (2) r is greater than n and the block is hflushed/appended.,1,3,reliability-tradeoff,hdfs
2900,dfs.client.block.write.retries,"The number of retries for writing blocks to the data nodes, before we signal failure to the application.",0,0,others,hdfs
2901,dfs.client.cache.drop.behind.reads,"Just like dfs.datanode.drop.cache.behind.reads, this setting causes the page cache to be dropped behind HDFS reads, potentially freeing up more memory for other uses. Unlike dfs.datanode.drop.cache.behind.reads, this is a client-side setting rather than a setting for the entire datanode. If present, this setting will override the DataNode default. If the native libraries are not available to the DataNode, this configuration has no effect.",1,4,limited-side-effect,hdfs
2903,dfs.client.cache.readahead,"When using remote reads, this setting causes the datanode to read ahead in the block file using posix_fadvise, potentially decreasing I/O wait times. Unlike dfs.datanode.readahead.bytes, this is a client-side setting rather than a setting for the entire datanode. If present, this setting will override the DataNode default. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize. When using local reads, this setting determines how much readahead we do in BlockReaderLocal. If the native libraries are not available to the DataNode, this configuration has no effect.",1,4,limited-side-effect,hdfs
2904,dfs.client.cached.conn.retry,"The number of times the HDFS client will pull a socket from the cache. Once this number is exceeded, the client will try to create a new socket.",0,0,others,hdfs
2906,dfs.client.datanode-restart.timeout,"Expert only. The time to wait, in seconds, from reception of an datanode shutdown notification for quick restart, until declaring the datanode dead and invoking the normal recovery mechanisms. The notification is sent by a datanode when it is being shutdown using the shutdownDatanode admin command with the upgrade option. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.If no time unit is specified then seconds is assumed.",0,0,others,hdfs
2907,dfs.client.deadnode.detection.deadnode.queue.max,The max queue size of probing dead node.,1,5,workload-specific,hdfs
2908,dfs.client.deadnode.detection.enabled,Set to true to enable dead node detection in client side. Then all the DFSInputStreams of the same client can share the dead node information.,1,3,reliability-tradeoff,hdfs
2909,dfs.client.deadnode.detection.probe.connection.timeout.ms,Connection timeout for probing dead node in milliseconds.,0,0,others,hdfs
2910,dfs.client.deadnode.detection.probe.deadnode.interval.ms,Interval time in milliseconds for probing dead node behavior.,0,0,others,hdfs
2911,dfs.client.deadnode.detection.probe.deadnode.threads,The maximum number of threads to use for probing dead node.,1,1,resource,hdfs
2912,dfs.client.deadnode.detection.probe.suspectnode.interval.ms,Interval time in milliseconds for probing suspect node behavior.,0,0,others,hdfs
2913,dfs.client.deadnode.detection.probe.suspectnode.threads,The maximum number of threads to use for probing suspect node.,1,1,resource,hdfs
2914,dfs.client.deadnode.detection.rpc.threads,The maximum number of threads to use for calling RPC call to recheck the liveness of dead node.,1,1,resource,hdfs
2915,dfs.client.deadnode.detection.suspectnode.queue.max,The max queue size of probing suspect node.,1,5,workload-specific,hdfs
2916,dfs.client.domain.socket.data.traffic,This control whether we will try to pass normal data traffic over UNIX domain socket rather than over TCP socket on node-local data transfer. This is currently experimental and turned off by default.,0,0,others,hdfs
2919,dfs.client.failover.max.attempts,Expert only. The number of client failover attempts that should be made before the failover is considered failed.,0,0,others,hdfs
2920,dfs.client.failover.proxy.provider,"The prefix (plus a required nameservice ID) for the class name of the configured Failover proxy provider for the host. For normal HA mode, please consult the ""Configuration Details"" section of the HDFS High Availability documentation. For observer reading mode, please choose a custom class--ObserverReadProxyProvider.",0,0,others,hdfs
2921,dfs.client.failover.random.order,"Determines if the failover proxies are picked in random order instead of the configured order. Random order may be enabled for better load balancing or to avoid always hitting failed ones first if the failed ones appear in the beginning of the configured or resolved list. For example, In the case of multiple RBF routers or ObserverNameNodes, it is recommended to be turned on for load balancing. The config name can be extended with an optional nameservice ID (of form dfs.client.failover.random.order[.nameservice]) in case multiple nameservices exist and random order should be enabled for specific nameservices.",1,4,limited-side-effect,hdfs
2922,dfs.client.failover.resolve-needed,Determines if the given nameservice address is a domain name which needs to be resolved (using the resolver configured by dfs.client.failover.resolver-impl). This adds a transparency layer in the client so physical server address can change without changing the client. The config name can be extended with an optional nameservice ID (of form dfs.client.failover.resolve-needed[.nameservice]) to configure specific nameservices when multiple nameservices exist.,0,0,others,hdfs
2923,dfs.client.failover.resolver.impl,Determines what class to use to resolve nameservice name to specific machine address(es). The config name can be extended with an optional nameservice ID (of form dfs.client.failover.resolver.impl[.nameservice]) to configure specific nameservices when multiple nameservices exist.,0,0,others,hdfs
2924,dfs.client.failover.resolver.useFQDN,"Determines whether the resolved result is fully qualified domain name instead of pure IP address(es). The config name can be extended with an optional nameservice ID (of form dfs.client.failover.resolver.impl[.nameservice]) to configure specific nameservices when multiple nameservices exist. In secure environment, this has to be enabled since Kerberos is using fqdn in machine's principal therefore accessing servers by IP won't be recognized by the KDC.",0,0,others,hdfs
2926,dfs.client.failover.sleep.max.millis,"Expert only. The time to wait, in milliseconds, between failover attempts increases exponentially as a function of the number of attempts made so far, with a random factor of +/- 50%. This option specifies the maximum value to wait between failovers. Specifically, the time between two failover attempts will not exceed +/- 50% of dfs.client.failover.sleep.max.millis milliseconds.",0,0,others,hdfs
2927,dfs.client.hedged.read.threadpool.size,"Support 'hedged' reads in DFSClient. To enable this feature, set the parameter to a positive number. The threadpool size is how many threads to dedicate to the running of these 'hedged', concurrent reads in your client.",1,1,resource,hdfs
2928,dfs.client.hedged.read.threshold.millis,Configure 'hedged' reads in DFSClient. This is the number of milliseconds to wait before starting up a 'hedged' read.,0,0,others,hdfs
2929,dfs.client.https.keystore.resource,Resource file from which ssl client keystore information will be extracted,0,0,others,hdfs
2932,dfs.client.local.interfaces,"A comma separated list of network interface names to use for data transfer between the client and datanodes. When creating a connection to read from or write to a datanode, the client chooses one of the specified interfaces at random and binds its socket to the IP of that interface. Individual names may be specified as either an interface name (eg ""eth0""), a subinterface name (eg ""eth0:0""), or an IP address (which may be specified using CIDR notation to match a range of IPs).",0,0,others,hdfs
2933,dfs.client.max.block.acquire.failures,Maximum failures allowed when trying to get block information from a specific datanode.,0,0,others,hdfs
2935,dfs.client.mmap.cache.timeout.ms,"The minimum length of time that we will keep an mmap entry in the cache between uses. If an entry is in the cache longer than this, and nobody uses it, it will be removed by a background thread.",1,5,workload-specific,hdfs
2936,dfs.client.mmap.enabled,"If this is set to false, the client won't attempt to perform memory-mapped reads.",1,4,limited-side-effect,hdfs
2937,dfs.client.mmap.retry.timeout.ms,The minimum amount of time that we will wait before retrying a failed mmap operation.,0,0,others,hdfs
2938,dfs.client.read.prefetch.size,The number of bytes for the DFSClient will fetch from the Namenode during a read operation. Defaults to 10 * ${dfs.blocksize}.,1,1,resource,hdfs
2939,dfs.client.read.short.circuit.replica.stale.threshold.ms,Threshold in milliseconds for read entries during short-circuit local reads.,0,0,others,hdfs
2940,dfs.client.read.shortcircuit,This configuration parameter turns on short-circuit local reads.,1,4,limited-side-effect,hdfs
2941,dfs.client.read.shortcircuit.buffer.size,Buffer size in bytes for short-circuit local reads.,1,1,resource,hdfs
2942,dfs.client.read.shortcircuit.skip.checksum,"If this configuration parameter is set, short-circuit local reads will skip checksums. This is normally not recommended, but it may be useful for special setups. You might consider using this if you are doing your own checksumming outside of HDFS.",1,4,limited-side-effect,hdfs
2943,dfs.client.read.shortcircuit.streams.cache.expiry.ms,This controls the minimum amount of time file descriptors need to sit in the client cache context before they can be closed for being inactive for too long.,1,5,workload-specific,hdfs
2944,dfs.client.read.shortcircuit.streams.cache.size,"The DFSClient maintains a cache of recently opened file descriptors. This parameter controls the maximum number of file descriptors in the cache. Setting this higher will use more file descriptors, but potentially provide better performance on workloads involving lots of seeks.",1,1,resource,hdfs
2945,dfs.client.read.striped.threadpool.size,The maximum number of threads used for parallel reading in striped layout.,1,1,resource,hdfs
2946,dfs.client.refresh.read-block-locations.ms,Refreshing LocatedBlocks period. A value of 0 disables the feature.,0,0,others,hdfs
2947,dfs.client.replica.accessor.builder.classes,"Comma-separated classes for building ReplicaAccessor. If the classes are specified, client will use external BlockReader that uses the ReplicaAccessor built by the builder.",0,0,others,hdfs
2948,dfs.client.retry.interval-ms.get-last-block-length,Retry interval in milliseconds to wait between retries in getting block lengths from the datanodes.,0,0,others,hdfs
2949,dfs.client.retry.max.attempts,Max retry attempts for DFSClient talking to namenodes.,0,0,others,hdfs
2950,dfs.client.retry.policy.enabled,"If true, turns on DFSClient retry policy.",1,3,reliability-tradeoff,hdfs
2952,dfs.client.retry.times.get-last-block-length,Number of retries for calls to fetchLocatedBlocksAndGetLastBlockLength().,0,0,others,hdfs
2953,dfs.client.retry.window.base,"Base time window in ms for DFSClient retries. For each retry attempt, this value is extended linearly (e.g. 3000 ms for first attempt and first retry, 6000 ms for second retry, 9000 ms for third retry, etc.).",0,0,others,hdfs
2954,dfs.client.server-defaults.validity.period.ms,"The amount of milliseconds after which cached server defaults are updated. By default this parameter is set to 1 hour. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.",0,0,others,hdfs
2957,dfs.client.socket.send.buffer.size,"Socket send buffer size for a write pipeline in DFSClient side. This may affect TCP connection throughput. If it is set to zero or negative value, no buffer size will be set explicitly, thus enable tcp auto-tuning on some system. The default value is 0.",1,1,resource,hdfs
2958,dfs.client.socketcache.capacity,"Socket cache capacity (in entries) for short-circuit reads. If this value is set to 0, the client socket cache is disabled.",1,1,resource,hdfs
2959,dfs.client.socketcache.expiryMsec,Socket cache expiration for short-circuit reads in msec.,1,5,workload-specific,hdfs
2960,dfs.client.socket-timeout,Default timeout value in milliseconds for all sockets.,0,0,others,hdfs
2961,dfs.client.test.drop.namenode.response.number,The number of Namenode responses dropped by DFSClient for each RPC call. Used for testing the NN retry cache.,0,0,others,hdfs
2963,dfs.client.use.legacy.blockreader.local,Legacy short-circuit reader implementation based on HDFS-2246 is used if this configuration parameter is true. This is for the platforms other than Linux where the new implementation based on HDFS-347 is not available.,0,0,others,hdfs
2964,dfs.client.write.byte-array-manager.count-limit,The maximum number of arrays allowed for each array length.,1,5,workload-specific,hdfs
2965,dfs.client.write.byte-array-manager.count-reset-time-period-ms,The time period in milliseconds that the allocation count for each array length is reset to zero if there is no increment.,0,0,others,hdfs
2966,dfs.client.write.byte-array-manager.count-threshold,"The count threshold for each array length so that a manager is created only after the allocation count exceeds the threshold. In other words, the particular array length is not managed until the allocation count exceeds the threshold.",1,5,workload-specific,hdfs
2968,dfs.client.write.exclude.nodes.cache.expiry.interval.millis,"The maximum period to keep a DN in the excluded nodes list at a client. After this period, in milliseconds, the previously excluded node(s) will be removed automatically from the cache and will be considered good for block allocations again. Useful to lower or raise in situations where you keep a file open for very long periods (such as a Write-Ahead-Log (WAL) file) to make the writer tolerant to cluster maintenance restarts. Defaults to 10 minutes.",1,5,workload-specific,hdfs
2969,dfs.client.write.max-packets-in-flight,The maximum number of DFSPackets allowed in flight.,1,3,reliability-tradeoff,hdfs
2970,dfs.client-write-packet-size,Packet size for clients to write,1,1,resource,hdfs
2971,dfs.cluster.administrators,"ACL for the admins, this configuration is used to control who can access the default servlets in the namenode, etc. The value should be a comma separated list of users and groups. The user list comes first and is separated by a space followed by the group list, e.g. ""user1,user2 group1,group2"". Both users and groups are optional, so ""user1"", "" group1"", """", ""user1 group1"", ""user1,user2 group1,group2"" are all valid (note the leading space in "" group1""). '*' grants access to all users and groups, e.g. '*', '* ' and ' *' are all valid.",0,0,others,hdfs
2972,dfs.content-summary.limit,The maximum content summary counts allowed in one locking period. 0 or a negative number means no limit (i.e. no yielding).,1,5,workload-specific,hdfs
2973,dfs.content-summary.sleep-microsec,"The length of time in microseconds to put the thread to sleep, between reaquiring the locks in content summary computation.",0,0,others,hdfs
2975,dfs.data.transfer.protection,"A comma-separated list of SASL protection values used for secured connections to the DataNode when reading or writing block data. Possible values are authentication, integrity and privacy. authentication means authentication only and no integrity or privacy; integrity implies authentication and integrity are enabled; and privacy implies all of authentication, integrity and privacy are enabled. If dfs.encrypt.data.transfer is set to true, then it supersedes the setting for dfs.data.transfer.protection and enforces that all connections must use a specialized encrypted SASL handshake. This property is ignored for connections to a DataNode listening on a privileged port. In this case, it is assumed that the use of a privileged port establishes sufficient trust.",1,2,security-tradeoff,hdfs
2976,dfs.data.transfer.saslproperties.resolver.class,"SaslPropertiesResolver used to resolve the QOP used for a connection to the DataNode when reading or writing block data. If not specified, the value of hadoop.security.saslproperties.resolver.class is used as the default value.",0,0,others,hdfs
2977,dfs.data.transfer.server.tcpnodelay,"If true, set TCP_NODELAY to sockets for transferring data between Datanodes.",1,4,limited-side-effect,hdfs
2978,dfs.datanode.address,The datanode server address and port for data transfer.,0,0,others,hdfs
2979,dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction,"Only used when the dfs.datanode.fsdataset.volume.choosing.policy is set to org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy. This setting controls what percentage of new block allocations will be sent to volumes with more available disk space than others. This setting should be in the range 0.0 - 1.0, though in practice 0.5 - 1.0, since there should be no reason to prefer that volumes with less available disk space receive more block allocations.",1,5,workload-specific,hdfs
2980,dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold,"Only used when the dfs.datanode.fsdataset.volume.choosing.policy is set to org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy. This setting controls how much DN volumes are allowed to differ in terms of bytes of free disk space before they are considered imbalanced. If the free space of all the volumes are within this range of each other, the volumes will be considered balanced and block assignments will be done on a pure round robin basis. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize.",1,5,workload-specific,hdfs
2981,dfs.datanode.balance.bandwidthPerSec,"Specifies the maximum amount of bandwidth that each datanode can utilize for the balancing purpose in term of the number of bytes per second. You can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa)to specify the size (such as 128k, 512m, 1g, etc.). Or provide complete size in bytes (such as 134217728 for 128 MB).",1,1,resource,hdfs
2982,dfs.datanode.balance.max.concurrent.moves,"Maximum number of threads for Datanode balancer pending moves. This value is reconfigurable via the ""dfsadmin -reconfig"" command.",1,1,resource,hdfs
2983,dfs.datanode.block.id.layout.upgrade.threads,The number of threads to use when creating hard links from current to previous blocks during upgrade of a DataNode to block ID-based block layout (see HDFS-6482 for details on the layout).,1,1,resource,hdfs
2984,dfs.datanode.block-pinning.enabled,Whether pin blocks on favored DataNode.,0,0,others,hdfs
2985,dfs.datanode.bp-ready.timeout,"The maximum wait time for datanode to be ready before failing the received request. Setting this to 0 fails requests right away if the datanode is not yet registered with the namenode. This wait time reduces initial request failures after datanode restart. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.If no time unit is specified then seconds is assumed.",0,0,others,hdfs
2986,dfs.datanode.cache.revocation.polling.ms,How often the DataNode should poll to see if the clients have stopped using a replica that the DataNode wants to uncache.,1,5,workload-specific,hdfs
2988,dfs.datanode.cached-dfsused.check.interval.ms,"The interval check time of loading DU_CACHE_FILE in each volume. When the cluster doing the rolling upgrade operations, it will usually lead dfsUsed cache file of each volume expired and redo the du operations in datanode and that makes datanode start slowly. Adjust this property can make cache file be available for the time as you want.",1,6,function-tradeoff,hdfs
2989,dfs.datanode.data.dir,"Determines where on the local filesystem an DFS data node should store its blocks. If this is a comma-delimited list of directories, then data will be stored in all named directories, typically on different devices. The directories should be tagged with corresponding storage types ([SSD]/[DISK]/[ARCHIVE]/[RAM_DISK]) for HDFS storage policies. The default storage type will be DISK if the directory does not have a storage type tagged explicitly. Directories that do not exist will be created if local filesystem permission allows.",0,0,others,hdfs
2990,dfs.datanode.data.dir.perm,Permissions for the directories on on the local filesystem where the DFS data node store its blocks. The permissions can either be octal or symbolic.,0,0,others,hdfs
2991,dfs.datanode.data.transfer.bandwidthPerSec,"Specifies the maximum amount of bandwidth that the data transfering can utilize for transfering block when BlockConstructionStage is PIPELINE_SETUP_CREATE and clientName is empty. When the bandwidth value is zero, there is no limit.",1,1,resource,hdfs
2993,dfs.datanode.directoryscan.interval,"Interval in seconds for Datanode to scan data directories and reconcile the difference between blocks in memory and on the disk. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.If no time unit is specified then seconds is assumed.",0,0,others,hdfs
2996,dfs.datanode.disk.check.min.gap,The minimum gap between two successive checks of the same DataNode volume. This setting supports multiple time unit suffixes as described in dfs.heartbeat.interval. If no suffix is specified then milliseconds is assumed.,0,0,others,hdfs
2997,dfs.datanode.disk.check.timeout,Maximum allowed time for a disk check to complete. If the check does not complete within this time interval then the disk is declared as failed. This setting supports multiple time unit suffixes as described in dfs.heartbeat.interval. If no suffix is specified then milliseconds is assumed.,0,0,others,hdfs
2998,dfs.datanode.dns.interface,The name of the Network Interface from which a data node should report its IP address. e.g. eth2. This setting may be required for some multi-homed nodes where the DataNodes are assigned multiple hostnames and it is desirable for the DataNodes to use a non-default hostname. Prefer using hadoop.security.dns.interface over dfs.datanode.dns.interface.,0,0,others,hdfs
2999,dfs.datanode.dns.nameserver,The host name or IP address of the name server (DNS) which a DataNode should use to determine its own host name. Prefer using hadoop.security.dns.nameserver over dfs.datanode.dns.nameserver.,0,0,others,hdfs
3000,dfs.datanode.drop.cache.behind.reads,"In some workloads, the data read from HDFS is known to be significantly large enough that it is unlikely to be useful to cache it in the operating system buffer cache. In this case, the DataNode may be configured to automatically purge all data from the buffer cache after it is delivered to the client. This behavior is automatically disabled for workloads which read only short sections of a block (e.g HBase random-IO workloads). This may improve performance for some workloads by freeing buffer cache space usage for more cacheable data. If the Hadoop native libraries are not available, this configuration has no effect.",1,4,limited-side-effect,hdfs
3001,dfs.datanode.drop.cache.behind.writes,"In some workloads, the data written to HDFS is known to be significantly large enough that it is unlikely to be useful to cache it in the operating system buffer cache. In this case, the DataNode may be configured to automatically purge all data from the buffer cache after it is written to disk. This may improve performance for some workloads by freeing buffer cache space usage for more cacheable data. If the Hadoop native libraries are not available, this configuration has no effect.",1,4,limited-side-effect,hdfs
3002,dfs.datanode.du.reserved,"Reserved space in bytes per volume. Always leave this much space free for non dfs use. Specific storage type based reservation is also supported. The property can be followed with corresponding storage types ([ssd]/[disk]/[archive]/[ram_disk]) for cluster with heterogeneous storage. For example, reserved space for RAM_DISK storage can be configured using property 'dfs.datanode.du.reserved.ram_disk'. If specific storage type reservation is not configured then dfs.datanode.du.reserved will be used. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize. Note: In case of using tune2fs to set reserved-blocks-percentage, or other filesystem tools, then you can possibly run into out of disk errors because hadoop will not check those external tool configurations.",1,5,workload-specific,hdfs
3003,dfs.datanode.du.reserved.calculator,"Determines the class of ReservedSpaceCalculator to be used for calculating disk space reserved for non-HDFS data. The default calculator is ReservedSpaceCalculatorAbsolute which will use dfs.datanode.du.reserved for a static reserved number of bytes. ReservedSpaceCalculatorPercentage will use dfs.datanode.du.reserved.pct to calculate the reserved number of bytes based on the size of the storage. ReservedSpaceCalculatorConservative and ReservedSpaceCalculatorAggressive will use their combination, Conservative will use maximum, Aggressive minimum. For more details see ReservedSpaceCalculator.",1,5,workload-specific,hdfs
3005,dfs.datanode.ec.reconstruction.stripedread.buffer.size,Datanode striped read buffer size.,1,1,resource,hdfs
3006,dfs.datanode.ec.reconstruction.stripedread.timeout.millis,Datanode striped read timeout in milliseconds.,0,0,others,hdfs
3007,dfs.datanode.ec.reconstruction.threads,Number of threads used by the Datanode for background reconstruction work.,1,1,resource,hdfs
3008,dfs.datanode.ec.reconstruction.xmits.weight,"Datanode uses xmits weight to calculate the relative cost of EC recovery tasks comparing to replicated block recovery, of which xmits is always 1. Namenode then uses xmits reported from datanode to throttle recovery tasks for EC and replicated blocks. The xmits of an erasure coding recovery task is calculated as the maximum value between the number of read streams and the number of write streams.",0,0,others,hdfs
3009,dfs.datanode.failed.volumes.tolerated,"The number of volumes that are allowed to fail before a datanode stops offering service. By default any volume failure will cause a datanode to shutdown. The value should be greater than or equal to -1 , -1 represents minimum 1 valid volume.",0,0,others,hdfs
3010,dfs.datanode.fileio.profiling.sampling.percentage,This setting controls the percentage of file I/O events which will be profiled for DataNode disk statistics. The default value of 0 disables disk statistics. Set to an integer value between 1 and 100 to enable disk statistics.,1,5,workload-specific,hdfs
3011,dfs.datanode.fixed.volume.size,"If false, call function getTotalSpace of File to get capacity of volume during every heartbeat. If true, cache the capacity when when the first call, and reuse it later.",1,4,limited-side-effect,hdfs
3012,dfs.datanode.fsdataset.factory,The class name for the underlying storage that stores replicas for a Datanode. Defaults to org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.,0,0,others,hdfs
3013,dfs.datanode.fsdataset.volume.choosing.policy,"The class name of the policy for choosing volumes in the list of directories. Defaults to org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy. If you would like to take into account available disk space, set the value to ""org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy"".",0,0,others,hdfs
3014,dfs.datanode.fsdatasetcache.max.threads.per.volume,The maximum number of threads per volume to use for caching new data on the datanode. These threads consume both I/O and CPU. This can affect normal datanode operations.,1,1,resource,hdfs
3015,dfs.datanode.handler.count,The number of server threads for the datanode.,1,1,resource,hdfs
3016,dfs.datanode.hostname,Optional. The hostname for the Datanode containing this configuration file. Will be different for each machine. Defaults to current hostname.,0,0,others,hdfs
3017,dfs.datanode.http.address,The datanode http server address and port.,0,0,others,hdfs
3018,dfs.datanode.http.internal-proxy.port,The datanode's internal web proxy port. By default it selects a random port available in runtime.,0,0,others,hdfs
3019,dfs.datanode.https.address,The datanode secure http server address and port.,0,0,others,hdfs
3020,dfs.datanode.httpserver.filter.handlers,Comma separated list of Netty servlet-style filter handlers to inject into the Datanode WebHDFS I/O path,0,0,others,hdfs
3021,dfs.datanode.ipc.address,The datanode ipc server address and port.,0,0,others,hdfs
3022,dfs.datanode.kerberos.principal,The DataNode service principal. This is typically set to dn/_HOST@REALM.TLD. Each DataNode will substitute _HOST with its own fully qualified hostname at startup. The _HOST placeholder allows using the same configuration setting on all DataNodes.,0,0,others,hdfs
3023,dfs.datanode.keytab.file,The keytab file used by each DataNode daemon to login as its service principal. The principal name is configured with dfs.datanode.kerberos.principal.,0,0,others,hdfs
3024,dfs.datanode.lazywriter.interval.sec,Interval in seconds for Datanodes for lazy persist writes.,0,0,others,hdfs
3025,dfs.datanode.lifeline.interval.seconds,"Sets the interval in seconds between sending DataNode Lifeline Protocol messages from the DataNode to the NameNode. The value must be greater than the value of dfs.heartbeat.interval. If this property is not defined, then the default behavior is to calculate the interval as 3x the value of dfs.heartbeat.interval. Note that normal heartbeat processing may cause the DataNode to postpone sending lifeline messages if they are not required. Under normal operations with speedy heartbeat processing, it is possible that no lifeline messages will need to be sent at all. This property has no effect if dfs.namenode.lifeline.rpc-address is not defined.",0,0,others,hdfs
3026,dfs.datanode.lock.fair,"If this is true, the Datanode FsDataset lock will be used in Fair mode, which will help to prevent writer threads from being starved, but can lower lock throughput. See java.util.concurrent.locks.ReentrantReadWriteLock for more information on fair/non-fair locks.",1,3,reliability-tradeoff,hdfs
3027,dfs.datanode.lock-reporting-threshold-ms,"When thread waits to obtain a lock, or a thread holds a lock for more than the threshold, a log message will be written. Note that dfs.lock.suppress.warning.interval ensures a single log message is emitted per interval for waiting threads and a single message for holding threads to avoid excessive logging.",0,0,others,hdfs
3028,dfs.datanode.max.locked.memory,"The amount of memory in bytes to use for caching of block replicas in memory on the datanode. The datanode's maximum locked memory soft ulimit (RLIMIT_MEMLOCK) must be set to at least this value, else the datanode will abort on startup. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize. By default, this parameter is set to 0, which disables in-memory caching. If the native libraries are not available to the DataNode, this configuration has no effect.",1,1,resource,hdfs
3029,dfs.datanode.max.transfer.threads,Specifies the maximum number of threads to use for transferring data in and out of the DN.,1,1,resource,hdfs
3030,dfs.datanode.metrics.logger.period.seconds,This setting controls how frequently the DataNode logs its metrics. The logging configuration must also define one or more appenders for DataNodeMetricsLog for the metrics to be logged. DataNode metrics logging is disabled if this value is set to zero or less than zero.,0,0,others,hdfs
3032,dfs.datanode.oob.timeout-ms,"Timeout value when sending OOB response for each OOB type, which are OOB_RESTART, OOB_RESERVED1, OOB_RESERVED2, and OOB_RESERVED3, respectively. Currently, only OOB_RESTART is used.",0,0,others,hdfs
3033,dfs.datanode.outliers.report.interval,This setting controls how frequently DataNodes will report their peer latencies to the NameNode via heartbeats. This setting supports multiple time unit suffixes as described in dfs.heartbeat.interval. If no suffix is specified then milliseconds is assumed. It is ignored if dfs.datanode.peer.stats.enabled is false.,0,0,others,hdfs
3036,dfs.datanode.peer.stats.enabled,A switch to turn on/off tracking DataNode peer statistics.,1,6,function-tradeoff,hdfs
3037,dfs.datanode.plugins,Comma-separated list of datanode plug-ins to be activated.,0,0,others,hdfs
3038,dfs.datanode.pmem.cache.dirs,"This value specifies the persistent memory directory used for caching block replica. Multiple directories separated by "","" are acceptable.",0,0,others,hdfs
3041,dfs.datanode.ram.disk.replica.tracker,Name of the class implementing the RamDiskReplicaTracker interface. Defaults to org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaLruTracker.,0,0,others,hdfs
3042,dfs.datanode.readahead.bytes,"While reading block files, if the Hadoop native libraries are available, the datanode can use the posix_fadvise system call to explicitly page data into the operating system buffer cache ahead of the current reader's position. This can improve performance especially when disks are highly contended. This configuration specifies the number of bytes ahead of the current read position which the datanode will attempt to read ahead. This feature may be disabled by configuring this property to 0. If the native libraries are not available, this configuration has no effect.",1,4,limited-side-effect,hdfs
3043,dfs.datanode.replica.cache.expiry.time,Living time of replica cached files in milliseconds.,1,5,workload-specific,hdfs
3044,dfs.datanode.replica.cache.root.dir,Use this key to change root dir of replica cache. The default root dir is currentDir.,0,0,others,hdfs
3045,dfs.datanode.restart.replica.expiration,"During shutdown for restart, the amount of time in seconds budgeted for datanode restart.",0,0,others,hdfs
3046,dfs.datanode.scan.period.hours,"If this is positive, the DataNode will not scan any individual block more than once in the specified scan period. If this is negative, the block scanner is disabled. If this is set to zero, then the default value of 504 hours or 3 weeks is used. Prior versions of HDFS incorrectly documented that setting this key to zero will disable the block scanner.",0,0,others,hdfs
3049,dfs.datanode.socket.reuse.keepalive,"The window of time in ms before the DataXceiver closes a socket for a single request. If a second request occurs within that window, the socket can be reused.",0,0,others,hdfs
3050,dfs.datanode.socket.write.timeout,Timeout in ms for clients socket writes to DataNodes.,0,0,others,hdfs
3051,dfs.datanode.sync.behind.writes,"If this configuration is enabled, the datanode will instruct the operating system to enqueue all written data to the disk immediately after it is written. This differs from the usual OS policy which may wait for up to 30 seconds before triggering writeback. This may improve performance for some workloads by smoothing the IO profile for data written to disk. If the Hadoop native libraries are not available, this configuration has no effect.",1,5,workload-specific,hdfs
3052,dfs.datanode.sync.behind.writes.in.background,"If set to true, then sync_file_range() system call will occur asynchronously. This property is only valid when the property dfs.datanode.sync.behind.writes is true.",1,4,limited-side-effect,hdfs
3053,dfs.datanode.transfer.socket.recv.buffer.size,"Socket receive buffer size for DataXceiver (receiving packets from client during block writing). This may affect TCP connection throughput. If it is set to zero or negative value, no buffer size will be set explicitly, thus enable tcp auto-tuning on some system. The default value is 0.",1,1,resource,hdfs
3054,dfs.datanode.transfer.socket.send.buffer.size,"Socket send buffer size for DataXceiver (mirroring packets to downstream in pipeline). This may affect TCP connection throughput. If it is set to zero or negative value, no buffer size will be set explicitly, thus enable tcp auto-tuning on some system. The default value is 0.",1,1,resource,hdfs
3055,dfs.datanode.transferTo.allowed,"If false, break block transfers on 32-bit machines greater than or equal to 2GB into smaller chunks.",1,4,limited-side-effect,hdfs
3056,dfs.datanode.use.datanode.hostname,Whether datanodes should use datanode hostnames when connecting to other datanodes for data transfer.,0,0,others,hdfs
3057,dfs.datanode.volumes.replica-add.threadpool.size,"Specifies the maximum number of threads to use for adding block in volume. Default value for this configuration is max of (volume * number of bp_service, number of processor).",1,1,resource,hdfs
3059,dfs.disk.balancer.block.tolerance.percent,"When a disk balancer copy operation is proceeding, the datanode is still active. So it might not be possible to move the exactly specified amount of data. So tolerance allows us to define a percentage which defines a good enough move.",1,5,workload-specific,hdfs
3060,dfs.disk.balancer.enabled,"This enables the diskbalancer feature on a cluster. By default, disk balancer is enabled.",1,4,limited-side-effect,hdfs
3062,dfs.disk.balancer.max.disk.throughputInMBperSec,Maximum disk bandwidth used by diskbalancer during read from a source disk. The unit is MB/sec.,1,1,resource,hdfs
3064,dfs.disk.balancer.plan.valid.interval,Maximum amount of time disk balancer plan is valid. This setting supports multiple time unit suffixes as described in dfs.heartbeat.interval. If no suffix is specified then milliseconds is assumed.,0,0,others,hdfs
3065,dfs.domain.socket.disable.interval.seconds,"The interval that a DataNode is disabled for future Short-Circuit Reads, after an error happens during a Short-Circuit Read. Setting this to 0 will not disable Short-Circuit Reads at all after errors happen. Negative values are invalid.",0,0,others,hdfs
3066,dfs.domain.socket.path,"Optional. This is a path to a UNIX domain socket that will be used for communication between the DataNode and local HDFS clients. If the string ""_PORT"" is present in this path, it will be replaced by the TCP port of the DataNode.",0,0,others,hdfs
3067,dfs.edit.log.transfer.bandwidthPerSec,"Maximum bandwidth used for transferring edit log to between journal nodes for syncing, in bytes per second. A default value of 0 indicates that throttling is disabled.",1,1,resource,hdfs
3068,dfs.edit.log.transfer.timeout,Socket timeout for edit log transfer in milliseconds. This timeout should be configured such that normal edit log transfer for journal node syncing can complete successfully.,0,0,others,hdfs
3069,dfs.encrypt.data.overwrite.downstream.derived.qop,"A boolean specifies whether DN should overwrite the downstream QOP in a write pipeline. This is used in the case where client talks to first DN with a QOP, but inter-DN communication needs to be using a different QOP. If set to false, the default behaviour is that inter-DN communication will use the same QOP as client-DN connection.",0,0,others,hdfs
3070,dfs.encrypt.data.overwrite.downstream.new.qop,"When dfs.datanode.overwrite.downstream.derived.qop is set to true, this configuration specifies the new QOP to be used to overwrite inter-DN QOP.",0,0,others,hdfs
3071,dfs.encrypt.data.transfer,"Whether or not actual block data that is read/written from/to HDFS should be encrypted on the wire. This only needs to be set on the NN and DNs, clients will deduce this automatically. It is possible to override this setting per connection by specifying custom logic via dfs.trustedchannel.resolver.class.",1,2,security-tradeoff,hdfs
3074,dfs.encrypt.data.transfer.cipher.suites,"This value may be either undefined or AES/CTR/NoPadding. If defined, then dfs.encrypt.data.transfer uses the specified cipher suite for data encryption. If not defined, then only the algorithm specified in dfs.encrypt.data.transfer.algorithm is used. By default, the property is not defined.",1,2,security-tradeoff,hdfs
3075,dfs.ha.automatic-failover.enabled,Whether automatic failover is enabled. See the HDFS High Availability documentation for details on automatic HA configuration.,0,0,others,hdfs
3076,dfs.ha.fencing.methods,A list of scripts or Java classes which will be used to fence the Active NameNode during a failover. See the HDFS High Availability documentation for details on automatic HA configuration.,0,0,others,hdfs
3077,dfs.ha.log-roll.period,"How often, in seconds, the StandbyNode should ask the active to roll edit logs. Since the StandbyNode only reads from finalized log segments, the StandbyNode will only be as up-to-date as how often the logs are rolled. Note that failover triggers a log roll so the StandbyNode will be up to date before it becomes active. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.If no time unit is specified then seconds is assumed.",0,0,others,hdfs
3078,dfs.ha.namenode.id,The ID of this namenode. If the namenode ID is not configured it is determined automatically by matching the local node's address with the configured address.,0,0,others,hdfs
3079,dfs.ha.namenodes.EXAMPLENAMESERVICE,"The prefix for a given nameservice, contains a comma-separated list of namenodes for a given nameservice (eg EXAMPLENAMESERVICE). Unique identifiers for each NameNode in the nameservice, delimited by commas. This will be used by DataNodes to determine all the NameNodes in the cluster. For example, if you used ""mycluster"" as the nameservice ID previously, and you wanted to use ""nn1"" and ""nn2"" as the individual IDs of the NameNodes, you would configure a property dfs.ha.namenodes.mycluster, and its value ""nn1,nn2"".",0,0,others,hdfs
3080,dfs.ha.nn.not-become-active-in-safemode,This will prevent safe mode namenodes to become active while other standby namenodes might be ready to serve requests when it is set to true.,0,0,others,hdfs
3081,dfs.ha.standby.checkpoints,"If true, a NameNode in Standby state periodically takes a checkpoint of the namespace, saves it to its local storage and then upload to the remote NameNode.",1,3,reliability-tradeoff,hdfs
3082,dfs.ha.tail-edits.in-progress,"Whether enable standby namenode to tail in-progress edit logs. Clients might want to turn it on when they want Standby NN to have more up-to-date data. When using the QuorumJournalManager, this enables tailing of edit logs via the RPC-based mechanism, rather than streaming, which allows for much fresher data.",1,3,reliability-tradeoff,hdfs
3083,dfs.ha.tail-edits.namenode-retries,Number of retries to use when contacting the namenode when tailing the log.,0,0,others,hdfs
3084,dfs.ha.tail-edits.period,"How often, the StandbyNode and ObserverNode should check if there are new edit log entries ready to be consumed. This is the minimum period between checking; exponential backoff will be applied if no edits are found and dfs.ha.tail-edits.period.backoff-max is configured. By default, no backoff is applied. Supports multiple time unit suffix (case insensitive), as described in dfs.heartbeat.interval.",0,0,others,hdfs
3085,dfs.ha.tail-edits.period.backoff-max,"The maximum time the tailer should wait between checking for new edit log entries. Exponential backoff will be applied when an edit log tail is performed but no edits are available to be read. Values less than or equal to zero disable backoff entirely; this is the default behavior. Supports multiple time unit suffix (case insensitive), as described in dfs.heartbeat.interval.",0,0,others,hdfs
3086,dfs.ha.tail-edits.rolledits.timeout,The timeout in seconds of calling rollEdits RPC on Active NN.,0,0,others,hdfs
3087,dfs.ha.zkfc.nn.http.timeout.ms,"The HTTP connection and read timeout value (unit is ms ) when DFS ZKFC tries to get local NN thread dump after local NN becomes SERVICE_NOT_RESPONDING or SERVICE_UNHEALTHY. If it is set to zero, DFS ZKFC won't get local NN thread dump.",0,0,others,hdfs
3088,dfs.ha.zkfc.port,The port number that the zookeeper failover controller RPC server binds to.,0,0,others,hdfs
3090,dfs.hosts,"Names a file that contains a list of hosts that are permitted to connect to the namenode. The full pathname of the file must be specified. If the value is empty, all hosts are permitted.",0,0,others,hdfs
3091,dfs.hosts.exclude,"Names a file that contains a list of hosts that are not permitted to connect to the namenode. The full pathname of the file must be specified. If the value is empty, no hosts are excluded.",0,0,others,hdfs
3092,dfs.http.client.failover.max.attempts,Specify the max number of failover attempts for WebHDFS client in case of network exception.,0,0,others,hdfs
3093,dfs.http.client.failover.sleep.base.millis,Specify the base amount of time in milliseconds upon which the exponentially increased sleep time between retries or failovers is calculated for WebHDFS client.,0,0,others,hdfs
3094,dfs.http.client.failover.sleep.max.millis,Specify the upper bound of sleep time in milliseconds between retries or failovers for WebHDFS client.,0,0,others,hdfs
3097,dfs.http.client.retry.policy.spec,"Specify a policy of multiple linear random retry for WebHDFS client, e.g. given pairs of number of retries and sleep time (n0, t0), (n1, t1), ..., the first n0 retries sleep t0 milliseconds on average, the following n1 retries sleep t1 milliseconds on average, and so on.",0,0,others,hdfs
3098,dfs.http.policy,Decide if HTTPS(SSL) is supported on HDFS This configures the HTTP endpoint for HDFS daemons: The following values are supported: - HTTP_ONLY : Service is provided only on http - HTTPS_ONLY : Service is provided only on https - HTTP_AND_HTTPS : Service is provided both on http and https,1,2,security-tradeoff,hdfs
3099,dfs.https.server.keystore.resource,Resource file from which ssl server keystore information will be extracted,0,0,others,hdfs
3101,dfs.image.compression.codec,"If the dfs image is compressed, how should they be compressed? This has to be a codec defined in io.compression.codecs.",1,6,function-tradeoff,hdfs
3102,dfs.image.parallel.inode.threshold,"If the image contains less inodes than this setting, then do not write sub-sections and hence disable parallel loading. This is because small images load very quickly in serial and parallel loading is not needed.",1,5,workload-specific,hdfs
3104,dfs.image.parallel.target.sections,"Controls the number of sub-sections that will be written to fsimage for each section. This should be larger than dfs.image.parallel.threads, otherwise all threads will not be used when loading. Ideally, have at least twice the number of target sections as threads, so each thread must load more than one section to avoid one long running section affecting the load time.",0,0,others,hdfs
3105,dfs.image.parallel.threads,The number of threads to use when dfs.image.parallel.load is enabled. This setting should be less than dfs.image.parallel.target.sections. The optimal number of threads will depend on the hardware and environment.,1,1,resource,hdfs
3106,dfs.image.transfer.bandwidthPerSec,"Maximum bandwidth used for regular image transfers (instead of bootstrapping the standby namenode), in bytes per second. This can help keep normal namenode operations responsive during checkpointing. A default value of 0 indicates that throttling is disabled. The maximum bandwidth used for bootstrapping standby namenode is configured with dfs.image.transfer-bootstrap-standby.bandwidthPerSec. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize.",1,1,resource,hdfs
3107,dfs.image.transfer.chunksize,"Chunksize in bytes to upload the checkpoint. Chunked streaming is used to avoid internal buffering of contents of image file of huge size. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize.",1,5,workload-specific,hdfs
3108,dfs.image.transfer.timeout,"Socket timeout for the HttpURLConnection instance used in the image transfer. This is measured in milliseconds. This timeout prevents client hangs if the connection is idle for this configured timeout, during image transfer.",0,0,others,hdfs
3109,dfs.image.transfer-bootstrap-standby.bandwidthPerSec,"Maximum bandwidth used for transferring image to bootstrap standby namenode, in bytes per second. A default value of 0 indicates that throttling is disabled. This default value should be used in most cases, to ensure timely HA operations. The maximum bandwidth used for regular image transfers is configured with dfs.image.transfer.bandwidthPerSec. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize.",1,1,resource,hdfs
3111,dfs.journalnode.edit-cache-size.bytes,"The size, in bytes, of the in-memory cache of edits to keep on the JournalNode. This cache is used to serve edits for tailing via the RPC-based mechanism, and is only enabled when dfs.ha.tail-edits.in-progress is true. Transactions range in size but are around 200 bytes on average, so the default of 1MB can store around 5000 transactions.",1,1,resource,hdfs
3112,dfs.journalnode.edits.dir,The directory where the journal edit files are stored.,0,0,others,hdfs
3113,dfs.journalnode.edits.dir.perm,Permissions for the directories on on the local filesystem where the DFS journal node stores the edits. The permissions can either be octal or symbolic.,0,0,others,hdfs
3114,dfs.journalnode.enable.sync,"If true, the journal nodes wil sync with each other. The journal nodes will periodically gossip with other journal nodes to compare edit log manifests and if they detect any missing log segment, they will download it from the other journal nodes.",1,3,reliability-tradeoff,hdfs
3115,dfs.journalnode.http-address,The address and port the JournalNode HTTP server listens on. If the port is 0 then the server will start on a free port.,0,0,others,hdfs
3116,dfs.journalnode.http-bind-host,"The actual address the HTTP server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.journalnode.http-address. This is useful for making the JournalNode HTTP server listen on allinterfaces by setting it to 0.0.0.0.",0,0,others,hdfs
3117,dfs.journalnode.https-address,The address and port the JournalNode HTTPS server listens on. If the port is 0 then the server will start on a free port.,0,0,others,hdfs
3118,dfs.journalnode.https-bind-host,"The actual address the HTTP server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.journalnode.https-address. This is useful for making the JournalNode HTTP server listen on all interfaces by setting it to 0.0.0.0.",0,0,others,hdfs
3119,dfs.journalnode.kerberos.internal.spnego.principal,"The server principal used by the JournalNode HTTP Server for SPNEGO authentication when Kerberos security is enabled. This is typically set to HTTP/_HOST@REALM.TLD. The SPNEGO server principal begins with the prefix HTTP/ by convention. If the value is '*', the web server will attempt to login with every principal specified in the keytab file dfs.web.authentication.kerberos.keytab. For most deployments this can be set to ${dfs.web.authentication.kerberos.principal} i.e use the value of dfs.web.authentication.kerberos.principal.",0,0,others,hdfs
3120,dfs.journalnode.kerberos.principal,The JournalNode service principal. This is typically set to jn/_HOST@REALM.TLD. Each JournalNode will substitute _HOST with its own fully qualified hostname at startup. The _HOST placeholder allows using the same configuration setting on all JournalNodes.,0,0,others,hdfs
3121,dfs.journalnode.keytab.file.A,Kerberos keytab file for the journal node.,0,0,others,hdfs
3122,dfs.journalnode.keytab.file.B,The keytab file used by each JournalNode daemon to login as its service principal. The principal name is configured with dfs.journalnode.kerberos.principal.,0,0,others,hdfs
3123,dfs.journalnode.rpc-address,The JournalNode RPC server address and port.,0,0,others,hdfs
3124,dfs.journalnode.rpc-bind-host,"The actual address the RPC server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.journalnode.rpc-address. This is useful for making the JournalNode listen on all interfaces by setting it to 0.0.0.0.",0,0,others,hdfs
3125,dfs.journalnode.sync.interval,"Time interval, in milliseconds, between two Journal Node syncs. This configuration takes effect only if the journalnode sync is enabled by setting the configuration parameter dfs.journalnode.enable.sync to true.",0,0,others,hdfs
3127,dfs.ls.limit,"Limit the number of files printed by ls. If less or equal to zero, at most DFS_LIST_LIMIT_DEFAULT (= 1000) will be printed.",0,0,others,hdfs
3128,dfs.metrics.percentiles.intervals,"Comma-delimited set of integers denoting the desired rollover intervals (in seconds) for percentile latency metrics on the Namenode and Datanode. By default, percentile latency metrics are disabled.",0,0,others,hdfs
3129,dfs.mover.address,The hostname used for a keytab based Kerberos login. Keytab based login can be enabled with dfs.mover.keytab.enabled.,0,0,others,hdfs
3130,dfs.mover.kerberos.principal,The Mover principal. This is typically set to mover/_HOST@REALM.TLD. The Mover will substitute _HOST with its own fully qualified hostname at startup. The _HOST placeholder allows using the same configuration setting on different servers. Keytab based login can be enabled with dfs.mover.keytab.enabled.,0,0,others,hdfs
3131,dfs.mover.keytab.enabled,Set to true to enable login using a keytab for Kerberized Hadoop.,0,0,others,hdfs
3132,dfs.mover.keytab.file,The keytab file used by the Mover to login as its service principal. The principal name is configured with dfs.mover.kerberos.principal. Keytab based login can be enabled with dfs.mover.keytab.enabled.,0,0,others,hdfs
3134,dfs.mover.movedWinWidth,"The minimum time interval, in milliseconds, that a block can be moved to another location again.",0,0,others,hdfs
3135,dfs.mover.moverThreads,Configure the balancer's mover thread pool size.,1,1,resource,hdfs
3137,dfs.namenode.accesstime.precision,The access time for HDFS file is precise upto this value. The default value is 1 hour. Setting a value of 0 disables access times for HDFS.,0,0,others,hdfs
3138,dfs.namenode.acls.enabled,"Set to true to enable support for HDFS ACLs (Access Control Lists). By default, ACLs are enabled. When ACLs are disabled, the NameNode rejects all RPCs related to setting or getting ACLs.",0,0,others,hdfs
3139,dfs.namenode.audit.log.async,"If true, enables asynchronous audit log.",1,4,limited-side-effect,hdfs
3140,dfs.namenode.audit.log.debug.cmdlist,A comma separated list of NameNode commands that are written to the HDFS namenode audit log only if the audit log level is debug.,0,0,others,hdfs
3142,dfs.namenode.audit.loggers,"List of classes implementing audit loggers that will receive audit events. These should be implementations of org.apache.hadoop.hdfs.server.namenode.AuditLogger. The special value ""default"" can be used to reference the default audit logger, which uses the configured log system. Installing custom audit loggers may affect the performance and stability of the NameNode. Refer to the custom logger's documentation for more details.",1,6,function-tradeoff,hdfs
3143,dfs.namenode.available-space-block-placement-policy.balanced-space-preference-fraction,"Only used when the dfs.block.replicator.classname is set to org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy. Special value between 0 and 1, noninclusive. Increases chance of placing blocks on Datanodes with less disk space used.",1,3,reliability-tradeoff,hdfs
3144,dfs.namenode.available-space-block-placement-policy.balance-local-node,"Only used when the dfs.block.replicator.classname is set to org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy. If true, balances the local node too.",1,4,limited-side-effect,hdfs
3145,dfs.namenode.avoid.read.stale.datanode,"Indicate whether or not to avoid reading from ""stale"" datanodes whose heartbeat messages have not been received by the namenode for more than a specified time interval. Stale datanodes will be moved to the end of the node list returned for reading. See dfs.namenode.avoid.write.stale.datanode for a similar setting for writes.",1,3,reliability-tradeoff,hdfs
3146,dfs.namenode.avoid.write.stale.datanode,"Indicate whether or not to avoid writing to ""stale"" datanodes whose heartbeat messages have not been received by the namenode for more than a specified time interval. Writes will avoid using stale datanodes unless more than a configured ratio (dfs.namenode.write.stale.datanode.ratio) of datanodes are marked as stale. See dfs.namenode.avoid.read.stale.datanode for a similar setting for reads.",1,3,reliability-tradeoff,hdfs
3147,dfs.namenode.backup.address,The backup node server address and port. If the port is 0 then the server will start on a free port.,0,0,others,hdfs
3148,dfs.namenode.backup.dnrpc-address,Service RPC address for the backup Namenode.,0,0,others,hdfs
3149,dfs.namenode.backup.http-address,The backup node http server address and port. If the port is 0 then the server will start on a free port.,0,0,others,hdfs
3152,dfs.namenode.blockreport.max.lock.hold.time,The BlockReportProcessingThread max write lock hold time in ms.,0,0,others,hdfs
3153,dfs.namenode.blockreport.queue.size,The queue size of BlockReportProcessingThread in BlockManager.,1,5,workload-specific,hdfs
3154,dfs.namenode.blocks.per.postponedblocks.rescan,Number of blocks to rescan for each iteration of postponedMisreplicatedBlocks.,1,6,function-tradeoff,hdfs
3155,dfs.namenode.caching.enabled,"Set to true to enable block caching. This flag enables the NameNode to maintain a mapping of cached blocks to DataNodes via processing DataNode cache reports. Based on these reports and addition and removal of caching directives, the NameNode will schedule caching and uncaching work.",1,4,limited-side-effect,hdfs
3156,dfs.namenode.checkpoint.check.period,"The SecondaryNameNode and CheckpointNode will poll the NameNode every 'dfs.namenode.checkpoint.check.period' seconds to query the number of uncheckpointed transactions. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.If no time unit is specified then seconds is assumed.",0,0,others,hdfs
3157,dfs.namenode.checkpoint.check.quiet-multiplier,"Used to calculate the amount of time between retries when in the 'quiet' period for creating checkpoints (active namenode already has an up-to-date image from another checkpointer), so we wait a multiplier of the dfs.namenode.checkpoint.check.period before retrying the checkpoint because another node likely is already managing the checkpoints, allowing us to save bandwidth to transfer checkpoints that don't need to be used.",0,0,others,hdfs
3159,dfs.namenode.checkpoint.edits.dir,Determines where on the local filesystem the DFS secondary name node should store the temporary edits to merge. If this is a comma-delimited list of directories then the edits is replicated in all of the directories for redundancy. Default value is same as dfs.namenode.checkpoint.dir,0,0,others,hdfs
3161,dfs.namenode.checkpoint.period,"The number of seconds between two periodic checkpoints. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.If no time unit is specified then seconds is assumed.",0,0,others,hdfs
3162,dfs.namenode.checkpoint.txns,"The Secondary NameNode or CheckpointNode will create a checkpoint of the namespace every 'dfs.namenode.checkpoint.txns' transactions, regardless of whether 'dfs.namenode.checkpoint.period' has expired.",0,0,others,hdfs
3163,dfs.namenode.corrupt.block.delete.immediately.enabled,"Whether the corrupt replicas should be deleted immediately, irrespective of other replicas on stale storages.",1,4,limited-side-effect,hdfs
3166,dfs.namenode.decommission.backoff.monitor.pending.limit,"When the Backoff monitor is enabled, determines the maximum number of blocks related to decommission and maintenance operations that can be loaded into the replication queue at any given time. Every dfs.namenode.decommission.interval seconds, the list is checked to see if the blocks have become fully replicated and then further blocks are added to reach the limit defined in this parameter.",0,0,others,hdfs
3167,dfs.namenode.decommission.blocks.per.interval,"The approximate number of blocks to process per decommission or maintenance interval, as defined in dfs.namenode.decommission.interval.",1,1,resource,hdfs
3168,dfs.namenode.decommission.interval,"Namenode periodicity in seconds to check if decommission or maintenance is complete. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval. If no time unit is specified then seconds is assumed.",0,0,others,hdfs
3169,dfs.namenode.decommission.max.concurrent.tracked.nodes,The maximum number of decommission-in-progress or entering-maintenance datanodes nodes that will be tracked at one time by the namenode. Tracking these datanode consumes additional NN memory proportional to the number of blocks on the datnode. Having a conservative limit reduces the potential impact of decommissioning or maintenance of a large number of nodes at once. A value of 0 means no limit will be enforced.,1,1,resource,hdfs
3170,dfs.namenode.decommission.monitor.class,Determines the implementation used for the decommission manager. The only valid options are: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor,0,0,others,hdfs
3171,dfs.namenode.delegation.key.update-interval,The update interval for master key for delegation tokens in the namenode in milliseconds.,0,0,others,hdfs
3172,dfs.namenode.delegation.token.always-use,"For testing. Setting to true always allows the DT secret manager to be used, even if security is disabled.",0,0,others,hdfs
3173,dfs.namenode.delegation.token.max-lifetime,The maximum lifetime in milliseconds for which a delegation token is valid.,0,0,others,hdfs
3174,dfs.namenode.delegation.token.renew-interval,The renewal interval for delegation token in milliseconds.,0,0,others,hdfs
3175,dfs.namenode.ec.policies.max.cellsize,The maximum cell size of erasure coding policy. Default is 4MB.,0,0,others,hdfs
3176,dfs.namenode.ec.system.default.policy,The default erasure coding policy name will be used on the path if no policy name is passed.,0,0,others,hdfs
3178,dfs.namenode.edekcacheloader.initial.delay.ms,"When KeyProvider is configured, the time delayed until the first attempt to warm up edek cache on NN start up / become active.",1,5,workload-specific,hdfs
3179,dfs.namenode.edekcacheloader.interval.ms,"When KeyProvider is configured, the interval time of warming up edek cache on NN starts up / becomes active. All edeks will be loaded from KMS into provider cache. The edek cache loader will try to warm up the cache until succeed or NN leaves active state.",1,5,workload-specific,hdfs
3180,dfs.namenode.edit.log.autoroll.check.interval.ms,"How often an active namenode will check if it needs to roll its edit log, in milliseconds.",0,0,others,hdfs
3181,dfs.namenode.edit.log.autoroll.multiplier.threshold,"Determines when an active namenode will roll its own edit log. The actual threshold (in number of edits) is determined by multiplying this value by dfs.namenode.checkpoint.txns. This prevents extremely large edit files from accumulating on the active namenode, which can cause timeouts during namenode startup and pose an administrative hassle. This behavior is intended as a failsafe for when the standby or secondary namenode fail to roll the edit log by the normal checkpoint threshold.",1,5,workload-specific,hdfs
3182,dfs.namenode.edits.asynclogging,"If set to true, enables asynchronous edit logs in the Namenode. If set to false, the Namenode uses the traditional synchronous edit logs.",1,4,limited-side-effect,hdfs
3183,dfs.namenode.edits.dir,"Determines where on the local filesystem the DFS name node should store the transaction (edits) file. If this is a comma-delimited list of directories then the transaction file is replicated in all of the directories, for redundancy. Default value is same as dfs.namenode.name.dir",0,0,others,hdfs
3184,dfs.namenode.edits.dir.minimum,"dfs.namenode.edits.dir includes both required directories (specified by dfs.namenode.edits.dir.required) and optional directories. The number of usable optional directories must be greater than or equal to this property. If the number of usable optional directories falls below dfs.namenode.edits.dir.minimum, HDFS will issue an error. This property defaults to 1.",0,0,others,hdfs
3185,dfs.namenode.edits.dir.required,"This should be a subset of dfs.namenode.edits.dir, to ensure that the transaction (edits) file in these places is always up-to-date.",0,0,others,hdfs
3186,dfs.namenode.edits.journal-plugin,"When FSEditLog is creating JournalManagers from dfs.namenode.edits.dir, and it encounters a URI with a schema different to ""file"" it loads the name of the implementing class from ""dfs.namenode.edits.journal-plugin.[schema]"". This class must implement JournalManager and have a constructor which takes (Configuration, URI).",0,0,others,hdfs
3187,dfs.namenode.edits.noeditlogchannelflush,"Specifies whether to flush edit log file channel. When set, expensive FileChannel#force calls are skipped and synchronous disk writes are enabled instead by opening the edit log file with RandomAccessFile(""rws"") flags. This can significantly improve the performance of edit log writes on the Windows platform. Note that the behavior of the ""rws"" flags is platform and hardware specific and might not provide the same level of guarantees as FileChannel#force. For example, the write will skip the disk-cache on SAS and SCSI devices while it might not on SATA devices. This is an expert level setting, change with caution.",1,4,limited-side-effect,hdfs
3188,dfs.namenode.enable.log.stale.datanode,Enable and disable logging datanode staleness. Disabled by default.,0,0,others,hdfs
3189,dfs.namenode.enable.retrycache,"This enables the retry cache on the namenode. Namenode tracks for non-idempotent requests the corresponding response. If a client retries the request, the response from the retry cache is sent. Such operations are tagged with annotation @AtMostOnce in namenode protocols. It is recommended that this flag be set to true. Setting it to false, will result in clients getting failure responses to retried request. This flag must be enabled in HA setup for transparent fail-overs. The entries in the cache have expiration time configurable using dfs.namenode.retrycache.expirytime.millis.",1,4,limited-side-effect,hdfs
3191,dfs.namenode.fs-limits.max-blocks-per-file,"Maximum number of blocks per file, enforced by the Namenode on write. This prevents the creation of extremely large files which can degrade performance.",0,0,others,hdfs
3192,dfs.namenode.fs-limits.max-component-length,"Defines the maximum number of bytes in UTF-8 encoding in each component of a path. A value of 0 will disable the check. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize.",1,5,workload-specific,hdfs
3193,dfs.namenode.fs-limits.max-directory-items,Defines the maximum number of items that a directory may contain. Cannot set the property to a value less than 1 or more than 6400000.,0,0,others,hdfs
3194,dfs.namenode.fs-limits.max-xattr-size,"The maximum combined size of the name and value of an extended attribute in bytes. It should be larger than 0, and less than or equal to maximum size hard limit which is 32768. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize.",0,0,others,hdfs
3197,dfs.namenode.fslock.fair,"If this is true, the FS Namesystem lock will be used in Fair mode, which will help to prevent writer threads from being starved, but can provide lower lock throughput. See java.util.concurrent.locks.ReentrantReadWriteLock for more information on fair/non-fair locks.",1,3,reliability-tradeoff,hdfs
3198,dfs.namenode.full.block.report.lease.length.ms,The number of milliseconds that the NameNode will wait before invalidating a full block report lease. This prevents a crashed DataNode from permanently using up a full block report lease.,0,0,others,hdfs
3199,dfs.namenode.gc.time.monitor.enable,Enable the GcTimePercentage metrics in NameNode's JvmMetrics. It will start a thread(GcTimeMonitor) computing the metric.,1,6,function-tradeoff,hdfs
3200,dfs.namenode.gc.time.monitor.observation.window.ms,Determines the windows size of GcTimeMonitor. A window is a period of time starts at now-windowSize and ends at now. The GcTimePercentage is the gc time proportion of the window.,0,0,others,hdfs
3201,dfs.namenode.gc.time.monitor.sleep.interval.ms,Determines the sleep interval in the window. The GcTimeMonitor wakes up in the sleep interval periodically to compute the gc time proportion. The shorter the interval the preciser the GcTimePercentage. The sleep interval must be shorter than the window size.,0,0,others,hdfs
3203,dfs.namenode.handler.count,The number of Namenode RPC server threads that listen to requests from clients. If dfs.namenode.servicerpc-address is not configured then Namenode RPC server threads listen to requests from all nodes.,1,1,resource,hdfs
3204,dfs.namenode.heartbeat.recheck-interval,"This time decides the interval to check for expired datanodes. With this value and dfs.heartbeat.interval, the interval of deciding the datanode is stale or not is also calculated. The unit of this configuration is millisecond.",0,0,others,hdfs
3205,dfs.namenode.hosts.provider.classname,"The class that provides access for host files. org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager is used by default which loads files specified by dfs.hosts and dfs.hosts.exclude. If org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager is used, it will load the JSON file defined in dfs.hosts. To change class name, nn restart is required. ""dfsadmin -refreshNodes"" only refreshes the configuration files used by the class.",0,0,others,hdfs
3206,dfs.namenode.http-address,The address and the base port where the dfs namenode web ui will listen on.,0,0,others,hdfs
3207,dfs.namenode.http-bind-host,"The actual address the HTTP server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.namenode.http-address. It can also be specified per name node or name service for HA/Federation. This is useful for making the name node HTTP server listen on all interfaces by setting it to 0.0.0.0.",0,0,others,hdfs
3208,dfs.namenode.https-address,The namenode secure http server address and port.,0,0,others,hdfs
3209,dfs.namenode.https-bind-host,"The actual address the HTTPS server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.namenode.https-address. It can also be specified per name node or name service for HA/Federation. This is useful for making the name node HTTPS server listen on all interfaces by setting it to 0.0.0.0.",0,0,others,hdfs
3210,dfs.namenode.inode.attributes.provider.bypass.users,A list of user principals (in secure cluster) or user names (in insecure cluster) for whom the external attributes provider will be bypassed for all operations. This means file attributes stored in HDFS instead of the external provider will be used for permission checking and be returned when requested.,0,0,others,hdfs
3211,dfs.namenode.inode.attributes.provider.class,Name of class to use for delegating HDFS authorization.,0,0,others,hdfs
3212,dfs.namenode.inotify.max.events.per.rpc,Maximum number of events that will be sent to an inotify client in a single RPC response. The default value attempts to amortize away the overhead for this RPC while avoiding huge memory requirements for the client and NameNode (1000 events should consume no more than 1 MB.),1,5,workload-specific,hdfs
3213,dfs.namenode.invalidate.work.pct.per.iteration,"*Note*: Advanced property. Change with caution. This determines the percentage amount of block invalidations (deletes) to do over a single DN heartbeat deletion command. The final deletion count is determined by applying this percentage to the number of live nodes in the system. The resultant number is the number of blocks from the deletion list chosen for proper invalidation over a single heartbeat of a single DN. Value should be a positive, non-zero percentage in float notation (X.Yf), with 1.0f meaning 100%.",1,5,workload-specific,hdfs
3215,dfs.namenode.kerberos.principal,The NameNode service principal. This is typically set to nn/_HOST@REALM.TLD. Each NameNode will substitute _HOST with its own fully qualified hostname at startup. The _HOST placeholder allows using the same configuration setting on both NameNodes in an HA setup.,0,0,others,hdfs
3216,dfs.namenode.kerberos.principal.pattern,A client-side RegEx that can be configured to control allowed realms to authenticate with (useful in cross-realm env.),0,0,others,hdfs
3217,dfs.namenode.keytab.file,The keytab file used by each NameNode daemon to login as its service principal. The principal name is configured with dfs.namenode.kerberos.principal.,0,0,others,hdfs
3218,dfs.namenode.lazypersist.file.scrub.interval.sec,"The NameNode periodically scans the namespace for LazyPersist files with missing blocks and unlinks them from the namespace. This configuration key controls the interval between successive scans. If this value is set to 0, the file scrubber is disabled.",0,0,others,hdfs
3219,dfs.namenode.lease-hard-limit-sec,Determines the namenode automatic lease recovery interval in seconds.,0,0,others,hdfs
3220,dfs.namenode.lease-recheck-interval-ms,During the release of lease a lock is hold that make any operations on the namenode stuck. In order to not block them during a too long duration we stop releasing lease after this max lock limit.,0,0,others,hdfs
3221,dfs.namenode.legacy-oiv-image.dir,"Determines where to save the namespace in the old fsimage format during checkpointing by standby NameNode or SecondaryNameNode. Users can dump the contents of the old format fsimage by oiv_legacy command. If the value is not specified, old format fsimage will not be saved in checkpoint.",0,0,others,hdfs
3222,dfs.namenode.lifeline.handler.count,"Sets an absolute number of RPC server threads the NameNode runs for handling the DataNode Lifeline Protocol and HA health check requests from ZKFC. If this property is defined, then it overrides the behavior of dfs.namenode.lifeline.handler.ratio. By default, it is not defined. This property has no effect if dfs.namenode.lifeline.rpc-address is not defined.",1,1,resource,hdfs
3224,dfs.namenode.lifeline.rpc-address,"NameNode RPC lifeline address. This is an optional separate RPC address that can be used to isolate health checks and liveness to protect against resource exhaustion in the main RPC handler pool. In the case of HA/Federation where multiple NameNodes exist, the name service ID is added to the name e.g. dfs.namenode.lifeline.rpc-address.ns1. The value of this property will take the form of nn-host1:rpc-port. If this property is not defined, then the NameNode will not start a lifeline RPC server. By default, the property is not defined.",0,0,others,hdfs
3225,dfs.namenode.lifeline.rpc-bind-host,"The actual address the lifeline RPC server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.namenode.lifeline.rpc-address. It can also be specified per name node or name service for HA/Federation. This is useful for making the name node listen on all interfaces by setting it to 0.0.0.0.",0,0,others,hdfs
3226,dfs.namenode.list.cache.directives.num.responses,This value controls the number of cache directives that the NameNode will send over the wire in response to a listDirectives RPC.,1,1,resource,hdfs
3228,dfs.namenode.list.encryption.zones.num.responses,"When listing encryption zones, the maximum number of zones that will be returned in a batch. Fetching the list incrementally in batches improves namenode performance.",1,1,resource,hdfs
3229,dfs.namenode.list.openfiles.num.responses,"When listing open files, the maximum number of open files that will be returned in a single batch. Fetching the list incrementally in batches improves namenode performance.",1,1,resource,hdfs
3230,dfs.namenode.list.reencryption.status.num.responses,"When listing re-encryption status, the maximum number of zones that will be returned in a batch. Fetching the list incrementally in batches improves namenode performance.",1,5,workload-specific,hdfs
3231,dfs.namenode.lock.detailed-metrics.enabled,"If true, the namenode will keep track of how long various operations hold the Namesystem lock for and emit this as metrics. These metrics have names of the form FSN(Read|Write)LockNanosOperationName, where OperationName denotes the name of the operation that initiated the lock hold (this will be OTHER for certain uncategorized operations) and they export the hold time values in nanoseconds.",1,6,function-tradeoff,hdfs
3232,dfs.namenode.maintenance.replication.min,Minimal live block replication in existence of maintenance mode.,0,0,others,hdfs
3233,dfs.namenode.max.extra.edits.segments.retained,"The maximum number of extra edit log segments which should be retained beyond what is minimally necessary for a NN restart. When used in conjunction with dfs.namenode.num.extra.edits.retained, this configuration property serves to cap the number of extra edits files to a reasonable value.",1,1,resource,hdfs
3235,dfs.namenode.max.objects,"The maximum number of files, directories and blocks dfs supports. A value of zero indicates no limit to the number of objects that dfs supports.",0,0,others,hdfs
3236,dfs.namenode.max.op.size,Maximum opcode size in bytes.,0,0,others,hdfs
3237,dfs.namenode.max-corrupt-file-blocks-returned,"The maximum number of corrupt file blocks listed by NameNode Web UI, JMX and other client request.",0,0,others,hdfs
3238,dfs.namenode.max-lock-hold-to-release-lease-ms,During the release of lease a lock is hold that make any operations on the namenode stuck. In order to not block them during a too long duration we stop releasing lease after this max lock limit.,0,0,others,hdfs
3241,dfs.namenode.missing.checkpoint.periods.before.shutdown,The number of checkpoint period windows (as defined by the property dfs.namenode.checkpoint.period) allowed by the Namenode to perform saving the namespace before shutdown.,0,0,others,hdfs
3242,dfs.namenode.name.cache.threshold,Frequently accessed files that are accessed more times than this threshold are cached in the FSDirectory nameCache.,1,4,limited-side-effect,hdfs
3243,dfs.namenode.name.dir,"Determines where on the local filesystem the DFS name node should store the name table(fsimage). If this is a comma-delimited list of directories then the name table is replicated in all of the directories, for redundancy.",0,0,others,hdfs
3244,dfs.namenode.name.dir.restore,"Set to true to enable NameNode to attempt recovering a previously failed dfs.namenode.name.dir. When enabled, a recovery of any failed directory is attempted during checkpoint.",0,0,others,hdfs
3245,dfs.namenode.num.checkpoints.retained,The number of image checkpoint files (fsimage_*) that will be retained by the NameNode and Secondary NameNode in their storage directories. All edit logs (stored on edits_* files) necessary to recover an up-to-date namespace from the oldest retained checkpoint will also be retained.,1,3,reliability-tradeoff,hdfs
3246,dfs.namenode.num.extra.edits.retained,"The number of extra transactions which should be retained beyond what is minimally necessary for a NN restart. It does not translate directly to file's age, or the number of files kept, but to the number of transactions (here ""edits"" means transactions). One edit file may contain several transactions (edits). During checkpoint, NameNode will identify the total number of edits to retain as extra by checking the latest checkpoint transaction value, subtracted by the value of this property. Then, it scans edits files to identify the older ones that don't include the computed range of retained transactions that are to be kept around, and purges them subsequently. The retainment can be useful for audit purposes or for an HA setup where a remote Standby Node may have been offline for some time and need to have a longer backlog of retained edits in order to start again. Typically each edit is on the order of a few hundred bytes, so the default of 1 million edits should be on the order of hundreds of MBs or low GBs. NOTE: Fewer extra edits may be retained than value specified for this setting if doing so would mean that more segments would be retained than the number configured by dfs.namenode.max.extra.edits.segments.retained.",1,5,workload-specific,hdfs
3247,dfs.namenode.path.based.cache.block.map.allocation.percent,The percentage of the Java heap which we will allocate to the cached blocks map. The cached blocks map is a hash map which uses chained hashing. Smaller maps may be accessed more slowly if the number of cached blocks is large; larger maps will consume more memory.,1,5,workload-specific,hdfs
3248,dfs.namenode.path.based.cache.refresh.interval.ms,"The amount of milliseconds between subsequent path cache rescans. Path cache rescans are when we calculate which blocks should be cached, and on what datanodes. By default, this parameter is set to 30 seconds.",1,5,workload-specific,hdfs
3249,dfs.namenode.path.based.cache.retry.interval.ms,"When the NameNode needs to uncache something that is cached, or cache something that is not cached, it must direct the DataNodes to do so by sending a DNA_CACHE or DNA_UNCACHE command in response to a DataNode heartbeat. This parameter controls how frequently the NameNode will resend these commands.",0,0,others,hdfs
3250,dfs.namenode.plugins,Comma-separated list of namenode plug-ins to be activated.,0,0,others,hdfs
3251,dfs.namenode.posix.acl.inheritance.enabled,"Set to true to enable POSIX style ACL inheritance. When it is enabled and the create request comes from a compatible client, the NameNode will apply default ACLs from the parent directory to the create mode and ignore the client umask. If no default ACL found, it will apply the client umask.",0,0,others,hdfs
3252,dfs.namenode.provided.enabled,Enables the Namenode to handle provided storages.,0,0,others,hdfs
3253,dfs.namenode.quota.init-threads,"The number of concurrent threads to be used in quota initialization. The speed of quota initialization also affects the namenode fail-over latency. If the size of name space is big, try increasing this.",1,1,resource,hdfs
3255,dfs.namenode.read-lock-reporting-threshold-ms,"When a read lock is held on the namenode for a long time, this will be logged as the lock is released. This sets how long the lock must be held for logging to occur.",0,0,others,hdfs
3256,dfs.namenode.reconstruction.pending.timeout-sec,"Timeout in seconds for block reconstruction. If this value is 0 or less, then it will default to 5 minutes.",0,0,others,hdfs
3257,dfs.namenode.redundancy.considerLoad,Decide if chooseTarget considers the target's load or not when write. Turn on by default.,1,4,limited-side-effect,hdfs
3258,dfs.namenode.redundancy.considerLoad.factor,"The factor by which a node's load can exceed the average before being rejected for writes, only if considerLoad is true.",1,5,workload-specific,hdfs
3259,dfs.namenode.redundancy.interval.seconds,"The periodicity in seconds with which the namenode computes low redundancy work for datanodes. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.",0,0,others,hdfs
3260,dfs.namenode.redundancy.queue.restart.iterations,"When picking blocks from the low redundancy queues, reset the bookmarked iterator after the set number of iterations to ensure any blocks which were not processed on the first pass are retried before the iterators would naturally reach their end point. This ensures blocks are retried more frequently when there are many pending blocks or blocks are continuously added to the queues preventing the iterator reaching its natural endpoint. The default setting of 2400 combined with the default of dfs.namenode.redundancy.interval.seconds means the iterators will be reset approximately every 2 hours. Setting this parameter to zero disables the feature and the iterators will be reset only when the end of all queues has been reached.",1,3,reliability-tradeoff,hdfs
3261,dfs.namenode.reencrypt.batch.size,How many EDEKs should the re-encrypt thread process in one batch.,0,0,others,hdfs
3262,dfs.namenode.reencrypt.edek.threads,Maximum number of re-encrypt threads to contact the KMS and re-encrypt the edeks.,1,1,resource,hdfs
3263,dfs.namenode.reencrypt.sleep.interval,"Interval the re-encrypt EDEK thread sleeps in the main loop. The interval accepts units. If none given, millisecond is assumed.",0,0,others,hdfs
3264,dfs.namenode.reencrypt.throttle.limit.handler.ratio,"Throttling ratio for the re-encryption, indicating what fraction of time should the re-encrypt handler thread work under NN read lock. Larger than 1.0 values are interpreted as 1.0. Negative value or 0 are invalid values and will fail NN startup.",1,5,workload-specific,hdfs
3266,dfs.namenode.reject-unresolved-dn-topology-mapping,"If the value is set to true, then namenode will reject datanode registration if the topology mapping for a datanode is not resolved and NULL is returned (script defined by net.topology.script.file.name fails to execute). Otherwise, datanode will be registered and the default rack will be assigned as the topology path. Topology paths are important for data resiliency, since they define fault domains. Thus it may be unwanted behavior to allow datanode registration with the default rack if the resolving topology failed.",0,0,others,hdfs
3271,dfs.namenode.resource.check.interval,"The interval in milliseconds at which the NameNode resource checker runs. The checker calculates the number of the NameNode storage volumes whose available spaces are more than dfs.namenode.resource.du.reserved, and enters safemode if the number becomes lower than the minimum value specified by dfs.namenode.resource.checked.volumes.minimum.",0,0,others,hdfs
3272,dfs.namenode.resource.checked.volumes,A list of local directories for the NameNode resource checker to check in addition to the local edits directories.,0,0,others,hdfs
3275,dfs.namenode.retrycache.expirytime.millis,The time for which retry cache entries are retained.,1,5,workload-specific,hdfs
3276,dfs.namenode.retrycache.heap.percent,"This parameter configures the heap size allocated for retry cache (excluding the response cached). This corresponds to approximately 4096 entries for every 64MB of namenode process java heap size. Assuming retry cache entry expiration time (configured using dfs.namenode.retrycache.expirytime.millis) of 10 minutes, this enables retry cache to support 7 operations per second sustained for 10 minutes. As the heap size is increased, the operation rate linearly increases.",1,1,resource,hdfs
3277,dfs.namenode.rpc-address,"RPC address that handles all clients requests. In the case of HA/Federation where multiple namenodes exist, the name service id is added to the name e.g. dfs.namenode.rpc-address.ns1 dfs.namenode.rpc-address.EXAMPLENAMESERVICE The value of this property will take the form of nn-host1:rpc-port. The NameNode's default RPC port is 8020.",0,0,others,hdfs
3278,dfs.namenode.rpc-address.auxiliary-ports,"A comma separated list of auxiliary ports for the NameNode to listen on. This allows exposing multiple NN addresses to clients. Particularly, it is used to enforce different SASL levels on different ports. Empty list indicates that auxiliary ports are disabled.",0,0,others,hdfs
3279,dfs.namenode.rpc-bind-host,"The actual address the RPC server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.namenode.rpc-address. It can also be specified per name node or name service for HA/Federation. This is useful for making the name node listen on all interfaces by setting it to 0.0.0.0.",0,0,others,hdfs
3280,dfs.namenode.safemode.extension,"Determines extension of safe mode in milliseconds after the threshold level is reached. Support multiple time unit suffix (case insensitive), as described in dfs.heartbeat.interval.",0,0,others,hdfs
3281,dfs.namenode.safemode.min.datanodes,Specifies the number of datanodes that must be considered alive before the name node exits safemode. Values less than or equal to 0 mean not to take the number of live datanodes into account when deciding whether to remain in safe mode during startup. Values greater than the number of datanodes in the cluster will make safe mode permanent.,0,0,others,hdfs
3282,dfs.namenode.safemode.replication.min,a separate minimum replication factor for calculating safe block count. This is an expert level setting. Setting this lower than the dfs.namenode.replication.min is not recommend and/or dangerous for production setups. When it's not set it takes value from dfs.namenode.replication.min,1,3,reliability-tradeoff,hdfs
3283,dfs.namenode.safemode.threshold-pct,Specifies the percentage of blocks that should satisfy the minimal replication requirement defined by dfs.namenode.replication.min. Values less than or equal to 0 mean not to wait for any particular percentage of blocks before exiting safemode. Values greater than 1 will make safe mode permanent.,1,5,workload-specific,hdfs
3284,dfs.namenode.secondary.http-address,The secondary namenode http server address and port.,0,0,others,hdfs
3285,dfs.namenode.secondary.https-address,The secondary namenode HTTPS server address and port.,0,0,others,hdfs
3286,dfs.namenode.send.qop.enabled,"A boolean specifies whether NameNode should encrypt the established QOP and include it in block token. The encrypted QOP will be used by DataNode as target QOP, overwriting DataNode configuration. This ensures DataNode will use exactly the same QOP NameNode and client has already agreed on.",1,2,security-tradeoff,hdfs
3287,dfs.namenode.service.handler.count,The number of Namenode RPC server threads that listen to requests from DataNodes and from all other non-client nodes. dfs.namenode.service.handler.count will be valid only if dfs.namenode.servicerpc-address is configured.,1,1,resource,hdfs
3288,dfs.namenode.servicerpc-address,"RPC address for HDFS Services communication. BackupNode, Datanodes and all other services should be connecting to this address if it is configured. In the case of HA/Federation where multiple namenodes exist, the name service id is added to the name e.g. dfs.namenode.servicerpc-address.ns1 dfs.namenode.rpc-address.EXAMPLENAMESERVICE The value of this property will take the form of nn-host1:rpc-port. If the value of this property is unset the value of dfs.namenode.rpc-address will be used as the default.",0,0,others,hdfs
3289,dfs.namenode.servicerpc-bind-host,"The actual address the service RPC server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.namenode.servicerpc-address. It can also be specified per name node or name service for HA/Federation. This is useful for making the name node listen on all interfaces by setting it to 0.0.0.0.",0,0,others,hdfs
3290,dfs.namenode.shared.edits.dir,A directory on shared storage between the multiple namenodes in an HA cluster. This directory will be written by the active and read by the standby in order to keep the namespaces synchronized. This directory does not need to be listed in dfs.namenode.edits.dir above. It should be left empty in a non-HA cluster.,0,0,others,hdfs
3291,dfs.namenode.snapshot.capture.openfiles,"If true, snapshots taken will have an immutable shared copy of the open files that have valid leases. Even after the open files grow or shrink in size, snapshot will always have the previous point-in-time version of the open files, just like all other closed files. Default is false. Note: The file length captured for open files in snapshot is whats recorded in NameNode at the time of snapshot and it may be shorter than what the client has written till then. In order to capture the latest length, the client can call hflush/hsync with the flag SyncFlag.UPDATE_LENGTH on the open files handles.",0,0,others,hdfs
3293,dfs.namenode.snapshot.skip.capture.accesstime-only-change,"If accessTime of a file/directory changed but there is no other modification made to the file/directory, the changed accesstime will not be captured in next snapshot. However, if there is other modification made to the file/directory, the latest access time will be captured together with the modification in next snapshot.",0,0,others,hdfs
3294,dfs.namenode.snapshot.skiplist.interval,"The interval after which the skip levels will be formed in the skip list for storing directory snapshot diffs. By default, value is set to 10.",0,0,others,hdfs
3295,dfs.namenode.snapshot.skiplist.max.levels,"Maximum no of the skip levels to be maintained in the skip list for storing directory snapshot diffs. By default, it is set to 0 and a linear list will be used to store the directory snapshot diffs.",0,0,others,hdfs
3296,dfs.namenode.snapshotdiff.allow.snap-root-descendant,"If enabled, snapshotDiff command can be run for any descendant directory under a snapshot root directory and the diff calculation will be scoped to the given descendant directory. Otherwise, snapshot diff command can only be run for a snapshot root directory.",0,0,others,hdfs
3297,dfs.namenode.snapshotdiff.listing.limit,"Limit the number of entries generated by getSnapshotDiffReportListing within one rpc call to the namenode.If less or equal to zero, at most DFS_NAMENODE_SNAPSHOT_DIFF_LISTING_LIMIT_DEFAULT (= 1000) will be sent across to the client within one rpc call.",0,0,others,hdfs
3300,dfs.namenode.startup.delay.block.deletion.sec,"The delay in seconds at which we will pause the blocks deletion after Namenode startup. By default it's disabled. In the case a directory has large number of directories and files are deleted, suggested delay is one hour to give the administrator enough time to notice large number of pending deletion blocks and take corrective action.",0,0,others,hdfs
3301,dfs.namenode.state.context.enabled,"Whether enable namenode sending back its current txnid back to client. Setting this to true is required by Consistent Read from Standby feature. But for regular cases, this should be set to false to avoid the overhead of updating and maintaining this state.",1,3,reliability-tradeoff,hdfs
3302,dfs.namenode.storage.dir.perm,Permissions for the directories on on the local filesystem where the DFS namenode stores the fsImage. The permissions can either be octal or symbolic.,0,0,others,hdfs
3303,dfs.namenode.storageinfo.defragment.interval.ms,The thread for checking the StorageInfo for defragmentation will run periodically. The time between runs is determined by this property.,0,0,others,hdfs
3305,dfs.namenode.storageinfo.defragment.timeout.ms,Timeout value in ms for the StorageInfo compaction run.,0,0,others,hdfs
3306,dfs.namenode.support.allow.format,"Does HDFS namenode allow itself to be formatted? You may consider setting this to false for any production cluster, to avoid any possibility of formatting a running DFS.",1,5,workload-specific,hdfs
3307,dfs.namenode.top.enabled,Enable nntop: reporting top users on namenode,0,0,others,hdfs
3309,dfs.namenode.top.window.num.buckets,Number of buckets in the rolling window implementation of nntop,0,0,others,hdfs
3310,dfs.namenode.top.windows.minutes,comma separated list of nntop reporting periods in minutes,0,0,others,hdfs
3311,dfs.namenode.upgrade.domain.factor,"This is valid only when block placement policy is set to BlockPlacementPolicyWithUpgradeDomain. It defines the number of unique upgrade domains any block's replicas should have. When the number of replicas is less or equal to this value, the policy ensures each replica has an unique upgrade domain. When the number of replicas is greater than this value, the policy ensures the number of unique domains is at least this value.",0,0,others,hdfs
3312,dfs.namenode.write.stale.datanode.ratio,"When the ratio of number stale datanodes to total datanodes marked is greater than this ratio, stop avoiding writing to stale nodes so as to prevent causing hotspots.",1,5,workload-specific,hdfs
3313,dfs.namenode.write-lock-reporting-threshold-ms,"When a write lock is held on the namenode for a long time, this will be logged as the lock is released. This sets how long the lock must be held for logging to occur.",0,0,others,hdfs
3314,dfs.namenode.xattrs.enabled,Whether support for extended attributes is enabled on the NameNode.,0,0,others,hdfs
3315,dfs.nameservice.id,The ID of this nameservice. If the nameservice ID is not configured or more than one nameservice is configured for dfs.nameservices it is determined automatically by matching the local node's address with the configured address.,0,0,others,hdfs
3316,dfs.nameservices,Comma-separated list of nameservices.,0,0,others,hdfs
3317,dfs.net.topology.impl,"The implementation class of NetworkTopology used in HDFS. By default, the class org.apache.hadoop.hdfs.net.DFSNetworkTopology is specified and used in block placement. This property only works when dfs.use.dfs.network.topology is true.",0,0,others,hdfs
3318,dfs.permissions.allow.owner.set.quota,Whether the owner(not superuser) of a directory can set quota of his sub directories when permissions is enabled. Default value is false;,0,0,others,hdfs
3319,dfs.permissions.ContentSummary.subAccess,"If ""true"", the ContentSummary permission checking will use subAccess. If ""false"", the ContentSummary permission checking will NOT use subAccess. subAccess means using recursion to check the access of all descendants.",1,2,security-tradeoff,hdfs
3320,dfs.permissions.enabled,"If ""true"", enable permission checking in HDFS. If ""false"", permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories.",0,0,others,hdfs
3321,dfs.permissions.superusergroup,The name of the group of super-users. The value should be a single group name.,0,0,others,hdfs
3323,dfs.provided.acls.import.enabled,"Set to true to inherit ACLs (Access Control Lists) from remote stores during mount. Disabled by default, i.e., ACLs are not inherited from remote stores. Note had HDFS ACLs have to be enabled (dfs.namenode.acls.enabled must be set to true) for this to take effect.",0,0,others,hdfs
3324,dfs.provided.aliasmap.class,"The class that is used to specify the input format of the blocks on provided storages. The default is org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap which uses file regions to describe blocks. The file regions are specified as a delimited text file. Each file region is a 6-tuple containing the block id, remote file path, offset into file, length of block, the block pool id containing the block, and the generation stamp of the block.",0,0,others,hdfs
3325,dfs.provided.aliasmap.inmemory.batch-size,The batch size when iterating over the database backing the aliasmap,1,5,workload-specific,hdfs
3326,dfs.provided.aliasmap.inmemory.dnrpc-address,"The address where the aliasmap server will be running. In the case of HA/Federation where multiple namenodes exist, and if the Namenode is configured to run the aliasmap server (dfs.provided.aliasmap.inmemory.enabled is set to true), the name service id is added to the name, e.g., dfs.provided.aliasmap.inmemory.rpc.address.EXAMPLENAMESERVICE. The value of this property will take the form of host:rpc-port.",0,0,others,hdfs
3327,dfs.provided.aliasmap.inmemory.enabled,Don't use the aliasmap by default. Some tests will fail because they try to start the namenode twice with the same parameters if you turn it on.,1,6,function-tradeoff,hdfs
3328,dfs.provided.aliasmap.inmemory.leveldb.dir,The directory where the leveldb files will be kept,0,0,others,hdfs
3330,dfs.provided.aliasmap.inmemory.server.log,Ensures that InMemoryAliasMap server logs every call to it. Set to false by default.,1,6,function-tradeoff,hdfs
3331,dfs.provided.aliasmap.leveldb.path,The read/write path for the leveldb-based alias map (org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.LevelDBFileRegionAliasMap). The path has to be explicitly configured when this alias map is used.,0,0,others,hdfs
3332,dfs.provided.aliasmap.load.retries,The number of retries on the Datanode to load the provided aliasmap; defaults to 0.,0,0,others,hdfs
3333,dfs.provided.aliasmap.text.codec,The codec used to de-compress the provided block map.,1,6,function-tradeoff,hdfs
3334,dfs.provided.aliasmap.text.delimiter,The delimiter used when the provided block map is specified as a text file.,0,0,others,hdfs
3335,dfs.provided.aliasmap.text.read.file,"The path specifying the provided block map as a text file, specified as a URI.",0,0,others,hdfs
3336,dfs.provided.aliasmap.text.write.dir,"The path to which the provided block map should be written as a text file, specified as a URI.",0,0,others,hdfs
3337,dfs.provided.storage.id,The storage ID used for provided stores.,0,0,others,hdfs
3338,dfs.qjm.operations.timeout,Common key to set timeout for related operations in QuorumJournalManager. This setting supports multiple time unit suffixes as described in dfs.heartbeat.interval. If no suffix is specified then milliseconds is assumed.,0,0,others,hdfs
3339,dfs.qjournal.accept-recovery.timeout.ms,Quorum timeout in milliseconds during accept phase of recovery/synchronization for a specific segment.,0,0,others,hdfs
3340,dfs.qjournal.finalize-segment.timeout.ms,Quorum timeout in milliseconds during finalizing for a specific segment.,0,0,others,hdfs
3341,dfs.qjournal.get-journal-state.timeout.ms,Timeout in milliseconds when calling getJournalState(). JournalNodes.,0,0,others,hdfs
3342,dfs.qjournal.http.open.timeout.ms,Timeout in milliseconds when open a new HTTP connection to remote journals.,0,0,others,hdfs
3343,dfs.qjournal.http.read.timeout.ms,Timeout in milliseconds when reading from a HTTP connection from remote journals.,0,0,others,hdfs
3344,dfs.qjournal.new-epoch.timeout.ms,Timeout in milliseconds when getting an epoch number for write access to JournalNodes.,0,0,others,hdfs
3345,dfs.qjournal.parallel-read.num-threads,Number of threads per JN to be used for tailing edits.,1,1,resource,hdfs
3346,dfs.qjournal.prepare-recovery.timeout.ms,Quorum timeout in milliseconds during preparation phase of recovery/synchronization for a specific segment.,0,0,others,hdfs
3347,dfs.qjournal.queued-edits.limit.mb,Queue size in MB for quorum journal edits.,1,5,workload-specific,hdfs
3350,dfs.qjournal.write-txns.timeout.ms,Write timeout in milliseconds when writing to a quorum of remote journals.,0,0,others,hdfs
3352,dfs.reformat.disabled,"Disable reformat of NameNode. If it's value is set to ""true"" and metadata directories already exist then attempt to format NameNode will throw NameNodeFormatException.",0,0,others,hdfs
3353,dfs.replication,Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time.,1,3,reliability-tradeoff,hdfs
3354,dfs.replication.max,Maximal block replication.,1,3,reliability-tradeoff,hdfs
3355,dfs.secondary.namenode.kerberos.internal.spnego.principal,"The server principal used by the Secondary NameNode for web UI SPNEGO authentication when Kerberos security is enabled. Like all other Secondary NameNode settings, it is ignored in an HA setup. If the value is '*', the web server will attempt to login with every principal specified in the keytab file dfs.web.authentication.kerberos.keytab.",0,0,others,hdfs
3356,dfs.secondary.namenode.kerberos.principal,Kerberos principal name for the Secondary NameNode.,0,0,others,hdfs
3357,dfs.secondary.namenode.keytab.file,Kerberos keytab file for the Secondary NameNode.,0,0,others,hdfs
3358,dfs.short.circuit.shared.memory.watcher.interrupt.check.ms,The length of time in milliseconds that the short-circuit shared memory watcher will go between checking for java interruptions sent from other threads. This is provided mainly for unit tests.,0,0,others,hdfs
3359,dfs.storage.policy.enabled,Allow users to change the storage policy on files and directories.,0,0,others,hdfs
3360,dfs.storage.policy.permissions.superuser-only,Allow only superuser role to change the storage policy on files and directories.,0,0,others,hdfs
3361,dfs.storage.policy.satisfier.address,The hostname used for a keytab based Kerberos login. Keytab based login is required when dfs.storage.policy.satisfier.mode is external.,0,0,others,hdfs
3362,dfs.storage.policy.satisfier.datanode.cache.refresh.interval.ms,"How often to refresh the datanode storages cache in milliseconds. This cache keeps live datanode storage reports fetched from namenode. After elapsed time, it will again fetch latest datanodes from namenode. By default, this parameter is set to 5 minutes.",1,5,workload-specific,hdfs
3365,dfs.storage.policy.satisfier.keytab.file,The keytab file used by external StoragePolicySatisfier to login as its service principal. The principal name is configured with dfs.storage.policy.satisfier.kerberos.principal. Keytab based login is required when dfs.storage.policy.satisfier.mode is external.,0,0,others,hdfs
3366,dfs.storage.policy.satisfier.max.outstanding.paths,Defines the maximum number of paths to satisfy that can be queued up in the Satisfier call queue in a period of time. Default value is 10000.,1,5,workload-specific,hdfs
3367,dfs.storage.policy.satisfier.queue.limit,Storage policy satisfier queue size. This queue contains the currently scheduled file's inode ID for statisfy the policy. Default value is 1000.,0,0,others,hdfs
3368,dfs.storage.policy.satisfier.recheck.timeout.millis,Blocks storage movements monitor re-check interval in milliseconds. This check will verify whether any blocks storage movement results arrived from DN and also verify if any of file blocks movements not at all reported to DN since dfs.storage.policy.satisfier.self.retry.timeout. The default value is 1 * 60 * 1000 (1 mins),0,0,others,hdfs
3369,dfs.storage.policy.satisfier.retry.max.attempts,Max retry to satisfy the block storage policy. After this retry block will be removed from the movement needed queue.,0,0,others,hdfs
3370,dfs.storage.policy.satisfier.self.retry.timeout.millis,"If any of file related block movements not at all reported by datanode, then after this timeout(in milliseconds), the item will be added back to movement needed list at namenode which will be retried for block movements. The default value is 5 * 60 * 1000 (5 mins)",0,0,others,hdfs
3372,dfs.stream-buffer-size,"The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",1,1,resource,hdfs
3373,dfs.trustedchannel.resolver.class,"TrustedChannelResolver is used to determine whether a channel is trusted for plain data transfer. The TrustedChannelResolver is invoked on both client and server side. If the resolver indicates that the channel is trusted, then the data transfer will not be encrypted even if dfs.encrypt.data.transfer is set to true. The default implementation returns false indicating that the channel is not trusted.",0,0,others,hdfs
3374,dfs.use.dfs.network.topology,"Enables DFSNetworkTopology to choose nodes for placing replicas. When enabled, NetworkTopology will be instantiated as class defined in property dfs.net.topology.impl, otherwise NetworkTopology will be instantiated as class defined in property net.topology.impl.",1,4,limited-side-effect,hdfs
3375,dfs.user.home.dir.prefix,The directory to prepend to user name to get the user's home direcotry.,0,0,others,hdfs
3376,dfs.web.authentication.kerberos.keytab,The keytab file for the principal corresponding to dfs.web.authentication.kerberos.principal.,0,0,others,hdfs
3379,dfs.web.ugi,dfs.web.ugi is deprecated. Use hadoop.http.staticuser.user instead.,0,0,others,hdfs
3380,dfs.webhdfs.acl.provider.permission.pattern,"Valid pattern for user and group names in webhdfs acl operations, it must be a valid java regex.",0,0,others,hdfs
3381,dfs.webhdfs.netty.high.watermark,High watermark configuration to Netty for Datanode WebHdfs.,1,5,workload-specific,hdfs
3382,dfs.webhdfs.netty.low.watermark,Low watermark configuration to Netty for Datanode WebHdfs.,1,5,workload-specific,hdfs
3383,dfs.webhdfs.oauth2.access.token.provider,Access token provider class for WebHDFS using OAuth2. Defaults to org.apache.hadoop.hdfs.web.oauth2.ConfCredentialBasedAccessTokenProvider.,0,0,others,hdfs
3384,dfs.webhdfs.oauth2.client.id,Client id used to obtain access token with either credential or refresh token.,0,0,others,hdfs
3385,dfs.webhdfs.oauth2.enabled,"If true, enables OAuth2 in WebHDFS",1,2,security-tradeoff,hdfs
3386,dfs.webhdfs.oauth2.refresh.url,URL against which to post for obtaining bearer token with either credential or refresh token.,0,0,others,hdfs
3389,dfs.webhdfs.rest-csrf.enabled,"If true, then enables WebHDFS protection against cross-site request forgery (CSRF). The WebHDFS client also uses this property to determine whether or not it needs to send the custom CSRF prevention header in its HTTP requests.",0,0,others,hdfs
3390,dfs.webhdfs.rest-csrf.methods-to-ignore,A comma-separated list of HTTP methods that do not require HTTP requests to include a custom header when protection against cross-site request forgery (CSRF) is enabled for WebHDFS by setting dfs.webhdfs.rest-csrf.enabled to true. The WebHDFS client also uses this property to determine whether or not it needs to send the custom CSRF prevention header in its HTTP requests.,1,2,security-tradeoff,hdfs
3391,dfs.webhdfs.socket.connect-timeout,"Socket timeout for connecting to WebHDFS servers. This prevents a WebHDFS client from hanging if the server hostname is misconfigured, or the server does not response before the timeout expires. Value is followed by a unit specifier: ns, us, ms, s, m, h, d for nanoseconds, microseconds, milliseconds, seconds, minutes, hours, days respectively. Values should provide units, but milliseconds are assumed.",0,0,others,hdfs
3392,dfs.webhdfs.socket.read-timeout,"Socket timeout for reading data from WebHDFS servers. This prevents a WebHDFS client from hanging if the server stops sending data. Value is followed by a unit specifier: ns, us, ms, s, m, h, d for nanoseconds, microseconds, milliseconds, seconds, minutes, hours, days respectively. Values should provide units, but milliseconds are assumed.",0,0,others,hdfs
3396,dfs.xframe.enabled,"If true, then enables protection against clickjacking by returning X_FRAME_OPTIONS header value set to SAMEORIGIN. Clickjacking protection prevents an attacker from using transparent or opaque layers to trick a user into clicking on a button or link on another page.",0,0,others,hdfs
3397,dfs.xframe.value,"This configration value allows user to specify the value for the X-FRAME-OPTIONS. The possible values for this field are DENY, SAMEORIGIN and ALLOW-FROM. Any other value will throw an exception when namenode and datanodes are starting up.",1,3,reliability-tradeoff,hdfs
3400,hadoop.hdfs.configuration.version,version of this configuration file,0,0,others,hdfs
3402,httpfs.buffer.size,The size buffer to be used when creating or opening httpfs filesystem IO stream.,1,1,resource,hdfs
3403,nfs.allow.insecure.ports,"When set to false, client connections originating from unprivileged ports (those above 1023) will be rejected. This is to ensure that clients connecting to this NFS Gateway must have had root privilege on the machine where they're connecting from.",0,0,others,hdfs
3404,nfs.dump.dir,"This directory is used to temporarily save out-of-order writes before writing to HDFS. For each file, the out-of-order writes are dumped after they are accumulated to exceed certain threshold (e.g., 1MB) in memory. One needs to make sure the directory has enough space.",0,0,others,hdfs
3405,nfs.kerberos.principal,*Note*: Advanced property. Change with caution. This is the name of the kerberos principal. This is required when the cluster is kerberized.It must be of this format: nfs-gateway-user/nfs-gateway-host@kerberos-realm,0,0,others,hdfs
3406,nfs.keytab.file,*Note*: Advanced property. Change with caution. This is the path to the keytab file for the hdfs-nfs gateway. This is required when the cluster is kerberized.,0,0,others,hdfs
3407,nfs.mountd.port,Specify the port number used by Hadoop mount daemon.,0,0,others,hdfs
3408,nfs.rtmax,"This is the maximum size in bytes of a READ request supported by the NFS gateway. If you change this, make sure you also update the nfs mount's rsize(add rsize= # of bytes to the mount directive).",1,1,resource,hdfs
3410,nfs.wtmax,"This is the maximum size in bytes of a WRITE request supported by the NFS gateway. If you change this, make sure you also update the nfs mount's wsize(add wsize= # of bytes to the mount directive).",1,1,resource,hdfs
3411,ssl.server.keystore.keypassword,Keystore key password for HTTPS SSL configuration,0,0,others,hdfs
3412,ssl.server.keystore.location,Keystore location for HTTPS SSL configuration,0,0,others,hdfs
3413,ssl.server.keystore.password,Keystore password for HTTPS SSL configuration,0,0,others,hdfs
3414,ssl.server.truststore.location,Truststore location for HTTPS SSL configuration,0,0,others,hdfs
3417,AcceptPathInfo,This directive controls whether requests that contain trailing pathname information that follows an actual filename (or non-existent file in an existing directory) will be accepted or rejected. The trailing pathname information can be made available to scripts in the PATH_INFO environment variable.,0,0,others,httpd
3418,AccessFileName,"While processing a request, the server looks for the first existing configuration file from this list of names in every directory of the path to the document, if distributed configuration files are enabled for that directory. For example:",0,0,others,httpd
3419,Action,"This directive adds an action, which will activate cgi-script when action-type is triggered by the request. The cgi-script is the URL-path to a resource that has been designated as a CGI script using ScriptAlias or AddHandler. The action-type can be either a handler or a MIME content type. It sends the URL and file path of the requested document using the standard CGI PATH_INFO and PATH_TRANSLATED environment variables. The handler used for the particular request is passed using the REDIRECT_HANDLER variable.",0,0,others,httpd
3420,AddAlt,"AddAlt provides the alternate text to display for a file, instead of an icon, for FancyIndexing. File is a file extension, partial filename, wild-card expression or full filename for files to describe. If String contains any whitespace, you have to enclose it in quotes ("" or '). This alternate text is displayed if the client is image-incapable, has image loading disabled, or fails to retrieve the icon.",0,0,others,httpd
3422,AddAltByType,"AddAltByType sets the alternate text to display for a file, instead of an icon, for FancyIndexing. MIME-type is a valid content-type, such as text/html. If String contains any whitespace, you have to enclose it in quotes ("" or '). This alternate text is displayed if the client is image-incapable, has image loading disabled, or fails to retrieve the icon.",0,0,others,httpd
3423,AddCharset,"The AddCharset directive maps the given filename extensions to the specified content charset (the Internet registered name for a given character encoding). charset is the media type's charset parameter for resources with filenames containing extension. This mapping is added to any already in force, overriding any mappings that already exist for the same extension.",0,0,others,httpd
3424,AddDefaultCharset,"This directive specifies a default value for the media type charset parameter (the name of a character encoding) to be added to a response if and only if the response's content-type is either text/plain or text/html. This should override any charset specified in the body of the response via a META element, though the exact behavior is often dependent on the user's client configuration. A setting of AddDefaultCharset Off disables this functionality. AddDefaultCharset On enables a default charset of iso-8859-1. Any other value is assumed to be the charset to be used, which should be one of the IANA registered charset values for use in Internet media types (MIME types). For example:",0,0,others,httpd
3425,AddDescription,"This sets the description to display for a file, for FancyIndexing. File is a file extension, partial filename, wild-card expression or full filename for files to describe. String is enclosed in double quotes ("").",0,0,others,httpd
3426,AddEncoding,"The AddEncoding directive maps the given filename extensions to the specified HTTP content-encoding. encoding is the HTTP content coding to append to the value of the Content-Encoding header field for documents named with the extension. This mapping is added to any already in force, overriding any mappings that already exist for the same extension.",0,0,others,httpd
3427,AddHandler,"Files having the name extension will be served by the specified handler-name. This mapping is added to any already in force, overriding any mappings that already exist for the same extension. For example, to activate CGI scripts with the file extension .cgi, you might use:",0,0,others,httpd
3428,AddIcon,"This sets the icon to display next to a file ending in name for FancyIndexing. Icon is either a (%-escaped) relative URL to the icon, a fully qualified remote URL, or of the format (alttext,url) where alttext is the text tag given for an icon for non-graphical browsers.",0,0,others,httpd
3429,AddIconByEncoding,"This sets the icon to display next to files with FancyIndexing. Icon is either a (%-escaped) relative URL to the icon, a fully qualified remote URL, or of the format (alttext,url) where alttext is the text tag given for an icon for non-graphical browsers.",0,0,others,httpd
3430,AddIconByType,"This sets the icon to display next to files of type MIME-type for FancyIndexing. Icon is either a (%-escaped) relative URL to the icon, a fully qualified remote URL, or of the format (alttext,url) where alttext is the text tag given for an icon for non-graphical browsers.",0,0,others,httpd
3431,AddInputFilter,"AddInputFilter maps the filename extension extension to the filters which will process client requests and POST input when they are received by the server. This is in addition to any filters defined elsewhere, including the SetInputFilter directive. This mapping is merged over any already in force, overriding any mappings that already exist for the same extension.",0,0,others,httpd
3432,AddLanguage,The AddLanguage directive maps the given filename extension to the specified content language. Files with the filename extension are assigned an HTTP Content-Language value of language-tag corresponding to the language identifiers defined by RFC 3066. This directive overrides any mappings that already exist for the same extension.,0,0,others,httpd
3433,AddModuleInfo,"This allows the content of string to be shown as HTML interpreted, Additional Information for the module module-name. Example:",0,0,others,httpd
3434,AddOutputFilter,"The AddOutputFilter directive maps the filename extension extension to the filters which will process responses from the server before they are sent to the client. This is in addition to any filters defined elsewhere, including SetOutputFilter and AddOutputFilterByType directive. This mapping is merged over any already in force, overriding any mappings that already exist for the same extension.",0,0,others,httpd
3435,AddOutputFilterByType,This directive activates a particular output filter for a request depending on the response media-type.,0,0,others,httpd
3436,AddType,"The AddType directive maps the given filename extensions onto the specified content type. media-type is the media type to use for filenames containing extension. This mapping is added to any already in force, overriding any mappings that already exist for the same extension.",0,0,others,httpd
3437,Alias,"The Alias directive allows documents to be stored in the local filesystem other than under the DocumentRoot. URLs with a (%-decoded) path beginning with URL-path will be mapped to local files beginning with directory-path. The URL-path is case-sensitive, even on case-insensitive file systems.",0,0,others,httpd
3438,AliasMatch,"This directive is equivalent to Alias, but makes use of regular expressions, instead of simple prefix matching. The supplied regular expression is matched against the URL-path, and if it matches, the server will substitute any parenthesized matches into the given string and use it as a filename. For example, to activate the /icons directory, one might use:",0,0,others,httpd
3440,AllowEncodedSlashes,The AllowEncodedSlashes directive allows URLs which contain encoded path separators (%2F for / and additionally %5C for \ on accordant systems) to be used in the path info.,0,0,others,httpd
3442,AuthDigestShmemSize,"The AuthDigestShmemSize directive defines the amount of shared memory, that will be allocated at the server startup for keeping track of clients. Note that the shared memory segment cannot be set less than the space that is necessary for tracking at least one client. This value is dependent on your system. If you want to find out the exact value, you may simply set AuthDigestShmemSize to the value of 0 and read the error message after trying to start the server.",1,1,resource,httpd
3443,AuthLDAPCharsetConfig,"The AuthLDAPCharsetConfig directive sets the location of the language to charset conversion configuration file. File-path is relative to the ServerRoot. This file specifies the list of language extensions to character sets. Most administrators use the provided charset.conv file, which associates common language extensions to character sets.",0,0,others,httpd
3444,AuthnCacheEnable,"This directive is not normally necessary: it is implied if authentication caching is enabled anywhere in httpd.conf. However, if it is not enabled anywhere in httpd.conf it will by default not be initialised, and is therefore not available in a .htaccess context. This directive ensures it is initialised so it can be used in .htaccess.",0,0,others,httpd
3445,AuthnCacheSOCache,"This is a server-wide setting to select a provider for the shared object cache, followed by optional arguments for that provider. Some possible values for provider-name are ""dbm"", ""dc"", ""memcache"", or ""shmcb"", each subject to the appropriate module being loaded. If not set, your platform's default will be used.",0,0,others,httpd
3447,AuthnzFcgiDefineProvider,This directive is used to define a FastCGI application as a provider for a particular phase of authentication or authorization.,0,0,others,httpd
3449,BalancerGrowth,This directive allows for growth potential in the number of Balancers available for a virtualhost in addition to the number pre-configured. It only takes effect if there is at least one pre-configured Balancer.,0,0,others,httpd
3450,BalancerInherit,"This directive will cause the current server/vhost to ""inherit"" ProxyPass Balancers and Workers defined in the main server. This can cause issues and inconsistent behavior if using the Balancer Manager and so should be disabled if using that feature.",0,0,others,httpd
3451,BalancerPersist,This directive will cause the shared memory storage associated with the balancers and balancer members to be persisted across restarts. This allows these local changes to not be lost during the normal restart/graceful state transitions.,1,4,limited-side-effect,httpd
3452,BrotliAlterETag,The BrotliAlterETag directive specifies how the ETag hader should be altered when a response is compressed.,0,0,others,httpd
3453,BrotliCompressionMaxInputBlock,"The BrotliCompressionMaxInputBlock directive specifies the maximum input block size between 16 and 24, with the caveat that larger block sizes require more memory.",1,1,resource,httpd
3455,BrotliCompressionWindow,"The BrotliCompressionWindow directive specifies the brotli sliding compression window size (a value between 10 and 24). Larger window sizes can improve compression quality, but require more memory.",1,6,function-tradeoff,httpd
3456,BrotliFilterNote,The BrotliFilterNote directive specifies that a note about compression ratios should be attached to the request. The name of the note is the value specified for the directive. You can use that note for statistical purposes by adding the value to your access log.,0,0,others,httpd
3458,BrowserMatchNoCase,"The BrowserMatchNoCase directive is semantically identical to the BrowserMatch directive. However, it provides for case-insensitive matching. For example:",0,0,others,httpd
3459,BufferedLogs,"The BufferedLogs directive causes mod_log_config to store several log entries in memory and write them together to disk, rather than writing them after each request. On some systems, this may result in more efficient disk access and hence higher performance. It may be set only once for the entire server; it cannot be configured per virtual-host.",1,4,limited-side-effect,httpd
3461,CacheDefaultExpire,"The CacheDefaultExpire directive specifies a default time, in seconds, to cache a document if neither an expiry date nor last-modified date are provided with the document. The value specified with the CacheMaxExpire directive does not override this setting.",1,5,workload-specific,httpd
3462,CacheDetailHeader,"When the CacheDetailHeader directive is switched on, an X-Cache-Detail header will be added to the response containing the detailed reason for a particular caching decision.",1,4,limited-side-effect,httpd
3463,CacheDirLength,The CacheDirLength directive sets the number of characters for each subdirectory name in the cache hierarchy. It can be used in conjunction with CacheDirLevels to determine the approximate structure of your cache hierarchy.,1,1,resource,httpd
3465,CacheDisable,The CacheDisable directive instructs mod_cache to not cache urls at or below url-string.,1,4,limited-side-effect,httpd
3466,CacheEnable,The CacheEnable directive instructs mod_cache to cache urls at or below url-string. The cache storage manager is specified with the cache_type argument. The CacheEnable directive can alternatively be placed inside either <Location> or <LocationMatch> sections to indicate the content is cacheable. cache_type disk instructs mod_cache to use the disk based storage manager implemented by mod_cache_disk. cache_type socache instructs mod_cache to use the shared object cache based storage manager implemented by mod_cache_socache.,1,4,limited-side-effect,httpd
3467,CacheFile,"The CacheFile directive opens handles to one or more files (given as whitespace separated arguments) and places these handles into the cache at server startup time. Handles to cached files are automatically closed on a server shutdown. When the files have changed on the filesystem, the server should be restarted to re-cache them.",0,0,others,httpd
3468,CacheHeader,"When the CacheHeader directive is switched on, an X-Cache header will be added to the response with the cache status of this response. If the normal handler is used, this directive may appear within a <Directory> or <Location> directive. If the quick handler is used, this directive must appear within a server or virtual host context, otherwise the setting will be ignored.",1,4,limited-side-effect,httpd
3470,CacheIgnoreHeaders,"According to RFC 2616, hop-by-hop HTTP headers are not stored in the cache. The following HTTP headers are hop-by-hop headers and thus do not get stored in the cache in any case regardless of the setting of CacheIgnoreHeaders:",1,4,limited-side-effect,httpd
3471,CacheIgnoreNoLastMod,"Ordinarily, documents without a last-modified date are not cached. Under some circumstances the last-modified date is removed (during mod_include processing for example) or not provided at all. The CacheIgnoreNoLastMod directive provides a way to specify that documents without last-modified dates should be considered for caching, even without a last-modified date. If neither a last-modified date nor an expiry date are provided with the document then the value specified by the CacheDefaultExpire directive will be used to generate an expiration date.",1,4,limited-side-effect,httpd
3472,CacheIgnoreQueryString,"Ordinarily, requests with query string parameters are cached separately for each unique query string. This is according to RFC 2616/13.9 done only if an expiration time is specified. The CacheIgnoreQueryString directive tells the cache to cache requests even if no expiration time is specified, and to reply with a cached reply even if the query string differs. From a caching point of view the request is treated as if having no query string when this directive is enabled.",1,4,limited-side-effect,httpd
3473,CacheIgnoreURLSessionIdentifiers,"This causes cacheable resources to be stored separately for each session, which is often not desired. CacheIgnoreURLSessionIdentifiers lets define a list of identifiers that are removed from the key that is used to identify an entity in the cache, such that cacheable resources are not stored separately for each session.",1,6,function-tradeoff,httpd
3474,CacheKeyBaseURL,"When the CacheKeyBaseURL directive is specified, the URL provided will be used as the base URL to calculate the URL of the cache keys in the reverse proxy configuration. When not specified, the scheme, hostname and port of the current virtual host is used to construct the cache key. When a cluster of machines is present, and all cached entries should be cached beneath the same cache key, a new base URL can be specified with this directive.",0,0,others,httpd
3475,CacheLastModifiedFactor,"In the event that a document does not provide an expiry date but does provide a last-modified date, an expiry date can be calculated based on the time since the document was last modified. The CacheLastModifiedFactor directive specifies a factor to be used in the generation of this expiry date according to the following formula: expiry-period = time-since-last-modified-date * factor expiry-date = current-date + expiry-period For example, if the document was last modified 10 hours ago, and factor is 0.1 then the expiry-period will be set to 10*0.1 = 1 hour. If the current time was 3:00pm then the computed expiry-date would be 3:00pm + 1hour = 4:00pm. If the expiry-period would be longer than that set by CacheMaxExpire, then the latter takes precedence.",0,0,others,httpd
3476,CacheLock,The CacheLock directive enables the thundering herd lock for the given URL space.,0,0,others,httpd
3477,CacheLockMaxAge,The CacheLockMaxAge directive specifies the maximum age of any cache lock.,0,0,others,httpd
3478,CacheLockPath,"The CacheLockPath directive allows you to specify the directory in which the locks are created. By default, the system's temporary folder is used. Locks consist of empty files that only exist for stale URLs in flight, so is significantly less resource intensive than the traditional disk cache.",0,0,others,httpd
3479,CacheMaxExpire,"The CacheMaxExpire directive specifies the maximum number of seconds for which cacheable HTTP documents will be retained without checking the origin server. Thus, documents will be out of date at most this number of seconds. This maximum value is enforced even if an expiry date was supplied with the document.",1,5,workload-specific,httpd
3480,CacheMaxFileSize,"The CacheMaxFileSize directive sets the maximum size, in bytes, for a document to be considered for storage in the cache.",1,1,resource,httpd
3481,CacheMinExpire,The CacheMinExpire directive specifies the minimum number of seconds for which cacheable HTTP documents will be retained without checking the origin server. This is only used if no valid expire time was supplied with the document.,1,5,workload-specific,httpd
3482,CacheMinFileSize,"The CacheMinFileSize directive sets the minimum size, in bytes, for a document to be considered for storage in the cache.",1,1,resource,httpd
3483,CacheNegotiatedDocs,"If set, this directive allows content-negotiated documents to be cached by proxy servers. This could mean that clients behind those proxys could retrieve versions of the documents that are not the best match for their abilities, but it will make caching more efficient.",1,4,limited-side-effect,httpd
3485,CacheReadSize,"The CacheReadSize directive sets the minimum amount of data, in bytes, to be read from the backend before the data is sent to the client. The default of zero causes all data read of any size to be passed downstream to the client immediately as it arrives. Setting this to a higher value causes the disk cache to buffer at least this amount before sending the result to the client. This can improve performance when caching content from a reverse proxy.",1,1,resource,httpd
3486,CacheReadTime,"The CacheReadTime directive sets the minimum amount of elapsed time that should pass before making an attempt to send data downstream to the client. During the time period, data will be buffered before sending the result to the client. This can improve performance when caching content from a reverse proxy.",1,5,workload-specific,httpd
3488,CacheSocache,"The CacheSocache directive defines the name of the shared object cache implementation to use, followed by optional arguments for that implementation. A number of implementations of shared object caches are available to choose from.",0,0,others,httpd
3490,CacheSocacheMaxTime,"The CacheSocacheMaxTime directive sets the maximum freshness lifetime, in seconds, for a document to be stored in the cache. This value overrides the freshness lifetime defined for the document by the HTTP protocol.",1,5,workload-specific,httpd
3491,CacheSocacheMinTime,"The CacheSocacheMinTime directive sets the amount of seconds beyond the freshness lifetime of the response that the response should be cached for in the shared object cache. If a response is only stored for its freshness lifetime, there will be no opportunity to revalidate the response to make it fresh again.",1,5,workload-specific,httpd
3492,CacheSocacheReadSize,"The CacheSocacheReadSize directive sets the minimum amount of data, in bytes, to be read from the backend before the data is sent to the client. The default of zero causes all data read of any size to be passed downstream to the client immediately as it arrives. Setting this to a higher value causes the disk cache to buffer at least this amount before sending the result to the client. This can improve performance when caching content from a slow reverse proxy.",1,1,resource,httpd
3493,CacheSocacheReadTime,"The CacheSocacheReadTime directive sets the minimum amount of elapsed time that should pass before making an attempt to send data downstream to the client. During the time period, data will be buffered before sending the result to the client. This can improve performance when caching content from a reverse proxy.",1,5,workload-specific,httpd
3494,CacheStaleOnError,"When the CacheStaleOnError directive is switched on, and when stale data is available in the cache, the cache will respond to 5xx responses from the backend by returning the stale data instead of the 5xx response. While the Cache-Control headers sent by clients will be respected, and the raw 5xx responses returned to the client on request, the 5xx response so returned to the client will not invalidate the content in the cache.",1,3,reliability-tradeoff,httpd
3497,CacheStorePrivate,"Ordinarily, responses with Cache-Control: private header values will not be stored in the cache. The CacheStorePrivate directive allows this behavior to be overridden. CacheStorePrivate On tells the server to attempt to cache the resource even if it contains private header values. Resources requiring authorization will never be cached.",0,0,others,httpd
3499,CharsetDefault,The CharsetDefault directive specifies the charset that content in the associated container should be translated to.,0,0,others,httpd
3502,CheckCaseOnly,"When set, this directive limits the action of the spelling correction to lower/upper case changes. Other potential corrections are not performed.",0,0,others,httpd
3503,CheckSpelling,This directive enables or disables the spelling module.,1,6,function-tradeoff,httpd
3504,ChrootDir,"This directive tells the server to chroot(8) to the specified directory after startup, but before accepting requests over the 'net.",0,0,others,httpd
3505,ContentDigest,This directive enables the generation of Content-MD5 headers as defined in RFC1864 respectively RFC2616.,0,0,others,httpd
3506,CookieDomain,"This directive controls the setting of the domain to which the tracking cookie applies. If not present, no domain is included in the cookie header field.",0,0,others,httpd
3507,CookieExpires,"When used, this directive sets an expiry time on the cookie generated by the usertrack module. The expiry-period can be given either as a number of seconds, or in the format such as ""2 weeks 3 days 7 hours"". Valid denominations are: years, months, weeks, days, hours, minutes and seconds. If the expiry time is in any format other than one number indicating the number of seconds, it must be enclosed by double quotes.",0,0,others,httpd
3509,CookieName,"This directive allows you to change the name of the cookie this module uses for its tracking purposes. By default the cookie is named ""Apache"".",0,0,others,httpd
3510,CookieSameSite,"When set to 'None', 'Lax', or 'Strict', the 'SameSite' cookie attribute is added to this modules tracking cookie with the corresponding value. This attribute instructs browser on how to treat the cookie when it is requested in a cross-site context.",0,0,others,httpd
3515,CustomLog,"The CustomLog directive is used to log requests to the server. A log format is specified, and the logging can optionally be made conditional on request characteristics using environment variables.",0,0,others,httpd
3516,DavDepthInfinity,"Use the DavDepthInfinity directive to allow the processing of PROPFIND requests containing the header 'Depth: Infinity'. Because this type of request could constitute a denial-of-service attack, by default it is not allowed.",0,0,others,httpd
3518,DavLockDB,"Use the DavLockDB directive to specify the full path to the lock database, excluding an extension. If the path is not absolute, it will be taken relative to ServerRoot. The implementation of mod_dav_fs uses a SDBM database to track user locks.",0,0,others,httpd
3519,DavMinTimeout,"When a client requests a DAV resource lock, it can also specify a time when the lock will be automatically removed by the server. This value is only a request, and the server can ignore it or inform the client of an arbitrary value.",0,0,others,httpd
3520,DBDExptime,Set the time to keep idle connections alive when the number of connections specified in DBDKeep has been exceeded (threaded platforms only).,0,0,others,httpd
3521,DBDInitSQL,"Modules, that wish it, can have one or more SQL statements executed when a connection to a database is created. Example usage could be initializing certain values or adding a log entry when a new connection is made to the database.",0,0,others,httpd
3524,DBDMin,Set the minimum number of connections per process (threaded platforms only).,0,0,others,httpd
3525,DBDParams,"As required by the underlying driver. Typically this will be used to pass whatever cannot be defaulted amongst username, password, database name, hostname and port number for connection.",0,0,others,httpd
3528,DBDriver,"Selects an apr_dbd driver by name. The driver must be installed on your system (on most systems, it will be a shared object or dll). For example, DBDriver mysql will select the MySQL driver in apr_dbd_mysql.so.",0,0,others,httpd
3529,DefaultIcon,"The DefaultIcon directive sets the icon to display for files when no specific icon is known, for FancyIndexing. Url-path is a (%-escaped) relative URL to the icon, or a fully qualified remote URL.",0,0,others,httpd
3530,DefaultLanguage,"The DefaultLanguage directive tells Apache that all resources in the directive's scope (e.g., all resources covered by the current <Directory> container) that don't have an explicit language extension (such as .fr or .de as configured by AddLanguage) should be assigned a Content-Language of language-tag. This allows entire directory trees to be marked as containing Dutch content, for instance, without having to rename each file. Note that unlike using extensions to specify languages, DefaultLanguage can only specify a single language.",0,0,others,httpd
3531,DefaultRuntimeDir,"The DefaultRuntimeDir directive sets the directory in which the server will create various run-time files (shared memory, locks, etc.). If set as a relative path, the full path will be relative to ServerRoot.",0,0,others,httpd
3532,DefaultType,"This directive has been disabled. For backwards compatibility of configuration files, it may be specified with the value none, meaning no default media type. For example:",0,0,others,httpd
3534,DeflateBufferSize,"The DeflateBufferSize directive specifies the size in bytes of the fragments that zlib should compress at one time. If the compressed response size is bigger than the one specified by this directive then httpd will switch to chunked encoding (HTTP header Transfer-Encoding set to Chunked), with the side effect of not setting any Content-Length HTTP header. This is particularly important when httpd works behind reverse caching proxies or when httpd is configured with mod_cache and mod_cache_disk because HTTP responses without any Content-Length header might not be cached.",1,1,resource,httpd
3535,DeflateCompressionLevel,"The DeflateCompressionLevel directive specifies what level of compression should be used, the higher the value, the better the compression, but the more CPU time is required to achieve this.",1,6,function-tradeoff,httpd
3536,DeflateFilterNote,The DeflateFilterNote directive specifies that a note about compression ratios should be attached to the request. The name of the note is the value specified for the directive. You can use that note for statistical purposes by adding the value to your access log.,0,0,others,httpd
3538,DeflateInflateRatioBurst,The DeflateInflateRatioBurst directive specifies the maximum number of times the DeflateInflateRatioLimit can be crossed before terminating the request.,0,0,others,httpd
3539,DeflateInflateRatioLimit,"The DeflateInflateRatioLimit directive specifies the maximum ratio of deflated to inflated size of an inflated request body. This ratio is checked as the body is streamed in, and if crossed more than DeflateInflateRatioBurst times, the request will be terminated.",0,0,others,httpd
3540,DeflateMemLevel,The DeflateMemLevel directive specifies how much memory should be used by zlib for compression (a value between 1 and 9).,1,6,function-tradeoff,httpd
3542,Directory,"<Directory> and </Directory> are used to enclose a group of directives that will apply only to the named directory, sub-directories of that directory, and the files within the respective directories. Any directive that is allowed in a directory context may be used. Directory-path is either the full path to a directory, or a wild-card string using Unix shell-style matching. In a wild-card string, ? matches any single character, and * matches any sequences of characters. You may also use [] character ranges. None of the wildcards match a `/' character, so <Directory ""/*/public_html""> will not match /home/user/public_html, but <Directory ""/home/*/public_html""> will match. Example:",0,0,others,httpd
3543,DirectoryCheckHandler,"The DirectoryCheckHandler directive determines whether mod_dir should check for directory indexes or add trailing slashes when some other handler has been configured for the current URL. Handlers can be set by directives such as SetHandler or by other modules, such as mod_rewrite during per-directory substitutions.",0,0,others,httpd
3544,DirectoryIndex,"The DirectoryIndex directive sets the list of resources to look for, when the client requests an index of the directory by specifying a / at the end of the directory name. Local-url is the (%-encoded) URL of a document on the server relative to the requested directory; it is usually the name of a file in the directory. Several URLs may be given, in which case the server will return the first one that it finds. If none of the resources exist and the Indexes option is set, the server will generate its own listing of the directory.",0,0,others,httpd
3545,DirectoryIndexRedirect,"By default, the DirectoryIndex is selected and returned transparently to the client. DirectoryIndexRedirect causes an external redirect to instead be issued.",0,0,others,httpd
3546,DirectoryMatch,"<DirectoryMatch> and </DirectoryMatch> are used to enclose a group of directives which will apply only to the named directory (and the files within), the same as <Directory>. However, it takes as an argument a regular expression. For example:",0,0,others,httpd
3547,DirectorySlash,The DirectorySlash directive determines whether mod_dir should fixup URLs pointing to a directory or not.,0,0,others,httpd
3548,DocumentRoot,"This directive sets the directory from which httpd will serve files. Unless matched by a directive like Alias, the server appends the path from the requested URL to the document root to make the path to the document. Example:",0,0,others,httpd
3549,DTracePrivileges,"This server-wide directive determines whether Apache will run with the privileges required to run dtrace. Note that DTracePrivileges On will not in itself activate DTrace, but DTracePrivileges Off will prevent it working.",1,6,function-tradeoff,httpd
3550,DumpIOInput,Enable dumping of all input.,1,6,function-tradeoff,httpd
3551,DumpIOOutput,Enable dumping of all output.,1,6,function-tradeoff,httpd
3553,ElseIf,The <ElseIf> applies the enclosed directives if and only if both the given condition evaluates to true and the most recent <If> or <ElseIf> section in the same scope has not been applied. For example: In,0,0,others,httpd
3554,EnableExceptionHook,For safety reasons this directive is only available if the server was configured with the --enable-exception-hook option. It enables a hook that allows external modules to plug in and do something after a child crashed.,0,0,others,httpd
3555,EnableMMAP,"This directive controls whether the httpd may use memory-mapping if it needs to read the contents of a file during delivery. By default, when the handling of a request requires access to the data within a file -- for example, when delivering a server-parsed file using mod_include -- Apache httpd memory-maps the file if the OS supports it.",1,4,limited-side-effect,httpd
3556,EnableSendfile,"This directive controls whether httpd may use the sendfile support from the kernel to transmit file contents to the client. By default, when the handling of a request requires no access to the data within a file -- for example, when delivering a static file -- Apache httpd uses sendfile to deliver the file contents without ever reading the file if the OS supports it.",1,6,function-tradeoff,httpd
3557,Error,"If an error can be detected within the configuration, this directive can be used to generate a custom error message, and halt configuration parsing. The typical use is for reporting required modules which are missing from the configuration.",0,0,others,httpd
3558,ErrorDocument,"In the event of a problem or error, Apache httpd can be configured to do one of four things,",0,0,others,httpd
3559,ErrorLog,The ErrorLog directive sets the name of the file to which the server will log any errors it encounters. If the file-path is not absolute then it is assumed to be relative to the ServerRoot.,0,0,others,httpd
3561,Example,"The Example directive just sets a demonstration flag which the example module's content handler displays. It takes no arguments. If you browse to an URL to which the example-hooks content-handler applies, you will get a display of the routines within the module and how and in what order they were called to service the document request. The effect of this directive one can observe under the point ""Example directive declared here: YES/NO"".",0,0,others,httpd
3563,ExpiresByType,"This directive defines the value of the Expires header and the max-age directive of the Cache-Control header generated for documents of the specified type (e.g., text/html). The second argument sets the number of seconds that will be added to a base time to construct the expiration date. The Cache-Control: max-age is calculated by subtracting the request time from the expiration date and expressing the result in seconds.",0,0,others,httpd
3564,ExpiresDefault,"This directive sets the default algorithm for calculating the expiration time for all documents in the affected realm. It can be overridden on a type-by-type basis by the ExpiresByType directive. See the description of that directive for details about the syntax of the argument, and the alternate syntax description as well.",0,0,others,httpd
3566,ExtFilterDefine,"The ExtFilterDefine directive defines the characteristics of an external filter, including the program to run and its arguments.",0,0,others,httpd
3567,FallbackResource,"Use this to set a handler for any URL that doesn't map to anything in your filesystem, and would otherwise return HTTP 404 (Not Found). For example",0,0,others,httpd
3568,FileETag,The FileETag directive configures the file attributes that are used to create the ETag (entity tag) response header field when the document is based on a static file. (The ETag value is used in cache management to save network bandwidth.) The FileETag directive allows you to choose which of these -- if any -- should be used. The recognized keywords are:,0,0,others,httpd
3569,Files,"The <Files> directive limits the scope of the enclosed directives by filename. It is comparable to the <Directory> and <Location> directives. It should be matched with a </Files> directive. The directives given within this section will be applied to any object with a basename (last component of filename) matching the specified filename. <Files> sections are processed in the order they appear in the configuration file, after the <Directory> sections and .htaccess files are read, but before <Location> sections. Note that <Files> can be nested inside <Directory> sections to restrict the portion of the filesystem they apply to.",0,0,others,httpd
3570,FilesMatch,"The <FilesMatch> directive limits the scope of the enclosed directives by filename, just as the <Files> directive does. However, it accepts a regular expression. For example:",0,0,others,httpd
3571,FilterChain,"This configures an actual filter chain, from declared filters. FilterChain takes any number of arguments, each optionally preceded with a single-character control that determines what to do:",0,0,others,httpd
3572,FilterDeclare,"This directive declares an output filter together with a header or environment variable that will determine runtime configuration. The first argument is a filter-name for use in FilterProvider, FilterChain and FilterProtocol directives.",0,0,others,httpd
3573,FilterProtocol,"This directs mod_filter to deal with ensuring the filter doesn't run when it shouldn't, and that the HTTP response headers are correctly set taking into account the effects of the filter.",0,0,others,httpd
3574,FilterProvider,This directive registers a provider for the smart filter. The provider will be called if and only if the expression declared evaluates to true when the harness is first called.,0,0,others,httpd
3575,FilterTrace,"This directive generates debug information from mod_filter. It is designed to help test and debug providers (filter modules), although it may also help with mod_filter itself.",0,0,others,httpd
3576,ForceLanguagePriority,The ForceLanguagePriority directive uses the given LanguagePriority to satisfy negotiation where the server could otherwise not return a single matching document.,0,0,others,httpd
3577,ForensicLog,"The ForensicLog directive is used to log requests to the server for forensic analysis. Each log entry is assigned a unique ID which can be associated with the request using the normal CustomLog directive. mod_log_forensic creates a token called forensic-id, which can be added to the transfer log using the %{forensic-id}n format string.",0,0,others,httpd
3579,GprofDir,"When the server has been compiled with gprof profiling support, GprofDir causes gmon.out files to be written to the specified directory when the process exits. If the argument ends with a percent symbol ('%'), subdirectories are created for each process id.",0,0,others,httpd
3580,GracefulShutdownTimeout,"The GracefulShutdownTimeout specifies how many seconds after receiving a ""graceful-stop"" signal, a server should continue to run, handling the existing connections.",0,0,others,httpd
3581,Group,"The Group directive sets the group under which the server will answer requests. In order to use this directive, the server must be run initially as root. If you start the server as a non-root user, it will fail to change to the specified group, and will instead continue to run as the group of the original user. Unix-group is one of:",0,0,others,httpd
3582,H2CopyFiles,"This directive influences how file content is handled in responses. When off, which is the default, file handles are passed from the requestion processing down to the main connection, using the usual Apache setaside handling for managing the lifetime of the file.",0,0,others,httpd
3583,H2Direct,This directive toggles the usage of the HTTP/2 Direct Mode. This should be used inside a <VirtualHost> section to enable direct HTTP/2 communication for that virtual host.,0,0,others,httpd
3585,H2MaxSessionStreams,This directive sets the maximum number of active streams per HTTP/2 session (e.g. connection) that the server allows. A stream is active if it is not idle or closed according to RFC 7540.,1,1,resource,httpd
3586,H2MaxWorkerIdleSeconds,This directive sets the maximum number of seconds a h2 worker may idle until it shuts itself down. This only happens while the number of h2 workers exceeds H2MinWorkers.,0,0,others,httpd
3588,H2MinWorkers,"This directive sets the minimum number of worker threads to spawn per child process for HTTP/2 processing. If this directive is not used, mod_http2 will chose a value suitable for the mpm module loaded.",1,1,resource,httpd
3589,H2ModernTLSOnly,This directive toggles the security checks on HTTP/2 connections in TLS mode (https:). This can be used server wide or for specific <VirtualHost>s.,1,2,security-tradeoff,httpd
3591,H2Push,This directive toggles the usage of the HTTP/2 server push protocol feature.,0,0,others,httpd
3592,H2PushDiarySize,This directive toggles the maximum number of HTTP/2 server pushes that are remembered per HTTP/2 connection. This can be used inside the <VirtualHost> section to influence the number for all connections to that virtual host.,0,0,others,httpd
3593,H2PushPriority,"This directive defines the priority handling of pushed responses based on the content-type of the response. This is usually defined per server config, but may also appear in a virtual host.",0,0,others,httpd
3594,H2PushResource,When added to a directory/location HTTP/2 PUSHes will be attempted for all paths added via this directive. This directive can be used several times for the same location.,0,0,others,httpd
3595,H2SerializeHeaders,This directive toggles if HTTP/2 requests shall be serialized in HTTP/1.1 format for processing by httpd core or if received binary data shall be passed into the request_recs directly.,0,0,others,httpd
3596,H2StreamMaxMemSize,This directive sets the maximum number of outgoing data bytes buffered in memory for an active streams. This memory is not allocated per stream as such. Allocations are counted against this limit when they are about to be done. Stream processing freezes when the limit has been reached and will only continue when buffered data has been sent out to the client.,1,1,resource,httpd
3597,H2TLSCoolDownSecs,This directive sets the number of seconds of idle time on a TLS connection before the TLS write size falls back to small (~1300 bytes) length. This can be used server wide or for specific <VirtualHost>s.,0,0,others,httpd
3598,H2TLSWarmUpSize,This directive sets the number of bytes to be sent in small TLS records (~1300 bytes) until doing maximum sized writes (16k) on https: HTTP/2 connections. This can be used server wide or for specific <VirtualHost>s.,1,1,resource,httpd
3599,H2Upgrade,This directive toggles the usage of the HTTP/1.1 Upgrade method for switching to HTTP/2. This should be used inside a <VirtualHost> section to enable Upgrades to HTTP/2 for that virtual host.,0,0,others,httpd
3600,H2WindowSize,This directive sets the size of the window that is used for flow control from client to server and limits the amount of data the server has to buffer. The client will stop sending on a stream once the limit has been reached until the server announces more available space (as it has processed some of the data).,1,1,resource,httpd
3601,Header,"This directive can replace, merge or remove HTTP response headers. The header is modified just after the content handler and output filters are run, allowing outgoing headers to be modified.",0,0,others,httpd
3602,HeaderName,The HeaderName directive sets the name of the file that will be inserted at the top of the index listing. Filename is the name of the file to include.,0,0,others,httpd
3603,HeartbeatAddress,The HeartbeatAddress directive specifies the multicast address to which mod_heartbeat will send status information. This address will usually correspond to a configured HeartbeatListen on a frontend proxy system.,0,0,others,httpd
3604,HeartbeatListen,The HeartbeatListen directive specifies the multicast address on which the server will listen for status information from mod_heartbeat-enabled servers. This address will usually correspond to a configured HeartbeatAddress on an origin server.,0,0,others,httpd
3606,HeartbeatStorage.A,The HeartbeatStorage directive specifies the path to read heartbeat data. This flat-file is used only when mod_slotmem_shm is not loaded.,0,0,others,httpd
3607,HeartbeatStorage.B,The HeartbeatStorage directive specifies the path to store heartbeat data. This flat-file is used only when mod_slotmem_shm is not loaded.,0,0,others,httpd
3608,HostnameLookups,"This directive enables DNS lookups so that host names can be logged (and passed to CGIs/SSIs in REMOTE_HOST). The value Double refers to doing double-reverse DNS lookup. That is, after a reverse lookup is performed, a forward lookup is then performed on that result. At least one of the IP addresses in the forward lookup must match the original address. (In ""tcpwrappers"" terminology this is called PARANOID.)",0,0,others,httpd
3609,HttpProtocolOptions,"This directive changes the rules applied to the HTTP Request Line (RFC 7230 v.1.1) and the HTTP Request Header Fields (RFC 7230 v.2), which are now applied by default or using the Strict option. Due to legacy modules, applications or custom user-agents which must be deprecated the Unsafe option has been added to revert to the legacy behaviors.",0,0,others,httpd
3611,IdentityCheckTimeout,"This directive specifies the timeout duration of an ident request. The default value of 30 seconds is recommended by RFC 1413, mainly because of possible network latency. However, you may want to adjust the timeout value according to your local network speed.",0,0,others,httpd
3612,If,"The <If> directive evaluates an expression at runtime, and applies the enclosed directives if and only if the expression evaluates to true. For example:",0,0,others,httpd
3613,IfDefine,"The <IfDefine test>...</IfDefine> section is used to mark directives that are conditional. The directives within an <IfDefine> section are only processed if the test is true. If test is false, everything between the start and end markers is ignored.",0,0,others,httpd
3614,IfDirective,"The <IfDirective test>...</IfDirective> section is used to mark directives that are conditional on the presence of a specific directive. The directives within an <IfDirective> section are only processed if the test is true. If test is false, everything between the start and end markers is ignored.",0,0,others,httpd
3615,IfFile,"The <IfFile filename>...</IfFile> section is used to mark directives that are conditional on the existence of a file on disk. The directives within an <IfFile> section are only processed if filename exists. If filename doesn't exist, everything between the start and end markers is ignored. filename can be an absolute path or a path relative to the server root.",0,0,others,httpd
3616,IfModule,"The <IfModule test>...</IfModule> section is used to mark directives that are conditional on the presence of a specific module. The directives within an <IfModule> section are only processed if the test is true. If test is false, everything between the start and end markers is ignored.",0,0,others,httpd
3617,IfSection,"The <IfSection test>...</IfSection> section is used to mark directives that are conditional on the presence of a specific section directive. A section directive is any directive such as <VirtualHost> which encloses other directives, and has a directive name with a leading ""<"".",0,0,others,httpd
3618,IfVersion,"The <IfVersion> section encloses configuration directives which are executed only if the httpd version matches the desired criteria. For normal (numeric) comparisons the version argument has the format major[.minor[.patch]], e.g. 2.1.0 or 2.2. minor and patch are optional. If these numbers are omitted, they are assumed to be zero. The following numerical operators are possible:",0,0,others,httpd
3619,ImapBase,"The ImapBase directive sets the default base used in the imagemap files. Its value is overridden by a base directive within the imagemap file. If not present, the base defaults to http://servername/.",0,0,others,httpd
3620,ImapDefault,"The ImapDefault directive sets the default default used in the imagemap files. Its value is overridden by a default directive within the imagemap file. If not present, the default action is nocontent, which means that a 204 No Content is sent to the client. In this case, the client should continue to display the original page.",0,0,others,httpd
3621,ImapMenu,The ImapMenu directive determines the action taken if an imagemap file is called without valid coordinates.,0,0,others,httpd
3622,Include,This directive allows inclusion of other configuration files from within the server configuration files.,0,0,others,httpd
3623,IncludeOptional,"This directive allows inclusion of other configuration files from within the server configuration files. It works identically to the Include directive, but it will be silently ignored (instead of causing an error) if wildcards are used and they do not match any file or directory or if a file path does not exist on the file system.",0,0,others,httpd
3624,IndexHeadInsert,The IndexHeadInsert directive specifies a string to insert in the <head> section of the HTML generated for the index page.,0,0,others,httpd
3625,IndexIgnore,"The IndexIgnore directive adds to the list of files to hide when listing a directory. File is a shell-style wildcard expression or full filename. Multiple IndexIgnore directives add to the list, rather than replacing the list of ignored files. By default, the list contains . (the current directory).",0,0,others,httpd
3626,IndexIgnoreReset,The IndexIgnoreReset directive removes any files ignored by IndexIgnore otherwise inherited from other configuration sections.,0,0,others,httpd
3627,IndexOptions,The IndexOptions directive specifies the behavior of the directory indexing. Option can be one of,0,0,others,httpd
3628,IndexOrderDefault,"The IndexOrderDefault directive is used in combination with the FancyIndexing index option. By default, fancyindexed directory listings are displayed in ascending order by filename; the IndexOrderDefault allows you to change this initial display order.",0,0,others,httpd
3629,IndexStyleSheet,The IndexStyleSheet directive sets the name of the file that will be used as the CSS for the index listing.,0,0,others,httpd
3630,ISAPIAppendLogToErrors,Record HSE_APPEND_LOG_PARAMETER requests from ISAPI extensions to the server error log.,0,0,others,httpd
3631,ISAPIAppendLogToQuery,Record HSE_APPEND_LOG_PARAMETER requests from ISAPI extensions to the query field (appended to the CustomLog %q component).,0,0,others,httpd
3632,ISAPICacheFile,"Specifies a space-separated list of file names to be loaded when the Apache server is launched, and remain loaded until the server is shut down. This directive may be repeated for every ISAPI .dll file desired. The full path name of each file should be specified. If the path name is not absolute, it will be treated relative to ServerRoot.",0,0,others,httpd
3633,ISAPIFakeAsync,"While set to on, asynchronous support for ISAPI callbacks is simulated.",1,4,limited-side-effect,httpd
3634,ISAPILogNotSupported,"Logs all requests for unsupported features from ISAPI extensions in the server error log. This may help administrators to track down problems. Once set to on and all desired ISAPI modules are functioning, it should be set back to off.",0,0,others,httpd
3635,ISAPIReadAheadBuffer,Defines the maximum size of the Read Ahead Buffer sent to ISAPI extensions when they are initially invoked. All remaining data must be retrieved using the ReadClient callback; some ISAPI extensions may not support the ReadClient function. Refer questions to the ISAPI extension's author.,1,1,resource,httpd
3636,KeepAlive,"The Keep-Alive extension to HTTP/1.0 and the persistent connection feature of HTTP/1.1 provide long-lived HTTP sessions which allow multiple requests to be sent over the same TCP connection. In some cases this has been shown to result in an almost 50% speedup in latency times for HTML documents with many images. To enable Keep-Alive connections, set KeepAlive On.",1,5,workload-specific,httpd
3637,KeepAliveTimeout,"The number of seconds Apache httpd will wait for a subsequent request before closing the connection. By adding a postfix of ms the timeout can be also set in milliseconds. Once a request has been received, the timeout value specified by the Timeout directive applies.",0,0,others,httpd
3638,LanguagePriority,"The LanguagePriority sets the precedence of language variants for the case where the client does not express a preference, when handling a Multiviews request. The list of MIME-lang are in order of decreasing preference.",0,0,others,httpd
3640,LDAPCacheTTL,Specifies the time (in seconds) that an item in the search/bind cache remains valid. The default is 600 seconds (10 minutes).,1,5,workload-specific,httpd
3641,LDAPConnectionPoolTTL,"Specifies the maximum age, in seconds, that a pooled LDAP connection can remain idle and still be available for use. Connections are cleaned up when they are next needed, not asynchronously.",1,5,workload-specific,httpd
3642,LDAPConnectionTimeout,"This directive configures the LDAP_OPT_NETWORK_TIMEOUT (or LDAP_OPT_CONNECT_TIMEOUT) option in the underlying LDAP client library, when available. This value typically controls how long the LDAP client library will wait for the TCP connection to the LDAP server to complete.",0,0,others,httpd
3643,LDAPLibraryDebug,Turns on SDK-specific LDAP debug options that generally cause the LDAP SDK to log verbose trace information to the main Apache error log. The trace messages from the LDAP SDK provide gory details that can be useful during debugging of connectivity problems with backend LDAP servers,0,0,others,httpd
3644,LDAPOpCacheEntries,This specifies the number of entries mod_ldap will use to cache LDAP compare operations. The default is 1024 entries. Setting it to 0 disables operation caching.,1,1,resource,httpd
3645,LDAPOpCacheTTL,Specifies the time (in seconds) that entries in the operation cache remain valid. The default is 600 seconds.,1,5,workload-specific,httpd
3646,LDAPRetries,The server will retry failed LDAP requests up to LDAPRetries times. Setting this directive to 0 disables retries.,0,0,others,httpd
3647,LDAPRetryDelay,"If LDAPRetryDelay is set to a non-zero value, the server will delay retrying an LDAP request for the specified amount of time. Setting this directive to 0 will result in any retry to occur without delay.",0,0,others,httpd
3650,LDAPTimeout,"This directive configures the timeout for bind and search operations, as well as the LDAP_OPT_TIMEOUT option in the underlying LDAP client library, when available.",0,0,others,httpd
3652,LDAPTrustedMode,The following modes are supported:,0,0,others,httpd
3653,LDAPVerifyServerCert,Specifies whether to force the verification of a server certificate when establishing an SSL connection to the LDAP server.,0,0,others,httpd
3654,LimitInternalRecursion,"An internal redirect happens, for example, when using the Action directive, which internally redirects the original request to a CGI script. A subrequest is Apache httpd's mechanism to find out what would happen for some URI if it were requested. For example, mod_dir uses subrequests to look for the files listed in the DirectoryIndex directive.",0,0,others,httpd
3655,LimitRequestBody,This directive specifies the number of bytes from 0 (meaning unlimited) to 2147483647 (2GB) that are allowed in a request body. See the note below for the limited applicability to proxy requests.,1,5,workload-specific,httpd
3656,LimitRequestFields,Number is an integer from 0 (meaning unlimited) to 32767. The default value is defined by the compile-time constant DEFAULT_LIMIT_REQUEST_FIELDS (100 as distributed).,0,0,others,httpd
3657,LimitRequestFieldSize,This directive specifies the number of bytes that will be allowed in an HTTP request header.,1,5,workload-specific,httpd
3658,LimitRequestLine,This directive sets the number of bytes that will be allowed on the HTTP request-line.,1,5,workload-specific,httpd
3659,LimitXMLRequestBody,Limit (in bytes) on maximum size of an XML-based request body. A value of 0 will disable any checking.,1,5,workload-specific,httpd
3663,LoadFile,The LoadFile directive links in the named object files or libraries when the server is started or restarted; this is used to load additional code which may be required for some module to work. Filename is either an absolute path or relative to ServerRoot.,0,0,others,httpd
3664,LoadModule,"The LoadModule directive links in the object file or library filename and adds the module structure named module to the list of active modules. Module is the name of the external variable of type module in the file, and is listed as the Module Identifier in the module documentation.",0,0,others,httpd
3665,Location,"The <Location> directive limits the scope of the enclosed directives by URL. It is similar to the <Directory> directive, and starts a subsection which is terminated with a </Location> directive. <Location> sections are processed in the order they appear in the configuration file, after the <Directory> sections and .htaccess files are read, and after the <Files> sections.",0,0,others,httpd
3666,LocationMatch,"The <LocationMatch> directive limits the scope of the enclosed directives by URL, in an identical manner to <Location>. However, it takes a regular expression as an argument instead of a simple string. For example:",0,0,others,httpd
3668,LogIOTrackTTFB,This directive configures whether this module tracks the delay between the request being read and the first byte of the response headers being written. The resulting value may be logged with the %^FB format.,0,0,others,httpd
3669,LogLevel,"LogLevel adjusts the verbosity of the messages recorded in the error logs (see ErrorLog directive). The following levels are available, in order of decreasing significance:",0,0,others,httpd
3670,LuaAuthzProvider,"After a lua function has been registered as authorization provider, it can be used with the Require directive:",0,0,others,httpd
3671,LuaCodeCache,"Specify the behavior of the in-memory code cache. The default is stat, which stats the top level script (not any included ones) each time that file is needed, and reloads it if the modified time indicates it is newer than the one it has already loaded. The other values cause it to keep the file cached forever (don't stat and replace) or to never cache the file.",1,4,limited-side-effect,httpd
3672,LuaHookAccessChecker,"Add your hook to the access_checker phase. An access checker hook function usually returns OK, DECLINED, or HTTP_FORBIDDEN.",0,0,others,httpd
3674,LuaHookFixups,"Just like LuaHookTranslateName, but executed at the fixups phase",0,0,others,httpd
3675,LuaHookInsertFilter,Not Yet Implemented,0,0,others,httpd
3677,LuaHookMapToStorage,"Like LuaHookTranslateName but executed at the map-to-storage phase of a request. Modules like mod_cache run at this phase, which makes for an interesting example on what to do here:",0,0,others,httpd
3678,LuaHookTranslateName,"Add a hook (at APR_HOOK_MIDDLE) to the translate name phase of request processing. The hook function receives a single argument, the request_rec, and should return a status code, which is either an HTTP error code, or the constants defined in the apache2 module: apache2.OK, apache2.DECLINED, or apache2.DONE.",0,0,others,httpd
3679,LuaHookTypeChecker,"This directive provides a hook for the type_checker phase of the request processing. This phase is where requests are assigned a content type and a handler, and thus can be used to modify the type and handler based on input:",0,0,others,httpd
3680,LuaInherit,"By default, if LuaHook* directives are used in overlapping Directory or Location configuration sections, the scripts defined in the more specific section are run after those defined in the more generic section (LuaInherit parent-first). You can reverse this order, or make the parent context not apply at all.",0,0,others,httpd
3682,LuaMapHandler,"This directive matches a uri pattern to invoke a specific handler function in a specific file. It uses PCRE regular expressions to match the uri, and supports interpolating match groups into both the file path and the function name. Be careful writing your regular expressions to avoid security issues.",0,0,others,httpd
3683,LuaOutputFilter,"Provides a means of adding a Lua function as an output filter. As with input filters, output filters work as coroutines, first yielding before buffers are sent, then yielding whenever a bucket needs to be passed down the chain, and finally (optionally) yielding anything that needs to be appended to the input data. The global variable bucket holds the buckets as they are passed onto the Lua script:",0,0,others,httpd
3684,LuaPackageCPath,Add a path to lua's shared library search path. Follows the same conventions as lua. This just munges the package.cpath in the lua vms.,0,0,others,httpd
3685,LuaPackagePath,Add a path to lua's module search path. Follows the same conventions as lua. This just munges the package.path in the lua vms.,0,0,others,httpd
3686,LuaQuickHandler,"This phase is run immediately after the request has been mapped to a virtal host, and can be used to either do some request processing before the other phases kick in, or to serve a request without the need to translate, map to storage et cetera. As this phase is run before anything else, directives such as <Location> or <Directory> are void in this phase, just as URIs have not been properly parsed yet.",0,0,others,httpd
3687,LuaRoot,"Specify the base path which will be used to evaluate all relative paths within mod_lua. If not specified they will be resolved relative to the current working directory, which may not always work well for a server.",0,0,others,httpd
3688,LuaScope,"Specify the life cycle scope of the Lua interpreter which will be used by handlers in this ""Directory."" The default is ""once""",0,0,others,httpd
3689,Macro,"The <Macro> directive controls the definition of a macro within the server runtime configuration files. The first argument is the name of the macro. Other arguments are parameters to the macro. It is good practice to prefix parameter names with any of '$%@', and not macro names with such characters.",0,0,others,httpd
3690,MaxConnectionsPerChild,"The MaxConnectionsPerChild directive sets the limit on the number of connections that an individual child server process will handle. After MaxConnectionsPerChild connections, the child process will die. If MaxConnectionsPerChild is 0, then the process will never expire.",1,1,resource,httpd
3691,MaxKeepAliveRequests,"The MaxKeepAliveRequests directive limits the number of requests allowed per connection when KeepAlive is on. If it is set to 0, unlimited requests will be allowed. We recommend that this setting be kept to a high value for maximum server performance.",1,1,resource,httpd
3692,MaxMemFree,"The MaxMemFree directive sets the maximum number of free Kbytes that every allocator is allowed to hold without calling free(). In threaded MPMs, every thread has its own allocator. When set to zero, the threshold will be set to unlimited.",1,1,resource,httpd
3694,MaxRangeReversals,"The MaxRangeReversals directive limits the number of HTTP Range reversals the server is willing to return to the client. If more ranges reversals than permitted are requested, the complete resource is returned instead.",0,0,others,httpd
3695,MaxRanges,"The MaxRanges directive limits the number of HTTP ranges the server is willing to return to the client. If more ranges than permitted are requested, the complete resource is returned instead.",0,0,others,httpd
3699,MaxThreads,"The MaxThreads directive sets the desired maximum number worker threads allowable. The default value is also the compiled in hard limit. Therefore it can only be lowered, for example:",1,1,resource,httpd
3700,MDBaseServer,"Controls if the base server, the one outside all VirtualHosts should be managed by mod_md or not. By default, it will not. For the very reason that it may have confusing side-effects. It is recommended that you have virtual hosts for all managed domains and do not rely on the global, fallback server configuration.",0,0,others,httpd
3702,MDCertificateAgreement,"When you use mod_md to obtain a certificate, you become a customer of the CA (e.g. Let's Encrypt). That means you need to read and agree to their Terms of Service, so that you understand what they offer and what they might exclude or require from you. mod_md cannot, by itself, agree to such a thing.",0,0,others,httpd
3703,MDCertificateAuthority,The URL where the CA offers its service.,0,0,others,httpd
3704,MDCertificateFile,This is used inside a MDomainSet and specifies the file holding the certificate chain for the Managed Domain. The matching key is specified via MDCertificateKeyFile.,0,0,others,httpd
3705,MDCertificateKeyFile,This is used inside a MDomainSet and specifies the file holding the private key for the Managed Domain. The matching certificate is specified via MDCertificateFile.,0,0,others,httpd
3707,MDCertificateProtocol,"Specifies the protocol to use. Currently, only ACME is supported.",0,0,others,httpd
3708,MDCertificateStatus,"When enabled, a resources is available in Managed Domains at 'https://domain/.httpd/certificate-status' that returns a JSON document list key properties of the current and of a renewed certificate - when available.",0,0,others,httpd
3709,MDChallengeDns01,Define a program to be called when the `dns-01` challenge needs to be setup/torn down. The program is given the argument `setup` or `teardown` followed by the domain name. For `setup` the challenge content is additionally given.,0,0,others,httpd
3711,MDDriveMode,This directive exists for backward compatibility as the old name for MDRenewMode.,0,0,others,httpd
3712,MDHttpProxy,Use a http proxy to connect to the MDCertificateAuthority. Define this if your webserver can only reach the internet with a forward proxy.,0,0,others,httpd
3713,MDMember,"Instead of listing all dns names on the same line, you may use MDMember to add such names to a managed domain.",0,0,others,httpd
3714,MDMembers,Defines if the ServerName and ServerAlias values of a VirtualHost are automatically added to the members of a Managed Domain or not.,0,0,others,httpd
3715,MDMessageCmd,"This command gets called when one of the following events happen for a Managed Domain: ""renewed"", ""installed"", ""expiring"", ""errored"". The command may be invoked for more than these in the future and ignore events it is not prepared to handle.",0,0,others,httpd
3716,MDMustStaple,"Defines if newly requested certificate should have the OCSP Must Staple flag set or not. If a certificate has this flag, the server is required to send a OCSP stapling response to every client. This only works if you configure mod_ssl to generate this (see SSLUseStapling and friends).",0,0,others,httpd
3717,MDNotifyCmd,The configured executable is run when a Managed Domain has signed up or renewed its certificate. It is given the name of the processed MD as additional arguments (after the parameters specified here). It should return status code 0 to indicate that it has run successfully.,0,0,others,httpd
3718,MDomain,"All the names in the list are managed as one Managed Domain (MD). mod_md will request one single certificate that is valid for all these names. This directive uses the global settings (see other MD directives below). If you need specific settings for one MD, use the <MDomainSet>.",0,0,others,httpd
3719,MDomainSetsection,"This is the directive MDomain with the added possibility to add setting just for this MD. In fact, you may also use ""<MDomain ..>"" as a shortcut.",0,0,others,httpd
3720,MDPortMap,"The ACME protocol provides two methods to verify domain ownership via HTTP: one that uses 'http:' urls (port 80) and one for 'https:' urls (port 443). If your server is not reachable by at least one of the two, ACME may only work by configuring your DNS server, see MDChallengeDns01.",0,0,others,httpd
3721,MDPrivateKeys,Defines what kind of private keys are generated for a managed domain and with what parameters. The only supported type right now is 'RSA' and the only parameter it takes is the number of bits used for the key.,0,0,others,httpd
3722,MDRenewMode,"In the default 'auto' mode, the module will do what makes most sense of each Managed Domain. For a domain without any certificates, it will obtain them from the Certificate Authority.",0,0,others,httpd
3723,MDRenewWindow,"If the validity of the certificate falls below duration, mod_md will get a new signed certificate.",0,0,others,httpd
3724,MDRequireHttps,This is a convenience directive to ease http: to https: migration of your Managed Domains. With:,0,0,others,httpd
3725,MDServerStatus,"Apaches 'server-status' handler allows you configure a resource to monitor what is going on. This includes now a section listing all Managed Domains with the DNS names, renewal status, lifetimes and main properties.",0,0,others,httpd
3726,MDStapleOthers,"This setting only takes effect when MDStapling is enabled. It controls if mod_md should also provide stapling information for certificates that are not directly controlled by it, e.g. renewed via an ACME CA.",0,0,others,httpd
3728,MDStaplingKeepResponse,This time window specifies when OCSP response data used in stapling shall be removed from the store again. Response information older than 7 days (default) is deleted on server restart/reload. This keeps the store from growing when certificates are renewed/reconfigured frequently.,0,0,others,httpd
3729,MDStaplingRenewWindow,"If the validity of the OCSP response used in stapling falls below duration, mod_md will obtain a new OCSP response.",0,0,others,httpd
3730,MDStoreDir,Defines where on the local file system the Managed Domain data is stored. This is an absolute path or interpreted relative to the server root. The default will create a directory 'md' in your server root.,0,0,others,httpd
3731,MDWarnWindow,See MDRenewWindow for a description on how you can specify the time.,0,0,others,httpd
3732,MemcacheConnTTL,Set the time to keep idle connections with the memcache server(s) alive (threaded platforms only).,1,5,workload-specific,httpd
3733,MergeSlashes,"By default, the server merges (or collapses) multiple consecutive slash ('/') characters in the path component of the request URL.",0,0,others,httpd
3734,MergeTrailers,"This directive controls whether HTTP trailers are copied into the internal representation of HTTP headers. This merging occurs when the request body has been completely consumed, long after most header processing would have a chance to examine or modify request headers.",0,0,others,httpd
3735,MetaDir,"Specifies the name of the directory in which Apache can find meta information files. The directory is usually a 'hidden' subdirectory of the directory that contains the file being accessed. Set to ""."" to look in the same directory as the file:",0,0,others,httpd
3736,MetaFiles,Turns on/off Meta file processing on a per-directory basis.,0,0,others,httpd
3737,MetaSuffix,"Specifies the file name suffix for the file containing the meta information. For example, the default values for the two directives will cause a request to DOCUMENT_ROOT/somedir/index.html to look in DOCUMENT_ROOT/somedir/.web/index.html.meta and will use its contents to generate additional MIME header information.",0,0,others,httpd
3738,MimeMagicFile,"The MimeMagicFile directive can be used to enable this module, the default file is distributed at conf/magic. Non-rooted paths are relative to the ServerRoot. Virtual hosts will use the same file as the main server unless a more specific setting is used, in which case the more specific setting overrides the main server's file.",0,0,others,httpd
3740,MinSpareThreads,Minimum number of idle threads to handle request spikes. Different MPMs deal with this directive differently.,1,1,resource,httpd
3741,MMapFile,The MMapFile directive maps one or more files (given as whitespace separated arguments) into memory at server startup time. They are automatically unmapped on a server shutdown. When the files have changed on the filesystem at least a HUP or USR1 signal should be send to the server to re-mmap() them.,0,0,others,httpd
3742,MultiviewsMatch,"MultiviewsMatch permits three different behaviors for mod_negotiation's Multiviews feature. Multiviews allows a request for a file, e.g. index.html, to match any negotiated extensions following the base request, e.g. index.html.en, index.html.fr, or index.html.gz.",0,0,others,httpd
3743,Mutex,"The Mutex directive sets the mechanism, and optionally the lock file location, that httpd and modules use to serialize access to resources. Specify default as the second argument to change the settings for all mutexes; specify a mutex name (see table below) as the second argument to override defaults only for that mutex.",0,0,others,httpd
3744,NameVirtualHost,"Prior to 2.3.11, NameVirtualHost was required to instruct the server that a particular IP address and port combination was usable as a name-based virtual host. In 2.3.11 and later, any time an IP address and port combination is used in multiple virtual hosts, name-based virtual hosting is automatically enabled for that address.",0,0,others,httpd
3745,NoProxy,"This directive is only useful for Apache httpd proxy servers within intranets. The NoProxy directive specifies a list of subnets, IP addresses, hosts and/or domains, separated by spaces. A request to a host which matches one or more of these is always served directly, without forwarding to the configured ProxyRemote proxy server(s).",0,0,others,httpd
3746,NWSSLTrustedCerts,Specifies a list of client certificate files (DER format) that are used when creating a proxied SSL connection. Each client certificate used by a server must be listed separately in its own .der file.,0,0,others,httpd
3747,NWSSLUpgradeable,Allow a connection that was created on the specified address and/or port to be upgraded to an SSL connection upon request from the client. The address and/or port must have already be defined previously with a Listen directive.,0,0,others,httpd
3748,Options,The Options directive controls which server features are available in a particular directory.,0,0,others,httpd
3749,PassEnv,"Specifies one or more native system environment variables to make available as internal environment variables, which are available to Apache HTTP Server modules as well as propagated to CGI scripts and SSI pages. Values come from the native OS environment of the shell which invoked the httpd process.",0,0,others,httpd
3750,PidFile,"The PidFile directive sets the file to which the server records the process id of the daemon. If the filename is not absolute, then it is assumed to be relative to the ServerRoot.",0,0,others,httpd
3751,PrivilegesMode,"This directive trades off performance vs security against malicious, privileges-aware code. In SECURE mode, each request runs in a secure subprocess, incurring a substantial performance penalty. In FAST mode, the server is not protected against escalation of privileges as discussed above.",1,2,security-tradeoff,httpd
3752,Protocol,This directive specifies the protocol used for a specific listening socket. The protocol is used to determine which module should handle a request and to apply protocol specific optimizations with the AcceptFilter directive.,1,4,limited-side-effect,httpd
3753,ProtocolEcho,The ProtocolEcho directive enables or disables the echo server.,0,0,others,httpd
3754,Protocols,This directive specifies the list of protocols supported for a server/virtual host. The list determines the allowed protocols a client may negotiate for this server/host.,0,0,others,httpd
3755,ProtocolsHonorOrder,This directive specifies if the server should honor the order in which the Protocols directive lists protocols.,0,0,others,httpd
3756,Proxy,Directives placed in <Proxy> sections apply only to matching proxied content. Shell-style wildcards are allowed.,0,0,others,httpd
3757,Proxy100Continue,"This directive determines whether the proxy should forward 100-continue Expect:ation to the origin server and thus let it decide when/if the HTTP request body should be read, or when Off the proxy should generate 100 Continue intermediate response by itself before forwarding the request body.",0,0,others,httpd
3758,ProxyAddHeaders,"This directive determines whether or not proxy related information should be passed to the backend server through X-Forwarded-For, X-Forwarded-Host and X-Forwarded-Server HTTP headers.",0,0,others,httpd
3759,ProxyBadHeader,The ProxyBadHeader directive determines the behavior of mod_proxy if it receives syntactically invalid response header lines (i.e. containing no colon) from the origin server. The following arguments are possible:,0,0,others,httpd
3761,ProxyDomain,"This directive is only useful for Apache httpd proxy servers within intranets. The ProxyDomain directive specifies the default domain which the apache proxy server will belong to. If a request to a host without a domain name is encountered, a redirection response to the same host with the configured Domain appended will be generated.",0,0,others,httpd
3762,ProxyErrorOverride,This directive is useful for reverse-proxy setups where you want to have a common look and feel on the error pages seen by the end user. This also allows for included files (via mod_include's SSI) to get the error code and act accordingly. (Default behavior would display the error page of the proxied server. Turning this on shows the SSI Error message.),0,0,others,httpd
3763,ProxyExpressDBMFile,"The ProxyExpressDBMFile directive points to the location of the Express map DBM file. This file serves to map the incoming server name, obtained from the Host: header, to a backend URL.",0,0,others,httpd
3765,ProxyExpressEnable,The ProxyExpressEnable directive controls whether the module will be active.,0,0,others,httpd
3766,ProxyFCGIBackendType,"This directive allows the type of backend FastCGI application to be specified. Some FastCGI servers, such as PHP-FPM, use historical quirks of environment variables to identify the type of proxy server being used. Set this directive to ""GENERIC"" if your non PHP-FPM application has trouble interpreting environment variables such as SCRIPT_FILENAME or PATH_TRANSLATED as set by the server.",0,0,others,httpd
3767,ProxyFCGISetEnvIf,"Just before passing a request to the configured FastCGI server, the core of the web server sets a number of environment variables based on details of the current request. FastCGI programs often uses these environment variables as inputs that determine what underlying scripts they will process, or what output they directly produce.",0,0,others,httpd
3768,ProxyFtpDirCharset,The ProxyFtpDirCharset directive defines the character set to be set for FTP directory listings in HTML generated by mod_proxy_ftp.,0,0,others,httpd
3769,ProxyFtpEscapeWildcards,"The ProxyFtpEscapeWildcards directive controls whether wildcard characters (""*?[{~"") in requested filenames are escaped with backslash before sending them to the FTP server. That is the default behavior, but many FTP servers don't know about the escaping and try to serve the literal filenames they were sent, including the backslashes in the names.",0,0,others,httpd
3770,ProxyFtpListOnWildcard,"The ProxyFtpListOnWildcard directive controls whether wildcard characters (""*?[{~"") in requested filenames cause mod_proxy_ftp to return a listing of files instead of downloading a file. By default (value on), they do.",0,0,others,httpd
3772,ProxyHCTemplate,The ProxyHCTemplate directive allows for creating a named set (template) of health check parameters that can then be assigned to balancer members via the hctemplate parameter.,0,0,others,httpd
3773,ProxyHCTPsize,"If Apache httpd and APR are built with thread support, the health check module will offload the work of the actual checking to a threadpool associated with the Watchdog process, allowing for parallel checks. The ProxyHCTPsize directive determines the size of this threadpool. If set to 0, no threadpool is used at all, resulting in serialized health checks.",1,1,resource,httpd
3775,ProxyHTMLCharsetOut,"This selects an encoding for mod_proxy_html output. It should not normally be used, as any change from the default UTF-8 (Unicode - as used internally by libxml2) will impose an additional processing overhead. The special token ProxyHTMLCharsetOut * will generate output using the same encoding as the input.",0,0,others,httpd
3776,ProxyHTMLDocType,"In the first form, documents will be declared as HTML 4.01 or XHTML 1.0 according to the option selected. This option also determines whether HTML or XHTML syntax is used for output. Note that the format of the documents coming from the backend server is immaterial: the parser will deal with it automatically. If the optional second argument is set to Legacy, documents will be declared ""Transitional"", an option that may be necessary if you are proxying pre-1998 content or working with defective authoring/publishing tools.",0,0,others,httpd
3777,ProxyHTMLEnable,A simple switch to enable or disable the proxy_html filter. If mod_xml2enc is loaded it will also automatically set up internationalisation support.,0,0,others,httpd
3778,ProxyHTMLEvents,Specifies one or more attributes to treat as scripting events and apply ProxyHTMLURLMaps to where enabled. You can specify any number of attributes in one or more ProxyHTMLEvents directives.,0,0,others,httpd
3779,ProxyHTMLExtended,"Set to Off, HTML links are rewritten according to the ProxyHTMLURLMap directives, but links appearing in Javascript and CSS are ignored.",0,0,others,httpd
3780,ProxyHTMLFixups,This directive takes one to three arguments as follows:,0,0,others,httpd
3781,ProxyHTMLInterp,This enables per-request interpolation in ProxyHTMLURLMap to- and from- patterns.,0,0,others,httpd
3782,ProxyHTMLLinks,"Specifies elements that have URL attributes that should be rewritten using standard ProxyHTMLURLMaps. You will need one ProxyHTMLLinks directive per element, but it can have any number of attributes.",0,0,others,httpd
3784,ProxyHTMLStripComments,"This directive will cause mod_proxy_html to strip HTML comments. Note that this will also kill off any scripts or styles embedded in comments (a bogosity introduced in 1995/6 with Netscape 2 for the benefit of then-older browsers, but still in use today). It may also interfere with comment-based processors such as SSI or ESI: be sure to run any of those before mod_proxy_html in the filter chain if stripping comments!",0,0,others,httpd
3785,ProxyHTMLURLMap,"This is the key directive for rewriting HTML links. When parsing a document, whenever a link target matches from-pattern, the matching portion will be rewritten to to-pattern, as modified by any flags supplied and by the ProxyHTMLExtended directive. Only the elements specified using the ProxyHTMLLinks directive will be considered as HTML links.",0,0,others,httpd
3786,ProxyIOBufferSize,The ProxyIOBufferSize directive adjusts the size of the internal buffer which is used as a scratchpad for the data between input and output. The size must be at least 512.,1,1,resource,httpd
3787,ProxyMatch,"The <ProxyMatch> directive is identical to the <Proxy> directive, except that it matches URLs using regular expressions.",0,0,others,httpd
3788,ProxyMaxForwards,The ProxyMaxForwards directive specifies the maximum number of proxies through which a request may pass if there's no Max-Forwards header supplied with the request. This may be set to prevent infinite proxy loops or a DoS attack.,0,0,others,httpd
3789,ProxyPass,This directive allows remote servers to be mapped into the space of the local server. The local server does not act as a proxy in the conventional sense but appears to be a mirror of the remote server. The local server is often called a reverse proxy or gateway. The path is the name of a local virtual path; url is a partial URL for the remote server and cannot include a query string.,0,0,others,httpd
3790,ProxyPassInherit,"This directive will cause the current server/vhost to ""inherit"" ProxyPass directives defined in the main server. This can cause issues and inconsistent behavior if using the Balancer Manager for dynamic changes and so should be disabled if using that feature.",0,0,others,httpd
3792,ProxyPassMatch,"This directive is equivalent to ProxyPass but makes use of regular expressions instead of simple prefix matching. The supplied regular expression is matched against the url, and if it matches, the server will substitute any parenthesized matches into the given string and use it as a new url.",0,0,others,httpd
3793,ProxyPassReverse,"This directive lets Apache httpd adjust the URL in the Location, Content-Location and URI headers on HTTP redirect responses. This is essential when Apache httpd is used as a reverse proxy (or gateway) to avoid bypassing the reverse proxy because of HTTP redirects on the backend servers which stay behind the reverse proxy.",0,0,others,httpd
3794,ProxyPassReverseCookieDomain,"Usage is basically similar to ProxyPassReverse, but instead of rewriting headers that are a URL, this rewrites the domain string in Set-Cookie headers.",0,0,others,httpd
3795,ProxyPassReverseCookiePath,"Useful in conjunction with ProxyPassReverse in situations where backend URL paths are mapped to public paths on the reverse proxy. This directive rewrites the path string in Set-Cookie headers. If the beginning of the cookie path matches internal-path, the cookie path will be replaced with public-path.",0,0,others,httpd
3796,ProxyPreserveHost,"When enabled, this option will pass the Host: line from the incoming request to the proxied host, instead of the hostname specified in the ProxyPass line.",0,0,others,httpd
3797,ProxyReceiveBufferSize,"The ProxyReceiveBufferSize directive specifies an explicit (TCP/IP) network buffer size for proxied HTTP and FTP connections, for increased throughput. It has to be greater than 512 or set to 0 to indicate that the system's default buffer size should be used.",1,1,resource,httpd
3798,ProxyRemote,"This defines remote proxies to this proxy. match is either the name of a URL-scheme that the remote server supports, or a partial URL for which the remote server should be used, or * to indicate the server should be contacted for all requests. remote-server is a partial URL for the remote server. Syntax:",0,0,others,httpd
3799,ProxyRemoteMatch,"The ProxyRemoteMatch is identical to the ProxyRemote directive, except that the first argument is a regular expression match against the requested URL.",0,0,others,httpd
3801,ProxySCGIInternalRedirect,"The ProxySCGIInternalRedirect enables the backend to internally redirect the gateway to a different URL. This feature originates in mod_cgi, which internally redirects the response if the response status is OK (200) and the response contains a Location (or configured alternate header) and its value starts with a slash (/). This value is interpreted as a new local URL that Apache httpd internally redirects to.",0,0,others,httpd
3803,ProxySet,"This directive is used as an alternate method of setting any of the parameters available to Proxy balancers and workers normally done via the ProxyPass directive. If used within a <Proxy balancer url|worker url> container directive, the url argument is not required. As a side effect the respective balancer or worker gets created. This can be useful when doing reverse proxying via a RewriteRule instead of a ProxyPass directive.",0,0,others,httpd
3804,ProxySourceAddress,This directive allows to set a specific local address to bind to when connecting to a backend server.,0,0,others,httpd
3805,ProxyStatus,This directive determines whether or not proxy loadbalancer status data is displayed via the mod_status server-status page.,0,0,others,httpd
3806,ProxyTimeout,"This directive allows a user to specifiy a timeout on proxy requests. This is useful when you have a slow/buggy appserver which hangs, and you would rather just return a timeout and fail gracefully instead of waiting however long it takes the server to return.",0,0,others,httpd
3807,ProxyVia,"This directive controls the use of the Via: HTTP header by the proxy. Its intended use is to control the flow of proxy requests along a chain of proxy servers. See RFC 2616 (HTTP/1.1), section 14.45 for an explanation of Via: header lines.",0,0,others,httpd
3808,QualifyRedirectURL,"This directive controls whether the server will ensure that the REDIRECT_URL environment variable is fully qualified. By default, the variable contains the verbatim URL requested by the client, such as ""/index.html"". With QualifyRedirectURL On, the same request would result in a value such as ""http://www.example.com/index.html"".",0,0,others,httpd
3809,ReadmeName,"The ReadmeName directive sets the name of the file that will be appended to the end of the index listing. Filename is the name of the file to include, and is taken to be relative to the location being indexed. If Filename begins with a slash, as in example 2, it will be taken to be relative to the DocumentRoot.",0,0,others,httpd
3810,ReceiveBufferSize,The server will set the TCP receive buffer size to the number of bytes specified.,1,1,resource,httpd
3811,Redirect,The Redirect directive maps an old URL into a new one by asking the client to refetch the resource at the new location.,0,0,others,httpd
3812,RedirectMatch,"This directive is equivalent to Redirect, but makes use of regular expressions, instead of simple prefix matching. The supplied regular expression is matched against the URL-path, and if it matches, the server will substitute any parenthesized matches into the given string and use it as a filename. For example, to redirect all GIF files to like-named JPEG files on another server, one might use:",0,0,others,httpd
3813,RedirectPermanent,This directive makes the client know that the Redirect is permanent (status 301). Exactly equivalent to Redirect permanent.,0,0,others,httpd
3815,RedisConnPoolTTL,Set the time to keep idle connections with the Redis server(s) alive (threaded platforms only).,0,0,others,httpd
3816,RedisTimeout,Set the Read/Write timeout used for the connection with the Redis server(s).,0,0,others,httpd
3817,ReflectorHeader,"This directive controls the reflection of request headers to the response. The first argument is the name of the request header to copy. If the optional second argument is specified, it will be used as the name of the response header, otherwise the original request header name will be used.",0,0,others,httpd
3818,RegexDefaultOptions,This directive adds some default behavior to ANY regular expression used afterwards.,0,0,others,httpd
3819,RegisterHttpMethod,"This directive may be used to register additional HTTP methods. This is necessary if non-standard methods need to be used with directives that accept method names as parameters, or to allow particular non-standard methods to be used via proxy or CGI script when the server has been configured to only pass recognized methods to modules.",0,0,others,httpd
3820,RemoteIPHeader,"The RemoteIPHeader directive triggers mod_remoteip to treat the value of the specified header-field header as the useragent IP address, or list of intermediate useragent IP addresses, subject to further configuration of the RemoteIPInternalProxy and RemoteIPTrustedProxy directives. Unless these other directives are used, mod_remoteip will trust all hosts presenting a RemoteIPHeader IP value.",0,0,others,httpd
3821,RemoteIPInternalProxy,"The RemoteIPInternalProxy directive adds one or more addresses (or address blocks) to trust as presenting a valid RemoteIPHeader value of the useragent IP. Unlike the RemoteIPTrustedProxy directive, any IP address presented in this header, including private intranet addresses, are trusted when passed from these proxies.",0,0,others,httpd
3822,RemoteIPInternalProxyList,"The RemoteIPInternalProxyList directive specifies a file parsed at startup, and builds a list of addresses (or address blocks) to trust as presenting a valid RemoteIPHeader value of the useragent IP.",0,0,others,httpd
3823,RemoteIPProxiesHeader,"The RemoteIPProxiesHeader directive specifies a header into which mod_remoteip will collect a list of all of the intermediate client IP addresses trusted to resolve the useragent IP of the request. Note that intermediate RemoteIPTrustedProxy addresses are recorded in this header, while any intermediate RemoteIPInternalProxy addresses are discarded.",0,0,others,httpd
3824,RemoteIPProxyProtocol,"The RemoteIPProxyProtocol directive enables or disables the reading and handling of the PROXY protocol connection header. If enabled with the On flag, the upstream client must send the header every time it opens a connection or the connection will be aborted unless it is in the list of disabled hosts provided by the RemoteIPProxyProtocolExceptions directive.",0,0,others,httpd
3825,RemoteIPProxyProtocolExceptions,"The RemoteIPProxyProtocol directive enables or disables the reading and handling of the PROXY protocol connection header. Sometimes it is desirable to require clients to provide the PROXY header, but permit other clients to connect without it. This directive allows a server administrator to configure a single host or CIDR range of hosts that may do so. This is generally useful for monitoring and administrative traffic to a virtual host direct to the server behind the upstream load balancer.",0,0,others,httpd
3826,RemoteIPTrustedProxy,"The RemoteIPTrustedProxy directive adds one or more addresses (or address blocks) to trust as presenting a valid RemoteIPHeader value of the useragent IP. Unlike the RemoteIPInternalProxy directive, any intranet or private IP address reported by such proxies, including the 10/8, 172.16/12, 192.168/16, 169.254/16 and 127/8 blocks (or outside of the IPv6 public 2000::/3 block) are not trusted as the useragent IP, and are left in the RemoteIPHeader header's value.",0,0,others,httpd
3827,RemoteIPTrustedProxyList,"The RemoteIPTrustedProxyList directive specifies a file parsed at startup, and builds a list of addresses (or address blocks) to trust as presenting a valid RemoteIPHeader value of the useragent IP.",0,0,others,httpd
3828,RequestHeader,"This directive can replace, merge, change or remove HTTP request headers. The header is modified just before the content handler is run, allowing incoming headers to be modified. The action it performs is determined by the first argument. This can be one of the following values:",0,0,others,httpd
3829,RequestReadTimeout,"This directive can set various timeouts for completing the TLS handshake, receiving the request headers and/or the request body from the client. If the client fails to complete each of these stages within the configured time, a 408 REQUEST TIME OUT error is sent.",0,0,others,httpd
3830,RewriteCond,"The RewriteCond directive defines a rule condition. One or more RewriteCond can precede a RewriteRule directive. The following rule is then only used if both the current state of the URI matches its pattern, and if these conditions are met.",0,0,others,httpd
3831,RewriteEngine,The RewriteEngine directive enables or disables the runtime rewriting engine. If it is set to off this module does no runtime processing at all. It does not even update the SCRIPT_URx environment variables.,1,6,function-tradeoff,httpd
3832,RewriteMap,The RewriteMap directive defines a Rewriting Map which can be used inside rule substitution strings by the mapping-functions to insert/substitute fields through a key lookup. The source of this lookup can be of various types.,0,0,others,httpd
3833,RewriteOptions,The RewriteOptions directive sets some special options for the current per-server or per-directory configuration. The Option string can currently only be one of the following:,0,0,others,httpd
3834,RewriteRule,"The RewriteRule directive is the real rewriting workhorse. The directive can occur more than once, with each instance defining a single rewrite rule. The order in which these rules are defined is important - this is the order in which they will be applied at run-time.",0,0,others,httpd
3835,RLimitCPU,"Takes 1 or 2 parameters. The first parameter sets the soft resource limit for all processes and the second parameter sets the maximum resource limit. Either parameter can be a number, or max to indicate to the server that the limit should be set to the maximum allowed by the operating system configuration. Raising the maximum resource limit requires that the server is running as root or in the initial startup phase.",0,0,others,httpd
3837,RLimitNPROC,"Takes 1 or 2 parameters. The first parameter sets the soft resource limit for all processes, and the second parameter sets the maximum resource limit. Either parameter can be a number, or max to indicate to the server that the limit should be set to the maximum allowed by the operating system configuration. Raising the maximum resource limit requires that the server is running as root or in the initial startup phase.",1,1,resource,httpd
3839,Script,"This directive adds an action, which will activate cgi-script when a file is requested using the method of method. The cgi-script is the URL-path to a resource that has been designated as a CGI script using ScriptAlias or AddHandler. The URL and file path of the requested document is sent using the standard CGI PATH_INFO and PATH_TRANSLATED environment variables.",0,0,others,httpd
3840,ScriptAlias,"The ScriptAlias directive has the same behavior as the Alias directive, except that in addition it marks the target directory as containing CGI scripts that will be processed by mod_cgi's cgi-script handler. URLs with a case-sensitive (%-decoded) path beginning with URL-path will be mapped to scripts beginning with the second argument, which is a full pathname in the local filesystem.",0,0,others,httpd
3841,ScriptAliasMatch,"This directive is equivalent to ScriptAlias, but makes use of regular expressions, instead of simple prefix matching. The supplied regular expression is matched against the URL-path, and if it matches, the server will substitute any parenthesized matches into the given string and use it as a filename. For example, to activate the standard /cgi-bin, one might use:",0,0,others,httpd
3842,ScriptInterpreterSource,"This directive is used to control how Apache httpd finds the interpreter used to run CGI scripts. The default setting is Script. This causes Apache httpd to use the interpreter pointed to by the shebang line (first line, starting with #!) in the script. On Win32 systems this line usually looks like:",0,0,others,httpd
3843,ScriptLog,"The ScriptLog directive sets the CGI script error logfile. If no ScriptLog is given, no error log is created. If given, any CGI errors are logged into the filename given as argument. If this is a relative file or path it is taken relative to the ServerRoot.",0,0,others,httpd
3845,ScriptLogLength,"ScriptLogLength can be used to limit the size of the CGI script logfile. Since the logfile logs a lot of information per CGI error (all request headers, all script output) it can grow to be a big file. To prevent problems due to unbounded growth, this directive can be used to set an maximum file-size for the CGI logfile. If the file exceeds this size, no more information will be written to it.",1,3,reliability-tradeoff,httpd
3846,ScriptSock,"This directive sets the filename prefix of the socket to use for communication with the CGI daemon, an extension corresponding to the process ID of the server will be appended. The socket will be opened using the permissions of the user who starts Apache (usually root). To maintain the security of communications with CGI scripts, it is important that no other user has permission to write in the directory where the socket is located.",0,0,others,httpd
3847,SecureListen,Specifies the port and the eDirectory based certificate name that will be used to enable SSL encryption. An optional third parameter also enables mutual authentication.,0,0,others,httpd
3848,SeeRequestTail,"mod_status with ExtendedStatus On displays the actual request being handled. For historical purposes, only 63 characters of the request are actually stored for display purposes. This directive controls whether the 1st 63 characters are stored (the previous behavior and the default) or if the last 63 characters are. This is only applicable, of course, if the length of the request is 64 characters or greater.",0,0,others,httpd
3849,SendBufferSize,"Sets the server's TCP send buffer size to the number of bytes specified. It is often useful to set this past the OS's standard default value on high speed, high latency connections (i.e., 100ms or so, such as transcontinental fast pipes).",1,1,resource,httpd
3850,ServerAdmin,"The ServerAdmin sets the contact address that the server includes in any error messages it returns to the client. If the httpd doesn't recognize the supplied argument as an URL, it assumes, that it's an email-address and prepends it with mailto: in hyperlink targets. However, it's recommended to actually use an email address, since there are a lot of CGI scripts that make that assumption. If you want to use an URL, it should point to another server under your control. Otherwise users may not be able to contact you in case of errors.",0,0,others,httpd
3852,ServerName,"The ServerName directive sets the request scheme, hostname and port that the server uses to identify itself.",0,0,others,httpd
3853,ServerRoot,"The ServerRoot directive sets the directory in which the server lives. Typically it will contain the subdirectories conf/ and logs/. Relative paths in other configuration directives (such as Include or LoadModule, for example) are taken as relative to this directory.",0,0,others,httpd
3854,ServerSignature,"The ServerSignature directive allows the configuration of a trailing footer line under server-generated documents (error messages, mod_proxy ftp directory listings, mod_info output, ...). The reason why you would want to enable such a footer line is that in a chain of proxies, the user often has no possibility to tell which of the chained servers actually produced a returned error message.",0,0,others,httpd
3855,ServerTokens,This directive controls whether Server response header field which is sent back to clients includes a description of the generic OS-type of the server as well as information about compiled-in modules.,0,0,others,httpd
3856,Session,The Session directive enables a session for the directory or location container. Further directives control where the session will be stored and how privacy is maintained.,0,0,others,httpd
3857,SessionCookieName,The SessionCookieName directive specifies the name and optional attributes of an RFC2109 compliant cookie inside which the session will be stored. RFC2109 cookies are set using the Set-Cookie HTTP header.,0,0,others,httpd
3858,SessionCookieName2,The SessionCookieName2 directive specifies the name and optional attributes of an RFC2965 compliant cookie inside which the session will be stored. RFC2965 cookies are set using the Set-Cookie2 HTTP header.,0,0,others,httpd
3859,SessionCookieRemove,The SessionCookieRemove flag controls whether the cookies containing the session will be removed from the headers during request processing.,1,6,function-tradeoff,httpd
3860,SessionCryptoCipher,"The SessionCryptoCipher directive allows the cipher to be used during encryption. If not specified, the cipher defaults to aes256.",0,0,others,httpd
3861,SessionCryptoDriver,"The SessionCryptoDriver directive specifies the name of the crypto driver to be used for encryption. If not specified, the driver defaults to the recommended driver compiled into APR-util.",0,0,others,httpd
3862,SessionCryptoPassphrase,"The SessionCryptoPassphrase directive specifies the keys to be used to enable symmetrical encryption on the contents of the session before writing the session, or decrypting the contents of the session after reading the session.",0,0,others,httpd
3864,SessionDBDCookieName,The SessionDBDCookieName directive specifies the name and optional attributes of an RFC2109 compliant cookie inside which the session ID will be stored. RFC2109 cookies are set using the Set-Cookie HTTP header.,0,0,others,httpd
3865,SessionDBDCookieName2,The SessionDBDCookieName2 directive specifies the name and optional attributes of an RFC2965 compliant cookie inside which the session ID will be stored. RFC2965 cookies are set using the Set-Cookie2 HTTP header.,0,0,others,httpd
3866,SessionDBDCookieRemove,The SessionDBDCookieRemove flag controls whether the cookies containing the session ID will be removed from the headers during request processing.,1,6,function-tradeoff,httpd
3867,SessionDBDDeleteLabel,The SessionDBDDeleteLabel directive sets the default delete query label to be used to delete an expired or empty session. This label must have been previously defined using the DBDPrepareSQL directive.,0,0,others,httpd
3868,SessionDBDInsertLabel,The SessionDBDInsertLabel directive sets the default insert query label to be used to load in a session. This label must have been previously defined using the DBDPrepareSQL directive.,0,0,others,httpd
3871,SessionDBDUpdateLabel,The SessionDBDUpdateLabel directive sets the default update query label to be used to load in a session. This label must have been previously defined using the DBDPrepareSQL directive.,0,0,others,httpd
3872,SessionEnv,"If set to On, the SessionEnv directive causes the contents of the session to be written to a CGI environment variable called HTTP_SESSION.",0,0,others,httpd
3873,SessionExclude,"The SessionExclude directive allows sessions to be disabled relative to URL prefixes only. This can be used to make a website more efficient, by targeting a more precise URL space for which a session should be maintained. By default, all URLs within the directory or location are included in the session. The SessionExclude directive takes precedence over the SessionInclude directive.",0,0,others,httpd
3874,SessionExpiryUpdateInterval,The SessionExpiryUpdateInterval directive allows sessions to avoid the cost associated with writing the session each request when only the expiry time has changed. This can be used to make a website more efficient or reduce load on a database when using mod_session_dbd. The session is always written if the data stored in the session has changed or the expiry has changed by more than the configured interval.,0,0,others,httpd
3875,SessionHeader,"The SessionHeader directive defines the name of an HTTP response header which, if present, will be parsed and written to the current session.",0,0,others,httpd
3876,SessionInclude,"The SessionInclude directive allows sessions to be made valid for specific URL prefixes only. This can be used to make a website more efficient, by targeting a more precise URL space for which a session should be maintained. By default, all URLs within the directory or location are included in the session.",0,0,others,httpd
3878,SetEnv,"Sets an internal environment variable, which is then available to Apache HTTP Server modules, and passed on to CGI scripts and SSI pages.",0,0,others,httpd
3879,SetEnvIf,The SetEnvIf directive defines environment variables based on attributes of the request. The attribute specified in the first argument can be one of four things:,0,0,others,httpd
3880,SetEnvIfExpr,"The SetEnvIfExpr directive defines environment variables based on an <If> ap_expr. These expressions will be evaluated at runtime, and applied env-variable in the same fashion as SetEnvIf.",0,0,others,httpd
3881,SetEnvIfNoCase,"The SetEnvIfNoCase is semantically identical to the SetEnvIf directive, and differs only in that the regular expression matching is performed in a case-insensitive manner. For example:",0,0,others,httpd
3882,SetHandler,"When placed into an .htaccess file or a <Directory> or <Location> section, this directive forces all matching files to be parsed through the handler given by handler-name. For example, if you had a directory you wanted to be parsed entirely as imagemap rule files, regardless of extension, you might put the following into an .htaccess file in that directory:",0,0,others,httpd
3883,SetInputFilter,"The SetInputFilter directive sets the filter or filters which will process client requests and POST input when they are received by the server. This is in addition to any filters defined elsewhere, including the AddInputFilter directive.",0,0,others,httpd
3884,SetOutputFilter,"The SetOutputFilter directive sets the filters which will process responses from the server before they are sent to the client. This is in addition to any filters defined elsewhere, including the AddOutputFilter directive.",0,0,others,httpd
3887,SSIStartTag,This directive changes the string that mod_include looks for to mark an include element to process.,0,0,others,httpd
3889,SSIUndefinedEcho,"This directive changes the string that mod_include displays when a variable is not set and ""echoed"".",0,0,others,httpd
3890,SSLCACertificateFile,"This directive sets the all-in-one file where you can assemble the Certificates of Certification Authorities (CA) whose clients you deal with. These are used for Client Authentication. Such a file is simply the concatenation of the various PEM-encoded Certificate files, in order of preference. This can be used alternatively and/or additionally to SSLCACertificatePath.",0,0,others,httpd
3891,SSLCACertificatePath,This directive sets the directory where you keep the Certificates of Certification Authorities (CAs) whose clients you deal with. These are used to verify the client certificate on Client Authentication.,0,0,others,httpd
3892,SSLCADNRequestFile,"When a client certificate is requested by mod_ssl, a list of acceptable Certificate Authority names is sent to the client in the SSL handshake. These CA names can be used by the client to select an appropriate client certificate out of those it has available.",0,0,others,httpd
3893,SSLCADNRequestPath,This optional directive can be used to specify the set of acceptable CA names which will be sent to the client when a client certificate is requested. See the SSLCADNRequestFile directive for more details.,0,0,others,httpd
3894,SSLCARevocationCheck,"Enables certificate revocation list (CRL) checking. At least one of SSLCARevocationFile or SSLCARevocationPath must be configured. When set to chain (recommended setting), CRL checks are applied to all certificates in the chain, while setting it to leaf limits the checks to the end-entity cert.",0,0,others,httpd
3895,SSLCARevocationFile,"This directive sets the all-in-one file where you can assemble the Certificate Revocation Lists (CRL) of Certification Authorities (CA) whose clients you deal with. These are used for Client Authentication. Such a file is simply the concatenation of the various PEM-encoded CRL files, in order of preference. This can be used alternatively and/or additionally to SSLCARevocationPath.",0,0,others,httpd
3896,SSLCARevocationPath,This directive sets the directory where you keep the Certificate Revocation Lists (CRL) of Certification Authorities (CAs) whose clients you deal with. These are used to revoke the client certificate on Client Authentication.,0,0,others,httpd
3897,SSLCertificateChainFile,"SSLCertificateChainFile became obsolete with version 2.4.8, when SSLCertificateFile was extended to also load intermediate CA certificates from the server certificate file.",0,0,others,httpd
3898,SSLCertificateFile,"This directive points to a file with certificate data in PEM format, or the certificate identifier through a configured cryptographic token. If using a PEM file, at minimum, the file must include an end-entity (leaf) certificate. The directive can be used multiple times (referencing different filenames) to support multiple algorithms for server authentication - typically RSA, DSA, and ECC. The number of supported algorithms depends on the OpenSSL version being used for mod_ssl: with version 1.0.0 or later, openssl list-public-key-algorithms will output a list of supported algorithms, see also the note below about limitations of OpenSSL versions prior to 1.0.2 and the ways to work around them.",0,0,others,httpd
3899,SSLCertificateKeyFile,"This directive points to the PEM-encoded private key file for the server, or the key ID through a configured cryptographic token. If the contained private key is encrypted, the pass phrase dialog is forced at startup time.",0,0,others,httpd
3901,SSLCompression,This directive allows to enable compression on the SSL level.,1,4,limited-side-effect,httpd
3903,SSLEngine,This directive toggles the usage of the SSL/TLS Protocol Engine. This is should be used inside a <VirtualHost> section to enable SSL/TLS for a that virtual host. By default the SSL/TLS Protocol Engine is disabled for both the main server and all configured virtual hosts.,0,0,others,httpd
3905,SSLHonorCipherOrder,"When choosing a cipher during an SSLv3 or TLSv1 handshake, normally the client's preference is used. If this directive is enabled, the server's preference will be used instead.",0,0,others,httpd
3906,SSLInsecureRenegotiation,"As originally specified, all versions of the SSL and TLS protocols (up to and including TLS/1.2) were vulnerable to a Man-in-the-Middle attack (CVE-2009-3555) during a renegotiation. This vulnerability allowed an attacker to ""prefix"" a chosen plaintext to the HTTP request as seen by the web server. A protocol extension was developed which fixed this vulnerability if supported by both client and server.",1,2,security-tradeoff,httpd
3908,SSLOCSPEnable,"This option enables OCSP validation of the client certificate chain. If this option is enabled, certificates in the client's certificate chain will be validated against an OCSP responder after normal verification (including CRL checks) have taken place. In mode 'leaf', only the client certificate itself will be validated.",1,2,security-tradeoff,httpd
3909,SSLOCSPNoverify,"Skip the OCSP responder certificates verification, mostly useful when testing an OCSP server.",1,2,security-tradeoff,httpd
3910,SSLOCSPOverrideResponder,"This option forces the configured default OCSP responder to be used during OCSP certificate validation, regardless of whether the certificate being validated references an OCSP responder.",0,0,others,httpd
3911,SSLOCSPProxyURL,This option allows to set the URL of a HTTP proxy that should be used for all queries to OCSP responders.,0,0,others,httpd
3912,SSLOCSPResponderCertificateFile,This supplies a list of trusted OCSP responder certificates to be used during OCSP responder certificate validation. The supplied certificates are implicitly trusted without any further validation. This is typically used where the OCSP responder certificate is self signed or omitted from the OCSP response.,0,0,others,httpd
3913,SSLOCSPResponderTimeout,"This option sets the timeout for queries to OCSP responders, when SSLOCSPEnable is turned on.",0,0,others,httpd
3914,SSLOCSPResponseMaxAge,"This option sets the maximum allowable age (""freshness"") for OCSP responses. The default value (-1) does not enforce a maximum age, which means that OCSP responses are considered valid as long as their nextUpdate field is in the future.",0,0,others,httpd
3917,SSLOpenSSLConfCmd,"This directive exposes OpenSSL's SSL_CONF API to mod_ssl, allowing a flexible configuration of OpenSSL parameters without the need of implementing additional mod_ssl directives when new features are added to OpenSSL.",0,0,others,httpd
3919,SSLPassPhraseDialog,"When Apache starts up it has to read the various Certificate (see SSLCertificateFile) and Private Key (see SSLCertificateKeyFile) files of the SSL-enabled virtual servers. Because for security reasons the Private Key files are usually encrypted, mod_ssl needs to query the administrator for a Pass Phrase in order to decrypt those files. This query can be done in two ways which can be configured by type:",0,0,others,httpd
3921,SSLProxyCACertificateFile,"This directive sets the all-in-one file where you can assemble the Certificates of Certification Authorities (CA) whose remote servers you deal with. These are used for Remote Server Authentication. Such a file is simply the concatenation of the various PEM-encoded Certificate files, in order of preference. This can be used alternatively and/or additionally to SSLProxyCACertificatePath.",0,0,others,httpd
3922,SSLProxyCACertificatePath,This directive sets the directory where you keep the Certificates of Certification Authorities (CAs) whose remote servers you deal with. These are used to verify the remote server certificate on Remote Server Authentication.,0,0,others,httpd
3924,SSLProxyCARevocationFile,"This directive sets the all-in-one file where you can assemble the Certificate Revocation Lists (CRL) of Certification Authorities (CA) whose remote servers you deal with. These are used for Remote Server Authentication. Such a file is simply the concatenation of the various PEM-encoded CRL files, in order of preference. This can be used alternatively and/or additionally to SSLProxyCARevocationPath.",0,0,others,httpd
3925,SSLProxyCARevocationPath,This directive sets the directory where you keep the Certificate Revocation Lists (CRL) of Certification Authorities (CAs) whose remote servers you deal with. These are used to revoke the remote server certificate on Remote Server Authentication.,0,0,others,httpd
3926,SSLProxyCheckPeerCN,This directive sets whether the remote server certificate's CN field is compared against the hostname of the request URL. If both are not equal a 502 status code (Bad Gateway) is sent. SSLProxyCheckPeerCN is superseded by SSLProxyCheckPeerName in release 2.4.5 and later.,0,0,others,httpd
3927,SSLProxyCheckPeerExpire,This directive sets whether it is checked if the remote server certificate is expired or not. If the check fails a 502 status code (Bad Gateway) is sent.,0,0,others,httpd
3928,SSLProxyCheckPeerName,"This directive configures host name checking for server certificates when mod_ssl is acting as an SSL client. The check will succeed if the host name from the request URI matches one of the CN attribute(s) of the certificate's subject, or matches the subjectAltName extension. If the check fails, the SSL request is aborted and a 502 status code (Bad Gateway) is returned.",0,0,others,httpd
3929,SSLProxyCipherSuite,"Equivalent to SSLCipherSuite, but for the proxy connection. Please refer to SSLCipherSuite for additional information.",0,0,others,httpd
3931,SSLProxyMachineCertificateChainFile,This directive sets the all-in-one file where you keep the certificate chain for all of the client certs in use. This directive will be needed if the remote server presents a list of CA certificates that are not direct signers of one of the configured client certificates.,0,0,others,httpd
3932,SSLProxyMachineCertificateFile,This directive sets the all-in-one file where you keep the certificates and keys used for authentication of the proxy server to remote servers.,0,0,others,httpd
3933,SSLProxyMachineCertificatePath,This directive sets the directory where you keep the certificates and keys used for authentication of the proxy server to remote servers.,0,0,others,httpd
3934,SSLProxyProtocol,This directive can be used to control the SSL protocol flavors mod_ssl should use when establishing its server environment for proxy . It will only connect to servers using one of the provided protocols.,0,0,others,httpd
3935,SSLProxyVerify,"When a proxy is configured to forward requests to a remote SSL server, this directive can be used to configure certificate verification of the remote server.",1,2,security-tradeoff,httpd
3936,SSLProxyVerifyDepth,This directive sets how deeply mod_ssl should verify before deciding that the remote server does not have a valid certificate.,1,2,security-tradeoff,httpd
3937,SSLRandomSeed,This configures one or more sources for seeding the Pseudo Random Number Generator (PRNG) in OpenSSL at startup time (context is startup) and/or just before a new SSL connection is established (context is connect). This directive can only be used in the global server context because the PRNG is a global facility.,0,0,others,httpd
3938,SSLSessionCache,"This configures the storage type of the global/inter-process SSL Session Cache. This cache is an optional facility which speeds up parallel request processing. For requests to the same server process (via HTTP keep-alive), OpenSSL already caches the SSL session information locally. But because modern clients request inlined images and other data via parallel requests (usually up to four parallel requests are common) those requests are served by different pre-forked server processes. Here an inter-process cache helps to avoid unnecessary session handshakes.",1,6,function-tradeoff,httpd
3941,SSLSessionTickets,This directive allows to enable or disable the use of TLS session tickets (RFC 5077).,0,0,others,httpd
3943,SSLSRPVerifierFile,"This directive enables TLS-SRP and sets the path to the OpenSSL SRP (Secure Remote Password) verifier file containing TLS-SRP usernames, verifiers, salts, and group parameters.",0,0,others,httpd
3945,SSLStaplingErrorCacheTimeout,"Sets the timeout in seconds before invalid responses in the OCSP stapling cache (configured through SSLStaplingCache) will expire. To set the cache timeout for valid responses, see SSLStaplingStandardCacheTimeout.",1,2,security-tradeoff,httpd
3947,SSLStaplingForceURL,This directive overrides the URI of an OCSP responder as obtained from the authorityInfoAccess (AIA) extension of the certificate. One potential use is when a proxy is used for retrieving OCSP queries.,0,0,others,httpd
3948,SSLStaplingResponderTimeout,This option sets the timeout for queries to OCSP responders when SSLUseStapling is enabled and mod_ssl is querying a responder for OCSP stapling purposes.,0,0,others,httpd
3949,SSLStaplingResponseMaxAge,"This option sets the maximum allowable age (""freshness"") when considering OCSP responses for stapling purposes, i.e. when SSLUseStapling is turned on. The default value (-1) does not enforce a maximum age, which means that OCSP responses are considered valid as long as their nextUpdate field is in the future.",0,0,others,httpd
3950,SSLStaplingResponseTimeSkew,This option sets the maximum allowable time skew when mod_ssl checks the thisUpdate and nextUpdate fields of OCSP responses which get included in the TLS handshake (OCSP stapling). Only applicable if SSLUseStapling is turned on.,0,0,others,httpd
3951,SSLStaplingReturnResponderErrors,"When enabled, mod_ssl will pass responses from unsuccessful stapling related OCSP queries (such as responses with an overall status other than ""successful"", responses with a certificate status other than ""good"", expired responses etc.) on to the client. If set to off, only responses indicating a certificate status of ""good"" will be included in the TLS handshake.",0,0,others,httpd
3952,SSLStaplingStandardCacheTimeout,"Sets the timeout in seconds before responses in the OCSP stapling cache (configured through SSLStaplingCache) will expire. This directive applies to valid responses, while SSLStaplingErrorCacheTimeout is used for controlling the timeout for invalid/unavailable responses.",1,5,workload-specific,httpd
3953,SSLStrictSNIVHostCheck,"This directive sets whether a non-SNI client is allowed to access a name-based virtual host. If set to on in the default name-based virtual host, clients that are SNI unaware will not be allowed to access any virtual host, belonging to this particular IP / port combination. If set to on in any other virtual host, SNI unaware clients are not allowed to access this particular virtual host.",0,0,others,httpd
3954,SSLUserName,"This directive sets the ""user"" field in the Apache request object. This is used by lower modules to identify the user with a character string. In particular, this may cause the environment variable REMOTE_USER to be set. The varname can be any of the SSL environment variables.",0,0,others,httpd
3955,SSLUseStapling,"This option enables OCSP stapling, as defined by the ""Certificate Status Request"" TLS extension specified in RFC 6066. If enabled (and requested by the client), mod_ssl will include an OCSP response for its own certificate in the TLS handshake. Configuring an SSLStaplingCache is a prerequisite for enabling OCSP stapling.",0,0,others,httpd
3956,SSLVerifyClient,This directive sets the Certificate verification level for the Client Authentication. Notice that this directive can be used both in per-server and per-directory context. In per-server context it applies to the client authentication process used in the standard SSL handshake when a connection is established. In per-directory context it forces a SSL renegotiation with the reconfigured client verification level after the HTTP request was read but before the HTTP response is sent.,1,2,security-tradeoff,httpd
3958,StartServers,"The StartServers directive sets the number of child server processes created on startup. As the number of processes is dynamically controlled depending on the load, (see MinSpareThreads, MaxSpareThreads, MinSpareServers, MaxSpareServers) there is usually little reason to adjust this parameter.",1,1,resource,httpd
3959,StartThreads,"Number of threads created on startup. As the number of threads is dynamically controlled depending on the load, (see MinSpareThreads, MaxSpareThreads, MinSpareServers, MaxSpareServers) there is usually little reason to adjust this parameter.",1,1,resource,httpd
3960,Suexec,"When On, startup will fail if the suexec binary doesn't exist or has an invalid owner or file mode.",0,0,others,httpd
3962,ThreadLimit,"This directive sets the maximum configured value for ThreadsPerChild for the lifetime of the Apache httpd process. Any attempts to change this directive during a restart will be ignored, but ThreadsPerChild can be modified during a restart up to the value of this directive.",1,1,resource,httpd
3965,TimeOut,The TimeOut directive defines the length of time Apache httpd will wait for I/O in various circumstances:,0,0,others,httpd
3966,TraceEnable,"This directive overrides the behavior of TRACE for both the core server and mod_proxy. The default TraceEnable on permits TRACE requests per RFC 2616, which disallows any request body to accompany the request. TraceEnable off causes the core server and mod_proxy to return a 405 (Method not allowed) error to the client.",0,0,others,httpd
3967,TransferLog,"This directive has exactly the same arguments and effect as the CustomLog directive, with the exception that it does not allow the log format to be specified explicitly or for conditional logging of requests. Instead, the log format is determined by the most recently specified LogFormat directive which does not define a nickname. Common Log Format is used if no other format has been specified.",0,0,others,httpd
3968,TypesConfig,"The TypesConfig directive sets the location of the media types configuration file. File-path is relative to the ServerRoot. This file sets the default list of mappings from filename extensions to content types. Most administrators use the mime.types file provided by their OS, which associates common filename extensions with the official list of IANA registered media types maintained at http://www.iana.org/assignments/media-types/index.html as well as a large number of unofficial types. This simplifies the httpd.conf file by providing the majority of media-type definitions, and may be overridden by AddType directives as needed. You should not edit the mime.types file, because it may be replaced when you upgrade your server.",0,0,others,httpd
3970,UndefMacro,The UndefMacro directive undefines a macro which has been defined before hand.,0,0,others,httpd
3971,UnsetEnv,Removes one or more internal environment variables from those passed on to CGI scripts and SSI pages.,0,0,others,httpd
3972,Use,The Use directive controls the use of a macro. The specified macro is expanded. It must be given the same number of arguments as in the macro definition. The provided values are associated to their corresponding initial parameters and are substituted before processing.,0,0,others,httpd
3973,UseCanonicalName,"In many situations Apache httpd must construct a self-referential URL -- that is, a URL that refers back to the same server. With UseCanonicalName On Apache httpd will use the hostname and port specified in the ServerName directive to construct the canonical name for the server. This name is used in all self-referential URLs, and for the values of SERVER_NAME and SERVER_PORT in CGIs.",0,0,others,httpd
3975,User,"The User directive sets the user ID as which the server will answer requests. In order to use this directive, the server must be run initially as root. If you start the server as a non-root user, it will fail to change to the lesser privileged user, and will instead continue to run as that original user. If you do start the server as root, then it is normal for the parent process to remain running as root. Unix-userid is one of:",0,0,others,httpd
3976,UserDir,The UserDir directive sets the real directory in a user's home directory to use when a request for a document for a user is received. Directory-filename is one of the following:,0,0,others,httpd
3977,VirtualDocumentRoot,The VirtualDocumentRoot directive allows you to determine where Apache HTTP Server will find your documents based on the value of the server name. The result of expanding interpolated-directory is used as the root of the document tree in a similar manner to the DocumentRoot directive's argument. If interpolated-directory is none then VirtualDocumentRoot is turned off. This directive cannot be used in the same context as VirtualDocumentRootIP.,0,0,others,httpd
3978,VirtualDocumentRootIP,"The VirtualDocumentRootIP directive is like the VirtualDocumentRoot directive, except that it uses the IP address of the server end of the connection for directory interpolation instead of the server name.",0,0,others,httpd
3980,VirtualScriptAlias,"The VirtualScriptAlias directive allows you to determine where Apache httpd will find CGI scripts in a similar manner to VirtualDocumentRoot does for other documents. It matches requests for URIs starting /cgi-bin/, much like ScriptAlias /cgi-bin/ would.",0,0,others,httpd
3981,VirtualScriptAliasIP,"The VirtualScriptAliasIP directive is like the VirtualScriptAlias directive, except that it uses the IP address of the server end of the connection for directory interpolation instead of the server name.",0,0,others,httpd
3982,WatchdogInterval,Sets the interval at which the watchdog_step hook runs. Default is to run every second.,0,0,others,httpd
3984,xml2EncAlias,"This server-wide directive aliases one or more encoding to another encoding. This enables encodings not recognised by libxml2 to be handled internally by libxml2's encoding support using the translation table for a recognised encoding. This serves two purposes: to support character sets (or names) not recognised either by libxml2 or iconv, and to skip conversion for an encoding where it is known to be unnecessary.",0,0,others,httpd
3985,xml2EncDefault,"If you are processing data with known encoding but no encoding information, you can set this default to help mod_xml2enc process the data correctly. For example, to work with the default value of Latin1 (iso-8859-1) specified in HTTP/1.0, use:",0,0,others,httpd
3986,xml2StartParse,Specify that the markup parser should start at the first instance of any of the elements specified. This can be used as a workaround where a broken backend inserts leading junk that messes up the parser (example here).,0,0,others,httpd
3987,access_token_duration,Number of seconds for the OAuth Access Token to remain valid after being created. This is the amount of time the consumer has to interact with the service provider (which is typically keystone). Setting this option to zero means that access tokens will last forever.,0,0,others,keystone
3988,addressing_mode,Indicates the addressing mode used by the driver. Permitted values: 'legacy' - use legacy non-routable addressing 'routable' - use routable addresses 'dynamic' - use legacy addresses if the message bus does not support routing otherwise use routable addressing,0,0,others,keystone
3989,admin_bind_host,The IP address of the network interface for the admin service to listen on.,0,0,others,keystone
3990,admin_endpoint,"The base admin endpoint URL for Keystone that is advertised to clients (NOTE: this does NOT affect how Keystone listens for connections). Defaults to the base host URL of the request. For example, if keystone receives a request to http://server:35357/v3/users, then this will option will be automatically treated as http://server:35357. You should only need to set option if either the value of the base URL contains a path that keystone does not automatically infer (/prefix/v3), or if the endpoint should be found on a different host.",0,0,others,keystone
3991,admin_port,The port number for the admin service to listen on.,0,0,others,keystone
3992,admin_project_domain_name,"Name of the domain that owns the admin_project_name. If left unset, then there is no admin project. [resource] admin_project_name must also be set to use this option.",0,0,others,keystone
3993,admin_project_name,"This is a special project which represents cloud-level administrator privileges across services. Tokens scoped to this project will contain a true is_admin_project attribute to indicate to policy systems that the role assignments on that specific project should apply equally across every project. If left unset, then there is no admin project, and thus no explicit means of cross-project role assignments. [resource] admin_project_domain_name must also be set to use this option.",0,0,others,keystone
3995,alias_dereferencing,The LDAP dereferencing option to use for queries involving aliases. A value of default falls back to using default dereferencing behavior configured by your ldap.conf. A value of never prevents aliases from being dereferenced at all. A value of searching dereferences aliases only after name resolution. A value of finding dereferences aliases only during name resolution. A value of always dereferences aliases in all cases.,0,0,others,keystone
3996,allow_credentials,Indicate that the actual request can include user credentials,0,0,others,keystone
3997,allow_expired_window,This controls the number of seconds that a token can be retrieved for beyond the built-in expiry time. This allows long running operations to succeed. Defaults to two days.,0,0,others,keystone
3999,allow_methods,Indicate which methods can be used during the actual request.,0,0,others,keystone
4000,allow_redelegation,"Allows authorization to be redelegated from one user to another, effectively chaining trusts together. When disabled, the remaining_uses attribute of a trust is constrained to be zero.",1,2,security-tradeoff,keystone
4002,allowed_origin,"Indicate whether this resource may be shared with the domain received in the requests ""origin"" header. Format: ""<protocol>://<host>[:<port>]"", no trailing slash. Example: https://horizon.example.com",0,0,others,keystone
4004,amqp_durable_queues,Use durable queues in AMQP.,1,5,workload-specific,keystone
4005,anycast_address,Appended to the address prefix when sending to a group of consumers. Used by the message bus to identify messages that should be delivered in a round-robin fashion across consumers.,0,0,others,keystone
4006,application_credential,Entry point for the application_credential auth plugin module in the keystone.auth.application_credential namespace. You do not need to set this unless you are overriding keystone's own application_credential authentication plugin.,0,0,others,keystone
4007,assertion_expiration_time,"Determines the lifetime for any SAML assertions generated by keystone, using NotOnOrAfter attributes.",0,0,others,keystone
4008,assertion_prefix,Prefix to use when filtering environment variable names for federated assertions. Matched variables are passed into the federated mapping engine.,0,0,others,keystone
4009,auth_pool_connection_lifetime,"The maximum end user authentication connection lifetime to the LDAP server in seconds. When this lifetime is exceeded, the connection will be unbound and removed from the connection pool. This option has no effect unless [ldap] use_auth_pool is also enabled.",0,0,others,keystone
4010,auth_pool_size,The size of the connection pool to use for end user authentication. This option has no effect unless [ldap] use_auth_pool is also enabled.,1,1,resource,keystone
4012,backend.B,"Cache backend module. For eventlet-based or environments with hundreds of threaded servers, Memcache with pooling (oslo_cache.memcache_pool) is recommended. For environments with less than 100 threaded servers, Memcached (dogpile.cache.memcached) or Redis (dogpile.cache.redis) is recommended. Test environments with a single instance of the server can use the dogpile.cache.memory backend.",0,0,others,keystone
4015,backends,Additional backends that can perform health checks and report that information back as part of a request.,0,0,others,keystone
4016,backward_compatible_ids,"The format of user and group IDs changed in Juno for backends that do not generate UUIDs (for example, LDAP), with keystone providing a hash mapping to the underlying attribute in LDAP. By default this mapping is disabled, which ensures that existing IDs will not change. Even when the mapping is enabled by using domain-specific drivers ([identity] domain_specific_drivers_enabled), any users and groups from the default domain being handled by LDAP will still not be mapped to ensure their IDs remain backward compatible. Setting this value to false will enable the new mapping for all backends, including the default LDAP driver. It is only guaranteed to be safe to enable this option if you do not already have assignments for users and groups from the default LDAP domain, and you consider it to be acceptable for Keystone to provide the different IDs to clients than it did previously (existing IDs in the API will suddenly change). Typically this means that the only time you can set this value to false is when configuring a fresh installation, although that is the recommended value.",0,0,others,keystone
4017,broadcast_prefix,address prefix used when broadcasting to all servers,0,0,others,keystone
4018,ca_certs,Absolute path to the public certificate authority (CA) file to use when creating self-signed certificates with keystone-manage pki_setup. Set this together with [signing] ca_key. There is no reason to set this option unless you are requesting revocation lists in a non-production environment. Use a [signing] certfile issued from a trusted certificate authority instead.,0,0,others,keystone
4019,ca_key,Absolute path to the private certificate authority (CA) key file to use when creating self-signed certificates with keystone-manage pki_setup. Set this together with [signing] ca_certs. There is no reason to set this option unless you are requesting revocation lists in a non-production environment. Use a [signing] certfile issued from a trusted certificate authority instead.,0,0,others,keystone
4020,cache_on_issue.B,Enable storing issued receipt data to receipt validation cache so that first receipt validation doesn't actually cause full validation cycle. This option has no effect unless global caching and receipt caching are enabled.,1,4,limited-side-effect,keystone
4021,cache_on_issue,Enable storing issued token data to token validation cache so that first token validation doesn't actually cause full validation cycle. This option has no effect unless global caching is enabled and will still cache tokens even if [token] caching = False.,1,4,limited-side-effect,keystone
4022,cache_time.B,The number of seconds to cache receipt creation and validation data. This has no effect unless both global and [receipt] caching are enabled.,1,5,workload-specific,keystone
4023,cache_time.C,The number of seconds to cache token creation and validation data. This has no effect unless both global and [token] caching are enabled.,1,5,workload-specific,keystone
4024,cache_time.D,Time to cache access rule data in seconds. This has no effect unless global caching is enabled.,1,5,workload-specific,keystone
4025,cache_time.E,Time to cache application credential data in seconds. This has no effect unless global caching is enabled.,1,5,workload-specific,keystone
4026,cache_time.F,"Time to cache catalog data (in seconds). This has no effect unless global and catalog caching are both enabled. Catalog data (services, endpoints, etc.) typically does not change frequently, and so a longer duration than the global default may be desirable.",1,5,workload-specific,keystone
4027,cache_time.G,Time to cache identity data (in seconds). This has no effect unless global and identity caching are enabled.,1,5,workload-specific,keystone
4028,cache_time.H,Time to cache resource data in seconds. This has no effect unless global caching is enabled.,1,5,workload-specific,keystone
4029,cache_time.I,"Time to cache role data, in seconds. This has no effect unless both global caching and [role] caching are enabled.",1,5,workload-specific,keystone
4030,cache_time.J,Time to cache the revocation list and the revocation events (in seconds). This has no effect unless global and [revoke] caching are both enabled.,1,5,workload-specific,keystone
4031,cache_time.K,"Time to cache unified limit data, in seconds. This has no effect unless both global caching and [unified_limit] caching are enabled.",1,5,workload-specific,keystone
4032,cache_time,"Time-to-live (TTL, in seconds) to cache domain-specific configuration data. This has no effect unless [domain_config] caching is enabled.",1,5,workload-specific,keystone
4033,caching.B,Toggle for access rules caching. This has no effect unless global caching is enabled.,1,4,limited-side-effect,keystone
4034,caching.C,Toggle for application credential caching. This has no effect unless global caching is enabled.,1,4,limited-side-effect,keystone
4035,caching.D,Toggle for caching of the domain-specific configuration backend. This has no effect unless global caching is enabled. There is normally no reason to disable this.,1,4,limited-side-effect,keystone
4036,caching.E,"Toggle for caching receipt creation and validation data. This has no effect unless global caching is enabled, or if cache_on_issue is disabled as we only cache receipts on issue.",1,4,limited-side-effect,keystone
4037,caching.F,Toggle for caching token creation and validation data. This has no effect unless global caching is enabled.,1,4,limited-side-effect,keystone
4038,caching.G,"Toggle for catalog caching. This has no effect unless global caching is enabled. In a typical deployment, there is no reason to disable this.",1,4,limited-side-effect,keystone
4039,caching.H,Toggle for federation caching. This has no effect unless global caching is enabled. There is typically no reason to disable this.,1,4,limited-side-effect,keystone
4041,caching.J,Toggle for resource caching. This has no effect unless global caching is enabled.,1,4,limited-side-effect,keystone
4042,caching.K,Toggle for revocation event caching. This has no effect unless global caching is enabled.,1,4,limited-side-effect,keystone
4043,caching.L,"Toggle for role caching. This has no effect unless global caching is enabled. In a typical deployment, there is no reason to disable this.",1,4,limited-side-effect,keystone
4044,caching,"Toggle for unified limit caching. This has no effect unless global caching is enabled. In a typical deployment, there is no reason to disable this.",1,4,limited-side-effect,keystone
4045,cert_subject,The certificate subject to use when generating a self-signed token signing certificate. There is no reason to set this option unless you are requesting revocation lists in a non-production environment. Use a [signing] certfile issued from a trusted certificate authority instead.,0,0,others,keystone
4046,certfile.B,"Absolute path to the public certificate file to use for SAML signing. The value cannot contain a comma (,).",0,0,others,keystone
4047,certfile,"Absolute path to the public certificate file to use for signing responses to revocation lists requests. Set this together with [signing] keyfile. For non-production environments, you may be interested in using keystone-manage pki_setup to generate self-signed certificates.",0,0,others,keystone
4048,change_password_upon_first_use,"Enabling this option requires users to change their password when the user is created, or upon administrative reset. Before accessing any services, affected users will have to change their password. To ignore this requirement for specific users, such as service users, set the options attribute ignore_change_password_upon_first_use to True for the desired user via the update user API. This feature is disabled by default. This feature is only applicable with the sql backend for the [identity] driver.",0,0,others,keystone
4049,chase_referrals,"Sets keystone's referral chasing behavior across directory partitions. If left unset, the system's default behavior will be used.",0,0,others,keystone
4050,config_prefix,Prefix for building the configuration dictionary for the cache region. This should not need to be changed unless there is another dogpile.cache region with the same configuration name.,0,0,others,keystone
4053,connection,The SQLAlchemy connection string to use to connect to the database.,0,0,others,keystone
4054,connection_debug,"Verbosity of SQL debugging information: 0=None, 100=Everything.",1,6,function-tradeoff,keystone
4055,connection_parameters,Optional URL parameters to append onto the connection URL at connect time; specify as param1=value1&param2=value2&??,0,0,others,keystone
4056,connection_recycle_time,Connections which have been present in the connection pool longer than this number of seconds will be replaced with a new one the next time they are checked out from the pool.,0,0,others,keystone
4057,connection_retry_backoff,Increase the connection_retry_interval by this many seconds after each unsuccessful failover attempt.,0,0,others,keystone
4058,connection_retry_interval,Seconds to pause before attempting to re-connect.,0,0,others,keystone
4060,connection_string,Connection string for a notifier backend.,0,0,others,keystone
4061,connection_timeout,The connection timeout to use with the LDAP server. A value of -1 means that connections will never timeout.,0,0,others,keystone
4062,connection_trace,Add Python stack traces to SQL as comment strings.,0,0,others,keystone
4063,consumer_group,Group id for Kafka consumer. Consumers in one group will coordinate message consumption,0,0,others,keystone
4064,container_name,Name for the AMQP container. must be globally unique. Defaults to a generated UUID,0,0,others,keystone
4065,control_exchange,The default exchange under which topics are scoped. May be overridden by an exchange name specified in the transport_url option.,0,0,others,keystone
4067,db_max_retries,Maximum retries in case of connection error or deadlock error before error is raised. Set to -1 to specify an infinite retry count.,0,0,others,keystone
4069,db_retry_interval,Seconds between retries of a database transaction.,0,0,others,keystone
4070,dead_retry,Number of seconds memcached server is considered dead before it is tried again. This is used by the key value store system.,0,0,others,keystone
4071,debug,"If set to true, the logging level will be set to DEBUG instead of the default INFO level.",1,6,function-tradeoff,keystone
4072,debug_cache_backend,"Extra debugging from the cache backend (cache keys, get/set/delete/etc calls). This is only really useful if you need to see the specific cache-backend get/set/delete calls with the keys/values. Typically this should be left set to false.",1,6,function-tradeoff,keystone
4073,debug_level,"Sets the LDAP debugging level for LDAP calls. A value of 0 means that debugging is not enabled. This value is a bitmask, consult your LDAP documentation for possible values.",1,6,function-tradeoff,keystone
4074,debug_middleware,"If set to true, this enables the oslo debug middleware in Keystone. This Middleware prints a lot of information about the request and the response. It is useful for getting information about the data on the wire (decoded) and passed to the WSGI application pipeline. This middleware has no effect on the ""debug"" setting in the [DEFAULT] section of the config file or setting Keystone's log-level to ""DEBUG""; it is specific to debugging the WSGI data as it enters and leaves Keystone (specific request-related data). This option is used for introspection on the request and response data between the web server (apache, nginx, etc) and Keystone. This middleware is inserted as the first element in the middleware chain and will show the data closest to the wire. WARNING: NOT INTENDED FOR USE IN PRODUCTION. THIS MIDDLEWARE CAN AND WILL EMIT SENSITIVE/PRIVILEGED DATA.",1,6,function-tradeoff,keystone
4075,default_domain_id,"This references the domain to use for all Identity API v2 requests (which are not aware of domains). A domain with this ID can optionally be created for you by keystone-manage bootstrap. The domain referenced by this ID cannot be deleted on the v3 API, to prevent accidentally breaking the v2 API. There is nothing special about this domain, other than the fact that it must exist to order to maintain support for your v2 clients. There is typically no reason to change this value.",0,0,others,keystone
4077,default_notification_exchange,Exchange name used in notification addresses. Exchange name resolution precedence: Target.exchange if set else default_notification_exchange if set else control_exchange if set else 'notify',0,0,others,keystone
4079,default_publisher_id,"Default publisher_id for outgoing notifications. If left undefined, Keystone will default to using the server's host name.",0,0,others,keystone
4080,default_reply_retry,The maximum number of attempts to re-send a reply message which failed due to a recoverable error.,0,0,others,keystone
4082,default_rpc_exchange,Exchange name used in RPC addresses. Exchange name resolution precedence: Target.exchange if set else default_rpc_exchange if set else control_exchange if set else 'rpc',0,0,others,keystone
4083,default_send_timeout,The deadline for an rpc cast or call message delivery. Only used when caller does not provide a timeout expiry.,0,0,others,keystone
4084,default_sender_link_timeout,The duration to schedule a purge of idle sender links. Detach link after expiry.,0,0,others,keystone
4085,detailed,Show more detailed information as part of the response. Security note: Enabling this option may expose sensitive details about the service being monitored. Be sure to verify that it will not violate your security policies.,1,2,security-tradeoff,keystone
4086,disable_by_file_path,Check the presence of a file to determine if an application is running on a port. Used by DisableByFileHealthcheck plugin.,0,0,others,keystone
4087,disable_by_file_paths,"Check the presence of a file based on a port to determine if an application is running on a port. Expects a ""port:path"" list of strings. Used by DisableByFilesPortsHealthcheck plugin.",0,0,others,keystone
4088,disable_user_account_days_inactive,"The maximum number of days a user can go without authenticating before being considered ""inactive"" and automatically disabled (locked). This feature is disabled by default; set any value to enable it. This feature depends on the sql backend for the [identity] driver. When a user exceeds this threshold and is considered ""inactive"", the user's enabled attribute in the HTTP API may not match the value of the user's enabled column in the user table.",0,0,others,keystone
4089,domain_config_dir,Absolute path where keystone should locate domain-specific [identity] configuration files. This option has no effect unless [identity] domain_specific_drivers_enabled is set to true. There is typically no reason to change this value.,0,0,others,keystone
4090,domain_configurations_from_database,"By default, domain-specific configuration data is read from files in the directory identified by [identity] domain_config_dir. Enabling this configuration option allows you to instead manage domain-specific configurations through the API, which are then persisted in the backend (typically, a SQL database), rather than using configuration files on disk.",0,0,others,keystone
4091,domain_name_url_safe,"This controls whether the names of domains are restricted from containing URL-reserved characters. If set to new, attempts to create or update a domain with a URL-unsafe name will fail. If set to strict, attempts to scope a token with a URL-unsafe domain name will fail, thereby forcing all domain names to be updated to be URL-safe.",0,0,others,keystone
4092,domain_specific_drivers_enabled,"A subset (or all) of domains can have their own identity driver, each with their own partial configuration options, stored in either the resource backend or in a file in a domain configuration directory (depending on the setting of [identity] domain_configurations_from_database). Only values specific to the domain need to be specified in this manner. This feature is disabled by default, but may be enabled by default in a future release; set to true to enable.",0,0,others,keystone
4093,driver.B,"The Drivers(s) to handle sending notifications. Possible values are messaging, messagingv2, routing, log, test, noop",0,0,others,keystone
4094,driver.C,"Entry point for the access rules config backend driver in the keystone.access_rules_config namespace. Keystone only provides a json driver, so there is no reason to change this unless you are providing a custom entry point.",0,0,others,keystone
4095,driver.D,"Entry point for the application credential backend driver in the keystone.application_credential namespace. Keystone only provides a sql driver, so there is no reason to change this unless you are providing a custom entry point.",0,0,others,keystone
4096,driver.E,"Entry point for the assignment backend driver (where role assignments are stored) in the keystone.assignment namespace. Only a SQL driver is supplied by keystone itself. Unless you are writing proprietary drivers for keystone, you do not need to set this option.",0,0,others,keystone
4097,driver.F,"Entry point for the catalog driver in the keystone.catalog namespace. Keystone provides a sql option (which supports basic CRUD operations through SQL), a templated option (which loads the catalog from a templated catalog file on disk), and a endpoint_filter.sql option (which supports arbitrary service catalogs per project).",0,0,others,keystone
4098,driver.G,"Entry point for the credential backend driver in the keystone.credential namespace. Keystone only provides a sql driver, so there's no reason to change this unless you are providing a custom entry point.",0,0,others,keystone
4099,driver.H,"Entry point for the domain-specific configuration driver in the keystone.resource.domain_config namespace. Only a sql option is provided by keystone, so there is no reason to set this unless you are providing a custom entry point.",0,0,others,keystone
4100,driver.I,"Entry point for the endpoint filter driver in the keystone.endpoint_filter namespace. Only a sql option is provided by keystone, so there is no reason to set this unless you are providing a custom entry point.",0,0,others,keystone
4101,driver.J,"Entry point for the endpoint policy driver in the keystone.endpoint_policy namespace. Only a sql driver is provided by keystone, so there is no reason to set this unless you are providing a custom entry point.",0,0,others,keystone
4102,driver.K,"Entry point for the federation backend driver in the keystone.federation namespace. Keystone only provides a sql driver, so there is no reason to set this option unless you are providing a custom entry point.",0,0,others,keystone
4103,driver.L,"Entry point for the identity backend driver in the keystone.identity namespace. Keystone provides a sql and ldap driver. This option is also used as the default driver selection (along with the other configuration variables in this section) in the event that [identity] domain_specific_drivers_enabled is enabled, but no applicable domain-specific configuration is defined for the domain in question. Unless your deployment primarily relies on ldap AND is not using domain-specific configuration, you should typically leave this set to sql.",0,0,others,keystone
4104,driver.M,"Entry point for the identity mapping backend driver in the keystone.identity.id_mapping namespace. Keystone only provides a sql driver, so there is no reason to change this unless you are providing a custom entry point.",0,0,others,keystone
4105,driver.N,"Entry point for the OAuth backend driver in the keystone.oauth1 namespace. Typically, there is no reason to set this option unless you are providing a custom entry point.",0,0,others,keystone
4106,driver.O,"Entry point for the policy backend driver in the keystone.policy namespace. Supplied drivers are rules (which does not support any CRUD operations for the v3 policy API) and sql. Typically, there is no reason to set this option unless you are providing a custom entry point.",0,0,others,keystone
4107,driver.P,"Entry point for the resource driver in the keystone.resource namespace. Only a sql driver is supplied by keystone. Unless you are writing proprietary drivers for keystone, you do not need to set this option.",0,0,others,keystone
4108,driver.Q,"Entry point for the role backend driver in the keystone.role namespace. Keystone only provides a sql driver, so there's no reason to change this unless you are providing a custom entry point.",0,0,others,keystone
4109,driver.R,"Entry point for the shadow users backend driver in the keystone.identity.shadow_users namespace. This driver is used for persisting local user references to externally-managed identities (via federation, LDAP, etc). Keystone only provides a sql driver, so there is no reason to change this option unless you are providing a custom entry point.",0,0,others,keystone
4110,driver.S,"Entry point for the token revocation backend driver in the keystone.revoke namespace. Keystone only provides a sql driver, so there is no reason to set this option unless you are providing a custom entry point.",0,0,others,keystone
4111,driver.T,"Entry point for the trust backend driver in the keystone.trust namespace. Keystone only provides a sql driver, so there is no reason to change this unless you are providing a custom entry point.",0,0,others,keystone
4112,driver,"Entry point for the unified limit backend driver in the keystone.unified_limit namespace. Keystone only provides a sql driver, so there's no reason to change this unless you are providing a custom entry point.",0,0,others,keystone
4113,enable_auto_commit,Enable asynchronous consumer commits,1,4,limited-side-effect,keystone
4114,enable_proxy_headers_parsing,Whether the application is behind a proxy or not. This determines if the middleware should parse the headers or not.,0,0,others,keystone
4115,enabled.B,Enable the profiling for all services on this node.,1,6,function-tradeoff,keystone
4116,enabled,Global toggle for caching.,1,4,limited-side-effect,keystone
4118,enforcement_model,"The enforcement model to use when validating limits associated to projects. Enforcement models will behave differently depending on the existing limits, which may result in backwards incompatible changes if a model is switched in a running deployment.",0,0,others,keystone
4119,es_doc_type,Document type for notification indexing in elasticsearch.,0,0,others,keystone
4121,es_scroll_time,"This parameter is a time value parameter (for example: es_scroll_time=2m), indicating for how long the nodes that participate in the search will maintain relevant resources in order to continue and support it.",0,0,others,keystone
4122,executor_thread_pool_size,Size of executor thread pool when executor is threading or eventlet.,1,1,resource,keystone
4123,expiration.B,"The amount of time that a receipt should remain valid (in seconds). This value should always be very short, as it represents how long a user has to reattempt auth with the missing auth methods.",0,0,others,keystone
4124,expiration,"The amount of time that a token should remain valid (in seconds). Drastically reducing this value may break ""long-running"" operations that involve multiple services to coordinate together, and will force users to authenticate with keystone more frequently. Drastically increasing this value will increase the number of tokens that will be simultaneously valid. Keystone tokens are also bearer tokens, so a shorter duration will also reduce the potential security impact of a compromised token.",0,0,others,keystone
4126,expiration_time,"Default TTL, in seconds, for any cached item in the dogpile.cache region. This applies to any cached method that doesn't have an explicit cache expiration time defined for it.",0,0,others,keystone
4127,expose_headers,Indicate which headers are safe to expose to the API. Defaults to HTTP Simple Headers.,0,0,others,keystone
4128,external,"Entry point for the external (REMOTE_USER) auth plugin module in the keystone.auth.external namespace. Supplied drivers are DefaultDomain and Domain. The default driver is DefaultDomain, which assumes that all users identified by the username specified to keystone in the REMOTE_USER variable exist within the context of the default domain. The Domain option expects an additional environment variable be presented to keystone, REMOTE_DOMAIN, containing the domain name of the REMOTE_USER (if REMOTE_DOMAIN is not set, then the default domain will be used instead). You do not need to set this unless you are taking advantage of ""external authentication"", where the application server (such as Apache) is handling authentication instead of keystone.",0,0,others,keystone
4129,fatal_deprecations,Enables or disables fatal status of deprecations.,0,0,others,keystone
4130,federated_domain_name,An arbitrary domain name that is reserved to allow federated ephemeral users to have a domain concept. Note that an admin will not be able to create a domain with this name or update an existing domain to this name. You are not advised to change this value unless you really have to.,0,0,others,keystone
4131,filter_error_trace,Enable filter traces that contain error/exception to a separated place.,0,0,others,keystone
4132,generator,"Entry point for the public ID generator for user and group entities in the keystone.identity.id_generator namespace. The Keystone identity mapper only supports generators that produce 64 bytes or less. Keystone only provides a sha256 entry point, so there is no reason to change this value unless you're providing a custom entry point.",0,0,others,keystone
4133,group_ad_nesting,"If enabled, group queries will use Active Directory specific filters for nested groups.",0,0,others,keystone
4134,group_additional_attribute_mapping,"A list of LDAP attribute to keystone group attribute pairs used for mapping additional attributes to groups in keystone. The expected format is <ldap_attr>:<group_attr>, where ldap_attr is the attribute in the LDAP object and group_attr is the attribute which should appear in the identity API.",0,0,others,keystone
4136,group_desc_attribute,The LDAP attribute mapped to group descriptions in keystone.,0,0,others,keystone
4137,group_filter,The LDAP search filter to use for groups.,0,0,others,keystone
4138,group_id_attribute,The LDAP attribute mapped to group IDs in keystone. This must NOT be a multivalued attribute. Group IDs are expected to be globally unique across keystone domains and URL-safe.,0,0,others,keystone
4139,group_member_attribute,The LDAP attribute used to indicate that a user is a member of the group.,0,0,others,keystone
4140,group_members_are_ids,Enable this option if the members of the group object class are keystone user IDs rather than LDAP DNs. This is the case when using posixGroup as the group object class in Open Directory.,0,0,others,keystone
4143,group_request_prefix,address prefix when sending to any server in group,0,0,others,keystone
4144,group_tree_dn,The search base to use for groups. Defaults to the [ldap] suffix value.,0,0,others,keystone
4145,heartbeat_rate,How often times during the heartbeat_timeout_threshold we check the heartbeat.,0,0,others,keystone
4146,heartbeat_timeout_threshold,Number of seconds after which the Rabbit broker is considered down if heartbeat's keep-alive fails (0 disable the heartbeat). EXPERIMENTAL,0,0,others,keystone
4147,hmac_keys,Secret key(s) to use for encrypting context data for performance profiling.,0,0,others,keystone
4148,idle_timeout,Timeout for inactive connections (in seconds),0,0,others,keystone
4149,idp_contact_company,This is the company name of the identity provider's contact person.,0,0,others,keystone
4150,idp_contact_email,This is the email address of the identity provider's contact person.,0,0,others,keystone
4151,idp_contact_name,This is the given name of the identity provider's contact person.,0,0,others,keystone
4152,idp_contact_surname,This is the surname of the identity provider's contact person.,0,0,others,keystone
4153,idp_contact_telephone,This is the telephone number of the identity provider's contact person.,0,0,others,keystone
4154,idp_contact_type,This is the type of contact that best describes the identity provider's contact person.,0,0,others,keystone
4155,idp_entity_id,This is the unique entity identifier of the identity provider (keystone) to use when generating SAML assertions. This value is required to generate identity provider metadata and must be a URI (a URL is recommended). For example: https://keystone.example.com/v3/OS-FEDERATION/saml2/idp.,0,0,others,keystone
4156,idp_lang,This is the language used by the identity provider's organization.,0,0,others,keystone
4157,idp_metadata_path,Absolute path to the identity provider metadata file. This file should be generated with the keystone-manage saml_idp_metadata command. There is typically no reason to change this value.,0,0,others,keystone
4158,idp_organization_display_name,This is the name of the identity provider's organization to be displayed.,0,0,others,keystone
4159,idp_organization_name,This is the name of the identity provider's organization.,0,0,others,keystone
4160,idp_organization_url,This is the URL of the identity provider's organization. The URL referenced here should be useful to humans.,0,0,others,keystone
4161,idp_sso_endpoint,This is the single sign-on (SSO) service location of the identity provider which accepts HTTP POST requests. A value is required to generate identity provider metadata. For example: https://keystone.example.com/v3/OS-FEDERATION/saml2/sso.,0,0,others,keystone
4162,infer_roles,"This controls whether roles should be included with tokens that are not directly assigned to the token's scope, but are instead linked implicitly to other role assignments.",0,0,others,keystone
4163,insecure_debug,"If set to true, then the server will return information in HTTP responses that may allow an unauthenticated or authenticated user to get more information than normal, such as additional details about why authentication failed. This may be useful for debugging but is insecure.",1,2,security-tradeoff,keystone
4164,instance_format,The format for an instance that is passed with the log message.,0,0,others,keystone
4165,instance_uuid_format,The format for an instance UUID that is passed with the log message.,0,0,others,keystone
4166,issuer_attribute,"The name of the WSGI environment variable used to pass the issuer of the client certificate to keystone. This attribute is used as an identity provider ID for the X.509 tokenless authorization along with the protocol to look up its corresponding mapping. In a typical deployment, there is no reason to change this value.",0,0,others,keystone
4167,jws_private_key_repository,"Directory containing private keys for signing JWS tokens. This directory must exist in order for keystone's server process to start. It must also be readable by keystone's server process. It must contain at least one private key that corresponds to a public key in keystone.conf [jwt_tokens] jws_public_key_repository. In the event there are multiple private keys in this directory, keystone will use a key named private.pem to sign tokens. In the future, keystone may support the ability to sign tokens with multiple private keys. For now, only a key named private.pem within this directory is required to issue JWS tokens. This option is only applicable in deployments issuing JWS tokens and setting keystone.conf [tokens] provider = jws.",0,0,others,keystone
4168,jws_public_key_repository,Directory containing public keys for validating JWS token signatures. This directory must exist in order for keystone's server process to start. It must also be readable by keystone's server process. It must contain at least one public key that corresponds to a private key in keystone.conf [jwt_tokens] jws_private_key_repository. This option is only applicable in deployments issuing JWS tokens and setting keystone.conf [tokens] provider = jws.,0,0,others,keystone
4170,kafka_max_fetch_bytes,Max fetch bytes of Kafka consumer,0,0,others,keystone
4171,key_repository.B,Directory containing Fernet keys used to encrypt and decrypt credentials stored in the credential backend. Fernet keys used to encrypt credentials have no relationship to Fernet keys used to encrypt Fernet tokens. Both sets of keys should be managed separately and require different rotation policies. Do not share this repository with the repository used to manage keys for Fernet tokens.,0,0,others,keystone
4172,key_repository.C,"Directory containing Fernet receipt keys. This directory must exist before using keystone-manage fernet_setup for the first time, must be writable by the user running keystone-manage fernet_setup or keystone-manage fernet_rotate, and of course must be readable by keystone's server process. The repository may contain keys in one of three states: a single staged key (always index 0) used for receipt validation, a single primary key (always the highest index) used for receipt creation and validation, and any number of secondary keys (all other index values) used for receipt validation. With multiple keystone nodes, each node must share the same key repository contents, with the exception of the staged key (index 0). It is safe to run keystone-manage fernet_rotate once on any one node to promote a staged key (index 0) to be the new primary (incremented from the previous highest index), and produce a new staged key (a new key with index 0); the resulting repository can then be atomically replicated to other nodes without any risk of race conditions (for example, it is safe to run keystone-manage fernet_rotate on host A, wait any amount of time, create a tarball of the directory on host A, unpack it on host B to a temporary location, and atomically move (mv) the directory into place on host B). Running keystone-manage fernet_rotate twice on a key repository without syncing other nodes will result in receipts that can not be validated by all nodes.",0,0,others,keystone
4173,key_repository,"Directory containing Fernet token keys. This directory must exist before using keystone-manage fernet_setup for the first time, must be writable by the user running keystone-manage fernet_setup or keystone-manage fernet_rotate, and of course must be readable by keystone's server process. The repository may contain keys in one of three states: a single staged key (always index 0) used for token validation, a single primary key (always the highest index) used for token creation and validation, and any number of secondary keys (all other index values) used for token validation. With multiple keystone nodes, each node must share the same key repository contents, with the exception of the staged key (index 0). It is safe to run keystone-manage fernet_rotate once on any one node to promote a staged key (index 0) to be the new primary (incremented from the previous highest index), and produce a new staged key (a new key with index 0); the resulting repository can then be atomically replicated to other nodes without any risk of race conditions (for example, it is safe to run keystone-manage fernet_rotate on host A, wait any amount of time, create a tarball of the directory on host A, unpack it on host B to a temporary location, and atomically move (mv) the directory into place on host B). Running keystone-manage fernet_rotate twice on a key repository without syncing other nodes will result in tokens that can not be validated by all nodes.",0,0,others,keystone
4174,key_size,Key size (in bits) to use when generating a self-signed token signing certificate. There is no reason to set this option unless you are requesting revocation lists in a non-production environment. Use a [signing] certfile issued from a trusted certificate authority instead.,0,0,others,keystone
4175,keyfile.B,"Absolute path to the private key file to use for SAML signing. The value cannot contain a comma (,).",0,0,others,keystone
4176,keyfile,Absolute path to the private key file to use for signing responses to revocation lists requests. Set this together with [signing] certfile.,0,0,others,keystone
4177,kombu_compression,"EXPERIMENTAL: Possible values are: gzip, bz2. If not set compression will not be used. This option may not be available in future versions.",1,6,function-tradeoff,keystone
4178,kombu_failover_strategy,Determines how the next RabbitMQ node is chosen in case the one we are currently connected to becomes unavailable. Takes effect only if more than one RabbitMQ node is provided in config.,0,0,others,keystone
4179,kombu_missing_consumer_retry_timeout,How long to wait a missing client before abandoning to send it its replies. This value should not be longer than rpc_response_timeout.,0,0,others,keystone
4180,kombu_reconnect_delay,How long to wait before reconnecting in response to an AMQP consumer cancel notification.,0,0,others,keystone
4181,link_retry_delay,Time to pause between re-connecting an AMQP 1.0 link that failed due to a recoverable error.,0,0,others,keystone
4182,list_limit.B,"Maximum number of entities that will be returned in a catalog collection. There is typically no reason to set this, as it would be unusual for a deployment to have enough services or endpoints to exceed a reasonable limit.",0,0,others,keystone
4183,list_limit.C,Maximum number of entities that will be returned in a policy collection.,0,0,others,keystone
4184,list_limit.D,Maximum number of entities that will be returned in a resource collection.,0,0,others,keystone
4185,list_limit.E,Maximum number of entities that will be returned in a role collection. This may be useful to tune if you have a large number of discrete roles in your deployment.,0,0,others,keystone
4186,list_limit.F,Maximum number of entities that will be returned in a role collection. This may be useful to tune if you have a large number of unified limits in your deployment.,0,0,others,keystone
4187,list_limit.G,Maximum number of entities that will be returned in an identity collection.,0,0,others,keystone
4188,list_limit,"The maximum number of entities that will be returned in a collection. This global limit may be then overridden for a specific driver, by specifying a list_limit in the appropriate section (for example, [assignment]). No limit is set by default. In larger deployments, it is recommended that you set this to a reasonable number to prevent operations like listing all users and projects from placing an unnecessary load on the system.",0,0,others,keystone
4190,lockout_failure_attempts,"The maximum number of times that a user can fail to authenticate before the user account is locked for the number of seconds specified by [security_compliance] lockout_duration. This feature is disabled by default. If this feature is enabled and [security_compliance] lockout_duration is not set, then users may be locked out indefinitely until the user is explicitly enabled via the API. This feature depends on the sql backend for the [identity] driver.",0,0,others,keystone
4191,log_config_append,"The name of a logging configuration file. This file is appended to any existing logging configuration files. For details about logging configuration files, see the Python logging module documentation. Note that when logging configuration files are used then all logging configuration is set in the configuration file and other logging configuration options are ignored (for example, log-date-format).",0,0,others,keystone
4192,log_date_format,Defines the format string for %(asctime)s in log records. Default: the value above . This option is ignored if log_config_append is set.,0,0,others,keystone
4193,log_dir,(Optional) The base directory used for relative log_file paths. This option is ignored if log_config_append is set.,0,0,others,keystone
4194,log_file,"(Optional) Name of log file to send logging output to. If no default is set, logging will go to stderr as defined by use_stderr. This option is ignored if log_config_append is set.",0,0,others,keystone
4195,log_rotate_interval,"The amount of time before the log files are rotated. This option is ignored unless log_rotation_type is setto ""interval"".",0,0,others,keystone
4196,log_rotate_interval_type,Rotation interval type. The time of the last file change (or the time when the service was started) is used when scheduling the next rotation.,0,0,others,keystone
4197,log_rotation_type,Log rotation type.,0,0,others,keystone
4198,logging_context_format_string,Format string to use for log messages with context. Used by oslo_log.formatters.ContextFormatter,0,0,others,keystone
4199,logging_debug_format_suffix,Additional data to append to log message when logging level for the message is DEBUG. Used by oslo_log.formatters.ContextFormatter,0,0,others,keystone
4200,logging_default_format_string,Format string to use for log messages when context is undefined. Used by oslo_log.formatters.ContextFormatter,0,0,others,keystone
4201,logging_exception_prefix,Prefix each line of exception output with this format. Used by oslo_log.formatters.ContextFormatter,0,0,others,keystone
4202,logging_user_identity_format,Defines the format string for %(user_identity)s that is used in logging_context_format_string. Used by oslo_log.formatters.ContextFormatter,0,0,others,keystone
4203,mapped,Entry point for the mapped auth plugin module in the keystone.auth.mapped namespace. You do not need to set this unless you are overriding keystone's own mapped authentication plugin.,0,0,others,keystone
4204,max_active_keys,"This controls how many keys are held in rotation by keystone-manage fernet_rotate before they are discarded. The default value of 3 means that keystone will maintain one staged key (always index 0), one primary key (the highest numerical index), and one secondary key (every other index). Increasing this value means that additional secondary keys will be kept in the rotation.",0,0,others,keystone
4205,max_age,Maximum cache age of CORS preflight requests.,0,0,others,keystone
4210,max_password_length,Maximum allowed length for user passwords. Decrease this value to improve performance. Changing this value does not effect existing passwords.,1,2,security-tradeoff,keystone
4211,max_poll_records,The maximum number of records returned in a poll call,0,0,others,keystone
4212,max_pool_size,Maximum number of SQL connections to keep open in a pool. Setting a value of 0 indicates no limit.,1,1,resource,keystone
4213,max_project_tree_depth,"Maximum depth of the project hierarchy, excluding the project acting as a domain at the top of the hierarchy. WARNING: Setting it to a large value may adversely impact performance.",1,3,reliability-tradeoff,keystone
4214,max_redelegation_count,Maximum number of times that authorization can be redelegated from one user to another in a chain of trusts. This number may be reduced further for a specific trust.,0,0,others,keystone
4215,max_request_body_size,"The maximum body size for each request, in bytes.",0,0,others,keystone
4216,max_retries,Maximum number of database connection retries during startup. Set to -1 to specify an infinite retry count.,0,0,others,keystone
4217,max_token_size,"Similar to [DEFAULT] max_param_size, but provides an exception for token values. With Fernet tokens, this can be set as low as 255. With UUID tokens, this should be set to 32).",0,0,others,keystone
4218,memcache_dead_retry,Number of seconds memcached server is considered dead before it is tried again. (dogpile.cache.memcache and oslo_cache.memcache_pool backends only).,0,0,others,keystone
4219,memcache_pool_connection_get_timeout,Number of seconds that an operation will wait to get a memcache client connection.,0,0,others,keystone
4220,memcache_pool_maxsize,Max total number of open connections to every memcached server. (oslo_cache.memcache_pool backend only).,1,1,resource,keystone
4221,memcache_pool_unused_timeout,Number of seconds a connection to memcached is held unused in the pool before it is closed. (oslo_cache.memcache_pool backend only).,0,0,others,keystone
4223,memcache_socket_timeout,Timeout in seconds for every call to a server. (dogpile.cache.memcache and oslo_cache.memcache_pool backends only).,0,0,others,keystone
4224,methods,"Allowed authentication methods. Note: You should disable the external auth method if you are currently using federation. External auth and federation both use the REMOTE_USER variable. Since both the mapped and external plugin are being invoked to validate attributes in the request environment, it can cause conflicts.",0,0,others,keystone
4225,min_pool_size,Minimum number of SQL connections to keep open in a pool.,0,0,others,keystone
4226,minimum_password_age,"The number of days that a password must be used before the user can change it. This prevents users from changing their passwords immediately in order to wipe out their password history and reuse an old password. This feature does not prevent administrators from manually resetting passwords. It is disabled by default and allows for immediate password changes. This feature depends on the sql backend for the [identity] driver. Note: If [security_compliance] password_expires_days is set, then the value for this option should be less than the password_expires_days.",0,0,others,keystone
4227,multicast_address,Appended to the address prefix when sending a fanout message. Used by the message bus to identify fanout messages.,0,0,others,keystone
4228,mysql_enable_ndb,"If True, transparently enables support for handling MySQL Cluster (NDB).",0,0,others,keystone
4229,mysql_sql_mode,"The SQL mode to be used for MySQL sessions. This option, including the default, overrides any server-set SQL mode. To use whatever SQL mode is set by the server configuration, set this to no value. Example: mysql_sql_mode=",0,0,others,keystone
4230,notification_format,"Define the notification format for identity service events. A basic notification only has information about the resource being operated on. A cadf notification has the same information, as well as information about the initiator of the event. The cadf option is entirely backwards compatible with the basic option, but is fully CADF-compliant, and is recommended for auditing use cases.",0,0,others,keystone
4231,notification_opt_out,"You can reduce the number of notifications keystone emits by explicitly opting out. Keystone will not emit notifications that match the patterns expressed in this list. Values are expected to be in the form of identity.<resource_type>.<operation>. By default, all notifications related to authentication are automatically suppressed. This field can be set multiple times in order to opt-out of multiple notification topics. For example, the following suppresses notifications describing user creation or successful authentication events: notification_opt_out=identity.user.create notification_opt_out=identity.authenticate.success",0,0,others,keystone
4232,notify_address_prefix,Address prefix for all generated Notification addresses,0,0,others,keystone
4233,notify_server_credit,Window size for incoming Notification messages,1,1,resource,keystone
4234,oauth1,Entry point for the OAuth 1.0a auth plugin module in the keystone.auth.oauth1 namespace. You do not need to set this unless you are overriding keystone's own oauth1 authentication plugin.,0,0,others,keystone
4235,page_size,Defines the maximum number of results per page that keystone should request from the LDAP server when listing objects. A value of zero (0) disables paging.,0,0,others,keystone
4236,password.B,Entry point for the password auth plugin module in the keystone.auth.password namespace. You do not need to set this unless you are overriding keystone's own password authentication plugin.,0,0,others,keystone
4237,password,"The password of the administrator bind DN to use when querying the LDAP server, if your LDAP server requires it.",0,0,others,keystone
4240,password_hash_rounds,"This option represents a trade off between security and performance. Higher values lead to slower performance, but higher security. Changing this option will only affect newly created passwords as existing password hashes already have a fixed number of rounds applied, so it is safe to tune this option in a running cluster. The default for bcrypt is 12, must be between 4 and 31, inclusive. The default for scrypt is 16, must be within range(1,32). The default for pbkdf_sha512 is 60000, must be within range(1,1<<32) WARNING: If using scrypt, increasing this value increases BOTH time AND memory requirements to hash a password.",1,2,security-tradeoff,keystone
4241,password_regex,"The regular expression used to validate password strength requirements. By default, the regular expression will match any password. The following is an example of a pattern which requires at least 1 letter, 1 digit, and have a minimum length of 7 characters: ^(?=.*d)(?=.*[a-zA-Z]).{7,}$ This feature depends on the sql backend for the [identity] driver.",0,0,others,keystone
4242,password_regex_description,"Describe your password regular expression here in language for humans. If a password fails to match the regular expression, the contents of this configuration variable will be returned to users to explain why their requested password was insufficient.",0,0,others,keystone
4243,path,The path to respond to healtcheck requests on.,0,0,others,keystone
4245,policy_default_rule,Default rule. Enforced when a requested rule is not found.,0,0,others,keystone
4246,policy_dirs,"Directories where policy configuration files are stored. They can be relative to any directory in the search path defined by the config_dir option, or absolute paths. The file defined by policy_file must exist for these directories to be searched. Missing or empty directories are ignored.",0,0,others,keystone
4247,policy_file,The file that defines policies.,0,0,others,keystone
4248,pool_connection_get_timeout,Number of seconds that an operation will wait to get a memcache client connection. This is used by the key value store system.,0,0,others,keystone
4249,pool_connection_lifetime,"The maximum connection lifetime to the LDAP server in seconds. When this lifetime is exceeded, the connection will be unbound and removed from the connection pool. This option has no effect unless [ldap] use_pool is also enabled.",0,0,others,keystone
4251,pool_maxsize,Max total number of open connections to every memcached server. This is used by the key value store system.,1,1,resource,keystone
4252,pool_retry_delay,The number of seconds to wait before attempting to reconnect to the LDAP server. This option has no effect unless [ldap] use_pool is also enabled.,0,0,others,keystone
4253,pool_retry_max,The maximum number of times to attempt reconnecting to the LDAP server before aborting. A value of zero prevents retries. This option has no effect unless [ldap] use_pool is also enabled.,0,0,others,keystone
4254,pool_size.B,Pool Size for Kafka Consumers,1,1,resource,keystone
4255,pool_size,The size of the LDAP connection pool. This option has no effect unless [ldap] use_pool is also enabled.,1,1,resource,keystone
4256,pool_timeout,"If set, use this value for pool_timeout with SQLAlchemy.",0,0,others,keystone
4257,pool_unused_timeout,Number of seconds a connection to memcached is held unused in the pool before it is closed. This is used by the key value store system.,0,0,others,keystone
4258,pre_settled,Send messages of this type pre-settled. Pre-settled messages will not receive acknowledgement from the peer. Note well: pre-settled messages may be silently discarded if the delivery fails. Permitted values: 'rpc-call' - send RPC Calls pre-settled 'rpc-reply'- send RPC Replies pre-settled 'rpc-cast' - Send RPC Casts pre-settled 'notify' - Send Notifications pre-settled,0,0,others,keystone
4259,producer_batch_size,Size of batch for the producer async send,0,0,others,keystone
4260,producer_batch_timeout,Upper bound on the delay for KafkaProducer batching in seconds,0,0,others,keystone
4261,prohibited_implied_role,A list of role names which are prohibited from being an implied role.,0,0,others,keystone
4263,protocol,"The federated protocol ID used to represent X.509 tokenless authorization. This is used in combination with the value of [tokenless_auth] issuer_attribute to find a corresponding federated mapping. In a typical deployment, there is no reason to change this value.",0,0,others,keystone
4264,provider.B,"Entry point for credential encryption and decryption operations in the keystone.credential.provider namespace. Keystone only provides a fernet driver, so there's no reason to change this unless you are providing a custom entry point to encrypt and decrypt credentials.",0,0,others,keystone
4265,provider.C,"Entry point for the receipt provider in the keystone.receipt.provider namespace. The receipt provider controls the receipt construction and validation operations. Keystone includes just the fernet receipt provider for now. fernet receipts do not need to be persisted at all, but require that you run keystone-manage fernet_setup (also see the keystone-manage fernet_rotate command).",0,0,others,keystone
4266,provider,"Entry point for the token provider in the keystone.token.provider namespace. The token provider controls the token construction, validation, and revocation operations. Supported upstream providers are fernet and jws. Neither fernet or jws tokens require persistence and both require additional setup. If using fernet, you're required to run keystone-manage fernet_setup, which creates symmetric keys used to encrypt tokens. If using jws, you're required to generate an ECDSA keypair using a SHA-256 hash algorithm for signing and validating token, which can be done with keystone-manage create_jws_keypair. Note that fernet tokens are encrypted and jws tokens are only signed. Please be sure to consider this if your deployment has security requirements regarding payload contents used to generate token IDs.",0,0,others,keystone
4267,proxies,Proxy classes to import that will affect the way the dogpile.cache backend functions. See the dogpile.cache documentation on changing-backend-behavior.,0,0,others,keystone
4268,pseudo_vhost,"Enable virtual host support for those message buses that do not natively support virtual hosting (such as qpidd). When set to true the virtual host name will be added to all message bus addresses, effectively creating a private 'subnet' per virtual host. Set to False if the message bus supports virtual hosting using the 'hostname' field in the AMQP 1.0 Open performative as the name of the virtual host.",0,0,others,keystone
4269,public_bind_host,The IP address of the network interface for the public service to listen on.,0,0,others,keystone
4270,public_endpoint,"The base public endpoint URL for Keystone that is advertised to clients (NOTE: this does NOT affect how Keystone listens for connections). Defaults to the base host URL of the request. For example, if keystone receives a request to http://server:5000/v3/users, then this will option will be automatically treated as http://server:5000. You should only need to set option if either the value of the base URL contains a path that keystone does not automatically infer (/prefix/v3), or if the endpoint should be found on a different host.",0,0,others,keystone
4272,publish_errors,Enables or disables publication of error events.,0,0,others,keystone
4273,query_scope,"The search scope which defines how deep to search within the search base. A value of one (representing oneLevel or singleLevel) indicates a search of objects immediately below to the base object, but does not include the base object itself. A value of sub (representing subtree or wholeSubtree) indicates a search of both the base object itself and the entire subtree below it.",0,0,others,keystone
4274,rabbit_ha_queues,"Try to use HA queues in RabbitMQ (x-ha-policy: all). If you change this option, you must wipe the RabbitMQ database. In RabbitMQ 3.0, queue mirroring is no longer controlled by the x-ha-policy argument when declaring a queue. If you just want to make sure that all queues (except those with auto-generated names) are mirrored across all nodes, run: ""rabbitmqctl set_policy HA '^(?!amq.).*' '{""ha-mode"": ""all""}' """,0,0,others,keystone
4277,rabbit_qos_prefetch_count,Specifies the number of messages to prefetch. Setting to zero allows unlimited messages.,1,1,resource,keystone
4278,rabbit_retry_backoff,How long to backoff for between retries when connecting to RabbitMQ.,0,0,others,keystone
4279,rabbit_retry_interval,How frequently to retry connecting with RabbitMQ.,0,0,others,keystone
4280,rabbit_transient_queues_ttl,Positive integer representing duration in seconds for queue TTL (x-expires). Queues which are unused for the duration of the TTL are automatically deleted. The parameter affects only reply and fanout queues.,0,0,others,keystone
4283,rate_limit_interval,"Interval, number of seconds, of log rate limiting.",0,0,others,keystone
4284,relay_state_prefix,"The prefix of the RelayState SAML attribute to use when generating enhanced client and proxy (ECP) assertions. In a typical deployment, there is no reason to change this value.",0,0,others,keystone
4286,remote_id_attribute,"Value to be used to obtain the entity ID of the Identity Provider from the environment. For mod_shib, this would be Shib-Identity-Provider. For mod_auth_openidc, this could be HTTP_OIDC_ISS. For mod_auth_mellon, this could be MELLON_IDP.",0,0,others,keystone
4287,remote_ssl_ca_crt_file,Absolute path to ca cert file for REST based policy check,0,0,others,keystone
4289,remote_ssl_client_key_file,Absolute path client key file REST based policy check,0,0,others,keystone
4290,remote_ssl_verify_server_crt,server identity verification for REST based policy check,0,0,others,keystone
4291,reply_link_credit,Window size for incoming RPC Reply messages.,1,1,resource,keystone
4293,retry,"The maximum number of attempts to re-send a notification message which failed to be delivered due to a recoverable error. 0 - No retry, -1 - indefinite",0,0,others,keystone
4295,return_all_endpoints_if_no_filter,"This controls keystone's behavior if the configured endpoint filters do not result in any endpoints for a user + project pair (and therefore a potentially empty service catalog). If set to true, keystone will return the entire service catalog. If set to false, keystone will return an empty service catalog.",0,0,others,keystone
4296,revoke_by_id,This toggles support for revoking individual tokens by the token identifier and thus various token enumeration operations (such as listing all tokens issued to a specific user). These operations are used to determine the list of tokens to consider revoked. Do not disable this option if you're using the kvs [revoke] driver.,0,0,others,keystone
4297,rpc_address_prefix,Address prefix for all generated RPC addresses,0,0,others,keystone
4298,rpc_conn_pool_size,Size of RPC connection pool.,1,1,resource,keystone
4300,rpc_server_credit,Window size for incoming RPC Request messages,1,1,resource,keystone
4301,rules_file,"Path to access rules configuration. If not present, no access rule configuration will be loaded and application credential access rules will be unavailable.",0,0,others,keystone
4302,salt_bytesize,Number of bytes to use in scrypt and pbkfd2_sha512 hashing salt. Default for scrypt is 16 bytes. Default for pbkfd2_sha512 is 16 bytes. Limited to a maximum of 96 bytes due to the size of the column used to store password hashes.,0,0,others,keystone
4303,sasl_config_dir,Path to directory that contains the SASL configuration,0,0,others,keystone
4304,sasl_config_name,Name of configuration file (without .conf suffix),0,0,others,keystone
4305,sasl_default_realm,SASL realm to use if no realm present in username,0,0,others,keystone
4306,sasl_mechanism,Mechanism when security protocol is SASL,1,2,security-tradeoff,keystone
4307,sasl_mechanisms,Space separated list of acceptable SASL mechanisms,0,0,others,keystone
4310,secure_proxy_ssl_header,"The HTTP Header that will be used to determine what the original request protocol scheme was, even if it was hidden by a SSL termination proxy.",0,0,others,keystone
4311,security_protocol,Protocol used to communicate with brokers,1,2,security-tradeoff,keystone
4312,sentinel_service_name,Redissentinel uses a service name to identify a master redis service. This parameter defines the name (for example: sentinal_service_name=mymaster).,0,0,others,keystone
4313,server_request_prefix,address prefix used when sending to a specific server,0,0,others,keystone
4314,slave_connection,The SQLAlchemy connection string to use to connect to the slave database.,0,0,others,keystone
4315,socket_timeout.B,Redissentinel provides a timeout option on the connections. This parameter defines that timeout (for example: socket_timeout=0.1).,0,0,others,keystone
4316,socket_timeout,Timeout in seconds for every call to a server. This is used by the key value store system.,0,0,others,keystone
4319,ssl,Connect over SSL.,1,2,security-tradeoff,keystone
4320,ssl_ca_file.B,CA certificate PEM file used to verify the server's certificate,0,0,others,keystone
4321,ssl_ca_file,SSL certification authority file (valid only if SSL enabled).,0,0,others,keystone
4322,ssl_cafile,CA certificate PEM file used to verify the server certificate,0,0,others,keystone
4323,ssl_cert_file.B,Self-identifying certificate PEM file for client authentication,0,0,others,keystone
4324,ssl_cert_file,SSL cert file (valid only if SSL enabled).,0,0,others,keystone
4325,ssl_key_file.B,SSL key file (valid only if SSL enabled).,0,0,others,keystone
4326,ssl_key_file,Private key PEM file used to sign ssl_cert_file certificate (optional),0,0,others,keystone
4327,ssl_key_password,Password for decrypting ssl_key_file (if encrypted),0,0,others,keystone
4328,ssl_verify_vhost,"By default SSL checks that the name in the server's certificate matches the hostname in the transport_url. In some configurations it may be preferable to use the virtual hostname instead, for example if the server uses the Server Name Indication TLS extension (rfc6066) to provide a certificate per virtual host. Set ssl_verify_vhost to True if the server's SSL certificate uses the virtual host name instead of the DNS name.",0,0,others,keystone
4329,ssl_version,"SSL version to use (valid only if SSL enabled). Valid values are TLSv1 and SSLv23. SSLv2, SSLv3, TLSv1_1, and TLSv1_2 may be available on some distributions.",0,0,others,keystone
4330,sso_callback_template,"Absolute path to an HTML file used as a Single Sign-On callback handler. This page is expected to redirect the user from keystone back to a trusted dashboard host, by form encoding a token in a POST request. Keystone's default value should be sufficient for most deployments.",0,0,others,keystone
4331,strict_password_check,"If set to true, strict password length checking is performed for password manipulation. If a password exceeds the maximum length, the operation will fail with an HTTP 403 Forbidden error. If set to false, passwords are automatically truncated to the maximum length.",0,0,others,keystone
4332,suffix,"The default LDAP server suffix to use, if a DN is not defined via either [ldap] user_tree_dn or [ldap] group_tree_dn.",0,0,others,keystone
4333,syslog_log_facility,Syslog facility to receive log lines. This option is ignored if log_config_append is set.,0,0,others,keystone
4334,template_file,Absolute path to the file used for the templated catalog backend. This option is only used if the [catalog] driver is set to templated.,0,0,others,keystone
4335,tls_cacertdir,An absolute path to a CA certificate directory to use when communicating with LDAP servers. There is no reason to set this option if you've also set [ldap] tls_cacertfile.,0,0,others,keystone
4336,tls_cacertfile,"An absolute path to a CA certificate file to use when communicating with LDAP servers. This option will take precedence over [ldap] tls_cacertdir, so there is no reason to set both.",0,0,others,keystone
4339,topics,AMQP topic used for OpenStack notifications.,0,0,others,keystone
4341,trace_sqlalchemy,Enable SQL requests profiling in services.,1,6,function-tradeoff,keystone
4342,transport_url.B,"A URL representing the messaging driver to use for notifications. If not set, we fall back to the same configuration used for RPC.",0,0,others,keystone
4343,transport_url,"The network address and optional user credentials for connecting to the messaging backend, in URL format. The expected format is:",0,0,others,keystone
4344,trusted_dashboard,"A list of trusted dashboard hosts. Before accepting a Single Sign-On request to return a token, the origin host must be a member of this list. This configuration option may be repeated for multiple values. You must set this in order to use web-based SSO flows. For example: trusted_dashboard=https://acme.example.com/auth/websso trusted_dashboard=https://beta.example.com/auth/websso",0,0,others,keystone
4345,trusted_issuer,"The list of distinguished names which identify trusted issuers of client certificates allowed to use X.509 tokenless authorization. If the option is absent then no certificates will be allowed. The format for the values of a distinguished name (DN) must be separated by a comma and contain no spaces. Furthermore, because an individual DN may contain commas, this configuration option may be repeated multiple times to represent multiple values. For example, keystone.conf would include two consecutive lines in order to trust two different DNs, such as trusted_issuer = CN=john,OU=keystone,O=openstack and trusted_issuer = CN=mary,OU=eng,O=abc.",0,0,others,keystone
4346,unicast_address,Appended to the address prefix when sending to a particular RPC/Notification server. Used by the message bus to identify messages sent to a single destination.,0,0,others,keystone
4347,unique_last_password_count,"This controls the number of previous user password iterations to keep in history, in order to enforce that newly created passwords are unique. The total number which includes the new password should not be greater or equal to this value. Setting the value to zero (the default) disables this feature. Thus, to enable this feature, values must be greater than 0. This feature depends on the sql backend for the [identity] driver.",0,0,others,keystone
4348,url,URL(s) for connecting to the LDAP server. Multiple LDAP URLs may be specified as a comma separated string. The first URL to successfully bind is used for the connection.,0,0,others,keystone
4350,use_db_reconnect,Enable the experimental use of database reconnect on connection lost.,0,0,others,keystone
4351,use_eventlog,Log output to Windows Event Log.,0,0,others,keystone
4352,use_journal,Enable journald for logging. If running in a systemd environment you may wish to enable journal support. Doing so will use the journal native protocol which includes structured metadata in addition to log messages.This option is ignored if log_config_append is set.,0,0,others,keystone
4353,use_json,Use JSON formatting for logging. This option is ignored if log_config_append is set.,0,0,others,keystone
4354,use_pool,Enable LDAP connection pooling for queries to the LDAP server. There is typically no reason to disable this.,1,4,limited-side-effect,keystone
4355,use_stderr,Log output to standard error. This option is ignored if log_config_append is set.,0,0,others,keystone
4356,use_syslog,Use syslog for logging. Existing syslog format is DEPRECATED and will be changed later to honor RFC5424. This option is ignored if log_config_append is set.,0,0,others,keystone
4357,use_tls,Enable TLS when communicating with LDAP servers. You should also set the [ldap] tls_cacertfile and [ldap] tls_cacertdir options when using this option. Do not set this option if you are using LDAP over SSL (LDAPS) instead of TLS.,1,2,security-tradeoff,keystone
4358,user,"The user name of the administrator bind DN to use when querying the LDAP server, if your LDAP server requires it.",0,0,others,keystone
4359,user_additional_attribute_mapping,"A list of LDAP attribute to keystone user attribute pairs used for mapping additional attributes to users in keystone. The expected format is <ldap_attr>:<user_attr>, where ldap_attr is the attribute in the LDAP object and user_attr is the attribute which should appear in the identity API.",0,0,others,keystone
4360,user_attribute_ignore,"List of user attributes to ignore on create and update, or whether a specific user attribute should be filtered for list or show user.",0,0,others,keystone
4362,user_description_attribute,The LDAP attribute mapped to user descriptions in keystone.,0,0,others,keystone
4364,user_enabled_default,"The default value to enable users. This should match an appropriate integer value if the LDAP server uses non-boolean (bitmask) values to indicate if a user is enabled or disabled. If this is not set to True, then the typical value is 512. This is typically used when [ldap] user_enabled_attribute = userAccountControl.",0,0,others,keystone
4365,user_enabled_emulation,"If enabled, keystone uses an alternative method to determine if a user is enabled or not by checking if they are a member of the group defined by the [ldap] user_enabled_emulation_dn option. Enabling this option causes keystone to ignore the value of [ldap] user_enabled_invert.",0,0,others,keystone
4366,user_enabled_emulation_dn,DN of the group entry to hold enabled users when using enabled emulation. Setting this option has no effect unless [ldap] user_enabled_emulation is also enabled.,0,0,others,keystone
4367,user_enabled_emulation_use_group_config,Use the [ldap] group_member_attribute and [ldap] group_objectclass settings to determine membership in the emulated enabled group. Enabling this option has no effect unless [ldap] user_enabled_emulation is also enabled.,0,0,others,keystone
4368,user_enabled_invert,"Logically negate the boolean value of the enabled attribute obtained from the LDAP server. Some LDAP servers use a boolean lock attribute where ""true"" means an account is disabled. Setting [ldap] user_enabled_invert = true will allow these lock attributes to be used. This option will have no effect if either the [ldap] user_enabled_mask or [ldap] user_enabled_emulation options are in use.",0,0,others,keystone
4369,user_enabled_mask,"Bitmask integer to select which bit indicates the enabled value if the LDAP server represents ""enabled"" as a bit on an integer rather than as a discrete boolean. A value of 0 indicates that the mask is not used. If this is not set to 0 the typical value is 2. This is typically used when [ldap] user_enabled_attribute = userAccountControl. Setting this option causes keystone to ignore the value of [ldap] user_enabled_invert.",0,0,others,keystone
4371,user_id_attribute,The LDAP attribute mapped to user IDs in keystone. This must NOT be a multivalued attribute. User IDs are expected to be globally unique across keystone domains and URL-safe.,0,0,others,keystone
4372,user_limit,"Maximum number of application credentials a user is permitted to create. A value of -1 means unlimited. If a limit is not set, users are permitted to create application credentials at will, which could lead to bloat in the keystone database or open keystone to a DoS attack.",1,3,reliability-tradeoff,keystone
4373,user_mail_attribute,The LDAP attribute mapped to user emails in keystone.,0,0,others,keystone
4375,user_objectclass,The LDAP object class to use for users.,0,0,others,keystone
4376,user_pass_attribute,The LDAP attribute mapped to user passwords in keystone.,0,0,others,keystone
4377,user_tree_dn,The search base to use for users. Defaults to the [ldap] suffix value.,0,0,others,keystone
4378,valid_days,The validity period (in days) to use when generating a self-signed token signing certificate. There is no reason to set this option unless you are requesting revocation lists in a non-production environment. Use a [signing] certfile issued from a trusted certificate authority instead.,0,0,others,keystone
4379,watch_log_file,Uses logging handler designed to watch file system. When log file is moved or removed this handler will open a new log file with specified path instantaneously. It makes sense only if log_file option is specified and Linux platform is used. This option is ignored if log_config_append is set.,0,0,others,keystone
4381,map.sort.class,The default sort class for sorting keys.,0,0,others,mapreduce
4384,mapreduce.admin.user.env,"Expert: Additional execution environment entries for map and reduce task processes. This is not an additive property. You must preserve the original value if you want your map and reduce tasks to have access to native libraries (compression, etc). When this value is empty, the command to set execution envrionment will be OS dependent: For linux, use LD_LIBRARY_PATH=$HADOOP_COMMON_HOME/lib/native. For windows, use PATH = %PATH%;%HADOOP_COMMON_HOME%\\bin.",0,0,others,mapreduce
4385,mapreduce.am.max-attempts,"The maximum number of application attempts. It is a application-specific setting. It should not be larger than the global number set by resourcemanager. Otherwise, it will be override. The default number is set to 2, to allow at least one retry for AM.",0,0,others,mapreduce
4386,mapreduce.application.classpath,"CLASSPATH for MR applications. A comma-separated list of CLASSPATH entries. If mapreduce.application.framework is set then this must specify the appropriate classpath for that archive, and the name of the archive must be present in the classpath. If mapreduce.app-submission.cross-platform is false, platform-specific environment vairable expansion syntax would be used to construct the default CLASSPATH entries. For Linux: $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*, $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*. For Windows: %HADOOP_MAPRED_HOME%/share/hadoop/mapreduce/*, %HADOOP_MAPRED_HOME%/share/hadoop/mapreduce/lib/*. If mapreduce.app-submission.cross-platform is true, platform-agnostic default CLASSPATH for MR applications would be used: {{HADOOP_MAPRED_HOME}}/share/hadoop/mapreduce/*, {{HADOOP_MAPRED_HOME}}/share/hadoop/mapreduce/lib/* Parameter expansion marker will be replaced by NodeManager on container launch based on the underlying OS accordingly.",0,0,others,mapreduce
4388,mapreduce.app-submission.cross-platform,"If enabled, user can submit an application cross-platform i.e. submit an application from a Windows client to a Linux/Unix server or vice versa.",0,0,others,mapreduce
4389,mapreduce.client.completion.pollinterval,The interval (in milliseconds) between which the JobClient polls the MapReduce ApplicationMaster for updates about job status. You may want to set this to a lower value to make tests run faster on a single node system. Adjusting this value in production may lead to unwanted client-server traffic.,0,0,others,mapreduce
4390,mapreduce.client.libjars.wildcard,"Whether the libjars cache files should be localized using a wildcarded directory instead of naming each archive independently. Using wildcards reduces the space needed for storing the job information in the case of a highly available resource manager configuration. This propery should only be set to false for specific jobs which are highly sensitive to the details of the archive localization. Having this property set to true will cause the archives to all be localized to the same local cache location. If false, each archive will be localized to its own local cache location. In both cases a symbolic link will be created to every archive from the job's working directory.",1,4,limited-side-effect,mapreduce
4391,mapreduce.client.output.filter,"The filter for controlling the output of the task's userlogs sent to the console of the JobClient. The permissible options are: NONE, KILLED, FAILED, SUCCEEDED and ALL.",1,6,function-tradeoff,mapreduce
4392,mapreduce.client.progressmonitor.pollinterval,The interval (in milliseconds) between which the JobClient reports status to the console and checks for job completion. You may want to set this to a lower value to make tests run faster on a single node system. Adjusting this value in production may lead to unwanted client-server traffic.,0,0,others,mapreduce
4394,mapreduce.cluster.acls.enabled,"Specifies whether ACLs should be checked for authorization of users for doing various queue and job level operations. ACLs are disabled by default. If enabled, access control checks are made by MapReduce ApplicationMaster when requests are made by users for queue operations like submit job to a queue and kill a job in the queue and job operations like viewing the job-details (See mapreduce.job.acl-view-job) or for modifying the job (See mapreduce.job.acl-modify-job) using Map/Reduce APIs, RPCs or via the console and web user interfaces. For enabling this flag, set to true in mapred-site.xml file of all MapReduce clients (MR job submitting nodes).",0,0,others,mapreduce
4395,mapreduce.cluster.local.dir,The local directory where MapReduce stores intermediate data files. May be a comma-separated list of directories on different devices in order to spread disk i/o. Directories that do not exist are ignored.,0,0,others,mapreduce
4396,mapreduce.cluster.temp.dir,A shared directory for temporary files.,0,0,others,mapreduce
4397,mapreduce.fileoutputcommitter.algorithm.version,"The file output committer algorithm version valid algorithm version number: 1 or 2 default to 1, which is the original algorithm In algorithm version 1, 1. commitTask will rename directory $joboutput/_temporary/$appAttemptID/_temporary/$taskAttemptID/ to $joboutput/_temporary/$appAttemptID/$taskID/ 2. recoverTask will also do a rename $joboutput/_temporary/$appAttemptID/$taskID/ to $joboutput/_temporary/($appAttemptID + 1)/$taskID/ 3. commitJob will merge every task output file in $joboutput/_temporary/$appAttemptID/$taskID/ to $joboutput/, then it will delete $joboutput/_temporary/ and write $joboutput/_SUCCESS It has a performance regression, which is discussed in MAPREDUCE-4815. If a job generates many files to commit then the commitJob method call at the end of the job can take minutes. the commit is single-threaded and waits until all tasks have completed before commencing. algorithm version 2 will change the behavior of commitTask, recoverTask, and commitJob. 1. commitTask will rename all files in $joboutput/_temporary/$appAttemptID/_temporary/$taskAttemptID/ to $joboutput/ 2. recoverTask actually doesn't require to do anything, but for upgrade from version 1 to version 2 case, it will check if there are any files in $joboutput/_temporary/($appAttemptID - 1)/$taskID/ and rename them to $joboutput/ 3. commitJob can simply delete $joboutput/_temporary and write $joboutput/_SUCCESS This algorithm will reduce the output commit time for large jobs by having the tasks commit directly to the final output directory as they were completing and commitJob had very little to do.",0,0,others,mapreduce
4398,mapreduce.framework.name,"The runtime framework for executing MapReduce jobs. Can be one of local, classic or yarn.",0,0,others,mapreduce
4400,mapreduce.ifile.readahead.bytes,Configuration key to set the IFile readahead length in bytes.,1,1,resource,mapreduce
4403,mapreduce.input.lineinputformat.linespermap,"When using NLineInputFormat, the number of lines of input data to include in each split.",0,0,others,mapreduce
4404,mapreduce.job.acl-modify-job,"Job specific access-control list for 'modifying' the job. It is only used if authorization is enabled in Map/Reduce by setting the configuration property mapreduce.cluster.acls.enabled to true. This specifies the list of users and/or groups who can do modification operations on the job. For specifying a list of users and groups the format to use is ""user1,user2 group1,group"". If set to '*', it allows all users/groups to modify this job. If set to ' '(i.e. space), it allows none. This configuration is used to guard all the modifications with respect to this job and takes care of all the following operations: o killing this job o killing a task of this job, failing a task of this job o setting the priority of this job Each of these operations are also protected by the per-queue level ACL ""acl-administer-jobs"" configured via mapred-queues.xml. So a caller should have the authorization to satisfy either the queue-level ACL or the job-level ACL. Irrespective of this ACL configuration, (a) job-owner, (b) the user who started the cluster, (c) members of an admin configured supergroup configured via mapreduce.cluster.permissions.supergroup and (d) queue administrators of the queue to which this job was submitted to configured via acl-administer-jobs for the specific queue in mapred-queues.xml can do all the modification operations on a job. By default, nobody else besides job-owner, the user who started the cluster, members of supergroup and queue administrators can perform modification operations on a job.",0,0,others,mapreduce
4405,mapreduce.job.acl-view-job,"Job specific access-control list for 'viewing' the job. It is only used if authorization is enabled in Map/Reduce by setting the configuration property mapreduce.cluster.acls.enabled to true. This specifies the list of users and/or groups who can view private details about the job. For specifying a list of users and groups the format to use is ""user1,user2 group1,group"". If set to '*', it allows all users/groups to modify this job. If set to ' '(i.e. space), it allows none. This configuration is used to guard some of the job-views and at present only protects APIs that can return possibly sensitive information of the job-owner like o job-level counters o task-level counters o tasks' diagnostic information o task-logs displayed on the HistoryServer's web-UI and o job.xml showed by the HistoryServer's web-UI Every other piece of information of jobs is still accessible by any other user, for e.g., JobStatus, JobProfile, list of jobs in the queue, etc. Irrespective of this ACL configuration, (a) job-owner, (b) the user who started the cluster, (c) members of an admin configured supergroup configured via mapreduce.cluster.permissions.supergroup and (d) queue administrators of the queue to which this job was submitted to configured via acl-administer-jobs for the specific queue in mapred-queues.xml can do all the view operations on a job. By default, nobody else besides job-owner, the user who started the cluster, memebers of supergroup and queue administrators can perform view operations on a job.",0,0,others,mapreduce
4406,mapreduce.job.am.node-label-expression,This is node-label configuration for Map Reduce Application Master container. If not configured it will make use of mapreduce.job.node-label-expression and if job's node-label expression is not configured then it will use queue's default-node-label-expression.,0,0,others,mapreduce
4408,mapreduce.job.cache.limit.max-resources-mb,"The maximum size (in MB) a map reduce job is allowed to submit for localization via files, libjars, archives, and jobjar command line arguments and through the distributed cache. If set to 0 the limit is ignored.",1,1,resource,mapreduce
4410,mapreduce.job.classloader,Whether to use a separate (isolated) classloader for user classes in the task JVM.,0,0,others,mapreduce
4411,mapreduce.job.classloader.system.classes,"Used to override the default definition of the system classes for the job classloader. The system classes are a comma-separated list of patterns that indicate whether to load a class from the system classpath, instead from the user-supplied JARs, when mapreduce.job.classloader is enabled. A positive pattern is defined as: 1. A single class name 'C' that matches 'C' and transitively all nested classes 'C$*' defined in C; 2. A package name ending with a '.' (e.g., ""com.example."") that matches all classes from that package. A negative pattern is defined by a '-' in front of a positive pattern (e.g., ""-com.example.""). A class is considered a system class if and only if it matches one of the positive patterns and none of the negative ones. More formally: A class is a member of the inclusion set I if it matches one of the positive patterns. A class is a member of the exclusion set E if it matches one of the negative patterns. The set of system classes S = I \ E.",0,0,others,mapreduce
4413,mapreduce.job.complete.cancel.delegation.tokens,"if false - do not unregister/cancel delegation tokens from renewal, because same tokens may be used by spawned jobs",0,0,others,mapreduce
4415,mapreduce.job.emit-timeline-data,Specifies if the Application Master should emit timeline data to the timeline server. Individual jobs can override this value.,0,0,others,mapreduce
4416,mapreduce.job.end-notification.max.attempts,"The maximum number of times a URL will be read for providing job end notification. Cluster administrators can set this to limit how long after end of a job, the Application Master waits before exiting. Must be marked as final to prevent users from overriding this.",0,0,others,mapreduce
4417,mapreduce.job.end-notification.max.retry.interval,The maximum amount of time (in milliseconds) to wait before retrying job end notification. Cluster administrators can set this to limit how long the Application Master waits before exiting. Must be marked as final to prevent users from overriding this.,0,0,others,mapreduce
4419,mapreduce.job.end-notification.retry.interval,The number of milliseconds the submitter of the job wants to wait before job end notification is retried if it fails. This is capped by mapreduce.job.end-notification.max.retry.interval,0,0,others,mapreduce
4420,mapreduce.job.end-notification.url,"Indicates url which will be called on completion of job to inform end status of job. User can give at most 2 variables with URI : $jobId and $jobStatus. If they are present in URI, then they will be replaced by their respective values.",0,0,others,mapreduce
4421,mapreduce.job.finish-when-all-reducers-done,"Specifies whether the job should complete once all reducers have finished, regardless of whether there are still running mappers.",1,3,reliability-tradeoff,mapreduce
4422,mapreduce.job.log4j-properties-file,"Used to override the default settings of log4j in container-log4j.properties for NodeManager. Like container-log4j.properties, it requires certain framework appenders properly defined in this overriden file. The file on the path will be added to distributed cache and classpath. If no-scheme is given in the path, it defaults to point to a log4j file on the local FS.",0,0,others,mapreduce
4423,mapreduce.job.map.output.collector.class,"The MapOutputCollector implementation(s) to use. This may be a comma-separated list of class names, in which case the map task will try to initialize each of the collectors in turn. The first to successfully initialize will be used.",0,0,others,mapreduce
4424,mapreduce.job.maps,"The default number of map tasks per job. Ignored when mapreduce.framework.name is ""local"".",1,1,resource,mapreduce
4425,mapreduce.job.max.map,Limit on the number of map tasks allowed per job. There is no limit if this value is negative.,1,1,resource,mapreduce
4426,mapreduce.job.max.split.locations,The max number of block locations to store for each split for locality calculation.,0,0,others,mapreduce
4427,mapreduce.job.maxtaskfailures.per.tracker,The number of task-failures on a node manager of a given job after which new tasks of that job aren't assigned to it. It MUST be less than mapreduce.map.maxattempts and mapreduce.reduce.maxattempts otherwise the failed task will never be tried on a different node.,1,3,reliability-tradeoff,mapreduce
4428,mapreduce.job.node-label-expression,"All the containers of the Map Reduce job will be run with this node label expression. If the node-label-expression for job is not set, then it will use queue's default-node-label-expression for all job's containers.",0,0,others,mapreduce
4429,mapreduce.job.queuename,"Queue to which a job is submitted. This must match one of the queues defined in mapred-queues.xml for the system. Also, the ACL setup for the queue must allow the current user to submit a job to the queue. Before specifying a queue, ensure that the system is configured with the queue, and access is allowed for submitting jobs to the queue.",0,0,others,mapreduce
4430,mapreduce.job.redacted-properties,The list of job configuration properties whose value will be redacted.,0,0,others,mapreduce
4431,mapreduce.job.reduce.shuffle.consumer.plugin.class,Name of the class whose instance will be used to send shuffle requests by reducetasks of this job. The class must be an instance of org.apache.hadoop.mapred.ShuffleConsumerPlugin.,0,0,others,mapreduce
4433,mapreduce.job.reducer.preempt.delay.sec,"The threshold (in seconds) after which an unsatisfied mapper request triggers reducer preemption when there is no anticipated headroom. If set to 0 or a negative value, the reducer is preempted as soon as lack of headroom is detected. Default is 0.",1,5,workload-specific,mapreduce
4434,mapreduce.job.reducer.unconditional-preempt.delay.sec,"The threshold (in seconds) after which an unsatisfied mapper request triggers a forced reducer preemption irrespective of the anticipated headroom. By default, it is set to 5 mins. Setting it to 0 leads to immediate reducer preemption. Setting to -1 disables this preemption altogether.",0,0,others,mapreduce
4435,mapreduce.job.reduces,"The default number of reduce tasks per job. Typically set to 99% of the cluster's reduce capacity, so that if a node fails the reduces can still be executed in a single wave. Ignored when mapreduce.framework.name is ""local"".",1,1,resource,mapreduce
4436,mapreduce.job.running.map.limit,The maximum number of simultaneous map tasks per job. There is no limit if this value is 0 or negative.,1,1,resource,mapreduce
4437,mapreduce.job.running.reduce.limit,The maximum number of simultaneous reduce tasks per job. There is no limit if this value is 0 or negative.,1,1,resource,mapreduce
4438,mapreduce.job.send-token-conf,"This configuration is a regex expression. The list of configurations that match the regex expression will be sent to RM. RM will use these configurations for renewing tokens. This configuration is added for below scenario: User needs to run distcp jobs across two clusters, but the RM does not have necessary hdfs configurations to connect to the remote hdfs cluster. Hence, user relies on this config to send the configurations to RM and RM uses these configurations to renew tokens. For example the following regex expression indicates the minimum required configs for RM to connect to a remote hdfs cluster: dfs.nameservices|^dfs.namenode.rpc-address.*$|^dfs.ha.namenodes.*$|^dfs.client.failover.proxy.provider.*$|dfs.namenode.kerberos.principal",0,0,others,mapreduce
4440,mapreduce.job.skip.outdir,"If no value is specified here, the skipped records are written to the output directory at _logs/skip. User can stop writing skipped records by giving the value ""none"".",0,0,others,mapreduce
4443,mapreduce.job.speculative.retry-after-speculate,The waiting time(ms) to do next round of speculation if there are tasks speculated in this round.,0,0,others,mapreduce
4444,mapreduce.job.speculative.slowtaskthreshold,The number of standard deviations by which a task's ave progress-rates must be lower than the average of all running tasks' for the task to be considered too slow.,1,5,workload-specific,mapreduce
4445,mapreduce.job.speculative.speculative-cap-running-tasks,The max percent (0-1) of running tasks that can be speculatively re-executed at any time.,1,5,workload-specific,mapreduce
4447,mapreduce.job.split.metainfo.maxsize,The maximum permissible size of the split metainfo file. The MapReduce ApplicationMaster won't attempt to read submitted split metainfo files bigger than this configured value. No limits if set to -1.,1,1,resource,mapreduce
4448,mapreduce.job.tags,Tags for the job that will be passed to YARN at submission time. Queries to YARN for applications can filter on these tags.,0,0,others,mapreduce
4449,mapreduce.job.token.tracking.ids,"When mapreduce.job.token.tracking.ids.enabled is set to true, this is set by the framework to the token-tracking-ids used by the job.",0,0,others,mapreduce
4450,mapreduce.job.token.tracking.ids.enabled,"Whether to write tracking ids of tokens to job-conf. When true, the configuration property ""mapreduce.job.token.tracking.ids"" is set to the token-tracking-ids of the job",0,0,others,mapreduce
4451,mapreduce.job.ubertask.enable,"Whether to enable the small-jobs ""ubertask"" optimization, which runs ""sufficiently small"" jobs sequentially within a single JVM. ""Small"" is defined by the following maxmaps, maxreduces, and maxbytes settings. Note that configurations for application masters also affect the ""Small"" definition - yarn.app.mapreduce.am.resource.mb must be larger than both mapreduce.map.memory.mb and mapreduce.reduce.memory.mb, and yarn.app.mapreduce.am.resource.cpu-vcores must be larger than both mapreduce.map.cpu.vcores and mapreduce.reduce.cpu.vcores to enable ubertask. Users may override this value.",1,4,limited-side-effect,mapreduce
4452,mapreduce.job.ubertask.maxbytes,"Threshold for number of input bytes, beyond which job is considered too big for the ubertasking optimization. If no value is specified, dfs.block.size is used as a default. Be sure to specify a default value in mapred-site.xml if the underlying filesystem is not HDFS. Users may override this value, but only downward.",1,5,workload-specific,mapreduce
4453,mapreduce.job.ubertask.maxmaps,"Threshold for number of maps, beyond which job is considered too big for the ubertasking optimization. Users may override this value, but only downward.",1,5,workload-specific,mapreduce
4454,mapreduce.job.ubertask.maxreduces,"Threshold for number of reduces, beyond which job is considered too big for the ubertasking optimization. CURRENTLY THE CODE CANNOT SUPPORT MORE THAN ONE REDUCE and will ignore larger values. (Zero is a valid max, however.) Users may override this value, but only downward.",1,5,workload-specific,mapreduce
4455,mapreduce.jobhistory.address,MapReduce JobHistory Server IPC host:port,0,0,others,mapreduce
4456,mapreduce.jobhistory.admin.acl,ACL of who can be admin of the History server.,0,0,others,mapreduce
4457,mapreduce.jobhistory.admin.address,The address of the History server admin interface.,0,0,others,mapreduce
4458,mapreduce.jobhistory.cleaner.interval-ms,"How often the job history cleaner checks for files to delete, in milliseconds. Defaults to 86400000 (one day). Files are only deleted if they are older than mapreduce.jobhistory.max-age-ms.",0,0,others,mapreduce
4460,mapreduce.jobhistory.datestring.cache.size,Size of the date string cache. Effects the number of directories which will be scanned to find a job.,1,1,resource,mapreduce
4461,mapreduce.jobhistory.http.policy,This configures the HTTP endpoint for JobHistoryServer web UI. The following values are supported: - HTTP_ONLY : Service is provided only on http - HTTPS_ONLY : Service is provided only on https,0,0,others,mapreduce
4462,mapreduce.jobhistory.jhist.format,"File format the AM will use when generating the .jhist file. Valid values are ""json"" for text output and ""binary"" for faster parsing.",0,0,others,mapreduce
4463,mapreduce.jobhistory.joblist.cache.size,Size of the job list cache,1,1,resource,mapreduce
4464,mapreduce.jobhistory.jobname.limit,Number of characters allowed for job name in Job History Server web page.,0,0,others,mapreduce
4465,mapreduce.jobhistory.keytab,Location of the kerberos keytab file for the MapReduce JobHistory Server.,0,0,others,mapreduce
4469,mapreduce.jobhistory.max-age-ms,Job history files older than this many milliseconds will be deleted when the history cleaner runs. Defaults to 604800000 (1 week).,0,0,others,mapreduce
4470,mapreduce.jobhistory.minicluster.fixed.ports,Whether to use fixed ports with the minicluster,0,0,others,mapreduce
4471,mapreduce.jobhistory.move.interval-ms,Scan for history files to move from intermediate done dir to done dir at this frequency.,0,0,others,mapreduce
4473,mapreduce.jobhistory.principal,Kerberos principal name for the MapReduce JobHistory Server.,0,0,others,mapreduce
4474,mapreduce.jobhistory.recovery.enable,Enable the history server to store server state and recover server state upon startup. If enabled then mapreduce.jobhistory.recovery.store.class must be specified.,1,3,reliability-tradeoff,mapreduce
4475,mapreduce.jobhistory.recovery.store.class,The HistoryServerStateStoreService class to store history server state for recovery.,1,3,reliability-tradeoff,mapreduce
4476,mapreduce.jobhistory.recovery.store.fs.uri,The URI where history server state will be stored if HistoryServerFileSystemStateStoreService is configured as the recovery storage class.,0,0,others,mapreduce
4477,mapreduce.jobhistory.recovery.store.leveldb.path,The URI where history server state will be stored if HistoryServerLeveldbSystemStateStoreService is configured as the recovery storage class.,0,0,others,mapreduce
4478,mapreduce.jobhistory.store.class,The HistoryStorage class to use to cache history data.,0,0,others,mapreduce
4479,mapreduce.jobhistory.webapp.address,MapReduce JobHistory Server Web UI host:port,0,0,others,mapreduce
4480,mapreduce.jobhistory.webapp.https.address,The https address the MapReduce JobHistory Server WebApp is on.,0,0,others,mapreduce
4481,mapreduce.jobhistory.webapp.rest-csrf.custom-header,Optional parameter that indicates the custom header name to use for CSRF protection.,0,0,others,mapreduce
4482,mapreduce.jobhistory.webapp.rest-csrf.enabled,Enable the CSRF filter for the job history web app,1,2,security-tradeoff,mapreduce
4483,mapreduce.jobhistory.webapp.rest-csrf.methods-to-ignore,Optional parameter that indicates the list of HTTP methods that do not require CSRF protection,0,0,others,mapreduce
4484,mapreduce.jobhistory.webapp.xfs-filter.xframe-options,Value of the xframe-options,0,0,others,mapreduce
4485,mapreduce.jobtracker.address,"The host and port that the MapReduce job tracker runs at. If ""local"", then jobs are run in-process as a single map and reduce task.",0,0,others,mapreduce
4486,mapreduce.jobtracker.staging.root.dir,"The root of the staging area for users' job files In practice, this should be the directory where users' home directories are located (usually /user)",0,0,others,mapreduce
4487,mapreduce.jobtracker.system.dir,The directory where MapReduce stores control files.,0,0,others,mapreduce
4488,mapreduce.jvm.system-properties-to-log,Comma-delimited list of system properties to log on mapreduce JVM start,0,0,others,mapreduce
4489,mapreduce.local.clientfactory.class.name,This the client factory that is responsible for creating local job runner client,0,0,others,mapreduce
4490,mapreduce.map.cpu.vcores,The number of virtual cores to request from the scheduler for each map task.,1,1,resource,mapreduce
4492,mapreduce.map.maxattempts,"Expert: The maximum number of attempts per map task. In other words, framework will try to execute a map task these many number of times before giving up on it.",0,0,others,mapreduce
4494,mapreduce.map.node-label-expression,This is node-label configuration for Map task containers. If not configured it will use mapreduce.job.node-label-expression and if job's node-label expression is not configured then it will use queue's default-node-label-expression.,0,0,others,mapreduce
4495,mapreduce.map.output.compress,Should the outputs of the maps be compressed before being sent across the network. Uses SequenceFile compression.,1,4,limited-side-effect,mapreduce
4496,mapreduce.map.output.compress.codec,"If the map outputs are compressed, how should they be compressed?",1,6,function-tradeoff,mapreduce
4497,mapreduce.map.skip.maxrecords,"The number of acceptable skip records surrounding the bad record PER bad record in mapper. The number includes the bad record as well. To turn the feature of detection/skipping of bad records off, set the value to 0. The framework tries to narrow down the skipped range by retrying until this threshold is met OR all attempts get exhausted for this task. Set the value to Long.MAX_VALUE to indicate that framework need not try to narrow down. Whatever records(depends on application) get skipped are acceptable.",0,0,others,mapreduce
4498,mapreduce.map.skip.proc-count.auto-incr,"The flag which if set to true, SkipBadRecords.COUNTER_MAP_PROCESSED_RECORDS is incremented by MapRunner after invoking the map function. This value must be set to false for applications which process the records asynchronously or buffer the input records. For example streaming. In such cases applications should increment this counter on their own.",0,0,others,mapreduce
4499,mapreduce.map.sort.spill.percent,"The soft limit in the serialization buffer. Once reached, a thread will begin to spill the contents to disk in the background. Note that collection will not block if this threshold is exceeded while a spill is already in progress, so spills may be larger than this threshold when it is set to less than .5",1,1,resource,mapreduce
4501,mapreduce.output.fileoutputformat.compress,Should the job outputs be compressed?,1,4,limited-side-effect,mapreduce
4502,mapreduce.output.fileoutputformat.compress.codec,"If the job outputs are compressed, how should they be compressed?",1,6,function-tradeoff,mapreduce
4503,mapreduce.output.fileoutputformat.compress.type,"If the job outputs are to compressed as SequenceFiles, how should they be compressed? Should be one of NONE, RECORD or BLOCK.",1,6,function-tradeoff,mapreduce
4504,mapreduce.reduce.cpu.vcores,The number of virtual cores to request from the scheduler for each reduce task.,1,1,resource,mapreduce
4506,mapreduce.reduce.log.level,"The logging level for the reduce task. The allowed levels are: OFF, FATAL, ERROR, WARN, INFO, DEBUG, TRACE and ALL. The setting here could be overridden if ""mapreduce.job.log4j-properties-file"" is set.",1,6,function-tradeoff,mapreduce
4507,mapreduce.reduce.markreset.buffer.percent,The percentage of memory -relative to the maximum heap size- to be used for caching values when using the mark-reset functionality.,1,1,resource,mapreduce
4509,mapreduce.reduce.memory.mb,The amount of memory to request from the scheduler for each reduce task.,1,1,resource,mapreduce
4510,mapreduce.reduce.merge.inmem.threshold,"The threshold, in terms of the number of files for the in-memory merge process. When we accumulate threshold number of files we initiate the in-memory merge and spill to disk. A value of 0 or less than 0 indicates we want to DON'T have any threshold and instead depend only on the ramfs's memory consumption to trigger the merge.",1,5,workload-specific,mapreduce
4512,mapreduce.reduce.shuffle.connect.timeout,Expert: The maximum amount of time (in milli seconds) reduce task spends in trying to connect to a remote node for getting map output.,1,5,workload-specific,mapreduce
4513,mapreduce.reduce.shuffle.fetch.retry.enabled,Set to enable fetch retry during host restart.,0,0,others,mapreduce
4514,mapreduce.reduce.shuffle.fetch.retry.interval-ms,Time of interval that fetcher retry to fetch again when some non-fatal failure happens because of some events like NM restart.,0,0,others,mapreduce
4515,mapreduce.reduce.shuffle.fetch.retry.timeout-ms,Timeout value for fetcher to retry to fetch again when some non-fatal failure happens because of some events like NM restart.,0,0,others,mapreduce
4516,mapreduce.reduce.shuffle.input.buffer.percent,The percentage of memory to be allocated from the maximum heap size to storing map outputs during the shuffle.,1,1,resource,mapreduce
4517,mapreduce.reduce.shuffle.memory.limit.percent,"Expert: Maximum percentage of the in-memory limit that a single shuffle can consume. Range of valid values is [0.0, 1.0]. If the value is 0.0 map outputs are shuffled directly to disk.",1,1,resource,mapreduce
4518,mapreduce.reduce.shuffle.merge.percent,"The usage threshold at which an in-memory merge will be initiated, expressed as a percentage of the total memory allocated to storing in-memory map outputs, as defined by mapreduce.reduce.shuffle.input.buffer.percent.",1,1,resource,mapreduce
4519,mapreduce.reduce.shuffle.parallelcopies,The default number of parallel transfers run by reduce during the copy(shuffle) phase.,1,1,resource,mapreduce
4520,mapreduce.reduce.shuffle.read.timeout,Expert: The maximum amount of time (in milli seconds) reduce task waits for map output data to be available for reading after obtaining connection.,0,0,others,mapreduce
4521,mapreduce.reduce.shuffle.retry-delay.max.ms,The maximum number of ms the reducer will delay before retrying to download map data.,0,0,others,mapreduce
4522,mapreduce.reduce.skip.maxgroups,"The number of acceptable skip groups surrounding the bad group PER bad group in reducer. The number includes the bad group as well. To turn the feature of detection/skipping of bad groups off, set the value to 0. The framework tries to narrow down the skipped range by retrying until this threshold is met OR all attempts get exhausted for this task. Set the value to Long.MAX_VALUE to indicate that framework need not try to narrow down. Whatever groups(depends on application) get skipped are acceptable.",0,0,others,mapreduce
4523,mapreduce.reduce.skip.proc-count.auto-incr,"The flag which if set to true, SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS is incremented by framework after invoking the reduce function. This value must be set to false for applications which process the records asynchronously or buffer the input records. For example streaming. In such cases applications should increment this counter on their own.",0,0,others,mapreduce
4524,mapreduce.reduce.speculative,"If true, then multiple instances of some reduce tasks may be executed in parallel.",1,4,limited-side-effect,mapreduce
4526,mapreduce.shuffle.connection-keep-alive.timeout,"The number of seconds a shuffle client attempts to retain http connection. Refer ""Keep-Alive: timeout="" header in Http specification",0,0,others,mapreduce
4527,mapreduce.shuffle.listen.queue.size,The length of the shuffle server listen queue.,1,1,resource,mapreduce
4528,mapreduce.shuffle.max.connections,Max allowed connections for the shuffle. Set to 0 (zero) to indicate no limit on the number of connections.,1,1,resource,mapreduce
4529,mapreduce.shuffle.max.threads,"Max allowed threads for serving shuffle connections. Set to zero to indicate the default of 2 times the number of available processors (as reported by Runtime.availableProcessors()). Netty is used to serve requests, so a thread is not needed for each connection.",1,1,resource,mapreduce
4530,mapreduce.shuffle.port,Default port that the ShuffleHandler will run on. ShuffleHandler is a service run at the NodeManager to facilitate transfers of intermediate Map outputs to requesting Reducers.,0,0,others,mapreduce
4531,mapreduce.shuffle.ssl.enabled,Whether to use SSL for for the Shuffle HTTP endpoints.,1,2,security-tradeoff,mapreduce
4532,mapreduce.shuffle.ssl.file.buffer.size,Buffer size for reading spills from file when using SSL.,1,1,resource,mapreduce
4533,mapreduce.shuffle.transfer.buffer.size,"This property is used only if mapreduce.shuffle.transferTo.allowed is set to false. In that case, this property defines the size of the buffer used in the buffer copy code for the shuffle phase. The size of this buffer determines the size of the IO requests.",1,1,resource,mapreduce
4534,mapreduce.shuffle.transferTo.allowed,"This option can enable/disable using nio transferTo method in the shuffle phase. NIO transferTo does not perform well on windows in the shuffle phase. Thus, with this configuration property it is possible to disable it, in which case custom transfer method will be used. Recommended value is false when running Hadoop on Windows. For Linux, it is recommended to set it to true. If nothing is set then the default value is false for Windows, and true for Linux.",1,1,resource,mapreduce
4535,mapreduce.task.combine.progress.records,The number of records to process during combine output collection before sending a progress notification.,1,1,resource,mapreduce
4537,mapreduce.task.exit.timeout.check-interval-ms,The interval in milliseconds between which the MR framework checks if task attempts stay in finishing state for too long.,0,0,others,mapreduce
4538,mapreduce.task.files.preserve.failedtasks,"Should the files for failed tasks be kept. This should only be used on jobs that are failing, because the storage is never reclaimed. It also prevents the map outputs from being erased from the reduce directory as they are consumed.",0,0,others,mapreduce
4539,mapreduce.task.io.sort.factor,The number of streams to merge at once while sorting files. This determines the number of open file handles.,1,1,resource,mapreduce
4540,mapreduce.task.io.sort.mb,"The total amount of buffer memory to use while sorting files, in megabytes. By default, gives each merge stream 1MB, which should minimize seeks.",1,1,resource,mapreduce
4541,mapreduce.task.local-fs.write-limit.bytes,"Limit on the byte written to the local file system by each task. This limit only applies to writes that go through the Hadoop filesystem APIs within the task process (i.e.: writes that will update the local filesystem's BYTES_WRITTEN counter). It does not cover other writes such as logging, sideband writes from subprocesses (e.g.: streaming jobs), etc. Negative values disable the limit. default is -1",1,3,reliability-tradeoff,mapreduce
4542,mapreduce.task.merge.progress.records,The number of records to process during merge before sending a progress notification to the MR ApplicationMaster.,1,1,resource,mapreduce
4543,mapreduce.task.profile,"To set whether the system should collect profiler information for some of the tasks in this job? The information is stored in the user log directory. The value is ""true"" if task profiling is enabled.",1,6,function-tradeoff,mapreduce
4544,mapreduce.task.profile.map.params,Map-task-specific JVM profiler parameters. See mapreduce.task.profile.params,0,0,others,mapreduce
4545,mapreduce.task.profile.maps,To set the ranges of map tasks to profile. mapreduce.task.profile has to be set to true for the value to be accounted.,0,0,others,mapreduce
4548,mapreduce.task.profile.reduces,To set the ranges of reduce tasks to profile. mapreduce.task.profile has to be set to true for the value to be accounted.,0,0,others,mapreduce
4550,mapreduce.task.timeout,"The number of milliseconds before a task will be terminated if it neither reads an input, writes an output, nor updates its status string. A value of 0 disables the timeout.",0,0,others,mapreduce
4551,mapreduce.task.userlog.limit.kb,The maximum size of user-logs of each task in KB. 0 disables the cap.,0,0,others,mapreduce
4552,yarn.app.mapreduce.am.admin.user.env,Environment variables for the MR App Master processes for admin purposes. These values are set first and can be overridden by the user env (yarn.app.mapreduce.am.env) Example : 1) A=foo This will set the env variable A to foo 2) B=$B:c This is inherit app master's B env variable.,0,0,others,mapreduce
4553,yarn.app.mapreduce.am.admin-command-opts,Java opts for the MR App Master processes for admin purposes. It will appears before the opts set by yarn.app.mapreduce.am.command-opts and thus its options can be overridden user. Usage of -Djava.library.path can cause programs to no longer function if hadoop native libraries are used. These values should instead be set as part of LD_LIBRARY_PATH in the map / reduce JVM env using the mapreduce.map.env and mapreduce.reduce.env config settings.,0,0,others,mapreduce
4554,yarn.app.mapreduce.am.command-opts,"Java opts for the MR App Master processes. The following symbol, if present, will be interpolated: @taskid@ is replaced by current TaskID. Any other occurrences of '@' will go unchanged. For example, to enable verbose gc logging to a file named for the taskid in /tmp and to set the heap maximum to be a gigabyte, pass a 'value' of: -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc Usage of -Djava.library.path can cause programs to no longer function if hadoop native libraries are used. These values should instead be set as part of LD_LIBRARY_PATH in the map / reduce JVM env using the mapreduce.map.env and mapreduce.reduce.env config settings.",0,0,others,mapreduce
4555,yarn.app.mapreduce.am.container.log.backups,"Number of backup files for the ApplicationMaster logs when using ContainerRollingLogAppender (CRLA). See org.apache.log4j.RollingFileAppender.maxBackupIndex. By default, ContainerLogAppender (CLA) is used, and container logs are not rolled. CRLA is enabled for the ApplicationMaster when both yarn.app.mapreduce.am.container.log.limit.kb and yarn.app.mapreduce.am.container.log.backups are greater than zero.",1,3,reliability-tradeoff,mapreduce
4556,yarn.app.mapreduce.am.container.log.limit.kb,The maximum size of the MRAppMaster attempt container logs in KB. 0 disables the cap.,0,0,others,mapreduce
4558,yarn.app.mapreduce.am.env,User added environment variables for the MR App Master processes. Example : 1) A=foo This will set the env variable A to foo 2) B=$B:c This is inherit tasktracker's B env variable.,0,0,others,mapreduce
4559,yarn.app.mapreduce.am.hard-kill-timeout-ms,Number of milliseconds to wait before the job client kills the application.,0,0,others,mapreduce
4560,yarn.app.mapreduce.am.job.client.port-range,"Range of ports that the MapReduce AM can use when binding. Leave blank if you want all possible ports. For example 50000-50050,50100-50200",0,0,others,mapreduce
4561,yarn.app.mapreduce.am.job.committer.cancel-timeout,The amount of time in milliseconds to wait for the output committer to cancel an operation if the job is killed,0,0,others,mapreduce
4562,yarn.app.mapreduce.am.job.committer.commit-window,"Defines a time window in milliseconds for output commit operations. If contact with the RM has occurred within this window then commits are allowed, otherwise the AM will not allow output commits until contact with the RM has been re-established.",0,0,others,mapreduce
4564,yarn.app.mapreduce.am.log.level,"The logging level for the MR ApplicationMaster. The allowed levels are: OFF, FATAL, ERROR, WARN, INFO, DEBUG, TRACE and ALL. The setting here could be overriden if ""mapreduce.job.log4j-properties-file"" is set.",1,6,function-tradeoff,mapreduce
4565,yarn.app.mapreduce.am.resource.cpu-vcores,The number of virtual CPU cores the MR AppMaster needs.,1,1,resource,mapreduce
4566,yarn.app.mapreduce.am.resource.mb,The amount of memory the MR AppMaster needs.,1,1,resource,mapreduce
4567,yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms,The interval in ms at which the MR AppMaster should send heartbeats to the ResourceManager,0,0,others,mapreduce
4568,yarn.app.mapreduce.am.staging-dir,The staging dir used while submitting jobs.,0,0,others,mapreduce
4570,yarn.app.mapreduce.client.job.max-retries,"The number of retries the client will make for getJob and dependent calls. This is needed for non-HDFS DFS where additional, high level retries are required to avoid spurious failures during the getJob call. 30 is a good value for WASB",0,0,others,mapreduce
4571,yarn.app.mapreduce.client.job.retry-interval,The delay between getJob retries in ms for retries configured with yarn.app.mapreduce.client.job.max-retries.,0,0,others,mapreduce
4573,yarn.app.mapreduce.client-am.ipc.max-retries,The number of client retries to the AM - before reconnecting to the RM to fetch Application Status.,0,0,others,mapreduce
4574,yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts,The number of client retries on socket timeouts to the AM - before reconnecting to the RM to fetch Application Status.,0,0,others,mapreduce
4576,yarn.app.mapreduce.shuffle.log.limit.kb,Maximum size of the syslog.shuffle file in kilobytes (0 for no limit).,0,0,others,mapreduce
4577,yarn.app.mapreduce.shuffle.log.separate,If enabled ('true') logging generated by the client-side shuffle classes in a reducer will be written in a dedicated log file 'syslog.shuffle' instead of 'syslog'.,0,0,others,mapreduce
4579,alter_algorithm,"The implied ALGORITHM for ALTER TABLE if no ALGORITHM clause is specified. The deprecated variable old_alter_table is an alias for this.
COPY corresponds to the pre-MySQL 5.1 approach of creating an intermediate table, copying data one row at a time, and renaming and dropping tables. 
INPLACE requests that the operation be refused if it cannot be done natively inside a the storage engine. 
DEFAULT (the default) chooses INPLACE if available, and falls back to COPY. 
NOCOPY refuses to copy a table. 
INSTANT refuses an operation that would involve any other than metadata changes.",0,0,others,mariadb
4580,analyze_sample_percentage,Percentage of rows from the table ANALYZE TABLE will sample to collect table statistics. Set to 0 to let MariaDB decide what percentage of rows to sample.,0,0,others,mariadb
4583,back_log,"Connections take a small amount of time to start, and this setting determines the number of outstanding connection requests MariaDB can have, or the size of the listen queue for incoming TCP/IP requests. Requests beyond this will be refused. Increase if you expect short bursts of connections. Cannot be set higher than the operating system limit (see the Unix listen() man page). If not set, set to 0, or the --autoset-back-log option is used, will be autoset to the lower of 900 and (50 + max_connections/5) (>= MariaDB 10.1.7).",1,5,workload-specific,mariadb
4584,basedir,Path to the MariaDB installation directory. Other paths are usually resolved relative to this base directory.,0,0,others,mariadb
4585,big_tables,"If this system variable is set to 1, then temporary tables will be saved to disk intead of memory.
This system variable's original intention was to allow result sets that were too big for memory-based temporary tables and to avoid the resulting 'table full' errors. 
This system variable is no longer needed, because the server can automatically convert large memory-based temporary tables into disk-based temporary tables when they exceed the value of the mp_memory_table_size system variable.
To prevent memory-based temporary tables from being used at all, set the tmp_memory_table_size system variable to 0.
In MariaDB 5.5 and earlier, sql_big_tables is a synonym.
In MariaDB 10.5, this system variable is deprecated.",0,0,others,mariadb
4586,bind_address,"By default, the MariaDB server listens for TCP/IP connections on a network socket bound to a single address, 0.0.0.0. You can specify an alternative when the server starts using this option; either a host name, an IPv4 or an IPv6 address. In Debian and Ubuntu, the default bind_address is 127.0.0.1, which binds the server to listen on localhost only. bind_address has always been available as a mysqld option, from MariaDB 10.3.3 its also available as a system variable.",0,0,others,mariadb
4587,bulk_insert_buffer_size,Size in bytes of the per-thread cache tree used to speed up bulk inserts into MyISAM and Aria tables. A value of 0 disables the cache tree.,1,1,resource,mariadb
4588,character_set_client,"Determines the character set for queries arriving from the client. It can be set per session by the client, although the server can be configured to ignore client requests with the --skip-character-set-client-handshake option. If the client does not request a character set, or requests a character set that the server does not support, the global value will be used. utf16, utf32 and ucs2 cannot be used as client character sets.",0,0,others,mariadb
4589,character_set_connection,"Character set used for number to string conversion, as well as for literals that don't have a character set introducer.",0,0,others,mariadb
4590,character_set_database,"Character set used by the default database, and set by the server whenever the default database is changed. If there's no default database, character_set_database contains the same value as character_set_server. This variable is dynamic, but should not be set manually, only by the server.",0,0,others,mariadb
4591,character_set_filesystem,"The character set for the filesystem. Used for converting file names specified as a string literal from character_set_client to character_set_filesystem before opening the file. By default set to binary, so no conversion takes place. This could be useful for statements such as LOAD_FILE() or LOAD DATA INFILE on system where multi-byte file names are use.",0,0,others,mariadb
4592,character_set_results,Character set used for results and error messages returned to the client.,0,0,others,mariadb
4593,character_set_server,"Default character set used by the server. See character_set_database for character sets used by the default database. Defaults may be different on some systems, see for example Differences in MariaDB in Debian.",0,0,others,mariadb
4594,character_set_system,"Character set used by the server to store identifiers, always set to utf8.",0,0,others,mariadb
4595,character_sets_dir,Directory where the character sets are installed.,0,0,others,mariadb
4596,check_constraint_checks,"If set to 0, will disable constraint checks, for example when loading a table that violates some constraints that you plan to fix later.",1,2,security-tradeoff,mariadb
4597,collation_connection,Collation used for the connection character set.,0,0,others,mariadb
4598,collation_database,"Collation used for the default database. Set by the server if the default database changes, if there is no default database the value from the collation_server variable is used. This variable is dynamic, but should not be set manually, only by the server.",0,0,others,mariadb
4599,collation_server,"Default collation used by the server. This is set to the default collation for a given character set automatically when character_set_server is changed, but it can also be set manually. Defaults may be different on some systems, see for example Differences in MariaDB in Debian.",0,0,others,mariadb
4600,completion_type,"The transaction completion type. If set to NO_CHAIN or 0 (the default), there is no effect on commits and rollbacks. If set to CHAIN or 1, a COMMIT statement is equivalent to COMMIT AND CHAIN, while a ROLLBACK is equivalent to ROLLBACK AND CHAIN, so a new transaction starts straight away with the same isolation level as transaction that's just finished. If set to RELEASE or 2, a COMMIT statement is equivalent to COMMIT RELEASE, while a ROLLBACK is equivalent to ROLLBACK RELEASE, so the server will disconnect after the transaction completes. Note that the transaction completion type only applies to explicit commits, not implicit commits.",1,3,reliability-tradeoff,mariadb
4601,concurrent_insert,"If set to AUTO or 1, the default, MariaDB allows concurrent INSERTs and SELECTs for MyISAM tables with no free blocks in the data (deleted rows in the middle). If set to NEVER or 0, concurrent inserts are disabled. If set to ALWAYS or 2, concurrent inserts are permitted for all MyISAM tables, even those with holes, in which case new rows are added at the end of a table if the table is being used by another thread. If the --skip-new option is used when starting the server, concurrent_insert is set to NEVER. Changing the variable only affects new opened tables. Use FLUSH TABLES If you want it to also affect cached tables. See Concurrent Inserts for more.",1,4,limited-side-effect,mariadb
4602,connect_timeout,"Time in seconds that the server waits for a connect packet before returning a 'Bad handshake'. Increasing may help if clients regularly encounter 'Lost connection to MySQL server at 'X', system error: error_number' type-errors.",0,0,others,mariadb
4603,core_file,"Write a core-file on crashes. The file name and location are system dependent. On Linux it is usually called core.${PID}, and it is usually written to the data directory. However, this can be changed.
See Enabling Core Dumps for more information.
Previously this system variable existed only as an option, but it was also made into a read-only system variable starting with MariaDB 10.3.9, MariaDB 10.2.17 and MariaDB 10.1.35.
On Windows >= MariaDB 10.4.3, this option is set by default. 
Note that the option accepts no arguments; specifying --core-file sets the value to ON. It cannot be disabled in the case of Windows >= MariaDB 10.4.3.",0,0,others,mariadb
4604,datadir,Directory where the data is stored.,0,0,others,mariadb
4605,date_format,Unused.,0,0,others,mariadb
4607,debug,Available in debug builds only (built with -DWITH_DEBUG=1). Used in debugging through the DBUG library to write to a trace file. Just using --debug will write a trace of what mysqld is doing to the default trace file.,0,0,others,mariadb
4608,debug_no_thread_alarm,"Disable system thread alarm calls. Disabling it may be useful in debugging or testing, never do it in production.",1,6,function-tradeoff,mariadb
4609,debug_sync,Used in debugging to show the interface to the Debug Sync facility. MariaDB needs to be configured with -DENABLE_DEBUG_SYNC=1 for this variable to be available.,1,3,reliability-tradeoff,mariadb
4610,default_password_lifetime,"This defines the global password expiration policy. 0 means automatic password expiration is disabled. If the value is a positive integer N, the passwords must be changed every N days. This behavior can be overridden using the password expiration options in ALTER USER.",0,0,others,mariadb
4611,default_regex_flags,Introduced to address remaining incompatibilities between PCRE and the old regex library. Accepts a comma-separated list of zero or more of the following values:,0,0,others,mariadb
4612,default_storage_engine,The default storage engine. The default storage engine must be enabled at server startup or the server won't start.,0,0,others,mariadb
4613,default_table_type,A synonym for default_storage_engine. Removed in MariaDB 5.5.,0,0,others,mariadb
4614,default_tmp_storage_engine,"Default storage engine that will be used for tables created with CREATE TEMPORARY TABLE where no engine is specified. For internal temporary tables see aria_used_for_temp_tables). The storage engine used must be active or the server will not start. See default_storage_engine for the default for non-temporary tables. Defaults to NULL, in which case the value from default_storage_engine is used.",0,0,others,mariadb
4616,delay_key_write,"Specifies how MyISAM tables handles CREATE TABLE DELAY_KEY_WRITE. If set to ON, the default, any DELAY KEY WRITEs are honored. The key buffer is then flushed only when the table closes, speeding up writes. MyISAM tables should be automatically checked upon startup in this case, and --external locking should not be used, as it can lead to index corruption. If set to OFF, DELAY KEY WRITEs are ignored, while if set to ALL, all new opened tables are treated as if created with DELAY KEY WRITEs enabled.",1,3,reliability-tradeoff,mariadb
4618,delayed_insert_timeout,Time in seconds that the INSERT DELAYED handler will wait for INSERTs before terminating.,0,0,others,mariadb
4620,disconnect_on_expired_password,"When a user password has expired (see User Password Expiry), this variable controls how the server handles clients that are not aware of the sandbox mode. If enabled, the client is not permitted to connect, otherwise the server puts the client in a sandbox mode.",1,2,security-tradeoff,mariadb
4621,div_precision_increment,"The precision of the result of the decimal division will be the larger than the precision of the dividend by that number. By default it's 4, so SELECT 2/15 would return 0.1333 and SELECT 2.0/15 would return 0.13333. After setting div_precision_increment to 6, for example, the same operation would return 0.133333 and 0.1333333 respectively.",0,0,others,mariadb
4622,encrypt_tmp_disk_tables,Enables automatic encryption of all internal on-disk temporary tables that are created during query execution if aria_used_for_temp_tables=ON is set. See Data at Rest Encryption and Enabling Encryption for Internal On-disk Temporary Tables.,1,2,security-tradeoff,mariadb
4624,encryption_algorithm,Which encryption algorithm to use for table encryption. aes_cbc is the recommended one. See Table and Tablespace Encryption.,1,2,security-tradeoff,mariadb
4625,enforce_storage_engine,"Force the use of a particular storage engine for new tables. Used to avoid unwanted creation of tables using another engine. For example, setting to InnoDB will prevent any MyISAM tables from being created. If another engine is specified in a CREATE TABLE statement, the outcome depends on whether the NO_ENGINE_SUBSTITUTION SQL_MODE has been set or not. If set (the default from MariaDB 10.1.7), the query will fail, while if not set, a warning will be returned and the table created according to the engine specified by this variable. The variable has a session scope, but is only modifiable by a user with the SUPER privilege.",0,0,others,mariadb
4627,eq_range_index_dive_limit,"Limit used for speeding up queries listed by long nested INs. The optimizer will use existing index statistics instead of doing index dives for equality ranges if the number of equality ranges for the index is larger than or equal to this number. If set to 0 (unlimited, the default), index dives are always used.",1,5,workload-specific,mariadb
4628,error_count,Read-only variable denoting the number of errors from the most recent statement in the current session that generated errors. See SHOW_ERRORS().,0,0,others,mariadb
4630,expensive_subquery_limit,"Number of rows to be examined for a query to be considered expensive, that is, maximum number of rows a subquery may examine in order to be executed during optimization and used for constant optimization.",0,0,others,mariadb
4631,explicit_defaults_for_timestamp,"This option causes CREATE TABLE to create all TIMESTAMP columns as NULL with the DEFAULT NULL attribute, Without this option, TIMESTAMP columns are NOT NULL and have implicit DEFAULT clauses.",0,0,others,mariadb
4632,external_user,External user name set by the plugin used to authenticate the client. NULL if native MariaDB authentication is used.,0,0,others,mariadb
4634,flush_time,"Interval in seconds that tables are closed to synchronize (flush) data to disk and free up resources. If set to 0, the default, there is no automatic synchronizing tables and closing of tables. This option should not be necessary on systems with sufficient resources.",0,0,others,mariadb
4635,foreign_key_checks,"If set to 1 (the default) foreign key constraints (including ON UPDATE and ON DELETE behavior) InnoDB tables are checked, while if set to 0, they are not checked. 0 is not recommended for normal use, though it can be useful in situations where you know the data is consistent, but want to reload data in a different order from that that specified by parent/child relationships. Setting this variable to 1 does not retrospectively check for inconsistencies introduced while set to 0.",1,3,reliability-tradeoff,mariadb
4636,ft_boolean_syntax,"List of operators supported by an IN BOOLEAN MODE full-text search. If you wish to change, note that each character must be ASCII and non-alphanumeric, the full string must be 14 characters and the first or second character must be a space. Positions 10, 13 and 14 are reserved for future extensions. Also, no duplicates are permitted except for the phrase quoting characters in positions 11 and 12, which may be the same.",0,0,others,mariadb
4637,ft_max_word_len,"Maximum length for a word to be included in the MyISAM full-text index. If this variable is changed, the full-text index must be rebuilt. The quickest way to do this is by issuing a REPAIR TABLE table_name QUICK statement. See innodb_ft_max_token_size for the InnoDB equivalent.",1,1,resource,mariadb
4638,ft_min_word_len,"Minimum length for a word to be included in the MyISAM full-text index. If this variable is changed, the full-text index must be rebuilt. The quickest way to do this is by issuing a REPAIR TABLE table_name QUICK statement. See innodb_ft_min_token_size for the InnoDB equivalent.",1,1,resource,mariadb
4639,ft_query_expansion_limit,"For full-text searches, denotes the numer of top matches when using WITH QUERY EXPANSION.",0,0,others,mariadb
4640,ft_stopword_file,"File containing a list of stopwords for use in MyISAM full-text searches. Unless an absolute path is specified the file will be looked for in the data directory. The file is not parsed for comments, so all words found become stopwords. By default, a built-in list of words (built from storage/myisam/ft_static.c file) is used. Stopwords can be disabled by setting this variable to '' (an empty string). If this variable is changed, the full-text index must be rebuilt. The quickest way to do this is by issuing a REPAIR TABLE table_name QUICK statement. See innodb_ft_server_stopword_table for the InnoDB equivalent.",0,0,others,mariadb
4641,general_log,"If set to 0, the default unless the --general-log option is used, the general query log is disabled, while if set to 1, the general query log is enabled. See log_output for how log files are written. If that variable is set to NONE, no logs will be written even if general_query_log is set to 1.",0,0,others,mariadb
4642,general_log_file,"Name of the general query log file. If this is not specified, the name is taken from the log-basename setting or from your system hostname with .log as a suffix.",0,0,others,mariadb
4643,group_concat_max_len,Maximum length in bytes of the returned result for a GROUP_CONCAT() function.,0,0,others,mariadb
4644,have_compress,"If the zlib compression library is accessible to the server, this will be set to YES, otherwise it will be NO. The COMPRESS() and UNCOMPRESS() functions will only be available if set to YES.",1,4,limited-side-effect,mariadb
4646,have_csv,"If the server supports CSV tables, will be set to YES, otherwise will be set to NO. Removed in MariaDB 10.0, use the Information Schema PLUGINS table or SHOW ENGINES instead.",0,0,others,mariadb
4647,have_dynamic_loading,"If the server supports dynamic loading of plugins, will be set to YES, otherwise will be set to NO.",0,0,others,mariadb
4648,have_geometry,"If the server supports spatial data types, will be set to YES, otherwise will be set to NO.",0,0,others,mariadb
4649,have_ndbcluster,If the server supports NDBCluster (disabled in MariaDB).,0,0,others,mariadb
4650,have_partitioning,"If the server supports partitioning, will be set to YES, unless the --skip-partition option is used, in which case will be set to DISABLED. Will be set to NO otherwise. Removed in MariaDB 10.0 - SHOW PLUGINS should be used instead.",0,0,others,mariadb
4652,have_query_cache,"If the server supports the query cache, will be set to YES, otherwise will be set to NO.",1,4,limited-side-effect,mariadb
4653,have_rtree_keys,"If RTREE indexes (used for spatial indexes) are available, will be set to YES, otherwise will be set to NO.",0,0,others,mariadb
4654,have_symlink,"This system variable can be used to determine whether the server supports symbolic links (note that it has no meaning on Windows).
If symbolic links are supported, then the value will be YES.
If symbolic links are not supported, then the value will be NO.
If symbolic links are disabled with the --symbolic-links option and the skip option prefix (i.e. --skip-symbolic-links), then the value will be DISABLED.
Symbolic link support is required for the INDEX DIRECTORY and DATA DIRECTORY table options.",0,0,others,mariadb
4655,histogram_size,"Number of bytes used for a histogram. If set to 0, no histograms are created by ANALYZE.",1,1,resource,mariadb
4656,histogram_type,"Specifies the type of histograms created by ANALYZE. 
SINGLE_PREC_HB - single precision height-balanced.
DOUBLE_PREC_HB - double precision height-balanced.",0,0,others,mariadb
4657,host_cache_size,"Number of host names that will be cached to avoid resolving. Setting to 0 disables the cache. Changing the value while the server is running causes an implicit FLUSH HOSTS, clearing the host cache and truncating the performance_schema.host_cache table. If you are connecting from a lot of different machines you should consider increasing.",1,1,resource,mariadb
4658,hostname,"When the server starts, this variable is set to the server host name.",0,0,others,mariadb
4660,idle_readonly_transaction_timeout,"Time in seconds that the server waits for idle read-only transactions before killing the connection. If set to 0, the default, connections are never killed. See also idle_transaction_timeout, idle_write_transaction_timeout and Transaction Timeouts.",0,0,others,mariadb
4661,idle_transaction_timeout,"Time in seconds that the server waits for idle transactions before killing the connection. If set to 0, the default, connections are never killed. See also idle_readonly_transaction_timeout, idle_write_transaction_timeout and Transaction Timeouts.",0,0,others,mariadb
4662,idle_write_transaction_timeout,"Time in seconds that the server waits for idle read-write transactions before killing the connection. If set to 0, the default, connections are never killed. See also idle_transaction_timeout, idle_readonly_transaction_timeout and Transaction Timeouts. Called idle_readwrite_transaction_timeout until MariaDB 10.3.2.",0,0,others,mariadb
4663,ignore_db_dirs,"Tells the server that this directory can never be a database. That means two things - firstly it is ignored by the SHOW DATABASES command and INFORMATION_SCHEMA tables. And secondly, USE, CREATE DATABASE and SELECT statements will return an error if the database from the ignored list specified. Use this option several times if you need to ignore more than one directory. To make the list empty set the void value to the option as --ignore-db-dir=. If the option or configuration is specified multiple times, viewing this value will list the ignore directories separated by a period.",0,0,others,mariadb
4664,in_predicate_conversion_threshold,The minimum number of scalar elements in the value list of an IN predicate that triggers its conversion to an IN subquery. Set to 0 to disable the conversion. See Conversion of Big IN Predicates Into Subqueries.,1,5,workload-specific,mariadb
4665,in_transaction,"Session-only and read-only variable that is set to 1 if a transaction is in progress, 0 if not.",0,0,others,mariadb
4666,init_connect,"String containing one or more SQL statements, separated by semicolons, that will be executed by the server for each client connecting. If there's a syntax error in the one of the statements, the client will fail to connect. For this reason, the statements are not executed for users with the SUPER privilege or, from MariaDB 10.5.2, the CONNECTION ADMIN privilege, who can then still connect and correct the error. See also init_file.",0,0,others,mariadb
4667,init_file,"Name of a file containing SQL statements that will be executed by the server on startup. Each statement should be on a new line, and end with a semicolon. See also init_connect.",0,0,others,mariadb
4669,interactive_timeout,Time in seconds that the server waits for an interactive connection (one that connects with the mysql_real_connect() CLIENT_INTERACTIVE option) to become active before closing it. See also wait_timeout.,0,0,others,mariadb
4670,join_buffer_size,"Minimum size in bytes of the buffer used for queries that cannot use an index, and instead perform a full table scan. Increase to get faster full joins when adding indexes is not possible, although be aware of memory issues, since joins will always allocate the minimum size. Best left low globally and set high in sessions that require large full joins. In 64-bit platforms, Windows truncates values above 4GB to 4GB with a warning. See also Block-Based Join Algorithms - Size of Join Buffers.",1,1,resource,mariadb
4673,keep_files_on_create,"If a MyISAM table is created with no DATA DIRECTORY option, the .MYD file is stored in the database directory. When set to 0, the default, if MariaDB finds another .MYD file in the database directory it will overwrite it. Setting this variable to 1 means that MariaDB will return an error instead, just as it usually does in the same situation outside of the database directory. The same applies for .MYI files and no INDEX DIRECTORY option.",0,0,others,mariadb
4674,large_files_support,"ON if the server if was compiled with large file support or not, else OFF",0,0,others,mariadb
4675,large_page_size,Indicates the size of memory page if large page support (Linux only) is enabled. The page size is determined from the Hugepagesize setting in /proc/meminfo. See large_pages. Deprecated and unused in MariaDB 10.5.3 since multiple page size support was added.,1,5,workload-specific,mariadb
4676,large_pages,"Indicates whether large page support (Linux only - called huge pages) is used. This is set with --large-pages or disabled with --skip-large-pages. Large pages are used for the innodb buffer pool and for online DDL (of size 3* innodb_sort_buffer_size (or 6 when encryption is used)). To use large pages, the Linux sysctl variable kernel.shmmax must be large than the llocation. Also the sysctl variable vm.nr_hugepages multipled by large-page) must be larger than the usage. The ulimit for locked memory must be sufficient to cover the amount used (ulimit -l and equalivent in /etc/security/limits.conf / or in systemd LimitMEMLOCK). If these operating system controls or insufficient free huge pages are available, the allocation of large pages will fall back to conventional memory allocation and a warning will appear in the logs. Only allocations of the default Hugepagesize currently occur (see /proc/meminfo).",1,4,limited-side-effect,mariadb
4678,lc_messages,"This system variable can be specified as a locale name. The language of the associated locale will be used for error messages. See Server Locales for a list of supported locales and their associated languages.
This system variable is set to en_US by default, which means that error messages are in English by default.
If this system variable is set to a valid locale name, but the server can't find an error message file for the language associated with the locale, then the default language will be used instead.
This system variable is used along with the lc_messages_dir system variable to construct the path to the error messages file.
See Setting the Language for Error Messages for more information.",0,0,others,mariadb
4679,lc_messages_dir,"This system variable can be specified either as the path to the directory storing the server's error message files or as the path to the directory storing the specific language's error message file. See Server Locales for a list of available locales and their related languages.
The server initially tries to interpret the value of this system variable as a path to the directory storing the server's error message files. Therefore, it constructs the path to the language's error message file by concatenating the value of this system variable with the language name of the locale specified by the lc_messages system variable .
If the server does not find the error message file for the language, then it tries to interpret the value of this system variable as a direct path to the directory storing the specific language's error message file.
See Setting the Language for Error Messages for more information.",0,0,others,mariadb
4680,lc_time_names,"The locale that determines the language used for the date and time functions DAYNAME(), MONTHNAME() and DATE_FORMAT(). Locale names are language and region subtags, for example 'en_ZA' (English - South Africa) or 'es_US: Spanish - United States'. The default is always 'en-US' regardless of the system's locale setting. See server locale for a full list of supported locales.",0,0,others,mariadb
4681,license,"Server license, for example GPL.",0,0,others,mariadb
4682,local_infile,"If set to 1, LOCAL is supported for LOAD DATA INFILE statements. If set to 0, usually for security reasons, attempts to perform a LOAD DATA LOCAL will fail with an error message.",1,2,security-tradeoff,mariadb
4683,lock_wait_timeout,"Timeout in seconds for attempts to acquire metadata locks. Statements using metadata locks include FLUSH TABLES WITH READ LOCK, LOCK TABLES, HANDLER and DML and DDL operations on tables, stored procedures and functions, and views. The timeout is separate for each attempt, of which there may be multiple in a single statement. 0 (from MariaDB 10.3.0) means no wait. See WAIT and NOWAIT.",0,0,others,mariadb
4684,locked_in_memory,Indicates whether --memlock was used to lock mysqld in memory.,0,0,others,mariadb
4685,log,"Deprecated and removed in MariaDB 10.0, use general_log instead.",0,0,others,mariadb
4686,log_disabled_statements,"If set, the specified type of statements (slave or stored procedure statements) will not be logged to the general log.",0,0,others,mariadb
4687,log_error,"Specifies the name of the error log. If --console is specified later in the configuration (Windows only) or this option isn't specified, errors will be logged to stderr. If no name is provided, errors will still be logged to hostname.err in the datadir directory by default. If a configuration file sets --log-error, one can reset it with --skip-log-error (useful to override a system wide configuration file). MariaDB always writes its error log, but the destination is configurable. See error log for details.",0,0,others,mariadb
4688,log_output,"How the output for the general query log and the slow query log is stored. By default written to file (FILE), it can also be stored in the general_log and slow_log tables in the mysql database (TABLE), or not stored at all (NONE). More than one option can be chosen at the same time, with NONE taking precedence if present. Logs will not be written if logging is not enabled. See Writing logs into tables, and the slow_query_log and general_log server system variables.",0,0,others,mariadb
4689,log_queries_not_using_indexes,"Queries that don't use an index, or that perform a full index scan where the index doesn't limit the number of rows, will be logged to the slow query log (regardless of time taken). The slow query log needs to be enabled for this to have an effect.",0,0,others,mariadb
4691,log_slow_disabled_statements,"If set, the specified type of statements will not be logged to the slow query log. See also log_slow_admin_statements and log_slow_filter.",0,0,others,mariadb
4693,log_slow_queries,"Deprecated and removed in MariaDB 10.0, use slow_query_log instead.",0,0,others,mariadb
4694,log_slow_rate_limit,"The slow query log will log every this many queries. The default is 1, or every query, while setting it to 20 would log every 20 queries, or five percent. Aims to reduce I/O usage and excessively large slow query logs. See also Slow Query Log Extended Statistics.",1,6,function-tradeoff,mariadb
4695,log_slow_verbosity,"Controls information to be added to the slow query log. Options are added in a comma-delimited string. See also Slow Query Log Extended Statistics. log_slow_verbosity is not supported when log_output='TABLE'.
query_plan logs query execution plan information
innodb an unused Percona XtraDB option for logging XtraDB/InnoDB statistics.
explain prints EXPLAIN output in the slow query log. See EXPLAIN in the Slow Query Log.",0,0,others,mariadb
4696,log_tc_size,"Defines the size in bytes of the memory-mapped file-based transaction coordinator log, which is only used if the binary log is disabled. If you have two or more XA-capable storage engines enabled, then a transaction coordinator log must be available. This size is defined in multiples of 4096. This size could always be set as a commandline option, but it was made into a system variable in MariaDB 10.1.3. See Transaction Coordinator Log for more information. Also see the --log-tc server option and the --tc-heuristic-recover option.",1,5,workload-specific,mariadb
4697,log_warnings,"Determines which additional warnings are logged. Setting to 0 disables additional warning logging. Note that this does not prevent all warnings, there is a core set of warnings that will always be written to the error log.",1,6,function-tradeoff,mariadb
4698,long_query_time,"If a query takes longer than this many seconds to execute (microseconds can be specified too), the Slow_queries status variable is incremented and, if enabled, the query is logged to the slow query log.",0,0,others,mariadb
4699,low_priority_updates,"If set to 1 (0 is the default), for storage engines that use only table-level locking (Aria, MyISAM, MEMORY and MERGE), all INSERTs, UPDATEs, DELETEs and LOCK TABLE WRITEs will wait until there are no more SELECTs or LOCK TABLE READs pending on the relevant tables. Set this to 1 if reads are prioritized over writes. 
In MariaDB 5.5 and earlier, sql_low_priority_updates is a synonym.",0,0,others,mariadb
4700,lower_case_file_system,"Read-only variable describing whether the file system is case-sensitive. If set to OFF, file names are case-sensitive. If set to ON, they are not case-sensitive.",0,0,others,mariadb
4701,lower_case_table_names,"If set to 0 (the default on Unix-based systems), table names and aliases and database names are compared in a case-sensitive manner. If set to 1 (the default on Windows), names are stored in lowercase and not compared in a case-sensitive manner. If set to 2 (the default on Mac OS X), names are stored as declared, but compared in lowercase.",0,0,others,mariadb
4702,max_allowed_packet,"Maximum size in bytes of a packet or a generated/intermediate string. The packet message buffer is initialized with the value from net_buffer_length, but can grow up to max_allowed_packet bytes. Set as large as the largest BLOB, in multiples of 1024. If this value is changed, it should be changed on the client side as well. See slave_max_allowed_packet for a specific limit for replication purposes.",1,1,resource,mariadb
4703,max_connect_errors,"Limit to the number of successive failed connects from a host before the host is blocked from making further connections. The count for a host is reset to zero if they successfully connect. To unblock, flush the host cache with a FLUSH HOSTS statement or mysqladmin flush-hosts.",1,3,reliability-tradeoff,mariadb
4704,max_connections,The maximum number of simultaneous client connections. See also Handling Too Many Connections. Note that this value affects the number of file descriptors required on the operating system. Minimum was changed from 1 to 10 to avoid possible unexpected results for the user (MDEV-18252).,1,1,resource,mariadb
4705,max_delayed_threads,"Limits to the number of INSERT DELAYED threads. Once this limit is reached, the insert is handled as if there was no DELAYED attribute. If set to 0, DELAYED is ignored entirely. The session value can only be set to 0 or to the same as the global value.",1,1,resource,mariadb
4706,max_digest_length,"Maximum length considered for computing a statement digest, such as used by the Performance Schema and query rewrite plugins. Statements that differ after this many bytes produce the same digest, and are aggregated for statistics purposes. The variable is allocated per session. Increasing will allow longer statements to be distinguished from each other, but increase memory use, while decreasing will reduce memory use, but more statements may become indistinguishable.",1,5,workload-specific,mariadb
4707,max_error_count,Specifies the maximum number of messages stored for display by SHOW ERRORS and SHOW WARNINGS statements.,0,0,others,mariadb
4708,max_heap_table_size,"Maximum size in bytes for user-created MEMORY tables. Setting the variable while the server is active has no effect on existing tables unless they are recreated or altered. The smaller of max_heap_table_size and tmp_table_size also limits internal in-memory tables. When the maximum size is reached, any further attempts to insert data will receive a ""table ... is full"" error. Temporary tables created with CREATE TEMPORARY will not be converted to Aria, as occurs with internal temporary tables, but will also receive a table full error.",1,1,resource,mariadb
4709,max_insert_delayed_threads,Synonym for max_delayed_threads.,0,0,others,mariadb
4710,max_join_size,"Statements will not be performed if they are likely to need to examine more than this number of rows, row combinations or do more disk seeks. Can prevent poorly-formatted queries from taking server resources. Changing this value to anything other the default will reset sql_big_selects to 0. If sql_big_selects is set again, max_join_size will be ignored. This limit is also ignored if the query result is sitting in the query cache. Previously named sql_max_join_size, which is still a synonym.",1,1,resource,mariadb
4711,max_length_for_sort_data,"Used to decide which algorithm to choose when sorting rows. If the total size of the column data, not including columns that are part of the sort, is less than max_length_for_sort_data, then we add these to the sort key. This can speed up the sort as we don't have to re-read the same row again later. Setting the value too high can slow things down as there will be a higher disk activity for doing the sort.",1,5,workload-specific,mariadb
4712,max_long_data_size,"Maximum size for parameter values sent with mysql_stmt_send_long_data(). If not set, will default to the value of max_allowed_packet. Deprecated in MariaDB 5.5 and removed in MariaDB 10.5.0; use max_allowed_packet instead.",0,0,others,mariadb
4713,max_password_errors,"The maximum permitted number of failed connection attempts due to an invalid password before a user is blocked from further connections. FLUSH_PRIVILEGES will permit the user to connect again. This limit is ignored for users with the SUPER privilege or, from MariaDB 10.5.2, the CONNECTION ADMIN privilege.",1,3,reliability-tradeoff,mariadb
4714,max_prepared_stmt_count,"Maximum number of prepared statements on the server. Can help prevent certain forms of denial-of-service attacks. If set to 0, no prepared statements are permitted on the server.",1,3,reliability-tradeoff,mariadb
4715,max_recursive_iterations,"Maximum number of iterations when executing recursive queries, used to prevent infinite loops in recursive CTEs.",1,3,reliability-tradeoff,mariadb
4716,max_rowid_filter_size,The maximum size of the container of a rowid filter.,1,5,workload-specific,mariadb
4718,max_session_mem_used,Amount of memory a single user session is allowed to allocate. This limits the value of the session variable Memory_used.,1,1,resource,mariadb
4719,max_sort_length,Maximum size in bytes used for sorting data values - anything exceeding this is ignored. The server uses only the first max_sort_length bytes of each value and ignores the rest.,1,1,resource,mariadb
4720,max_sp_recursion_depth,"Permitted number of recursive calls for a stored procedure. 0, the default, no recursion is permitted. Increasing this value increases the thread stack requirements, so you may need to increase thread_stack as well. This limit doesn't apply to stored functions.",1,3,reliability-tradeoff,mariadb
4721,max_statement_time,"Maximum time in seconds that a query can execute before being aborted. This includes all queries, not just SELECT statements, but excludes statements in stored procedures. If set to 0, no limit is applied. See Aborting statements that take longer than a certain time to execute for details and limitations. Useful when combined with SET STATEMENT for limiting the execution times of individual queries.",0,0,others,mariadb
4723,max_user_connections,"Maximum simultaneous connections permitted for each user account. When set to 0, there is no per user limit. Setting it to -1 stops users without the SUPER privilege or, from MariaDB 10.5.2, the CONNECTION ADMIN privilege, from connecting to the server. The session variable is always read-only and only privileged users can modify user limits. The session variable defaults to the global max_user_connections variable, unless the user's specific MAX_USER_CONNECTIONS resource option is non-zero. When both global variable and the user resource option are set, the user's MAX_USER_CONNECTIONS is used. Note: This variable does not affect users with the SUPER privilege or, from MariaDB 10.5.2, the CONNECTION ADMIN privilege.",1,3,reliability-tradeoff,mariadb
4724,max_write_lock_count,"Read lock requests will be permitted for processing after this many write locks. Applies only to storage engines that use table level locks (thr_lock), so no effect with InnoDB or Archive.",0,0,others,mariadb
4725,metadata_locks_cache_size,"Size of the metadata locks cache, used for reducing the need to create and destroy synchronization objects. It is particularly helpful on systems where this process is inefficient, such as Windows XP.",1,1,resource,mariadb
4726,metadata_locks_hash_instances,Number of hashes used by the set of metadata locks. The metadata locks are partitioned into separate hashes in order to reduce contention.,0,0,others,mariadb
4727,min_examined_row_limit,"If a query examines more than this number of rows, it is logged to the slow query log. If set to 0, the default, no row limit is used.",1,5,workload-specific,mariadb
4728,mrr_buffer_size,Size of buffer to use when using multi-range read with range access. See Multi Range Read optimization for more information.,1,1,resource,mariadb
4729,multi_range_count,Ignored. Use mrr_buffer_size instead.,0,0,others,mariadb
4730,mysql56_temporal_format,"If set (the default), MariaDB uses the MySQL 5.6 low level formats for TIME, DATETIME and TIMESTAMP instead of the MariaDB 5.3 version. The version MySQL introduced in 5.6 requires more storage, but potentially allows negative dates and has some advantages in replication. There should be no reason to revert to the old MariaDB 5.3 microsecond format. See also MDEV-10723.",0,0,others,mariadb
4731,named_pipe,"On Windows systems, determines whether connections over named pipes are permitted.",0,0,others,mariadb
4733,net_read_timeout,Time in seconds the server will wait for a client connection to send more data before aborting the read. See also net_write_timeout and slave_net_timeout,0,0,others,mariadb
4735,net_write_timeout,Time in seconds to wait on writing a block to a connection before aborting the write. See also net_read_timeout and slave_net_timeout.,0,0,others,mariadb
4737,old_alter_table,"From MariaDB 10.3.7, an alias for alter_algorithm. Prior to that, if set to 1 (0 is default), MariaDB reverts to the non-optimized, pre-MySQL 5.1, method of processing ALTER TABLE statements. A temporary table is created, the data is copied over, and then the temporary table is renamed to the original.",0,0,others,mariadb
4738,old_mode,Used for getting MariaDB to emulate behavior from an old version of MySQL or MariaDB. See OLD Mode. Will be used to replace the old variable over time.,0,0,others,mariadb
4739,old_passwords,"If set to 1 (0 is default), MariaDB reverts to using the mysql_old_password authentication plugin by default for newly created users and passwords, instead of the mysql_native_password authentication plugin.",1,2,security-tradeoff,mariadb
4740,open_files_limit,"The number of file descriptors available to MariaDB. If you are getting the Too many open files error, then you should increase this limit. If set to 0, then MariaDB will calculate a limit based on the following: 

MAX(max_connections*5, max_connections +table_open_cache*2) 

MariaDB sets the limit with setrlimit. MariaDB cannot set this to exceed the hard limit imposed by the operating system. Therefore, you may also need to change the hard limit. There are a few ways to do so. 
If you are using mysqld_safe to start mysqld, then see the instructions at mysqld_safe: Configuring the Open Files Limit. 
If you are using systemd to start mysqld, then see the instructions at systemd: Configuring the Open Files Limit. 
Otherwise, you can change the hard limit for the mysql user account by modifying /etc/security/limits.conf. See Configuring Linux for MariaDB: Configuring the Open Files Limit for more details.",0,0,others,mariadb
4742,optimizer_search_depth,"Maximum search depth by the query optimizer. Smaller values lead to less time spent on execution plans, but potentially less optimal results. If set to 0, MariaDB will automatically choose a reasonable value. Since the better results from more optimal planning usually offset the longer time spent on planning, this is set as high as possible by default. 63 is a valid value, but its effects (switching to the original find_best search) are deprecated.",1,3,reliability-tradeoff,mariadb
4743,optimizer_selectivity_sampling_limit,Controls number of record samples to check condition selectivity,0,0,others,mariadb
4744,optimizer_switch,"A series of flags for controlling the query optimizer. See Optimizer Switch for defaults, and a comparison to MySQL.",0,0,others,mariadb
4745,optimizer_trace,"Controls tracing of the optimizer: optimizer_trace=option=val[,option=val...], where option is one of {enabled} and val is one of {on, off, default}",1,4,limited-side-effect,mariadb
4746,optimizer_trace_max_mem_size,"Limits the memory used while tracing a query by specifying the maximum allowed cumulated size, in bytes, of stored optimizer traces.",1,1,resource,mariadb
4748,pid_file,Full path of the process ID file.,0,0,others,mariadb
4749,plugin_dir,"Path to the plugin directory. For security reasons, either make sure this directory can only be read by the server, or set secure_file_priv.",0,0,others,mariadb
4750,plugin_maturity,The lowest acceptable plugin maturity. MariaDB will not load plugins less mature than the specified level.,0,0,others,mariadb
4751,port,"Port to listen for TCP/IP connections. If set to 0, will default to, in order of preference, my.cnf, the MYSQL_TCP_PORT environment variable, /etc/services, built-in default (3306).",0,0,others,mariadb
4752,preload_buffer_size,Size in bytes of the buffer allocated when indexes are preloaded.,1,1,resource,mariadb
4753,profiling,"If set to 1 (0 is default), statement profiling will be enabled. See SHOW PROFILES() and SHOW PROFILE().",0,0,others,mariadb
4755,progress_report_time,"Time in seconds between sending progress reports to the client for time-consuming statements. If set to 0, progress reporting will be disabled.",0,0,others,mariadb
4756,protocol_version,The version of the client/server protocol used by the MariaDB server.,0,0,others,mariadb
4757,proxy_protocol_networks,"Enable proxy protocol for these source networks. The syntax is a comma separated list of IPv4 and IPv6 networks. If the network doesn't contain a mask, it is considered to be a single host. ""*"" represents all networks and must be the only directive on the line. String ""localhost"" represents non-TCP local connections (Unix domain socket, Windows named pipe or shared memory). See Proxy Protocol Support.",0,0,others,mariadb
4758,proxy_user,"Set to the proxy user account name if the current client is a proxy, else NULL.",0,0,others,mariadb
4759,pseudo_slave_mode,For internal use by the server.,0,0,others,mariadb
4760,pseudo_thread_id,For internal use only.,0,0,others,mariadb
4761,query_alloc_block_size,Size in bytes of the extra blocks allocated during query parsing and execution (after query_prealloc_size is used up).,1,1,resource,mariadb
4763,query_cache_min_res_unit,Minimum size in bytes of the blocks allocated for query cache results.,1,1,resource,mariadb
4764,query_cache_size,"Size in bytes available to the query cache. About 40KB is needed for query cache structures, so setting a size lower than this will result in a warning. 0, the default before MariaDB 10.1.7, effectively disables the query cache.",1,1,resource,mariadb
4766,query_cache_type,"If set to 0, the query cache is disabled (although a buffer of query_cache_size bytes is still allocated). If set to 1 all SELECT queries will be cached unless SQL_NO_CACHE is specified. If set to 2 (or DEMAND), only queries with the SQL CACHE clause will be cached. Note that if the server is started with the query cache disabled, it cannot be enabled at runtime.",1,4,limited-side-effect,mariadb
4767,query_cache_wlock_invalidate,"If set to 0, the default, results present in the query cache will be returned even if there's a write lock on the table. If set to 1, the client will first have to wait for the lock to be released.",1,4,limited-side-effect,mariadb
4768,query_prealloc_size,"Size in bytes of the persistent buffer for query parsing and execution, allocated on connect and freed on disconnect. Increasing may be useful if complex queries are being run, as this will reduce the need for more memory allocations during query operation. See also query_alloc_block_size.",1,1,resource,mariadb
4769,rand_seed1,"rand_seed1 and rand_seed2 facilitate replication of the RAND() function. The master passes the value of these to the slaves so that the random number generator is seeded in the same way, and generates the same value, on the slave as on the master. Until MariaDB 10.1.4, the variable value could not be viewed, with the SHOW VARIABLES output always displaying zero.",0,0,others,mariadb
4771,range_alloc_block_size,Size in bytes of blocks allocated during range optimization. The unit size in 1024.,1,1,resource,mariadb
4774,read_rnd_buffer_size,"Size in bytes of the buffer used when reading rows from a MyISAM table in sorted order after a key sort. Larger values improve ORDER BY performance, although rather increase the size by SESSION where the need arises to avoid excessive memory use.",1,1,resource,mariadb
4775,require_secure_transport,"When this option is enabled, connections attempted using insecure transport will be rejected. Secure transports are SSL/TLS, Unix sockets or named pipes. Note that per-account requirements take precedence.",1,2,security-tradeoff,mariadb
4776,rowid_merge_buff_size,The maximum size in bytes of the memory available to the Rowid-merge strategy. See Non-semi-join subquery optimizations for more information.,1,1,resource,mariadb
4778,safe_show_database,"This variable was removed in MariaDB 5.5, and has been replaced by the more flexible SHOW DATABASES privilege.",0,0,others,mariadb
4779,secure_auth,"Connections will be blocked if they use the the mysql_old_password authentication plugin. The server will also fail to start if the privilege tables are in the old, pre-MySQL 4.1 format.",1,2,security-tradeoff,mariadb
4780,secure_file_priv,"LOAD DATA, SELECT ... INTO and LOAD FILE() will only work with files in the specified path. If not set, the default, or set to empty string, the statements will work with any files that can be accessed.",0,0,others,mariadb
4782,session_track_schema,Whether to track changes to the default schema within the current session.,0,0,others,mariadb
4783,session_track_state_change,Whether to track changes to the session state.,0,0,others,mariadb
4784,session_track_system_variables,"Comma-separated list of session system variables for which to track changes. In MariaDB 10.2, by default no variables are tracked. For compatibility with MySQL defaults, this variable should be set to ""autocommit, character_set_client, character_set_connection, character_set_results, time_zone"" (the default from MariaDB 10.3.1). The * character tracks all session variables.",0,0,others,mariadb
4785,session_track_transaction_info,"Track changes to the transaction attributes. OFF to disable; STATE to track just transaction state (Is there an active transaction? Does it have any data? etc.); CHARACTERISTICS to track transaction state and report all statements needed to start a transaction with the same characteristics (isolation level, read only/read write,snapshot - but not any work done / data modified within the transaction).",1,6,function-tradeoff,mariadb
4786,shared_memory,"Windows only, determines whether the server permits shared memory connections. See also shared_memory_base_name.",0,0,others,mariadb
4788,skip_external_locking,"If this system variable is set, then some kinds of external table locks will be disabled for some storage engines.
If this system variable is set, then the MyISAM storage engine will not use file-based locks. Otherwise, it will use the fcntl() function with the F_SETLK option to get file-based locks on Unix, and it will use the LockFileEx() function to get file-based locks on Windows.
If this system variable is set, then the Aria storage engine will not lock a table when it decrements the table's in-file counter that keeps track of how many connections currently have the table open. See MDEV-19393 for more information.",0,0,others,mariadb
4789,skip_name_resolve,"If set to 1 (0 is the default), only IP addresses are used for connections. Host names are not resolved. All host values in the GRANT tables must be IP addresses (or localhost).",0,0,others,mariadb
4790,skip_networking,"If set to 1, (0 is the default), the server does not listen for TCP/IP connections. All interaction with the server by be through socket files (Unix) or named pipes or shared memory (Windows). It's recommended to use this option if only local clients are permitted to connect to the server. Enabling this option also prevents a server from functioning as a replication client.",1,6,function-tradeoff,mariadb
4791,skip_show_database,"If set to 1, (0 is the default), only users with the SHOW DATABASES privilege can use the SHOW DATABASES statement to see all database names.",0,0,others,mariadb
4793,slow_query_log,"If set to 0, the default unless the --slow-query-log option is used, the slow query log is disabled, while if set to 1 (both global and session variables), the slow query log is enabled. MariaDB 10.1 added support for session variables.",0,0,others,mariadb
4794,slow_query_log_file,Name of the slow query log file.,0,0,others,mariadb
4795,socket,"On Unix-like systems, this is the name of the socket file used for local client connections, by default /tmp/mysql.sock, often changed by the distribution, for example /var/lib/mysql/mysql.sock. On Windows, this is the name of the named pipe used for local client connections, by default MySQL. On Windows, this is not case-sensitive.",0,0,others,mariadb
4796,sort_buffer_size,"Each session performing a sort allocates a buffer with this amount of memory. Not specific to any storage engine. If the status variable sort_merge_passes is too high, you may need to look at improving your query indexes, or increasing this. Consider reducing where there are many small sorts, such as OLTP, and increasing where needed by session. 16k is a suggested minimum.",1,1,resource,mariadb
4797,sql_auto_is_null,"If set to 1, the query SELECT * FROM table_name WHERE auto_increment_column IS NULL will return an auto-increment that has just been successfully inserted, the same as the LAST_INSERT_ID() function. Some ODBC programs make use of this IS NULL comparison.",0,0,others,mariadb
4799,sql_big_tables,"Old variable, which if set to 1, allows large result sets by saving all temporary sets to disk, avoiding 'table full' errors. No longer needed, as the server now handles this automatically.
This is a synonym for big_tables.",0,0,others,mariadb
4800,sql_buffer_result,"If set to 1 (0 is default), results from SELECT statements are always placed into temporary tables. This can help the server when it takes a long time to send the results to the client by allowing the table locks to be freed early.",0,0,others,mariadb
4801,sql_if_exists,"If set to 1, adds an implicit IF EXISTS to ALTER, RENAME and DROP of TABLES, VIEWS, FUNCTIONS and PACKAGES. This variable is mainly used in replication to tag DDLs that can be ignored on the slave if the target table doesn't exist.",0,0,others,mariadb
4802,sql_log_off,"If set to 1 (0 is the default), no logging to the general query log is done for the client. Only clients with the SUPER privilege can update this variable.",1,6,function-tradeoff,mariadb
4805,sql_max_join_size,"Synonym for max_join_size, the preferred name.",0,0,others,mariadb
4806,sql_mode,"Sets the SQL Mode. Multiple modes can be set, separated by a comma.",0,0,others,mariadb
4807,sql_notes,"If set to 1, the default, warning_count is incremented each time a Note warning is encountered. If set to 0, Note warnings are not recorded. mysqldump has outputs to set this variable to 0 so that no unnecessary increments occur when data is reloaded.",0,0,others,mariadb
4808,sql_quote_show_create,"If set to 1, the default, the server will quote identifiers for SHOW CREATE DATABASE, SHOW CREATE TABLE and SHOW CREATE VIEW statements. Quoting is disabled if set to 0. Enable to ensure replications works when identifiers require quoting.",0,0,others,mariadb
4809,sql_safe_updates,"If set to 1, UPDATEs and DELETEs need either a key in the WHERE clause, or a LIMIT clause, or else they will aborted. Prevents the common mistake of accidentally deleting or updating every row in a table. Until MariaDB 10.3.11, could not be set as a command-line option or in my.cnf.",0,0,others,mariadb
4810,sql_select_limit,"Maximum number of rows that can be returned from a SELECT query. Default is the maximum number of rows permitted per table by the server, usually 232-1 or 264-1. Can be restored to the default value after being changed by assigning it a value of DEFAULT.",0,0,others,mariadb
4811,sql_warnings,"If set to 1, single-row INSERTs will produce a string containing warning information if a warning occurs.",0,0,others,mariadb
4812,standard_compliant_cte,"Allow only standard-compliant common table expressions. Prior to MariaDB 10.2.4, this variable was named standards_compliant_cte.",0,0,others,mariadb
4813,storage_engine,See default_storage_engine.,0,0,others,mariadb
4815,strict_password_validation,"When password validation plugins are enabled, reject passwords that cannot be validated (passwords specified as a hash). This excludes direct updates to the privilege tables.",1,2,security-tradeoff,mariadb
4816,sync_frm,"If set to 1, the default, each time a non-temporary table is created, its .frm definition file is synced to disk. Fractionally slower, but safer in case of a crash.",1,3,reliability-tradeoff,mariadb
4817,system_time_zone,"The system time zone is determined when the server starts. The system time zone is usually read from the operating system's environment. See Time Zones: System Time Zone for the various ways to change the system time zone. This variable is not the same as the time_zone system variable, which is the variable that actually controls a session's active time zone. The system time zone is used for a session when time_zone is set to the special value SYSTEM.",0,0,others,mariadb
4820,table_open_cache,Maximum number of open tables cached in one table cache instance. See Optimizing table_open_cache for suggestions on optimizing. Increasing table_open_cache increases the number of file descriptors required.,1,1,resource,mariadb
4821,table_open_cache_instances,"This system variable specifies the maximum number of table cache instances. MariaDB Server initially creates just a single instance. However, whenever it detects contention on the existing instances, it will automatically create a new instance. When the number of instances has been increased due to contention, it does not decrease again. The default value of this system variable is 8, which is expected to handle up to 100 CPU cores. If your system is larger than this, then you may benefit from increasing the value of this system variable.
Depending on the ratio of actual available file handles, and table_open_cache size, the max. instance count may be auto adjusted to a lower value on server startup.
The implementation and behavior of this feature is different than the same feature in MySQL 5.6.
See Optimizing table_open_cache: Automatic Creation of New Table Open Cache Instances for more information.",1,1,resource,mariadb
4822,table_type,Removed and replaced by storage_engine. Use default_storage_engine instead.,0,0,others,mariadb
4823,tcp_keepalive_interval,"The interval, in seconds, between when successive keep-alive packets are sent if no acknowledgement is received. If set to 0, the system dependent default is used.",0,0,others,mariadb
4824,tcp_keepalive_probes,"The number of unacknowledged probes to send before considering the connection dead and notifying the application layer. If set to 0, a system dependent default is used.",0,0,others,mariadb
4825,tcp_keepalive_time,"Timeout, in seconds, with no activity until the first TCP keep-alive packet is sent. If set to 0, a system dependent default is used.",0,0,others,mariadb
4826,tcp_nodelay,Set the TCP_NODELAY option (disable Nagle's algorithm) on socket.,0,0,others,mariadb
4828,thread_concurrency,"Allows applications to give the system a hint about the desired number of threads. Specific to Solaris only, invokes thr_setconcurrency(). Deprecated and has no effect from MariaDB 5.5.",1,1,resource,mariadb
4830,time_format,Unused.,0,0,others,mariadb
4831,time_zone,"The global value determines the default time zone for sessions that connect. The session value determines the session's active time zone. When it is set to SYSTEM, the session's time zone is determined by the system_time_zone system variable.",0,0,others,mariadb
4832,timed_mutexes,"Determines whether InnoDB mutexes are timed. OFF, the default, disables mutex timing, while ON enables it. See also SHOW ENGINE for more on mutex statistics. Deprecated and has no effect.",0,0,others,mariadb
4833,timestamp,"Sets the time for the client. This will affect the result returned by the NOW() function, not the SYSDATE() function, unless the server is started with the --sysdate-is-now option, in which case SYSDATE becomes an alias of NOW, and will also be affected. Also used to get the original timestamp when restoring rows from the binary log.",0,0,others,mariadb
4834,tmp_disk_table_size,Max size for data for an internal temporary on-disk MyISAM or Aria table. These tables are created as part of complex queries when the result doesn't fit into the memory engine. You can set this variable if you want to limit the size of temporary tables created in your temporary directory tmpdir.,1,1,resource,mariadb
4835,tmp_memory_table_size,An alias for tmp_table_size.,1,1,resource,mariadb
4836,tmp_table_size,"The largest size for temporary tables in memory (not MEMORY tables) although if max_heap_table_size is smaller the lower limit will apply. If a table exceeds the limit, MariaDB converts it to a MyISAM or Aria table. You can see if it's necessary to increase by comparing the status variables Created_tmp_disk_tables and Created_tmp_tables to see how many temporary tables out of the total created needed to be converted to disk. Often complex GROUP BY queries are responsible for exceeding the limit. Defaults may be different on some systems, see for example Differences in MariaDB in Debian. From MariaDB 10.2.7, tmp_memory_table_size is an alias.",1,1,resource,mariadb
4837,tmpdir,"Directory for storing temporary tables and files. Can specify a list (separated by semicolons in Windows, and colons in Unix) that will then be used in round-robin fashion. This can be used for load balancing across several disks. Note that if the server is a replication replica, and slave_load_tmpdir, which overrides tmpdir for replicas, is not set, you should not set tmpdir to a directory that is cleared when the machine restarts, or else replication may fail.",0,0,others,mariadb
4838,transaction_alloc_block_size,Size in bytes to increase the memory pool available to each transaction when the available pool is not large enough. See transaction_prealloc_size.,1,1,resource,mariadb
4839,transaction_prealloc_size,"Initial size of a memory pool available to each transaction for various memory allocations. If the memory pool is not large enough for an allocation, it is increased by transaction_alloc_block_size bytes, and truncated back to transaction_prealloc_size bytes when the transaction is completed. If set large enough to contain all statements in a transaction, extra malloc() calls are avoided.",1,1,resource,mariadb
4840,tx_isolation,The transaction isolation level. See also SET TRANSACTION ISOLATION LEVEL.,1,3,reliability-tradeoff,mariadb
4842,unique_checks,"If set to 1, the default, secondary indexes in InnoDB tables are performed. If set to 0, storage engines can (but are not required to) assume that duplicate keys are not present in input data. Set to 0 to speed up imports of large tables to InnoDB. The storage engine will still issue a duplicate key error if it detects one, even if set to 0.",0,0,others,mariadb
4843,updatable_views_with_limit,"Determines whether view updates can be made with an UPDATE or DELETE statement with a LIMIT clause if the view does not contain all primary or not null unique key columns from the underlying table. 0 prohibits this, while 1 permits it while issuing a warning (the default).",0,0,others,mariadb
4844,use_stat_tables,"Controls the use of engine-independent table statistics. 
never: The optimizer will not use data from statistics tables. 
complementary: The optimizer uses data from statistics tables if the same kind of data is not provided by the storage engine.
preferably: Prefer the data from statistics tables, if it's not available there, use the data from the storage engine.
complementary_for_queries: Same as complementary, but for queries only (to avoid needlessly collecting for ANALYZE TABLE). From MariaDB 10.4.1.
preferably_for_queries: Same as preferably, but for queries only (to avoid needlessly collecting for ANALYZE TABLE). From MariaDB 10.4.1.",0,0,others,mariadb
4845,version,"Server version number. It may also include a suffix with configuration or build information. -debug indicates debugging support was enabled on the server, and -log indicates at least one of the binary log, general log or slow query log are enabled, for example 10.0.1-MariaDB-mariadb1precise-log. From MariaDB 10.2.1, this variable can be set at startup in order to fake the server version.",0,0,others,mariadb
4846,version_comment,"Value of the COMPILATION_COMMENT option specified by CMake when building MariaDB, for example mariadb.org binary distribution.",0,0,others,mariadb
4848,version_compile_os,"Operating system that MariaDB was built on, for example debian-linux-gnu.",0,0,others,mariadb
4849,version_malloc_library,Version of the used malloc library.,0,0,others,mariadb
4850,version_source_revision,"Source control revision id for MariaDB source code, enabling one to see exactly which version of the source was used for a build.",0,0,others,mariadb
4851,wait_timeout,"Time in seconds that the server waits for a connection to become active before closing it. The session value is initialized when a thread starts up from either the global value, if the connection is non-interactive, or from the interactive_timeout value, if the connection is interactive.",0,0,others,mariadb
4853,abort-slave-event-count,"When this option is set to some positive integer value other than 0 (the default) it affects replication behavior as follows: After the replication SQL thread has started, value log events are permitted to be executed; after that, the replication SQL thread does not receive any more events, just as if the network connection from the source were cut. The replication SQL thread continues to run, and the output from SHOW REPLICA STATUS displays Yes in both the Replica_IO_Running and the Replica_SQL_Running columns, but no further events are read from the relay log.",1,5,workload-specific,mysql
4854,admin-ssl,"The --admin-ssl option is like the --ssl option, except that it applies to the administrative connection interface rather than the main connection interface.",1,2,security-tradeoff,mysql
4855,allow-suspicious-udfs,"This option controls whether loadable functions that have only an xxx symbol for the main function can be loaded. By default, the option is off and only loadable functions that have at least one auxiliary symbol can be loaded; this prevents attempts at loading functions from shared object files other than those containing legitimate functions. See Loadable Function Security Precautions.",1,2,security-tradeoff,mysql
4856,ansi,"Use standard (ANSI) SQL syntax instead of MySQL syntax. For more precise control over the server SQL mode, use the --sql-mode option instead.",0,0,others,mysql
4857,audit_log_buffer_size,"When the audit log plugin writes events to the log asynchronously, it uses a buffer to store event contents prior to writing them. This variable controls the size of that buffer, in bytes. The server adjusts the value to a multiple of 4096. The plugin uses a single buffer, which it allocates when it initializes and removes when it terminates. The plugin allocates this buffer only if logging is asynchronous.",1,1,resource,mysql
4858,audit_log_compression,"The type of compression for the audit log file. Permitted values are NONE (no compression; the default) and GZIP (GNU Zip compression). For more information, see Compressing Audit Log Files.",1,5,workload-specific,mysql
4859,audit_log_connection_policy,The policy controlling how the audit log plugin writes connection events to its log file. The following table shows the permitted values.,1,6,function-tradeoff,mysql
4860,audit_log_current_session,"Whether audit logging is enabled for the current session. The session value of this variable is read only. It is set when the session begins based on the values of the audit_log_include_accounts and audit_log_exclude_accounts system variables. The audit log plugin uses the session value to determine whether to audit events for the session. (There is a global value, but the plugin does not use it.)",1,6,function-tradeoff,mysql
4861,Audit_log_current_size,The size of the current audit log file. The value increases when an event is written to the log and is reset to 0 when the log is rotated.,0,0,others,mysql
4862,audit_log_encryption,"The type of encryption for the audit log file. Permitted values are NONE (no encryption; the default) and AES (AES-256-CBC cipher encryption). For more information, see Encrypting Audit Log Files.",1,2,security-tradeoff,mysql
4863,Audit_log_event_max_drop_size,The size of the largest dropped event in performance logging mode.,0,0,others,mysql
4864,Audit_log_events,"The number of events handled by the audit log plugin, whether or not they were written to the log based on filtering policy.",1,1,resource,mysql
4865,Audit_log_events_filtered,The number of events handled by the audit log plugin that were filtered (not written to the log) based on filtering policy .,1,1,resource,mysql
4867,Audit_log_events_written,The number of events written to the audit log.,0,0,others,mysql
4868,audit_log_exclude_accounts,The accounts for which events should not be logged. The value should be NULL or a string containing a list of one or more comma-separated account names.,0,0,others,mysql
4869,audit_log_file,"The base name and suffix of the file to which the audit log plugin writes events. The default value is audit.log, regardless of logging format. To have the name suffix correspond to the format, set the name explicitly, choosing a different suffix (for example, audit.xml for XML format, audit.json for JSON format).",0,0,others,mysql
4870,audit_log_filter_id,The session value of this variable indicates the internally maintained ID of the audit filter for the current session. A value of 0 means that the session has no filter assigned.,0,0,others,mysql
4871,audit_log_flush,"If audit_log_rotate_on_size is 0, automatic audit log file rotation is disabled and rotation occurs only when performed manually. In that case, enabling audit_log_flush by setting it to 1 or ON causes the audit log plugin to close and reopen its log file to flush it. (The variable value remains OFF so that you need not disable it explicitly before enabling it again to perform another flush.) For more information, see Section6.4.5.5, “Configuring Audit Logging Characteristics”.",1,3,reliability-tradeoff,mysql
4872,audit_log_format,"The audit log file format. Permitted values are OLD (old-style XML), NEW (new-style XML; the default), and JSON. For details about each format, see Section6.4.5.4, “Audit Log File Formats”.",0,0,others,mysql
4874,audit_log_include_accounts,"The accounts for which events should be logged. The value should be NULL or a string containing a list of one or more comma-separated account names. For more information, see Section6.4.5.7, “Audit Log Filtering”.",0,0,others,mysql
4875,audit_log_max_size,"audit_log_max_size pertains to audit log file pruning, which is supported for JSON-format log files only. It controls pruning based on combined log file size. A value of 0 (the default) disables size-based pruning. No size limit is enforced. A value greater than 0 enables size-based pruning. The value is the combined size above which audit log files become subject to pruning.",0,0,others,mysql
4876,audit_log_password_history_keep_days,"When the audit log plugin creates a new encryption password, it archives the previous password, if one exists, for later use. The audit_log_password_history_keep_days variable controls automatic removal of expired archived passwords. Its value indicates the number of days after which archived audit log encryption passwords are removed.",0,0,others,mysql
4877,audit_log_policy,The policy controlling how the audit log plugin writes events to its log file. The following table shows the permitted values.,1,6,function-tradeoff,mysql
4878,audit_log_prune_seconds,"audit_log_prune_seconds pertains to audit log file pruning, which is supported for JSON-format log files only. It controls pruning based on log file age. A value of 0 (the default) disables age-based pruning. No age limit is enforced. A value greater than 0 enables age-based pruning. The value is the number of seconds after which audit log files become subject to pruning.",1,5,workload-specific,mysql
4880,audit_log_rotate_on_size,"If audit_log_rotate_on_size is 0, the audit log plugin does not perform automatic size-based log file rotation. If rotation is to occur, you must perform it manually; see Manual Audit Log File Rotation.",0,0,others,mysql
4882,audit_log_strategy,The logging method used by the audit log plugin. These strategy values are permitted: ASYNCHRONOUS: Log asynchronously. Wait for space in the output buffer. PERFORMANCE: Log asynchronously. Drop requests for which there is insufficient space in the output buffer. SEMISYNCHRONOUS: Log synchronously. Permit caching by the operating system. SYNCHRONOUS: Log synchronously. Call sync() after each request.,1,3,reliability-tradeoff,mysql
4883,Audit_log_total_size,"The total size of events written to all audit log files. Unlike Audit_log_current_size, the value of Audit_log_total_size increases even when the log is rotated.",0,0,others,mysql
4887,authentication_kerberos_service_principal,The Kerberos service principal name (SPN) that the MySQL server sends to clients.,0,0,others,mysql
4890,authentication_ldap_sasl_bind_root_dn,"For SASL LDAP authentication, the root distinguished name (DN). This variable is used in conjunction with authentication_ldap_sasl_bind_root_pwd as the credentials for authenticating to the LDAP server for the purpose of performing searches. Authentication uses either one or two LDAP bind operations, depending on whether the MySQL account names an LDAP user DN:",0,0,others,mysql
4891,authentication_ldap_sasl_bind_root_pwd,"For SASL LDAP authentication, the password for the root distinguished name. This variable is used in conjunction with authentication_ldap_sasl_bind_root_dn. See the description of that variable.",0,0,others,mysql
4892,authentication_ldap_sasl_ca_path,"For SASL LDAP authentication, the absolute path of the certificate authority file. Specify this file if it is desired that the authentication plugin perform verification of the LDAP server certificate.",0,0,others,mysql
4894,authentication_ldap_sasl_group_search_filter,"For SASL LDAP authentication, the custom group search filter.",0,0,others,mysql
4895,authentication_ldap_sasl_init_pool_size,"For SASL LDAP authentication, the initial size of the pool of connections to the LDAP server. Choose the value for this variable based on the average number of concurrent authentication requests to the LDAP server.",0,0,others,mysql
4896,authentication_ldap_sasl_log_status,"For SASL LDAP authentication, the logging level for messages written to the error log. The following table shows the permitted level values and their meanings.",1,6,function-tradeoff,mysql
4897,authentication_ldap_sasl_max_pool_size,"For SASL LDAP authentication, the maximum size of the pool of connections to the LDAP server. To disable connection pooling, set this variable to 0.",1,1,resource,mysql
4899,authentication_ldap_sasl_server_host,"For SASL LDAP authentication, the LDAP server host. The permitted values for this variable depend on the authentication method:",0,0,others,mysql
4900,authentication_ldap_sasl_server_port,"For SASL LDAP authentication, the LDAP server TCP/IP port number.",0,0,others,mysql
4901,authentication_ldap_sasl_tls,"For SASL LDAP authentication, whether connections by the plugin to the LDAP server are secure. If this variable is enabled, the plugin uses TLS to connect securely to the LDAP server. This variable can be set to override the default OpenLDAP TLS configuration; see LDAP Pluggable Authentication and ldap.conf If you enable this variable, you may also wish to set the authentication_ldap_sasl_ca_path variable.",1,2,security-tradeoff,mysql
4902,authentication_ldap_sasl_user_search_attr,"For SASL LDAP authentication, the name of the attribute that specifies user names in LDAP directory entries. If a user distinguished name is not provided, the authentication plugin searches for the name using this attribute. For example, if the authentication_ldap_sasl_user_search_attr value is uid, a search for the user name user1 finds entries with a uid value of user1.",0,0,others,mysql
4903,authentication_ldap_simple_auth_method_name,"For simple LDAP authentication, the authentication method name. Communication between the authentication plugin and the LDAP server occurs according to this authentication method.",0,0,others,mysql
4904,authentication_ldap_simple_bind_base_dn,"For simple LDAP authentication, the base distinguished name (DN). This variable can be used to limit the scope of searches by anchoring them at a certain location (the “base”) within the search tree.",0,0,others,mysql
4905,authentication_ldap_simple_bind_root_dn,"For simple LDAP authentication, the root distinguished name (DN). This variable is used in conjunction with authentication_ldap_simple_bind_root_pwd as the credentials for authenticating to the LDAP server for the purpose of performing searches. Authentication uses either one or two LDAP bind operations, depending on whether the MySQL account names an LDAP user DN:",0,0,others,mysql
4906,authentication_ldap_simple_bind_root_pwd,"For simple LDAP authentication, the password for the root distinguished name. This variable is used in conjunction with authentication_ldap_simple_bind_root_dn. See the description of that variable.",0,0,others,mysql
4908,authentication_ldap_simple_group_search_attr,"For simple LDAP authentication, the name of the attribute that specifies group names in LDAP directory entries. If authentication_ldap_simple_group_search_attr has its default value of cn, searches return the cn value as the group name. For example, if an LDAP entry with a uid value of user1 has a cn attribute of mygroup, searches for user1 return mygroup as the group name.",0,0,others,mysql
4909,authentication_ldap_simple_group_search_filter,"For simple LDAP authentication, the custom group search filter.",0,0,others,mysql
4910,authentication_ldap_simple_init_pool_size,"For simple LDAP authentication, the initial size of the pool of connections to the LDAP server. Choose the value for this variable based on the average number of concurrent authentication requests to the LDAP server.",1,1,resource,mysql
4911,authentication_ldap_simple_log_status,"For simple LDAP authentication, the logging level for messages written to the error log. The following table shows the permitted level values and their meanings.",1,6,function-tradeoff,mysql
4913,authentication_ldap_simple_referral,"For simple LDAP authentication, whether to enable LDAP search referral. See LDAP Search Referral.",0,0,others,mysql
4915,authentication_ldap_simple_server_port,"For simple LDAP authentication, the LDAP server TCP/IP port number.",0,0,others,mysql
4916,authentication_ldap_simple_tls,"For simple LDAP authentication, whether connections by the plugin to the LDAP server are secure. If this variable is enabled, the plugin uses TLS to connect securely to the LDAP server. This variable can be set to override the default OpenLDAP TLS configuration; see LDAP Pluggable Authentication and ldap.conf If you enable this variable, you may also wish to set the authentication_ldap_simple_ca_path variable.",1,2,security-tradeoff,mysql
4917,authentication_ldap_simple_user_search_attr,"For simple LDAP authentication, the name of the attribute that specifies user names in LDAP directory entries. If a user distinguished name is not provided, the authentication plugin searches for the name using this attribute. For example, if the authentication_ldap_simple_user_search_attr value is uid, a search for the user name user1 finds entries with a uid value of user1.",0,0,others,mysql
4918,auto_increment_increment,"auto_increment_increment and auto_increment_offset are intended for use with circular (source-to-source) replication, and can be used to control the operation of AUTO_INCREMENT columns. Both variables have global and session values, and each can assume an integer value between 1 and 65,535 inclusive. Setting the value of either of these two variables to 0 causes its value to be set to 1 instead. Attempting to set the value of either of these two variables to an integer greater than 65,535 or less than 0 causes its value to be set to 65,535 instead. Attempting to set the value of auto_increment_increment or auto_increment_offset to a noninteger value produces an error, and the actual value of the variable remains unchanged.",0,0,others,mysql
4920,binlog_cache_size,The size of the memory buffer to hold changes to the binary log during a transaction. The value must be a multiple of 4096.,1,1,resource,mysql
4921,binlog_checksum,"When enabled, this variable causes the source to write a checksum for each event in the binary log. binlog_checksum supports the values NONE (which disables checksums) and CRC32. The default is CRC32. When binlog_checksum is disabled (value NONE), the server verifies that it is writing only complete events to the binary log by writing and checking the event length (rather than a checksum) for each event.",1,2,security-tradeoff,mysql
4922,binlog_direct_non_transactional_updates,"Due to concurrency issues, a replica can become inconsistent when a transaction contains updates to both transactional and nontransactional tables. MySQL tries to preserve causality among these statements by writing nontransactional statements to the transaction cache, which is flushed upon commit. However, problems arise when modifications done to nontransactional tables on behalf of a transaction become immediately visible to other connections because these changes may not be written immediately into the binary log.",0,0,others,mysql
4925,binlog_expire_logs_seconds,"Sets the binary log expiration period in seconds. After their expiration period ends, binary log files can be automatically removed. Possible removals happen at startup and when the binary log is flushed. Log flushing occurs as indicated in Section5.4, “MySQL Server Logs”.",0,0,others,mysql
4926,binlog_format,"This system variable sets the binary logging format, and can be any one of STATEMENT, ROW, or MIXED. See Section17.2.1, “Replication Formats”. The setting takes effect when binary logging is enabled on the server, which is the case when the log_bin system variable is set to ON. From MySQL 8.0, binary logging is enabled by default.",0,0,others,mysql
4927,binlog_group_commit_sync_delay,"Controls how many microseconds the binary log commit waits before synchronizing the binary log file to disk. By default binlog_group_commit_sync_delay is set to 0, meaning that there is no delay. Setting binlog_group_commit_sync_delay to a microsecond delay enables more transactions to be synchronized together to disk at once, reducing the overall time to commit a group of transactions because the larger groups require fewer time units per group.",0,0,others,mysql
4928,binlog_group_commit_sync_no_delay_count,"The maximum number of transactions to wait for before aborting the current delay as specified by binlog_group_commit_sync_delay. If binlog_group_commit_sync_delay is set to 0, then this option has no effect.",0,0,others,mysql
4929,binlog_gtid_simple_recovery,This variable controls how binary log files are iterated during the search for GTIDs when MySQL starts or restarts.,1,6,function-tradeoff,mysql
4930,binlog_max_flush_queue_time,"binlog_max_flush_queue_time is deprecated, and is marked for eventual removal in a future MySQL release. Formerly, this system variable controlled the time in microseconds to continue reading transactions from the flush queue before proceeding with group commit. It no longer has any effect.",0,0,others,mysql
4932,binlog_rotate_encryption_master_key_at_startup,"Specifies whether or not the binary log master key is rotated at server startup. The binary log master key is the binary log encryption key that is used to encrypt file passwords for the binary log files and relay log files on the server. When a server is started for the first time with binary log encryption enabled (binlog_encryption=ON), a new binary log encryption key is generated and used as the binary log master key. If the binlog_rotate_encryption_master_key_at_startup system variable is also set to ON, whenever the server is restarted, a further binary log encryption key is generated and used as the binary log master key for all subsequent binary log files and relay log files. If the binlog_rotate_encryption_master_key_at_startup system variable is set to OFF, which is the default, the existing binary log master key is used again after the server restarts. For more information on binary log encryption keys and the binary log master key, see Section17.3.2, “Encrypting Binary Log Files and Relay Log Files”.",1,2,security-tradeoff,mysql
4934,binlog_row_image,"For MySQL row-based replication, this variable determines how row images are written to the binary log.",1,6,function-tradeoff,mysql
4935,binlog_row_metadata,"Configures the amount of table metadata added to the binary log when using row-based logging. When set to MINIMAL, the default, only metadata related to SIGNED flags, column character set and geometry types are logged. When set to FULL complete metadata for tables is logged, such as column name, ENUM or SET string values, PRIMARY KEY information, and so on.",1,6,function-tradeoff,mysql
4936,binlog_row_value_options,"When set to PARTIAL_JSON, this enables use of a space-efficient binary log format for updates that modify only a small portion of a JSON document, which causes row-based replication to write only the modified parts of the JSON document to the after-image for the update in the binary log, rather than writing the full document (see Partial Updates of JSON Values). This works for an UPDATE statement which modifies a JSON column using any sequence of JSON_SET(), JSON_REPLACE(), and JSON_REMOVE(). If the server is unable to generate a partial update, the full document is used instead.",1,4,limited-side-effect,mysql
4937,binlog_rows_query_log_events,"This system variable affects row-based logging only. When enabled, it causes the server to write informational log events such as row query log events into its binary log. This information can be used for debugging and related purposes, such as obtaining the original query issued on the source when it cannot be reconstructed from the row updates.",1,6,function-tradeoff,mysql
4938,binlog_stmt_cache_size,The size of the memory buffer for the binary log to hold nontransactional statements issued during a transaction. The value must be a multiple of 4096.,1,1,resource,mysql
4939,binlog_transaction_compression,Enables compression for transactions that are written to binary log files on this server. OFF is the default. Use the binlog_transaction_compression_level_zstd system variable to set the level for the zstd algorithm that is used for compression.,1,4,limited-side-effect,mysql
4940,binlog_transaction_compression_level_zstd,"Sets the compression level for binary log transaction compression on this server, which is enabled by the binlog_transaction_compression system variable. The value is an integer that determines the compression effort, from 1 (the lowest effort) to 22 (the highest effort). If you do not specify this system variable, the compression level is set to 3.",1,5,workload-specific,mysql
4942,binlog_transaction_dependency_tracking,"For a replication source server that has multithreaded replicas (replicas on which replica_parallel_workers or slave_parallel_workers is set to a value greater than 0), binlog_transaction_dependency_tracking specifies the source of dependency information that the source records in the binary log to help replicas determine which transactions can be executed in parallel. The possible values are:",1,6,function-tradeoff,mysql
4943,binlog-checksum,"Enabling this option causes the source to write checksums for events written to the binary log. Set to NONE to disable, or the name of the algorithm to be used for generating checksums; currently, only CRC32 checksums are supported, and CRC32 is the default. You cannot change the setting for this option within a transaction.",1,2,security-tradeoff,mysql
4944,binlog-do-db,This option affects binary logging in a manner similar to the way that --replicate-do-db affects replication.,0,0,others,mysql
4945,binlog-ignore-db,This option affects binary logging in a manner similar to the way that --replicate-ignore-db affects replication.,0,0,others,mysql
4946,character-set-client-handshake,"Do not ignore character set information sent by the client. To ignore client information and use the default server character set, use --skip-character-set-client-handshake; this makes MySQL behave like MySQL 4.0.",0,0,others,mysql
4947,chroot,Put the mysqld server in a closed environment during startup by using the chroot() system call. This is a recommended security measure. Use of this option somewhat limits LOAD DATA and SELECT ... INTO OUTFILE.,0,0,others,mysql
4949,clone_buffer_size,"Defines the size of the intermediate buffer used when transferring data during a local cloning operation. The default value is 4 mebibytes (MiB). A larger buffer size may permit I/O device drivers to fetch data in parallel, which can improve cloning performance.",1,1,resource,mysql
4950,clone_ddl_timeout,"The time in seconds to wait for a backup lock when executing a cloning operation. This setting is applied on both the donor and recipient MySQL server instances. A cloning operation cannot run concurrently with DDL operations. A backup lock is required on the donor and recipient MySQL server instances. The cloning operation waits for current DDL operations to finish. Once backup locks are acquired, DDL operations must wait for the cloning operation to finish. A value of 0 means that no backup lock is to be taken for the cloning operation. In this case, the cloning operation fails with an error if a DDL operation is attempted concurrently.",0,0,others,mysql
4951,clone_donor_timeout_after_network_failure,"Defines the amount of time in minutes the donor allows for the recipient to reconnect and restart a cloning operation after a network failure. For more information, see Section5.6.7.8, “Remote Cloning Operation Failure Handling”.",0,0,others,mysql
4953,clone_max_concurrency,"Defines the maximum number of concurrent threads for a remote cloning operation. The default value is 16. A greater number of threads can improve cloning performance but also reduces the number of permitted simultaneous client connections, which can affect the performance of existing client connections. This setting is only applied on the recipient MySQL server instance.",1,1,resource,mysql
4954,clone_max_data_bandwidth,"Defines the maximum data transfer rate in mebibytes (MiB) per second for a remote cloning operation. This variable helps manage the performance impact of a cloning operation. A limit should be set only when donor disk I/O bandwidth is saturated, affecting performance. A value of 0 means “unlimited”, which permits cloning operations to run at the highest possible data transfer rate. This setting is only applicable to the recipient MySQL server instance.",1,1,resource,mysql
4955,clone_max_network_bandwidth,"Specifies the maximum approximate network transfer rate in mebibytes (MiB) per second for a remote cloning operation. This variable can be used to manage the performance impact of a cloning operation on network bandwidth. It should be set only when network bandwidth is saturated, affecting performance on the donor instance. A value of 0 means “unlimited”, which permits cloning at the highest possible data transfer rate over the network, providing the best performance. This setting is only applicable to the recipient MySQL server instance.",1,1,resource,mysql
4957,clone_ssl_cert,Specifies the path to the public key certificate. Used to configure an encrypted connection for a remote cloning operation. This setting configured on the recipient and used when connecting to the donor.,0,0,others,mysql
4958,clone_ssl_key,Specifies the path to the private key file. Used to configure an encrypted connection for a remote cloning operation. This setting configured on the recipient and used when connecting to the donor.,0,0,others,mysql
4959,clone_valid_donor_list,"Defines valid donor host addresses for remote cloning operations. This setting is applied on the recipient MySQL server instance. A comma-separated list of values is permitted in the following format: “HOST1:PORT1,HOST2:PORT2,HOST3:PORT3”. Spaces are not permitted.",0,0,others,mysql
4960,Connection_control_delay_generated,The number of times the server added a delay to its response to a failed connection attempt. This does not count attempts that occur before reaching the threshold defined by the connection_control_failed_connections_threshold system variable.,0,0,others,mysql
4961,connection_control_failed_connections_threshold,The number of consecutive failed connection attempts permitted to accounts before the server adds a delay for subsequent connection attempts:,0,0,others,mysql
4962,connection_control_max_connection_delay,"The maximum delay in milliseconds for server response to failed connection attempts, if connection_control_failed_connections_threshold is greater than zero.",0,0,others,mysql
4963,connection_control_min_connection_delay,"The minimum delay in milliseconds for server response to failed connection attempts, if connection_control_failed_connections_threshold is greater than zero.",0,0,others,mysql
4965,core-file,"Write a core file if mysqld dies. The name and location of the core file is system dependent. On Linux, a core file named core.pid is written to the current working directory of the process, which for mysqld is the data directory. pid represents the process ID of the server process. On macOS, a core file named core.pid is written to the /cores directory. On Solaris, use the coreadm command to specify where to write the core file and how to name it.",0,0,others,mysql
4967,daemon_memcached_engine_lib_name,Specifies the shared library that implements the InnoDB memcached plugin.,0,0,others,mysql
4968,daemon_memcached_engine_lib_path,"The path of the directory containing the shared library that implements the InnoDB memcached plugin. The default value is NULL, representing the MySQL plugin directory. You should not need to modify this parameter unless specifying a memcached plugin for a different storage engine that is located outside of the MySQL plugin directory.",0,0,others,mysql
4969,daemon_memcached_option,"Used to pass space-separated memcached options to the underlying memcached memory object caching daemon on startup. For example, you might change the port that memcached listens on, reduce the maximum number of simultaneous connections, change the maximum memory size for a key-value pair, or enable debugging messages for the error log.",1,5,workload-specific,mysql
4970,daemon_memcached_r_batch_size,Specifies how many memcached read operations (get operations) to perform before doing a COMMIT to start a new transaction. Counterpart of daemon_memcached_w_batch_size.,1,1,resource,mysql
4971,daemon_memcached_w_batch_size,"Specifies how many memcached write operations, such as add, set, and incr, to perform before doing a COMMIT to start a new transaction. Counterpart of daemon_memcached_r_batch_size.",1,1,resource,mysql
4972,daemonize,"This option causes the server to run as a traditional, forking daemon, permitting it to work with operating systems that use systemd for process control.",0,0,others,mysql
4973,ddl-rewriter,"This option controls how the server loads the ddl_rewriter plugin at startup. It is available only if the plugin has been previously registered with INSTALL PLUGIN or is loaded with --plugin-load or --plugin-load-add. See Section5.6.5.1, “Installing or Uninstalling ddl_rewriter”.",0,0,others,mysql
4974,debug,"If MySQL is configured with the -DWITH_DEBUG=1 CMake option, you can use this option to get a trace file of what mysqld is doing. A typical debug_options string is d:t:o,file_name. The default is d:t:i:o,/tmp/mysqld.trace on Unix and d:t:i:O,\mysqld.trace on Windows.",0,0,others,mysql
4976,defaults-extra-file,"Read this option file after the global option file but (on Unix) before the user option file. If the file does not exist or is otherwise inaccessible, an error occurs. If file_name is not an absolute path name, it is interpreted relative to the current directory. This must be the first option on the command line if it is used.",0,0,others,mysql
4977,defaults-file,"Read only the given option file. If the file does not exist or is otherwise inaccessible, an error occurs. If file_name is not an absolute path name, it is interpreted relative to the current directory.",0,0,others,mysql
4978,defaults-group-suffix,"Read not only the usual option groups, but also groups with the usual names and a suffix of str. For example, mysqld normally reads the [mysqld] group. If this option is given as --defaults-group-suffix=_other, mysqld also reads the [mysqld_other] group.",0,0,others,mysql
4979,default-time-zone,"Set the default server time zone. This option sets the global time_zone system variable. If this option is not given, the default time zone is the same as the system time zone (given by the value of the system_time_zone system variable.",0,0,others,mysql
4981,enforce_gtid_consistency,"Depending on the value of this variable, the server enforces GTID consistency by allowing execution of only statements that can be safely logged using a GTID. You must set this variable to ON before enabling GTID based replication.",1,3,reliability-tradeoff,mysql
4982,exit-info,This is a bitmask of different flags that you can use for debugging the mysqld server. Do not use this option unless you know exactly what it does!,0,0,others,mysql
4984,external-locking,"Enable external locking (system locking), which is disabled by default. If you use this option on a system on which lockd does not fully work (such as Linux), it is easy for mysqld to deadlock.",0,0,others,mysql
4985,Firewall_access_denied,The number of statements rejected by MySQL Enterprise Firewall.,0,0,others,mysql
4986,Firewall_access_granted,The number of statements accepted by MySQL Enterprise Firewall.,0,0,others,mysql
4987,Firewall_cached_entries,"The number of statements recorded by MySQL Enterprise Firewall, including duplicates.",0,0,others,mysql
4988,gdb,"Install an interrupt handler for SIGINT (needed to stop mysqld with ^C to set breakpoints) and disable stack tracing and core file handling. See Section5.9.1.4, “Debugging mysqld under gdb”.",0,0,others,mysql
4989,group_replication_advertise_recovery_endpoints,"The value of this system variable can be changed while Group Replication is running. The change takes effect immediately on the member. However, a joining member that already received the previous value of the system variable continues to use that value. Only members that join after the value change receive the new value.",0,0,others,mysql
4991,group_replication_auto_increment_increment,"This system variable should have the same value on all group members. You cannot change the value of this system variable while Group Replication is running. You must stop Group Replication, change the value of the system variable, then restart Group Replication, on each of the group members. During this process, the value of the system variable is permitted to differ between group members, but some transactions on group members might be rolled back.",0,0,others,mysql
4992,group_replication_autorejoin_tries,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately. The system variable's current value is read when an issue occurs that means the behavior is needed.",0,0,others,mysql
4993,group_replication_bootstrap_group,"group_replication_bootstrap_group configures this server to bootstrap the group. This system variable must only be set on one server, and only when starting the group for the first time or restarting the entire group. After the group has been bootstrapped, set this option to OFF. It should be set to OFF both dynamically and in the configuration files. Starting two servers or restarting one server with this option set while the group is running may lead to an artificial split brain situation, where two independent groups with the same name are bootstrapped.",0,0,others,mysql
4994,group_replication_clone_threshold,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
4995,group_replication_communication_debug_options,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
4996,group_replication_communication_max_message_size,"This system variable should have the same value on all group members. You cannot change the value of this system variable while Group Replication is running. You must stop Group Replication, change the value of the system variable, then restart Group Replication, on each of the group members. During this process, the value of the system variable is permitted to differ between group members, but some transactions on group members might be rolled back.",0,0,others,mysql
4997,group_replication_components_stop_timeout,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
4999,group_replication_consistency,"The value of this system variable can be changed while Group Replication is running. group_replication_consistency is a server system variable rather than a Group Replication plugin-specific variable, so a restart of Group Replication is not required for the change to take effect. Changing the session value of the system variable takes effect immediately, and changing the global value takes effect for new sessions that start after the change. The GROUP_REPLICATION_ADMIN privilege is required to change the global setting for this system variable.",0,0,others,mysql
5000,group_replication_enforce_update_everywhere_checks,"This system variable is a group-wide configuration setting. It must have the same value on all group members, cannot be changed while Group Replication is running, and requires a full reboot of the group (a bootstrap by a server with group_replication_bootstrap_group=ON) in order for the value change to take effect. If the group has a value set for this system variable, and a joining member has a different value set for the system variable, the joining member cannot join the group until the value is changed to match. If the group members have a value set for this system variable, and the joining member does not support the system variable, it cannot join the group.",0,0,others,mysql
5001,group_replication_exit_state_action,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately. The system variable's current value is read when an issue occurs that means the behavior is needed.",0,0,others,mysql
5003,group_replication_flow_control_certifier_threshold,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately.",0,0,others,mysql
5004,group_replication_flow_control_hold_percent,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately.",0,0,others,mysql
5005,group_replication_flow_control_max_commit_quota,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately.",0,0,others,mysql
5006,group_replication_flow_control_member_quota_percent,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately.",0,0,others,mysql
5007,group_replication_flow_control_min_quota,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately.",0,0,others,mysql
5008,group_replication_flow_control_min_recovery_quota,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately.",0,0,others,mysql
5009,group_replication_flow_control_mode,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately.",0,0,others,mysql
5010,group_replication_flow_control_period,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately.",0,0,others,mysql
5011,group_replication_flow_control_release_percent,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately.",0,0,others,mysql
5012,group_replication_force_members,"This system variable is used to force a new group membership. The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately. You only need to set the value of the system variable on one of the group members that is to remain in the group. For details of the situation in which you might need to force a new group membership, and a procedure to follow when using this system variable, see Section18.5.4, “Network Partitioning”.",0,0,others,mysql
5013,group_replication_group_name,The value of this system variable cannot be changed while Group Replication is running.,0,0,others,mysql
5014,group_replication_group_seeds,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
5015,group_replication_gtid_assignment_block_size,"This system variable is a group-wide configuration setting. It must have the same value on all group members, cannot be changed while Group Replication is running, and requires a full reboot of the group (a bootstrap by a server with group_replication_bootstrap_group=ON) in order for the value change to take effect. If the group has a value set for this system variable, and a joining member has a different value set for the system variable, the joining member cannot join the group until the value is changed to match. If the group members have a value set for this system variable, and the joining member does not support the system variable, it cannot join the group.",0,0,others,mysql
5016,group_replication_ip_allowlist,"group_replication_ip_allowlist is available from MySQL 8.0.22 to replace group_replication_ip_whitelist. From MySQL 8.0.24, the value of this system variable can be changed while Group Replication is running, and the change takes effect immediately on the member.",0,0,others,mysql
5017,group_replication_ip_whitelist,"At Group Replication startup, if either one of the system variables has been set to a user-defined value and the other has not, the changed value is used. If both of the system variables have been set to a user-defined value, the value of group_replication_ip_allowlist is used.",0,0,others,mysql
5018,group_replication_local_address,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
5019,group_replication_member_expel_timeout,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately. The current value of the system variable is read whenever Group Replication checks the timeout. It is not mandatory for all members of a group to have the same setting, but it is recommended in order to avoid unexpected expulsions.",1,2,security-tradeoff,mysql
5020,group_replication_member_weight,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately. The system variable's current value is read when a failover situation occurs.",0,0,others,mysql
5022,group_replication_poll_spin_loops,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
5023,group_replication_recovery_complete_at,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
5024,group_replication_recovery_compression_algorithms,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",1,4,limited-side-effect,mysql
5025,group_replication_recovery_get_public_key,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
5026,group_replication_recovery_public_key_path,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
5029,group_replication_recovery_ssl_ca,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
5030,group_replication_recovery_ssl_capath,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
5031,group_replication_recovery_ssl_cert,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
5032,group_replication_recovery_ssl_cipher,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
5033,group_replication_recovery_ssl_crl,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
5034,group_replication_recovery_ssl_crlpath,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
5035,group_replication_recovery_ssl_key,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
5036,group_replication_recovery_ssl_verify_server_cert,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
5037,group_replication_recovery_tls_ciphersuites,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
5038,group_replication_recovery_tls_version,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
5039,group_replication_recovery_use_ssl,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
5040,group_replication_recovery_zstd_compression_level,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
5041,group_replication_single_primary_mode,"This system variable is a group-wide configuration setting. It must have the same value on all group members, cannot be changed while Group Replication is running, and requires a full reboot of the group (a bootstrap by a server with group_replication_bootstrap_group=ON) in order for the value change to take effect. If the group has a value set for this system variable, and a joining member has a different value set for the system variable, the joining member cannot join the group until the value is changed to match. If the group members have a value set for this system variable, and the joining member does not support the system variable, it cannot join the group.",0,0,others,mysql
5042,group_replication_ssl_mode,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
5043,group_replication_start_on_boot,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql
5045,group_replication_transaction_size_limit,"This system variable should have the same value on all group members. The value of this system variable can be changed while Group Replication is running. The change takes effect immediately on the group member, and applies from the next transaction started on that member. During this process, the value of the system variable is permitted to differ between group members, but some transactions might be rejected.",0,0,others,mysql
5046,group_replication_unreachable_majority_timeout,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately. The current value of the system variable is read when an issue occurs that means the behavior is needed.",0,0,others,mysql
5048,gtid_executed,"When used with global scope, this variable contains a representation of the set of all transactions executed on the server and GTIDs that have been set by a SET gtid_purged statement. This is the same as the value of the Executed_Gtid_Set column in the output of SHOW MASTER STATUS and SHOW REPLICA STATUS. The value of this variable is a GTID set, see GTID Sets for more information.",0,0,others,mysql
5049,gtid_executed_compression_period,"Compress the mysql.gtid_executed table each time this many transactions have been processed. When binary logging is enabled on the server, this compression method is not used, and instead the mysql.gtid_executed table is compressed on each binary log rotation. When binary logging is disabled on the server, the compression thread sleeps until the specified number of transactions have been executed, then wakes up to perform compression of the mysql.gtid_executed table. Setting the value of this system variable to 0 means that the thread never wakes up, so this explicit compression method is not used. Instead, compression occurs implicitly as required.",0,0,others,mysql
5050,gtid_mode,"Controls whether GTID based logging is enabled and what type of transactions the logs can contain. You must have privileges sufficient to set global system variables. See Section5.1.9.1, “System Variable Privileges”. enforce_gtid_consistency must be set to ON before you can set gtid_mode=ON. Before modifying this variable, see Section17.1.4, “Changing GTID Mode on Online Servers”.",0,0,others,mysql
5051,gtid_next,This variable is used to specify whether and how the next GTID is obtained.,0,0,others,mysql
5052,gtid_owned,This read-only variable is primarily for internal use. Its contents depend on its scope.,0,0,others,mysql
5054,Handler_discover,The MySQL server can ask the NDBCLUSTER storage engine if it knows about a table with a given name. This is called discovery. Handler_discover indicates the number of times that tables have been discovered using this mechanism.,0,0,others,mysql
5055,help,Display a short help message and exit. Use both the --verbose and --help options to see the full message.,0,0,others,mysql
5056,immediate_server_version,"For internal use by replication. This session system variable holds the MySQL Server release number of the server that is the immediate source in a replication topology (for example, 80014 for a MySQL 8.0.14 server instance). If this immediate server is at a release that does not support the session system variable, the value of the variable is set to 0 (UNKNOWN_SERVER_VERSION).",0,0,others,mysql
5057,init_replica,"init_replica is similar to init_connect, but is a string to be executed by a replica server each time the replication SQL thread starts. The format of the string is the same as for the init_connect variable. The setting of this variable takes effect for subsequent START REPLICA statements.",0,0,others,mysql
5058,init_slave,"init_slave is similar to init_connect, but is a string to be executed by a replica server each time the replication SQL thread starts. The format of the string is the same as for the init_connect variable. The setting of this variable takes effect for subsequent START REPLICA statements.",0,0,others,mysql
5059,initialize,"This option is used to initialize a MySQL installation by creating the data directory and populating the tables in the mysql system schema. For more information, see Section2.10.1, “Initializing the Data Directory”.",0,0,others,mysql
5060,initialize-insecure,"This option is used to initialize a MySQL installation by creating the data directory and populating the tables in the mysql system schema. This option implies --initialize. For more information, see the description of that option, and Section2.10.1, “Initializing the Data Directory”.",0,0,others,mysql
5061,innodb,"Controls loading of the InnoDB storage engine, if the server was compiled with InnoDB support. This option has a tristate format, with possible values of OFF, ON, or FORCE. See Section5.6.1, “Installing and Uninstalling Plugins”.",0,0,others,mysql
5063,innodb_adaptive_flushing_lwm,"Defines the low water mark representing percentage of redo log capacity at which adaptive flushing is enabled. For more information, see Section15.8.3.5, “Configuring Buffer Pool Flushing”.",0,0,others,mysql
5064,innodb_adaptive_hash_index,"Whether the InnoDB adaptive hash index is enabled or disabled. It may be desirable, depending on your workload, to dynamically enable or disable adaptive hash indexing to improve query performance. Because the adaptive hash index may not be useful for all workloads, conduct benchmarks with it both enabled and disabled, using realistic workloads. See Section15.5.3, “Adaptive Hash Index” for details.",1,5,workload-specific,mysql
5066,innodb_adaptive_max_sleep_delay,"Permits InnoDB to automatically adjust the value of innodb_thread_sleep_delay up or down according to the current workload. Any nonzero value enables automated, dynamic adjustment of the innodb_thread_sleep_delay value, up to the maximum value specified in the innodb_adaptive_max_sleep_delay option. The value represents the number of microseconds. This option can be useful in busy systems, with greater than 16 InnoDB threads. (In practice, it is most valuable for MySQL systems with hundreds or thousands of simultaneous connections.)",0,0,others,mysql
5067,innodb_api_bk_commit_interval,"How often to auto-commit idle connections that use the InnoDB memcached interface, in seconds. For more information, see Section15.20.6.4, “Controlling Transactional Behavior of the InnoDB memcached Plugin”.",1,3,reliability-tradeoff,mysql
5068,innodb_api_disable_rowlock,"Use this option to disable row locks when InnoDB memcached performs DML operations. By default, innodb_api_disable_rowlock is disabled, which means that memcached requests row locks for get and set operations. When innodb_api_disable_rowlock is enabled, memcached requests a table lock instead of row locks.",0,0,others,mysql
5069,innodb_api_enable_binlog,"Lets you use the InnoDB memcached plugin with the MySQL binary log. For more information, see Enabling the InnoDB memcached Binary Log.",0,0,others,mysql
5070,innodb_api_enable_mdl,"Locks the table used by the InnoDB memcached plugin, so that it cannot be dropped or altered by DDL through the SQL interface. For more information, see Section15.20.6.4, “Controlling Transactional Behavior of the InnoDB memcached Plugin”.",0,0,others,mysql
5071,innodb_api_trx_level,Controls the transaction isolation level on queries processed by the memcached interface. The constants corresponding to the familiar names are:,0,0,others,mysql
5072,innodb_autoextend_increment,"The increment size (in megabytes) for extending the size of an auto-extending InnoDB system tablespace file when it becomes full. The default value is 64. For related information, see System Tablespace Data File Configuration, and Resizing the System Tablespace.",0,0,others,mysql
5073,innodb_autoinc_lock_mode,"The lock mode to use for generating auto-increment values. Permissible values are 0, 1, or 2, for traditional, consecutive, or interleaved, respectively.",0,0,others,mysql
5074,innodb_background_drop_list_empty,"Enabling the innodb_background_drop_list_empty debug option helps avoid test case failures by delaying table creation until the background drop list is empty. For example, if test case A places table t1 on the background drop list, test case B waits until the background drop list is empty before creating table t1.",0,0,others,mysql
5075,innodb_buffer_pool_chunk_size,innodb_buffer_pool_chunk_size defines the chunk size for InnoDB buffer pool resizing operations.,1,1,resource,mysql
5076,innodb_buffer_pool_debug,"Enabling this option permits multiple buffer pool instances when the buffer pool is less than 1GB in size, ignoring the 1GB minimum buffer pool size constraint imposed on innodb_buffer_pool_instances. The innodb_buffer_pool_debug option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.",0,0,others,mysql
5079,innodb_buffer_pool_dump_pct,"Specifies the percentage of the most recently used pages for each buffer pool to read out and dump. The range is 1 to 100. The default value is 25. For example, if there are 4 buffer pools with 100 pages each, and innodb_buffer_pool_dump_pct is set to 25, the 25 most recently used pages from each buffer pool are dumped.",0,0,others,mysql
5081,innodb_buffer_pool_in_core_file,"Disabling the innodb_buffer_pool_in_core_file variable reduces the size of core files by excluding InnoDB buffer pool pages. To use this variable, the core_file variable must be enabled and the operating system must support the MADV_DONTDUMP non-POSIX extension to madvise(), which is supported in Linux 3.4 and later. For more information, see Section15.8.3.7, “Excluding Buffer Pool Pages from Core Files”.",1,4,limited-side-effect,mysql
5082,innodb_buffer_pool_instances,"The number of regions that the InnoDB buffer pool is divided into. For systems with buffer pools in the multi-gigabyte range, dividing the buffer pool into separate instances can improve concurrency, by reducing contention as different threads read and write to cached pages. Each page that is stored in or read from the buffer pool is assigned to one of the buffer pool instances randomly, using a hashing function. Each buffer pool manages its own free lists, flush lists, LRUs, and all other data structures connected to a buffer pool, and is protected by its own buffer pool mutex.",0,0,others,mysql
5083,innodb_buffer_pool_load_abort,Interrupts the process of restoring InnoDB buffer pool contents triggered by innodb_buffer_pool_load_at_startup or innodb_buffer_pool_load_now.,0,0,others,mysql
5084,innodb_buffer_pool_load_at_startup,"Specifies that, on MySQL server startup, the InnoDB buffer pool is automatically warmed up by loading the same pages it held at an earlier time. Typically used in combination with innodb_buffer_pool_dump_at_shutdown.",0,0,others,mysql
5085,innodb_buffer_pool_load_now,"Immediately warms up the InnoDB buffer pool by loading a set of data pages, without waiting for a server restart. Can be useful to bring cache memory back to a known state during benchmarking, or to ready the MySQL server to resume its normal workload after running queries for reports or maintenance.",1,3,reliability-tradeoff,mysql
5086,innodb_buffer_pool_size,"The size in bytes of the buffer pool, the memory area where InnoDB caches table and index data. The default value is 134217728 bytes (128MB). The maximum value depends on the CPU architecture; the maximum is 4294967295 (232-1) on 32-bit systems and 18446744073709551615 (264-1) on 64-bit systems. On 32-bit systems, the CPU architecture and operating system may impose a lower practical maximum size than the stated maximum. When the size of the buffer pool is greater than 1GB, setting innodb_buffer_pool_instances to a value greater than 1 can improve the scalability on a busy server.",1,1,resource,mysql
5087,innodb_change_buffer_max_size,"Maximum size for the InnoDB change buffer, as a percentage of the total size of the buffer pool. You might increase this value for a MySQL server with heavy insert, update, and delete activity, or decrease it for a MySQL server with unchanging data used for reporting. For more information, see Section15.5.2, “Change Buffer”. For general I/O tuning advice, see Section8.5.8, “Optimizing InnoDB Disk I/O”.",1,1,resource,mysql
5088,innodb_change_buffering,"Whether InnoDB performs change buffering, an optimization that delays write operations to secondary indexes so that the I/O operations can be performed sequentially. Permitted values are described in the following table. Values may also be specified numerically.",1,6,function-tradeoff,mysql
5089,innodb_change_buffering_debug,Sets a debug flag for InnoDB change buffering. A value of 1 forces all changes to the change buffer. A value of 2 causes an unexpected exit at merge. A default value of 0 indicates that the change buffering debug flag is not set. This option is only available when debugging support is compiled in using the WITH_DEBUG CMake option.,0,0,others,mysql
5090,innodb_checkpoint_disabled,"This is a debug option that is only intended for expert debugging use. It disables checkpoints so that a deliberate server exit always initiates InnoDB recovery. It should only be enabled for a short interval, typically before running DML operations that write redo log entries that would require recovery following a server exit. This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.",0,0,others,mysql
5093,innodb_commit_concurrency,The number of threads that can commit at the same time. A value of 0 (the default) permits any number of transactions to commit simultaneously.,1,1,resource,mysql
5094,innodb_compress_debug,Compresses all tables using a specified compression algorithm without having to define a COMPRESSION attribute for each table. This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.,0,0,others,mysql
5097,innodb_compression_pad_pct_max,"Specifies the maximum percentage that can be reserved as free space within each compressed page, allowing room to reorganize the data and modification log within the page when a compressed table or index is updated and the data might be recompressed. Only applies when innodb_compression_failure_threshold_pct is set to a nonzero value, and the rate of compression failures passes the cutoff point.",0,0,others,mysql
5098,innodb_concurrency_tickets,"Determines the number of threads that can enter InnoDB concurrently. A thread is placed in a queue when it tries to enter InnoDB if the number of threads has already reached the concurrency limit. When a thread is permitted to enter InnoDB, it is given a number of “ tickets” equal to the value of innodb_concurrency_tickets, and the thread can enter and leave InnoDB freely until it has used up its tickets. After that point, the thread again becomes subject to the concurrency check (and possible queuing) the next time it tries to enter InnoDB. The default value is 5000.",1,1,resource,mysql
5099,innodb_data_file_path,"Defines the name, size, and attributes of InnoDB system tablespace data files. If you do not specify a value for innodb_data_file_path, the default behavior is to create a single auto-extending data file, slightly larger than 12MB, named ibdata1.",0,0,others,mysql
5100,innodb_data_home_dir,"The common part of the directory path for InnoDB system tablespace data files. The default value is the MySQL data directory. The setting is concatenated with the innodb_data_file_path setting, unless that setting is defined with an absolute path.",0,0,others,mysql
5102,innodb_deadlock_detect,"This option is used to disable deadlock detection. On high concurrency systems, deadlock detection can cause a slowdown when numerous threads wait for the same lock. At times, it may be more efficient to disable deadlock detection and rely on the innodb_lock_wait_timeout setting for transaction rollback when a deadlock occurs.",1,6,function-tradeoff,mysql
5104,innodb_default_row_format,"The innodb_default_row_format option defines the default row format for InnoDB tables and user-created temporary tables. The default setting is DYNAMIC. Other permitted values are COMPACT and REDUNDANT. The COMPRESSED row format, which is not supported for use in the system tablespace, cannot be defined as the default.",0,0,others,mysql
5105,innodb_directories,Defines directories to scan at startup for tablespace files. This option is used when moving or restoring tablespace files to a new location while the server is offline. It is also used to specify directories of tablespace files created using an absolute path or that reside outside of the data directory.,0,0,others,mysql
5106,innodb_disable_sort_file_cache,Disables the operating system file system cache for merge-sort temporary files. The effect is to open such files with the equivalent of O_DIRECT.,0,0,others,mysql
5108,innodb_doublewrite_batch_size,Defines the number of doublewrite pages to write in a batch.,1,1,resource,mysql
5109,innodb_doublewrite_dir,"Defines the directory for doublewrite files. If no directory is specified, doublewrite files are created in the innodb_data_home_dir directory, which defaults to the data directory if unspecified.",0,0,others,mysql
5111,innodb_doublewrite_pages,"Defines the maximum number of doublewrite pages per thread for a batch write. If no value is specified, innodb_doublewrite_pages is set to the innodb_write_io_threads value.",1,1,resource,mysql
5112,innodb_extend_and_initialize,Controls how space is allocated to file-per-table and general tablespaces on Linux systems.,1,4,limited-side-effect,mysql
5113,innodb_fast_shutdown,"The InnoDB shutdown mode. If the value is 0, InnoDB does a slow shutdown, a full purge and a change buffer merge before shutting down. If the value is 1 (the default), InnoDB skips these operations at shutdown, a process known as a fast shutdown. If the value is 2, InnoDB flushes its logs and shuts down cold, as if MySQL had crashed; no committed transactions are lost, but the crash recovery operation makes the next startup take longer.",0,0,others,mysql
5114,innodb_fil_make_page_dirty_debug,"By default, setting innodb_fil_make_page_dirty_debug to the ID of a tablespace immediately dirties the first page of the tablespace. If innodb_saved_page_number_debug is set to a non-default value, setting innodb_fil_make_page_dirty_debug dirties the specified page. The innodb_fil_make_page_dirty_debug option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.",0,0,others,mysql
5115,innodb_file_per_table,"When innodb_file_per_table is enabled, tables are created in file-per-table tablespaces by default. When disabled, tables are created in the system tablespace by default. For information about file-per-table tablespaces, see Section15.6.3.2, “File-Per-Table Tablespaces”. For information about the InnoDB system tablespace, see Section15.6.3.1, “The System Tablespace”.",1,5,workload-specific,mysql
5116,innodb_fill_factor,InnoDB performs a bulk load when creating or rebuilding indexes. This method of index creation is known as a “sorted index build”.,0,0,others,mysql
5117,innodb_flush_log_at_timeout,Write and flush the logs every N seconds. innodb_flush_log_at_timeout allows the timeout period between flushes to be increased in order to reduce flushing and avoid impacting performance of binary log group commit. The default setting for innodb_flush_log_at_timeout is once per second.,0,0,others,mysql
5122,innodb_flushing_avg_loops,"Number of iterations for which InnoDB keeps the previously calculated snapshot of the flushing state, controlling how quickly adaptive flushing responds to changing workloads. Increasing the value makes the rate of flush operations change smoothly and gradually as the workload changes. Decreasing the value makes adaptive flushing adjust quickly to workload changes, which can cause spikes in flushing activity if the workload increases and decreases suddenly.",1,5,workload-specific,mysql
5123,innodb_force_load_corrupted,"Permits InnoDB to load tables at startup that are marked as corrupted. Use only during troubleshooting, to recover data that is otherwise inaccessible. When troubleshooting is complete, disable this setting and restart the server.",0,0,others,mysql
5124,innodb_force_recovery,"The crash recovery mode, typically only changed in serious troubleshooting situations. Possible values are from 0 to 6. For the meanings of these values and important information about innodb_force_recovery, see Section15.21.3, “Forcing InnoDB Recovery”.",0,0,others,mysql
5125,innodb_fsync_threshold,"By default, when InnoDB creates a new data file, such as a new log file or tablespace file, the file is fully written to the operating system cache before it is flushed to disk, which can cause a large amount of disk write activity to occur at once. To force smaller, periodic flushes of data from the operating system cache, you can use the innodb_fsync_threshold variable to define a threshold value, in bytes. When the byte threshold is reached, the contents of the operating system cache are flushed to disk. The default value of 0 forces the default behavior, which is to flush data to disk only after a file is fully written to the cache.",1,5,workload-specific,mysql
5127,innodb_ft_cache_size,"The memory allocated, in bytes, for the InnoDB FULLTEXT search index cache, which holds a parsed document in memory while creating an InnoDB FULLTEXT index. Index inserts and updates are only committed to disk when the innodb_ft_cache_size size limit is reached. innodb_ft_cache_size defines the cache size on a per table basis. To set a global limit for all tables, see innodb_ft_total_cache_size.",1,1,resource,mysql
5128,innodb_ft_enable_diag_print,Whether to enable additional full-text search (FTS) diagnostic output. This option is primarily intended for advanced FTS debugging and is not of interest to most users. Output is printed to the error log and includes information such as:,0,0,others,mysql
5129,innodb_ft_enable_stopword,"Specifies that a set of stopwords is associated with an InnoDB FULLTEXT index at the time the index is created. If the innodb_ft_user_stopword_table option is set, the stopwords are taken from that table. Else, if the innodb_ft_server_stopword_table option is set, the stopwords are taken from that table. Otherwise, a built-in set of default stopwords is used.",0,0,others,mysql
5130,innodb_ft_max_token_size,"Maximum character length of words that are stored in an InnoDB FULLTEXT index. Setting a limit on this value reduces the size of the index, thus speeding up queries, by omitting long keywords or arbitrary collections of letters that are not real words and are not likely to be search terms.",1,1,resource,mysql
5131,innodb_ft_min_token_size,"Minimum length of words that are stored in an InnoDB FULLTEXT index. Increasing this value reduces the size of the index, thus speeding up queries, by omitting common words that are unlikely to be significant in a search context, such as the English words “a” and “to”. For content using a CJK (Chinese, Japanese, Korean) character set, specify a value of 1.",1,1,resource,mysql
5132,innodb_ft_num_word_optimize,"Number of words to process during each OPTIMIZE TABLE operation on an InnoDB FULLTEXT index. Because a bulk insert or update operation to a table containing a full-text search index could require substantial index maintenance to incorporate all changes, you might do a series of OPTIMIZE TABLE statements, each picking up where the last left off.",1,5,workload-specific,mysql
5133,innodb_ft_result_cache_limit,"The InnoDB full-text search query result cache limit (defined in bytes) per full-text search query or per thread. Intermediate and final InnoDB full-text search query results are handled in memory. Use innodb_ft_result_cache_limit to place a size limit on the full-text search query result cache to avoid excessive memory consumption in case of very large InnoDB full-text search query results (millions or hundreds of millions of rows, for example). Memory is allocated as required when a full-text search query is processed. If the result cache size limit is reached, an error is returned indicating that the query exceeds the maximum allowed memory.",1,5,workload-specific,mysql
5134,innodb_ft_server_stopword_table,"This option is used to specify your own InnoDB FULLTEXT index stopword list for all InnoDB tables. To configure your own stopword list for a specific InnoDB table, use innodb_ft_user_stopword_table.",0,0,others,mysql
5135,innodb_ft_sort_pll_degree,Number of threads used in parallel to index and tokenize text in an InnoDB FULLTEXT index when building a search index.,0,0,others,mysql
5136,innodb_ft_total_cache_size,"The total memory allocated, in bytes, for the InnoDB full-text search index cache for all tables. Creating numerous tables, each with a FULLTEXT search index, could consume a significant portion of available memory. innodb_ft_total_cache_size defines a global memory limit for all full-text search indexes to help avoid excessive memory consumption. If the global limit is reached by an index operation, a forced sync is triggered.",1,1,resource,mysql
5137,innodb_ft_user_stopword_table,"This option is used to specify your own InnoDB FULLTEXT index stopword list on a specific table. To configure your own stopword list for all InnoDB tables, use innodb_ft_server_stopword_table.",0,0,others,mysql
5138,innodb_idle_flush_pct,"Limits page flushing when InnoDB is idle. The innodb_idle_flush_pct value is a percentage of the innodb_io_capacity setting, which defines the number of I/O operations per second available to InnoDB. For more information, see Limiting Buffer Flushing During Idle Periods.",0,0,others,mysql
5139,innodb_io_capacity,"The innodb_io_capacity variable defines the number of I/O operations per second (IOPS) available to InnoDB background tasks, such as flushing pages from the buffer pool and merging data from the change buffer.",1,1,resource,mysql
5140,innodb_io_capacity_max,"If flushing activity falls behind, InnoDB can flush more aggressively, at a higher rate of I/O operations per second (IOPS) than defined by the innodb_io_capacity variable. The innodb_io_capacity_max variable defines a maximum number of IOPS performed by InnoDB background tasks in such situations.",1,3,reliability-tradeoff,mysql
5141,innodb_limit_optimistic_insert_debug,Limits the number of records per B-tree page. A default value of 0 means that no limit is imposed. This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.,1,1,resource,mysql
5142,innodb_lock_wait_timeout,The length of time in seconds an InnoDB transaction waits for a row lock before giving up. The default value is 50 seconds. A transaction that tries to access a row that is locked by another InnoDB transaction waits at most this many seconds for write access to the row before issuing the following error:,0,0,others,mysql
5143,innodb_log_buffer_size,"The size in bytes of the buffer that InnoDB uses to write to the log files on disk. The default is 16MB. A large log buffer enables large transactions to run without the need to write the log to disk before the transactions commit. Thus, if you have transactions that update, insert, or delete many rows, making the log buffer larger saves disk I/O. For related information, see Memory Configuration, and Section8.5.4, “Optimizing InnoDB Redo Logging”. For general I/O tuning advice, see Section8.5.8, “Optimizing InnoDB Disk I/O”.",1,1,resource,mysql
5144,innodb_log_checkpoint_fuzzy_now,Enable this debug option to force InnoDB to write a fuzzy checkpoint. This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.,1,6,function-tradeoff,mysql
5145,innodb_log_checkpoint_now,Enable this debug option to force InnoDB to write a checkpoint. This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.,1,6,function-tradeoff,mysql
5146,innodb_log_checksums,Enables or disables checksums for redo log pages.,1,6,function-tradeoff,mysql
5147,innodb_log_compressed_pages,Specifies whether images of re-compressed pages are written to the redo log. Re-compression may occur when changes are made to compressed data.,1,5,workload-specific,mysql
5149,innodb_log_files_in_group,The number of log files in the log group. InnoDB writes to the files in a circular fashion. The default (and recommended) value is 2. The location of the files is specified by innodb_log_group_home_dir. The combined size of log files (innodb_log_file_size * innodb_log_files_in_group) can be up to 512GB.,0,0,others,mysql
5150,innodb_log_group_home_dir,"The directory path to the InnoDB redo log files, whose number is specified by innodb_log_files_in_group. If you do not specify any InnoDB log variables, the default is to create two files named ib_logfile0 and ib_logfile1 in the MySQL data directory. Log file size is given by the innodb_log_file_size system variable.",0,0,others,mysql
5151,innodb_log_spin_cpu_abs_lwm,"Defines the minimum amount of CPU usage below which user threads no longer spin while waiting for flushed redo. The value is expressed as a sum of CPU core usage. For example, The default value of 80 is 80% of a single CPU core. On a system with a multi-core processor, a value of 150 represents 100% usage of one CPU core plus 50% usage of a second CPU core.",1,1,resource,mysql
5152,innodb_log_spin_cpu_pct_hwm,"Defines the maximum amount of CPU usage above which user threads no longer spin while waiting for flushed redo. The value is expressed as a percentage of the combined total processing power of all CPU cores. The default value is 50%. For example, 100% usage of two CPU cores is 50% of the combined CPU processing power on a server with four CPU cores.",1,1,resource,mysql
5153,innodb_log_wait_for_flush_spin_hwm,Defines the maximum average log flush time beyond which user threads no longer spin while waiting for flushed redo. The default value is 400 microseconds.,0,0,others,mysql
5155,innodb_log_writer_threads,"Enables dedicated log writer threads for writing redo log records from the log buffer to the system buffers and flushing the system buffers to the redo log files. Dedicated log writer threads can improve performance on high-concurrency systems, but for low-concurrency systems, disabling dedicated log writer threads provides better performance.",0,0,others,mysql
5158,innodb_max_dirty_pages_pct_lwm,"Defines a low water mark representing the percentage of dirty pages at which preflushing is enabled to control the dirty page ratio. A value of 0 disables the pre-flushing behavior entirely. The configured value should always be lower than the innodb_max_dirty_pages_pct value. For more information, see Section15.8.3.5, “Configuring Buffer Pool Flushing”.",1,4,limited-side-effect,mysql
5159,innodb_max_purge_lag,"Defines the desired maximum purge lag. If this value is exceeded, a delay is imposed on INSERT, UPDATE, and DELETE operations to allow time for purge to catch up. The default value is 0, which means there is no maximum purge lag and no delay.",1,3,reliability-tradeoff,mysql
5160,innodb_max_purge_lag_delay,Specifies the maximum delay in microseconds for the delay imposed when the innodb_max_purge_lag threshold is exceeded. The specified innodb_max_purge_lag_delay value is an upper limit on the delay period calculated by the innodb_max_purge_lag formula.,0,0,others,mysql
5161,innodb_max_undo_log_size,"Defines a threshold size for undo tablespaces. If an undo tablespace exceeds the threshold, it can be marked for truncation when innodb_undo_log_truncate is enabled. The default value is 1073741824 bytes (1024 MiB).",1,5,workload-specific,mysql
5162,innodb_merge_threshold_set_all_debug,"Defines a page-full percentage value for index pages that overrides the current MERGE_THRESHOLD setting for all indexes that are currently in the dictionary cache. This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option. For related information, see Section15.8.11, “Configuring the Merge Threshold for Index Pages”.",0,0,others,mysql
5163,innodb_monitor_disable,"Disables InnoDB metrics counters. Counter data may be queried using the INFORMATION_SCHEMA.INNODB_METRICS table. For usage information, see Section15.15.6, “InnoDB INFORMATION_SCHEMA Metrics Table”.",0,0,others,mysql
5165,innodb_monitor_reset,"Resets the count value for InnoDB metrics counters to zero. Counter data may be queried using the INFORMATION_SCHEMA.INNODB_METRICS table. For usage information, see Section15.15.6, “InnoDB INFORMATION_SCHEMA Metrics Table”.",0,0,others,mysql
5166,innodb_monitor_reset_all,"Resets all values (minimum, maximum, and so on) for InnoDB metrics counters. Counter data may be queried using the INFORMATION_SCHEMA.INNODB_METRICS table. For usage information, see Section15.15.6, “InnoDB INFORMATION_SCHEMA Metrics Table”.",0,0,others,mysql
5167,innodb_numa_interleave,"Enables the NUMA interleave memory policy for allocation of the InnoDB buffer pool. When innodb_numa_interleave is enabled, the NUMA memory policy is set to MPOL_INTERLEAVE for the mysqld process. After the InnoDB buffer pool is allocated, the NUMA memory policy is set back to MPOL_DEFAULT. For the innodb_numa_interleave option to be available, MySQL must be compiled on a NUMA-enabled Linux system.",0,0,others,mysql
5168,innodb_old_blocks_pct,"Specifies the approximate percentage of the InnoDB buffer pool used for the old block sublist. The range of values is 5 to 95. The default value is 37 (that is, 3/8 of the pool). Often used in combination with innodb_old_blocks_time.",0,0,others,mysql
5169,innodb_old_blocks_time,"Non-zero values protect against the buffer pool being filled by data that is referenced only for a brief period, such as during a full table scan. Increasing this value offers more protection against full table scans interfering with data cached in the buffer pool.",1,5,workload-specific,mysql
5170,innodb_online_alter_log_max_size,"Specifies an upper limit in bytes on the size of the temporary log files used during online DDL operations for InnoDB tables. There is one such log file for each index being created or table being altered. This log file stores data inserted, updated, or deleted in the table during the DDL operation. The temporary log file is extended when needed by the value of innodb_sort_buffer_size, up to the maximum specified by innodb_online_alter_log_max_size. If a temporary log file exceeds the upper size limit, the ALTER TABLE operation fails and all uncommitted concurrent DML operations are rolled back. Thus, a large value for this option allows more DML to happen during an online DDL operation, but also extends the period of time at the end of the DDL operation when the table is locked to apply the data from the log.",0,0,others,mysql
5171,innodb_open_files,"This variable is only relevant if you have numerous InnoDB tablespaces. It specifies the maximum number of .ibd files that MySQL can keep open at one time. The minimum value is 10. The default value is 300 if innodb_file_per_table is not enabled, and the higher of 300 and table_open_cache otherwise.",0,0,others,mysql
5172,innodb_optimize_fulltext_only,"Changes the way OPTIMIZE TABLE operates on InnoDB tables. Intended to be enabled temporarily, during maintenance operations for InnoDB tables with FULLTEXT indexes.",0,0,others,mysql
5173,innodb_page_cleaners,"The number of page cleaner threads that flush dirty pages from buffer pool instances. Page cleaner threads perform flush list and LRU flushing. When there are multiple page cleaner threads, buffer pool flushing tasks for each buffer pool instance are dispatched to idle page cleaner threads. The innodb_page_cleaners default value is 4. If the number of page cleaner threads exceeds the number of buffer pool instances, innodb_page_cleaners is automatically set to the same value as innodb_buffer_pool_instances.",1,1,resource,mysql
5174,innodb_page_size,"Specifies the page size for InnoDB tablespaces. Values can be specified in bytes or kilobytes. For example, a 16 kilobyte page size value can be specified as 16384, 16KB, or 16k.",0,0,others,mysql
5175,innodb_parallel_read_threads,"Defines the number of threads that can be used for parallel clustered index reads. Parallel scanning of partitions is supported as of MySQL 8.0.17. Parallel read threads can improve CHECK TABLE performance. InnoDB reads the clustered index twice during a CHECK TABLE operation. The second read can be performed in parallel. This feature does not apply to secondary index scans. The innodb_parallel_read_threads session variable must be set to a value greater than 1 for parallel clustered index reads to occur. The actual number of threads used to perform a parallel clustered index read is determined by the innodb_parallel_read_threads setting or the number of index subtrees to scan, whichever is smaller. The pages read into the buffer pool during the scan are kept at the tail of the buffer pool LRU list so that they can be discarded quickly when free buffer pool pages are required.",1,1,resource,mysql
5176,innodb_print_all_deadlocks,"When this option is enabled, information about all deadlocks in InnoDB user transactions is recorded in the mysqld error log. Otherwise, you see information about only the last deadlock, using the SHOW ENGINE INNODB STATUS command. An occasional InnoDB deadlock is not necessarily an issue, because InnoDB detects the condition immediately and rolls back one of the transactions automatically. You might use this option to troubleshoot why deadlocks are occurring if an application does not have appropriate error-handling logic to detect the rollback and retry its operation. A large number of deadlocks might indicate the need to restructure transactions that issue DML or SELECT ... FOR UPDATE statements for multiple tables, so that each transaction accesses the tables in the same order, thus avoiding the deadlock condition.",0,0,others,mysql
5177,innodb_print_ddl_logs,"Enabling this option causes MySQL to write DDL logs to stderr. For more information, see Viewing DDL Logs.",0,0,others,mysql
5178,innodb_purge_batch_size,"Defines the number of undo log pages that purge parses and processes in one batch from the history list. In a multithreaded purge configuration, the coordinator purge thread divides innodb_purge_batch_size by innodb_purge_threads and assigns that number of pages to each purge thread. The innodb_purge_batch_size variable also defines the number of undo log pages that purge frees after every 128 iterations through the undo logs.",1,5,workload-specific,mysql
5179,innodb_purge_rseg_truncate_frequency,"Defines the frequency with which the purge system frees rollback segments in terms of the number of times that purge is invoked. An undo tablespace cannot be truncated until its rollback segments are freed. Normally, the purge system frees rollback segments once every 128 times that purge is invoked. The default value is 128. Reducing this value increases the frequency with which the purge thread frees rollback segments.",1,5,workload-specific,mysql
5180,innodb_purge_threads,"The number of background threads devoted to the InnoDB purge operation. Increasing the value creates additional purge threads, which can improve efficiency on systems where DML operations are performed on multiple tables.",1,1,resource,mysql
5182,innodb_read_ahead_threshold,"Controls the sensitivity of linear read-ahead that InnoDB uses to prefetch pages into the buffer pool. If InnoDB reads at least innodb_read_ahead_threshold pages sequentially from an extent (64 pages), it initiates an asynchronous read for the entire following extent. The permissible range of values is 0 to 64. A value of 0 disables read-ahead. For the default of 56, InnoDB must read at least 56 pages sequentially from an extent to initiate an asynchronous read for the following extent.",0,0,others,mysql
5183,innodb_read_io_threads,"The number of I/O threads for read operations in InnoDB. Its counterpart for write threads is innodb_write_io_threads. For more information, see Section15.8.5, “Configuring the Number of Background InnoDB I/O Threads”. For general I/O tuning advice, see Section8.5.8, “Optimizing InnoDB Disk I/O”.",1,1,resource,mysql
5184,innodb_read_only,"Starts InnoDB in read-only mode. For distributing database applications or data sets on read-only media. Can also be used in data warehouses to share the same data directory between multiple instances. For more information, see Section15.8.2, “Configuring InnoDB for Read-Only Operation”.",0,0,others,mysql
5185,innodb_redo_log_archive_dirs,Defines labeled directories where redo log archive files can be created. You can define multiple labeled directories in a semicolon-separated list. For example:,0,0,others,mysql
5186,innodb_redo_log_encrypt,"Controls encryption of redo log data for tables encrypted using the InnoDB data-at-rest encryption feature. Encryption of redo log data is disabled by default. For more information, see Redo Log Encryption.",1,2,security-tradeoff,mysql
5189,innodb_rollback_segments,"innodb_rollback_segments defines the number of rollback segments allocated to each undo tablespace and the global temporary tablespace for transactions that generate undo records. The number of transactions that each rollback segment supports depends on the InnoDB page size and the number of undo logs assigned to each transaction. For more information, see Section15.6.6, “Undo Logs”.",0,0,others,mysql
5190,innodb_saved_page_number_debug,Saves a page number. Setting the innodb_fil_make_page_dirty_debug option dirties the page defined by innodb_saved_page_number_debug. The innodb_saved_page_number_debug option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.,0,0,others,mysql
5191,innodb_segment_reserve_factor,"Defines the percentage of tablespace file segment pages reserved as empty pages. The setting is applicable to file-per-table and general tablespaces. The innodb_segment_reserve_factor default setting is 12.5 percent, which is the same percentage of pages reserved in previous MySQL releases.",0,0,others,mysql
5192,innodb_sort_buffer_size,"Specifies the size of sort buffers used to sort data during creation of an InnoDB index. The specified size defines the amount of data that is read into memory for internal sorting and then written out to disk. This process is referred to as a “run”. During the merge phase, pairs of buffers of the specified size are read and merged. The larger the setting, the fewer runs and merges there are.",1,1,resource,mysql
5193,innodb_spin_wait_delay,"The maximum delay between polls for a spin lock. The low-level implementation of this mechanism varies depending on the combination of hardware and operating system, so the delay does not correspond to a fixed time interval.",0,0,others,mysql
5194,innodb_spin_wait_pause_multiplier,Defines a multiplier value used to determine the number of PAUSE instructions in spin-wait loops that occur when a thread waits to acquire a mutex or rw-lock.,0,0,others,mysql
5195,innodb_stats_auto_recalc,Causes InnoDB to automatically recalculate persistent statistics after the data in a table is changed substantially. The threshold value is 10% of the rows in the table. This setting applies to tables created when the innodb_stats_persistent option is enabled. Automatic statistics recalculation may also be configured by specifying STATS_PERSISTENT=1 in a CREATE TABLE or ALTER TABLE statement. The amount of data sampled to produce the statistics is controlled by the innodb_stats_persistent_sample_pages variable.,1,3,reliability-tradeoff,mysql
5196,innodb_stats_include_delete_marked,"By default, InnoDB reads uncommitted data when calculating statistics. In the case of an uncommitted transaction that deletes rows from a table, InnoDB excludes records that are delete-marked when calculating row estimates and index statistics, which can lead to non-optimal execution plans for other transactions that are operating on the table concurrently using a transaction isolation level other than READ UNCOMMITTED. To avoid this scenario, innodb_stats_include_delete_marked can be enabled to ensure that InnoDB includes delete-marked records when calculating persistent optimizer statistics.",1,3,reliability-tradeoff,mysql
5197,innodb_stats_method,"How the server treats NULL values when collecting statistics about the distribution of index values for InnoDB tables. Permitted values are nulls_equal, nulls_unequal, and nulls_ignored. For nulls_equal, all NULL index values are considered equal and form a single value group with a size equal to the number of NULL values. For nulls_unequal, NULL values are considered unequal, and each NULL forms a distinct value group of size 1. For nulls_ignored, NULL values are ignored.",1,5,workload-specific,mysql
5198,innodb_stats_on_metadata,"This option only applies when optimizer statistics are configured to be non-persistent. Optimizer statistics are not persisted to disk when innodb_stats_persistent is disabled or when individual tables are created or altered with STATS_PERSISTENT=0. For more information, see Section15.8.10.2, “Configuring Non-Persistent Optimizer Statistics Parameters”.",0,0,others,mysql
5199,innodb_stats_persistent,"Specifies whether InnoDB index statistics are persisted to disk. Otherwise, statistics may be recalculated frequently which can lead to variations in query execution plans. This setting is stored with each table when the table is created. You can set innodb_stats_persistent at the global level before creating a table, or use the STATS_PERSISTENT clause of the CREATE TABLE and ALTER TABLE statements to override the system-wide setting and configure persistent statistics for individual tables.",0,0,others,mysql
5200,innodb_stats_persistent_sample_pages,"The number of index pages to sample when estimating cardinality and other statistics for an indexed column, such as those calculated by ANALYZE TABLE. Increasing the value improves the accuracy of index statistics, which can improve the query execution plan, at the expense of increased I/O during the execution of ANALYZE TABLE for an InnoDB table. For more information, see Section15.8.10.1, “Configuring Persistent Optimizer Statistics Parameters”.",1,5,workload-specific,mysql
5201,innodb_stats_transient_sample_pages,"The number of index pages to sample when estimating cardinality and other statistics for an indexed column, such as those calculated by ANALYZE TABLE. The default value is 8. Increasing the value improves the accuracy of index statistics, which can improve the query execution plan, at the expense of increased I/O when opening an InnoDB table or recalculating statistics. For more information, see Section15.8.10.2, “Configuring Non-Persistent Optimizer Statistics Parameters”.",1,5,workload-specific,mysql
5202,innodb_status_output,"Enables or disables periodic output for the standard InnoDB Monitor. Also used in combination with innodb_status_output_locks to enable or disable periodic output for the InnoDB Lock Monitor. For more information, see Section15.17.2, “Enabling InnoDB Monitors”.",0,0,others,mysql
5203,innodb_status_output_locks,"Enables or disables the InnoDB Lock Monitor. When enabled, the InnoDB Lock Monitor prints additional information about locks in SHOW ENGINE INNODB STATUS output and in periodic output printed to the MySQL error log. Periodic output for the InnoDB Lock Monitor is printed as part of the standard InnoDB Monitor output. The standard InnoDB Monitor must therefore be enabled for the InnoDB Lock Monitor to print data to the MySQL error log periodically. For more information, see Section15.17.2, “Enabling InnoDB Monitors”.",1,6,function-tradeoff,mysql
5204,innodb_strict_mode,"When innodb_strict_mode is enabled, InnoDB returns errors rather than warnings for certain conditions.",0,0,others,mysql
5205,innodb_sync_array_size,"Defines the size of the mutex/lock wait array. Increasing the value splits the internal data structure used to coordinate threads, for higher concurrency in workloads with large numbers of waiting threads. This setting must be configured when the MySQL instance is starting up, and cannot be changed afterward. Increasing the value is recommended for workloads that frequently produce a large number of waiting threads, typically greater than 768.",1,5,workload-specific,mysql
5208,innodb_table_locks,"If autocommit = 0, InnoDB honors LOCK TABLES; MySQL does not return from LOCK TABLES ... WRITE until all other threads have released all their locks to the table. The default value of innodb_table_locks is 1, which means that LOCK TABLES causes InnoDB to lock a table internally if autocommit = 0.",0,0,others,mysql
5209,innodb_temp_data_file_path,"Defines the relative path, name, size, and attributes of global temporary tablespace data files. The global temporary tablespace stores rollback segments for changes made to user-created temporary tables.",0,0,others,mysql
5210,innodb_temp_tablespaces_dir,Defines the location where InnoDB creates a pool of session temporary tablespaces at startup. The default location is the #innodb_temp directory in the data directory. A fully qualified path or path relative to the data directory is permitted.,0,0,others,mysql
5211,innodb_thread_concurrency,Defines the maximum number of threads permitted inside of InnoDB. A value of 0 (the default) is interpreted as infinite concurrency (no limit). This variable is intended for performance tuning on high concurrency systems.,1,1,resource,mysql
5212,innodb_thread_sleep_delay,"How long InnoDB threads sleep before joining the InnoDB queue, in microseconds. The default value is 10000. A value of 0 disables sleep. You can set innodb_adaptive_max_sleep_delay to the highest value you would allow for innodb_thread_sleep_delay, and InnoDB automatically adjusts innodb_thread_sleep_delay up or down depending on current thread-scheduling activity. This dynamic adjustment helps the thread scheduling mechanism to work smoothly during times when the system is lightly loaded or when it is operating near full capacity.",0,0,others,mysql
5213,innodb_tmpdir,Used to define an alternate directory for temporary sort files created during online ALTER TABLE operations that rebuild the table.,0,0,others,mysql
5214,innodb_trx_purge_view_update_only_debug,Pauses purging of delete-marked records while allowing the purge view to be updated. This option artificially creates a situation in which the purge view is updated but purges have not yet been performed. This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.,1,2,security-tradeoff,mysql
5216,innodb_undo_directory,The path where InnoDB creates undo tablespaces. Typically used to place undo tablespaces on a different storage device.,0,0,others,mysql
5217,innodb_undo_log_encrypt,"Controls encryption of undo log data for tables encrypted using the InnoDB data-at-rest encryption feature. Only applies to undo logs that reside in separate undo tablespaces. See Section15.6.3.4, “Undo Tablespaces”. Encryption is not supported for undo log data that resides in the system tablespace. For more information, see Undo Log Encryption.",1,2,security-tradeoff,mysql
5218,innodb_undo_log_truncate,"When enabled, undo tablespaces that exceed the threshold value defined by innodb_max_undo_log_size are marked for truncation. Only undo tablespaces can be truncated. Truncating undo logs that reside in the system tablespace is not supported. For truncation to occur, there must be at least two undo tablespaces.",0,0,others,mysql
5219,innodb_undo_tablespaces,Defines the number of undo tablespaces used by InnoDB. The default and minimum value is 2.,1,1,resource,mysql
5220,innodb_use_fdatasync,"On platforms that support fdatasync() system calls, enabling the innodb_use_fdatasync variable permits using fdatasync() instead of fsync() system calls for operating system flushes. An fdatasync() call does not flush changes to file metadata unless required for subsequent data retrieval, providing a potential performance benefit.",0,0,others,mysql
5221,innodb_use_native_aio,"Specifies whether to use the Linux asynchronous I/O subsystem. This variable applies to Linux systems only, and cannot be changed while the server is running. Normally, you do not need to configure this option, because it is enabled by default.",0,0,others,mysql
5222,innodb_validate_tablespace_paths,"Controls tablespace file path validation. At startup, InnoDB validates the paths of known tablespace files against tablespace file paths stored in the data dictionary in case tablespace files have been moved to a different location. The innodb_validate_tablespace_paths variable permits disabling tablespace path validation. This feature is intended for environments where tablespaces files are not moved. Disabling path validation improves startup time on systems with a large number of tablespace files.",0,0,others,mysql
5223,innodb_version,"The InnoDB version number. In MySQL 8.0, separate version numbering for InnoDB does not apply and this value is the same the version number of the server.",0,0,others,mysql
5225,innodb-status-file,"The --innodb-status-file startup option controls whether InnoDB creates a file named innodb_status.pid in the data directory and writes SHOW ENGINE INNODB STATUS output to it every 15 seconds, approximately.",0,0,others,mysql
5226,install,"(Windows only) Install the server as a Windows service that starts automatically during Windows startup. The default service name is MySQL if no service_name value is given. For more information, see Section2.3.4.8, “Starting MySQL as a Windows Service”.",0,0,others,mysql
5229,keyring_aws_conf_file,The location of the configuration file for the keyring_aws plugin. This variable is unavailable unless that plugin is installed.,0,0,others,mysql
5230,keyring_aws_data_file,The location of the storage file for the keyring_aws plugin. This variable is unavailable unless that plugin is installed.,0,0,others,mysql
5231,keyring_aws_region,The AWS region for the keyring_aws plugin. This variable is unavailable unless that plugin is installed.,0,0,others,mysql
5233,keyring_encrypted_file_password,The password used by the keyring_encrypted_file plugin. This variable is unavailable unless that plugin is installed.,0,0,others,mysql
5234,keyring_file_data,"The path name of the data file used for secure data storage by the keyring_file plugin. This variable is unavailable unless that plugin is installed. The file location should be in a directory considered for use only by keyring plugins. For example, do not locate the file under the data directory.",0,0,others,mysql
5235,keyring_hashicorp_auth_path,"The authentication path where AppRole authentication is enabled within the HashiCorp Vault server, for use by the keyring_hashicorp plugin. This variable is unavailable unless that plugin is installed.",0,0,others,mysql
5238,keyring_hashicorp_commit_auth_path,"This variable is associated with keyring_hashicorp_auth_path, from which it takes its value during keyring_hashicorp plugin initialization. This variable is unavailable unless that plugin is installed. It reflects the “committed” value actually used for plugin operation if initialization succeeds. For additional information, see keyring_hashicorp Configuration.",0,0,others,mysql
5242,keyring_hashicorp_commit_server_url,"This variable is associated with keyring_hashicorp_server_url, from which it takes its value during keyring_hashicorp plugin initialization. This variable is unavailable unless that plugin is installed. It reflects the “committed” value actually used for plugin operation if initialization succeeds. For additional information, see keyring_hashicorp Configuration.",0,0,others,mysql
5243,keyring_hashicorp_commit_store_path,"This variable is associated with keyring_hashicorp_store_path, from which it takes its value during keyring_hashicorp plugin initialization. This variable is unavailable unless that plugin is installed. It reflects the “committed” value actually used for plugin operation if initialization succeeds. For additional information, see keyring_hashicorp Configuration.",0,0,others,mysql
5244,keyring_hashicorp_role_id,"The HashiCorp Vault AppRole authentication role ID, for use by the keyring_hashicorp plugin. This variable is unavailable unless that plugin is installed. The value must be in UUID format.",0,0,others,mysql
5245,keyring_hashicorp_secret_id,"The HashiCorp Vault AppRole authentication secret ID, for use by the keyring_hashicorp plugin. This variable is unavailable unless that plugin is installed. The value must be in UUID format.",0,0,others,mysql
5246,keyring_hashicorp_server_url,"The HashiCorp Vault server URL, for use by the keyring_hashicorp plugin. This variable is unavailable unless that plugin is installed. The value must begin with https://.",0,0,others,mysql
5247,keyring_hashicorp_store_path,"A store path within the HashiCorp Vault server that is writeable when appropiate AppRole credentials are provided by the keyring_hashicorp plugin. This variable is unavailable unless that plugin is installed. To specify the credentials, set the keyring_hashicorp_role_id and keyring_hashicorp_secret_id system variables (for example, as shown in keyring_hashicorp Configuration).",0,0,others,mysql
5250,keyring_oci_encryption_endpoint,The endpoint of the Oracle Cloud Infrastructure encryption server that the keyring_oci plugin uses for generating ciphertext for new keys. This variable is unavailable unless that plugin is installed.,0,0,others,mysql
5251,keyring_oci_key_file,The path name of the file containing the RSA private key that the keyring_oci plugin uses for Oracle Cloud Infrastructure authentication. This variable is unavailable unless that plugin is installed.,0,0,others,mysql
5252,keyring_oci_key_fingerprint,The fingerprint of the RSA private key that the keyring_oci plugin uses for Oracle Cloud Infrastructure authentication. This variable is unavailable unless that plugin is installed.,0,0,others,mysql
5253,keyring_oci_management_endpoint,The endpoint of the Oracle Cloud Infrastructure key management server that the keyring_oci plugin uses for listing existing keys. This variable is unavailable unless that plugin is installed.,0,0,others,mysql
5254,keyring_oci_master_key,The OCID of the Oracle Cloud Infrastructure master encryption key that the keyring_oci plugin uses for encryption of secrets. This variable is unavailable unless that plugin is installed.,0,0,others,mysql
5256,keyring_oci_tenancy,The OCID of the Oracle Cloud Infrastructure tenancy that the keyring_oci plugin uses as the location of the MySQL compartment. This variable is unavailable unless that plugin is installed.,0,0,others,mysql
5257,keyring_oci_user,The OCID of the Oracle Cloud Infrastructure user that the keyring_oci plugin uses for cloud connections. This variable is unavailable unless that plugin is installed.,0,0,others,mysql
5259,keyring_oci_virtual_vault,The OCID of the Oracle Cloud Infrastructure Vault that the keyring_oci plugin uses for encryption operations. This variable is unavailable unless that plugin is installed.,0,0,others,mysql
5260,keyring_okv_conf_dir,"The path name of the directory that stores configuration information used by the keyring_okv plugin. This variable is unavailable unless that plugin is installed. The location should be a directory considered for use only by the keyring_okv plugin. For example, do not locate the directory under the data directory.",0,0,others,mysql
5262,keyring-migration-destination,"The destination keyring plugin for key migration. See Section6.4.4.13, “Migrating Keys Between Keyring Keystores”. The option value interpretation depends on whether --keyring-migration-to-component is specified:",0,0,others,mysql
5263,keyring-migration-host,"The host location of the running server that is currently using one of the key migration keystores. See Section6.4.4.13, “Migrating Keys Between Keyring Keystores”. Migration always occurs on the local host, so the option always specifies a value for connecting to a local server, such as localhost, 127.0.0.1, ::1, or the local host IP address or host name.",0,0,others,mysql
5266,keyring-migration-socket,"For Unix socket file or Windows named pipe connections, the socket file or named pipe for connecting to the running server that is currently using one of the key migration keystores. See Section6.4.4.13, “Migrating Keys Between Keyring Keystores”.",0,0,others,mysql
5267,keyring-migration-source,"The source keyring plugin for key migration. See Section6.4.4.13, “Migrating Keys Between Keyring Keystores”.",0,0,others,mysql
5268,keyring-migration-to-component,"Indicates that a key migration is from a keyring plugin to a keyring component. This option makes it possible to migrate keys from any keyring plugin to any keyring component, which facilitates transitioning a MySQL installation from keyring plugins to keyring components.",0,0,others,mysql
5272,lc_messages,"The locale to use for error messages. The default is en_US. The server converts the argument to a language name and combines it with the value of --lc-messages-dir to produce the location for the error message file. See Section10.12, “Setting the Error Message Language”.",0,0,others,mysql
5273,lc_messages_dir,"The directory where error messages are located. The server uses the value together with the value of --lc-messages to produce the location for the error message file. See Section10.12, “Setting the Error Message Language”.",0,0,others,mysql
5274,local-service,"(Windows only) A --local-service option following the service name causes the server to run using the LocalService Windows account that has limited system privileges. If both --defaults-file and --local-service are given following the service name, they can be in any order. See Section2.3.4.8, “Starting MySQL as a Windows Service”.",0,0,others,mysql
5277,lock_order_debug_missing_arc,Whether the LOCK_ORDER tool causes a debug assertion failure when it encounters a dependency that is not declared in the lock-order graph.,0,0,others,mysql
5278,lock_order_debug_missing_key,Whether the LOCK_ORDER tool causes a debug assertion failure when it encounters an object that is not properly instrumented with the Performance Schema.,0,0,others,mysql
5279,lock_order_debug_missing_unlock,Whether the LOCK_ORDER tool causes a debug assertion failure when it encounters a lock that is destroyed while still held.,0,0,others,mysql
5280,lock_order_dependencies,The path to the lock_order_dependencies.txt file that defines the server lock-order dependency graph.,0,0,others,mysql
5281,lock_order_extra_dependencies,"The path to a file containing additional dependencies for the lock-order dependency graph. This is useful to amend the primary server dependency graph, defined in the lock_order_dependencies.txt file, with additional dependencies describing the behavior of third party code. (The alternative is to modify lock_order_dependencies.txt itself, which is not encouraged.)",0,0,others,mysql
5282,lock_order_output_directory,"The directory where the LOCK_ORDER tool writes its logs. If this variable is not set, the default is the current directory.",0,0,others,mysql
5283,lock_order_print_txt,Whether the LOCK_ORDER tool performs a lock-order graph analysis and prints a textual report. The report includes any lock-acquisition cycles detected.,0,0,others,mysql
5285,lock_order_trace_missing_arc,Whether the LOCK_ORDER tool prints a trace in the log file when it encounters a dependency that is not declared in the lock-order graph.,0,0,others,mysql
5287,lock_order_trace_missing_unlock,Whether the LOCK_ORDER tool prints a trace in the log file when it encounters a lock that is destroyed while still held.,0,0,others,mysql
5288,log_bin,"Shows the status of binary logging on the server, either enabled (ON) or disabled (OFF). With binary logging enabled, the server logs all statements that change data to the binary log, which is used for backup and replication. ON means that the binary log is available, OFF means that it is not in use. The --log-bin option can be used to specify a base name and location for the binary log.",1,6,function-tradeoff,mysql
5289,log_bin_basename,"Holds the base name and path for the binary log files, which can be set with the --log-bin server option. The maximum variable length is 256. In MySQL 8.0, if the --log-bin option is not supplied, the default base name is binlog. For compatibility with MySQL 5.7, if the --log-bin option is supplied with no string or with an empty string, the default base name is host_name-bin, using the name of the host machine. The default location is the data directory.",0,0,others,mysql
5290,log_bin_index,"The name for the binary log index file, which contains the names of the binary log files. By default, it has the same location and base name as the value specified for the binary log files using the --log-bin option, plus the extension .index. If you do not specify --log-bin, the default binary log index file name is binlog.index. If you specify --log-bin option with no string or an empty string, the default binary log index file name is host_name-bin.index, using the name of the host machine.",0,0,others,mysql
5292,log_bin_use_v1_row_events,"This read-only system variable is deprecated. Setting the system variable to ON at server startup enabled row-based replication with replicas running MySQL Server 5.5 and earlier by writing the binary log using Version 1 binary log row events, instead of Version 2 binary log row events which are the default as of MySQL 5.6.",0,0,others,mysql
5293,log_error,"Set the default error log destination to the named file. This affects log sinks that base their own output destination on the default destination. See Section5.4.2, “The Error Log”.",0,0,others,mysql
5294,log_raw,"Passwords in certain statements written to the general query log, slow query log, and binary log are rewritten by the server not to occur literally in plain text. Password rewriting can be suppressed for the general query log by starting the server with the --log-raw option. This option may be useful for diagnostic purposes, to see the exact text of statements as received by the server, but for security reasons is not recommended for production use.",0,0,others,mysql
5295,log_replica_updates,log_replica_updates specifies whether updates received by a replica server from a replication source server should be logged to the replica's own binary log.,0,0,others,mysql
5296,log_slave_updates,log_slave_updates specifies whether updates received by a replica server from a replication source server should be logged to the replica's own binary log.,0,0,others,mysql
5297,log_slow_replica_statements,"When the slow query log is enabled, log_slow_replica_statements enables logging for queries that have taken more than long_query_time seconds to execute on the replica. Note that if row-based replication is in use (binlog_format=ROW), log_slow_replica_statements has no effect. Queries are only added to the replica's slow query log when they are logged in statement format in the binary log, that is, when binlog_format=STATEMENT is set, or when binlog_format=MIXED is set and the statement is logged in statement format. Slow queries that are logged in row format when binlog_format=MIXED is set, or that are logged when binlog_format=ROW is set, are not added to the replica's slow query log, even if log_slow_replica_statements is enabled.",1,6,function-tradeoff,mysql
5298,log_slow_slave_statements,"When the slow query log is enabled, log_slow_slave_statements enables logging for queries that have taken more than long_query_time seconds to execute on the replica. Note that if row-based replication is in use (binlog_format=ROW), log_slow_slave_statements has no effect. Queries are only added to the replica's slow query log when they are logged in statement format in the binary log, that is, when binlog_format=STATEMENT is set, or when binlog_format=MIXED is set and the statement is logged in statement format. Slow queries that are logged in row format when binlog_format=MIXED is set, or that are logged when binlog_format=ROW is set, are not added to the replica's slow query log, even if log_slow_slave_statements is enabled.",1,6,function-tradeoff,mysql
5299,log_statements_unsafe_for_binlog,"If error 1592 is encountered, controls whether the generated warnings are added to the error log or not.",0,0,others,mysql
5300,log-bin,"Specifies the base name to use for binary log files. With binary logging enabled, the server logs all statements that change data to the binary log, which is used for backup and replication. The binary log is a sequence of files with a base name and numeric extension. The --log-bin option value is the base name for the log sequence. The server creates binary log files in sequence by adding a numeric suffix to the base name.",0,0,others,mysql
5301,log-isam,Log all MyISAM changes to this file (used only when debugging MyISAM).,0,0,others,mysql
5302,log-short-format,"Log less information to the slow query log, if it has been activated.",0,0,others,mysql
5305,master_info_repository,"The use of this system variable is now deprecated. The setting TABLE is the default, and is required when multiple replication channels are configured. The alternative setting FILE was previously deprecated.",0,0,others,mysql
5306,master_verify_checksum,"Enabling master_verify_checksum causes the source to verify events read from the binary log by examining checksums, and to stop with an error in the event of a mismatch. master_verify_checksum is disabled by default; in this case, the source uses the event length from the binary log to verify events, so that only complete events are read from the binary log.",1,2,security-tradeoff,mysql
5308,master-retry-count,"The number of times that the replica tries to reconnect to the source before giving up. The default value is 86400 times. A value of 0 means “infinite”, and the replica attempts to connect forever. Reconnection attempts are triggered when the replica reaches its connection timeout (specified by the replica_net_timeout or slave_net_timeout system variable) without receiving data or a heartbeat signal from the source. Reconnection is attempted at intervals set by the SOURCE_CONNECT_RETRY | MASTER_CONNECT_RETRY option of the CHANGE REPLICATION SOURCE TO | CHANGE MASTER TO statement (which defaults to every 60 seconds).",0,0,others,mysql
5309,max_binlog_cache_size,"If a transaction requires more than this many bytes of memory, the server generates a Multi-statement transaction required more than 'max_binlog_cache_size' bytes of storage error. The minimum value is 4096. The maximum possible value is 16EiB (exbibytes). The maximum recommended value is 4GB; this is due to the fact that MySQL currently cannot work with binary log positions greater than 4GB. The value must be a multiple of 4096.",1,5,workload-specific,mysql
5310,max_binlog_size,"If a write to the binary log causes the current log file size to exceed the value of this variable, the server rotates the binary logs (closes the current file and opens the next one). The minimum value is 4096 bytes. The maximum and default value is 1GB. Encrypted binary log files have an additional 512-byte header, which is included in max_binlog_size.",1,5,workload-specific,mysql
5312,max_relay_log_size,"If a write by a replica to its relay log causes the current log file size to exceed the value of this variable, the replica rotates the relay logs (closes the current file and opens the next one). If max_relay_log_size is 0, the server uses max_binlog_size for both the binary log and the relay log. If max_relay_log_size is greater than 0, it constrains the size of the relay log, which enables you to have different sizes for the two logs. You must set max_relay_log_size to between 4096 bytes and 1GB (inclusive), or to 0. The default value is 0. See Section17.2.3, “Replication Threads”.",1,5,workload-specific,mysql
5313,max-binlog-dump-events,This option is used internally by the MySQL test suite for replication testing and debugging.,0,0,others,mysql
5314,memlock,Lock the mysqld process in memory. This option might help if you have a problem where the operating system is causing mysqld to swap to disk.,1,6,function-tradeoff,mysql
5316,mysql_firewall_mode,Whether MySQL Enterprise Firewall is enabled (the default) or disabled.,0,0,others,mysql
5317,mysql_firewall_trace,"Whether the MySQL Enterprise Firewall trace is enabled or disabled (the default). When mysql_firewall_trace is enabled, for PROTECTING mode, the firewall writes rejected statements to the error log.",0,0,others,mysql
5319,mysqlx_bind_address,The network address on which X Plugin listens for TCP/IP connections. This variable is not dynamic and can be configured only at startup. This is the X Plugin equivalent of the bind_address system variable; see that variable description for more information.,0,0,others,mysql
5320,mysqlx_compression_algorithms,"The compression algorithms that are permitted for use on X Protocol connections. By default, the Deflate, LZ4, and zstd algorithms are all permitted. To disallow any of the algorithms, set mysqlx_compression_algorithms to include only the ones you permit. The algorithm names deflate_stream, lz4_message, and zstd_stream can be specified in any combination, and the order and case are not important. If you set the system variable to the empty string, no compression algorithms are permitted and only uncompressed connections are used. Use the algorithm-specific system variables to adjust the default and maximum compression level for each permitted algorithm.",1,5,workload-specific,mysql
5322,mysqlx_deflate_default_compression_level,"The default compression level that the server uses for the Deflate algorithm on X Protocol connections. Specify the level as an integer from 1 (the lowest compression effort) to 9 (the highest effort). This level is used if the client does not request a compression level during capability negotiation. If you do not specify this system variable, the server uses level 3 as the default.",1,6,function-tradeoff,mysql
5323,mysqlx_deflate_max_client_compression_level,"The maximum compression level that the server permits for the Deflate algorithm on X Protocol connections. The range is the same as for the default compression level for this algorithm. If the client requests a higher compression level than this, the server uses the level you set here. If you do not specify this system variable, the server sets a maximum compression level of 5.",1,6,function-tradeoff,mysql
5324,mysqlx_document_id_unique_prefix,"Sets the first 4 bytes of document IDs generated by the server when documents are added to a collection. By setting this variable to a unique value per instance, you can ensure document IDs are unique across instances.",0,0,others,mysql
5325,mysqlx_enable_hello_notice,"Controls messages sent to classic MySQL protocol clients that try to connect over X Protocol. When enabled, clients which do not support X Protocol that attempt to connect to the server X Protocol port receive an error explaining they are using the wrong protocol.",0,0,others,mysql
5326,mysqlx_idle_worker_thread_timeout,The number of seconds after which idle worker threads are terminated.,0,0,others,mysql
5327,mysqlx_interactive_timeout,The default value of the mysqlx_wait_timeout session variable for interactive clients. (The number of seconds to wait for interactive clients to timeout.),0,0,others,mysql
5328,mysqlx_lz4_default_compression_level,"The default compression level that the server uses for the LZ4 algorithm on X Protocol connections. Specify the level as an integer from 0 (the lowest compression effort) to 16 (the highest effort). This level is used if the client does not request a compression level during capability negotiation. If you do not specify this system variable, the server uses level 2 as the default. For more information, see Section20.5.5, “Connection Compression with X Plugin”.",1,6,function-tradeoff,mysql
5329,mysqlx_lz4_max_client_compression_level,"The maximum compression level that the server permits for the LZ4 algorithm on X Protocol connections. The range is the same as for the default compression level for this algorithm. If the client requests a higher compression level than this, the server uses the level you set here. If you do not specify this system variable, the server sets a maximum compression level of 8.",1,6,function-tradeoff,mysql
5330,mysqlx_max_allowed_packet,"The maximum size of network packets that can be received by X Plugin. This limit also applies when compression is used for the connection, so the network packet must be smaller than this size after the message has been decompressed.",1,5,workload-specific,mysql
5331,mysqlx_max_connections,The maximum number of concurrent client connections X Plugin can accept. This is the X Plugin equivalent of max_connections; see that variable description for more information.,1,1,resource,mysql
5332,mysqlx_min_worker_threads,The minimum number of worker threads used by X Plugin for handling client requests.,1,1,resource,mysql
5334,mysqlx_port_open_timeout,The number of seconds X Plugin waits for a TCP/IP port to become free.,0,0,others,mysql
5335,mysqlx_read_timeout,"The number of seconds that X Plugin waits for blocking read operations to complete. After this time, if the read operation is not successful, X Plugin closes the connection and returns a warning notice with the error code ER_IO_READ_ERROR to the client application.",0,0,others,mysql
5337,mysqlx_ssl_ca,"The mysqlx_ssl_ca system variable is like ssl_ca, except that it applies to X Plugin rather than the MySQL Server main connection interface.",0,0,others,mysql
5338,mysqlx_ssl_capath,"The mysqlx_ssl_capath system variable is like ssl_capath, except that it applies to X Plugin rather than the MySQL Server main connection interface.",0,0,others,mysql
5339,mysqlx_ssl_cert,"The mysqlx_ssl_cert system variable is like ssl_cert, except that it applies to X Plugin rather than the MySQL Server main connection interface. For information about configuring encryption support for X Plugin, see Section20.5.3, “Using Encrypted Connections with X Plugin”.",0,0,others,mysql
5340,mysqlx_ssl_cipher,"The mysqlx_ssl_cipher system variable is like ssl_cipher, except that it applies to X Plugin rather than the MySQL Server main connection interface.",0,0,others,mysql
5342,mysqlx_ssl_crlpath,"The mysqlx_ssl_crlpath system variable is like ssl_crlpath, except that it applies to X Plugin rather than the MySQL Server main connection interface. For information about configuring encryption support for X Plugin, see Section20.5.3, “Using Encrypted Connections with X Plugin”.",0,0,others,mysql
5343,mysqlx_ssl_key,"The mysqlx_ssl_key system variable is like ssl_key, except that it applies to X Plugin rather than the MySQL Server main connection interface. For information about configuring encryption support for X Plugin, see Section20.5.3, “Using Encrypted Connections with X Plugin”.",0,0,others,mysql
5344,mysqlx_wait_timeout,"The number of seconds that X Plugin waits for activity on a connection. After this time, if the read operation is not successful, X Plugin closes the connection. If the client is noninteractive, the initial value of the session variable is copied from the global mysqlx_wait_timeout variable. For interactive clients, the initial value is copied from the session mysqlx_interactive_timeout.",0,0,others,mysql
5345,mysqlx_write_timeout,"The number of seconds that X Plugin waits for blocking write operations to complete. After this time, if the write operation is not successful, X Plugin closes the connection.",0,0,others,mysql
5346,mysqlx_zstd_default_compression_level,"The default compression level that the server uses for the zstd algorithm on X Protocol connections. For versions of the zstd library from 1.4.0, you can set positive values from 1 to 22 (the highest compression effort), or negative values which represent progressively lower effort. A value of 0 is converted to a value of 1. For earlier versions of the zstd library, you can only specify the value 3. This level is used if the client does not request a compression level during capability negotiation. If you do not specify this system variable, the server uses level 3 as the default. For more information, see Section20.5.5, “Connection Compression with X Plugin”.",1,6,function-tradeoff,mysql
5347,mysqlx_zstd_max_client_compression_level,"The maximum compression level that the server permits for the zstd algorithm on X Protocol connections. The range is the same as for the default compression level for this algorithm. If the client requests a higher compression level than this, the server uses the level you set here. If you do not specify this system variable, the server sets a maximum compression level of 11.",1,6,function-tradeoff,mysql
5348,ndb_allow_copying_alter_table,Let ALTER TABLE and other DDL statements use copying operations on NDB tables. Set to OFF to keep this from happening; doing so may improve performance of critical applications.,1,6,function-tradeoff,mysql
5351,Ndb_api_adaptive_send_deferred_count_session,Number of adaptive send calls that were not actually sent.,0,0,others,mysql
5352,Ndb_api_adaptive_send_deferred_count_slave,Number of adaptive send calls that were not actually sent by this replica.,0,0,others,mysql
5353,Ndb_api_adaptive_send_forced_count,Number of adaptive send calls using forced-send sent by this MySQL Server (SQL node).,0,0,others,mysql
5354,Ndb_api_adaptive_send_forced_count_replica,Number of adaptive send calls using forced-send sent by this replica.,0,0,others,mysql
5356,Ndb_api_adaptive_send_forced_count_slave,Number of adaptive send calls using forced-send sent by this replica.,0,0,others,mysql
5358,Ndb_api_adaptive_send_unforced_count_replica,Number of adaptive send calls without forced-send sent by this replica.,0,0,others,mysql
5359,Ndb_api_adaptive_send_unforced_count_session,Number of adaptive send calls without forced-send sent in this client session.,0,0,others,mysql
5360,Ndb_api_adaptive_send_unforced_count_slave,Number of adaptive send calls without forced-send sent by this replica.,0,0,others,mysql
5361,Ndb_api_bytes_received_count,Amount of data (in bytes) received from the data nodes by this MySQL Server (SQL node).,0,0,others,mysql
5362,Ndb_api_bytes_received_count_replica,Amount of data (in bytes) received from the data nodes by this replica.,0,0,others,mysql
5363,Ndb_api_bytes_received_count_session,Amount of data (in bytes) received from the data nodes in this client session.,0,0,others,mysql
5364,Ndb_api_bytes_received_count_slave,Amount of data (in bytes) received from the data nodes by this replica.,0,0,others,mysql
5365,Ndb_api_bytes_sent_count,Amount of data (in bytes) sent to the data nodes by this MySQL Server (SQL node).,0,0,others,mysql
5366,Ndb_api_bytes_sent_count_replica,Amount of data (in bytes) sent to the data nodes by this replica.,0,0,others,mysql
5367,Ndb_api_bytes_sent_count_session,Amount of data (in bytes) sent to the data nodes in this client session.,0,0,others,mysql
5368,Ndb_api_bytes_sent_count_slave,Amount of data (in bytes) sent to the data nodes by this replica.,0,0,others,mysql
5369,Ndb_api_event_bytes_count,The number of bytes of events received by this MySQL Server (SQL node).,0,0,others,mysql
5370,Ndb_api_event_bytes_count_injector,The number of bytes of events received by the NDB binlog injector thread.,0,0,others,mysql
5373,Ndb_api_event_nondata_count,"The number of events received, other than row change events, by this MySQL Server (SQL node).",0,0,others,mysql
5376,Ndb_api_pk_op_count_replica,"The number of operations by this replica based on or using primary keys. This includes operations on blob tables, implicit unlock operations, and auto-increment operations, as well as user-visible primary key operations.",0,0,others,mysql
5377,Ndb_api_pk_op_count_session,"The number of operations in this client session based on or using primary keys. This includes operations on blob tables, implicit unlock operations, and auto-increment operations, as well as user-visible primary key operations.",0,0,others,mysql
5378,Ndb_api_pk_op_count_slave,"The number of operations by this replica based on or using primary keys. This includes operations on blob tables, implicit unlock operations, and auto-increment operations, as well as user-visible primary key operations.",0,0,others,mysql
5379,Ndb_api_pruned_scan_count,The number of scans by this MySQL Server (SQL node) that have been pruned to a single partition.,0,0,others,mysql
5380,Ndb_api_pruned_scan_count_replica,The number of scans by this replica that have been pruned to a single partition.,0,0,others,mysql
5381,Ndb_api_pruned_scan_count_session,The number of scans in this client session that have been pruned to a single partition.,0,0,others,mysql
5382,Ndb_api_pruned_scan_count_slave,The number of scans by this replica that have been pruned to a single partition.,0,0,others,mysql
5384,Ndb_api_range_scan_count_replica,The number of range scans that have been started by this replica.,0,0,others,mysql
5386,Ndb_api_range_scan_count_slave,The number of range scans that have been started by this replica.,0,0,others,mysql
5388,Ndb_api_read_row_count_replica,"The total number of rows that have been read by this replica. This includes all rows read by any primary key, unique key, or scan operation made by this replica.",0,0,others,mysql
5389,Ndb_api_read_row_count_session,"The total number of rows that have been read in this client session. This includes all rows read by any primary key, unique key, or scan operation made in this client session.",0,0,others,mysql
5391,Ndb_api_scan_batch_count,The number of batches of rows received by this MySQL Server (SQL node). 1 batch is defined as 1 set of scan results from a single fragment.,0,0,others,mysql
5392,Ndb_api_scan_batch_count_replica,The number of batches of rows received by this replica. 1 batch is defined as 1 set of scan results from a single fragment.,0,0,others,mysql
5393,Ndb_api_scan_batch_count_session,The number of batches of rows received in this client session. 1 batch is defined as 1 set of scan results from a single fragment.,0,0,others,mysql
5394,Ndb_api_scan_batch_count_slave,The number of batches of rows received by this replica. 1 batch is defined as 1 set of scan results from a single fragment.,0,0,others,mysql
5395,Ndb_api_table_scan_count,"The number of table scans that have been started by this MySQL Server (SQL node), including scans of internal tables,.",0,0,others,mysql
5396,Ndb_api_table_scan_count_replica,"The number of table scans that have been started by this replica, including scans of internal tables.",0,0,others,mysql
5397,Ndb_api_table_scan_count_session,"The number of table scans that have been started in this client session, including scans of internal tables.",0,0,others,mysql
5398,Ndb_api_table_scan_count_slave,"The number of table scans that have been started by this replica, including scans of internal tables.",0,0,others,mysql
5399,Ndb_api_trans_abort_count,The number of transactions aborted by this MySQL Server (SQL node).,0,0,others,mysql
5400,Ndb_api_trans_abort_count_replica,The number of transactions aborted by this replica.,0,0,others,mysql
5401,Ndb_api_trans_abort_count_session,The number of transactions aborted in this client session.,0,0,others,mysql
5402,Ndb_api_trans_abort_count_slave,The number of transactions aborted by this replica.,0,0,others,mysql
5403,Ndb_api_trans_close_count,"The number of transactions closed by this MySQL Server (SQL node). This value may be greater than the sum of Ndb_api_trans_commit_count and Ndb_api_trans_abort_count, since some transactions may have been rolled back.",0,0,others,mysql
5404,Ndb_api_trans_close_count_replica,"The number of transactions closed by this replica. This value may be greater than the sum of Ndb_api_trans_commit_count_replica and Ndb_api_trans_abort_count_replica, since some transactions may have been rolled back.",0,0,others,mysql
5405,Ndb_api_trans_close_count_session,"The number of transactions closed in this client session. This value may be greater than the sum of Ndb_api_trans_commit_count_session and Ndb_api_trans_abort_count_session, since some transactions may have been rolled back.",0,0,others,mysql
5406,Ndb_api_trans_close_count_slave,"The number of transactions closed by this replica. This value may be greater than the sum of Ndb_api_trans_commit_count_replica and Ndb_api_trans_abort_count_replica, since some transactions may have been rolled back.",0,0,others,mysql
5407,Ndb_api_trans_commit_count,The number of transactions committed by this MySQL Server (SQL node).,0,0,others,mysql
5411,Ndb_api_trans_local_read_row_count,"The total number of rows that have been read by this MySQL Server (SQL node). This includes all rows read by any primary key, unique key, or scan operation made by this MySQL Server (SQL node).",0,0,others,mysql
5412,Ndb_api_trans_local_read_row_count_replica,"The total number of rows that have been read by this replica. This includes all rows read by any primary key, unique key, or scan operation made by this replica.",0,0,others,mysql
5413,Ndb_api_trans_local_read_row_count_session,"The total number of rows that have been read in this client session. This includes all rows read by any primary key, unique key, or scan operation made in this client session.",0,0,others,mysql
5414,Ndb_api_trans_local_read_row_count_slave,"The total number of rows that have been read by this replica. This includes all rows read by any primary key, unique key, or scan operation made by this replica.",0,0,others,mysql
5415,Ndb_api_trans_start_count,The number of transactions started by this MySQL Server (SQL node).,0,0,others,mysql
5416,Ndb_api_trans_start_count_replica,The number of transactions started by this replica.,0,0,others,mysql
5417,Ndb_api_trans_start_count_session,The number of transactions started in this client session.,0,0,others,mysql
5418,Ndb_api_trans_start_count_slave,The number of transactions started by this replica.,0,0,others,mysql
5419,Ndb_api_uk_op_count,The number of operations by this MySQL Server (SQL node) based on or using unique keys.,0,0,others,mysql
5420,Ndb_api_uk_op_count_replica,The number of operations by this replica based on or using unique keys.,0,0,others,mysql
5421,Ndb_api_uk_op_count_session,The number of operations in this client session based on or using unique keys.,0,0,others,mysql
5422,Ndb_api_uk_op_count_slave,The number of operations by this replica based on or using unique keys.,0,0,others,mysql
5423,Ndb_api_wait_exec_complete_count,The number of times a thread has been blocked by this MySQL Server (SQL node) while waiting for execution of an operation to complete. This includes all execute() calls as well as implicit executes for blob and auto-increment operations not visible to clients.,0,0,others,mysql
5425,Ndb_api_wait_exec_complete_count_session,The number of times a thread has been blocked in this client session while waiting for execution of an operation to complete. This includes all execute() calls as well as implicit executes for blob and auto-increment operations not visible to clients.,0,0,others,mysql
5426,Ndb_api_wait_exec_complete_count_slave,The number of times a thread has been blocked by this replica while waiting for execution of an operation to complete. This includes all execute() calls as well as implicit executes for blob and auto-increment operations not visible to clients.,0,0,others,mysql
5427,Ndb_api_wait_meta_request_count,"The number of times a thread has been blocked by this MySQL Server (SQL node) waiting for a metadata-based signal, such as is expected for DDL requests, new epochs, and seizure of transaction records.",0,0,others,mysql
5428,Ndb_api_wait_meta_request_count_replica,"The number of times a thread has been blocked by this replica waiting for a metadata-based signal, such as is expected for DDL requests, new epochs, and seizure of transaction records.",0,0,others,mysql
5429,Ndb_api_wait_meta_request_count_session,"The number of times a thread has been blocked in this client session waiting for a metadata-based signal, such as is expected for DDL requests, new epochs, and seizure of transaction records.",0,0,others,mysql
5430,Ndb_api_wait_meta_request_count_slave,"The number of times a thread has been blocked by this replica waiting for a metadata-based signal, such as is expected for DDL requests, new epochs, and seizure of transaction records.",0,0,others,mysql
5431,Ndb_api_wait_nanos_count,Total time (in nanoseconds) spent by this MySQL Server (SQL node) waiting for any type of signal from the data nodes.,0,0,others,mysql
5432,Ndb_api_wait_nanos_count_replica,Total time (in nanoseconds) spent by this replica waiting for any type of signal from the data nodes.,0,0,others,mysql
5433,Ndb_api_wait_nanos_count_session,Total time (in nanoseconds) spent in this client session waiting for any type of signal from the data nodes.,0,0,others,mysql
5437,Ndb_api_wait_scan_result_count_session,"The number of times a thread has been blocked in this client session while waiting for a scan-based signal, such as when waiting for more results from a scan, or when waiting for a scan to close.",0,0,others,mysql
5438,Ndb_api_wait_scan_result_count_slave,"The number of times a thread has been blocked by this replica while waiting for a scan-based signal, such as when waiting for more results from a scan, or when waiting for a scan to close.",0,0,others,mysql
5439,ndb_autoincrement_prefetch_sz,"Determines the probability of gaps in an autoincremented column. Set it to 1 to minimize this. Setting it to a high value for optimization makes inserts faster, but decreases the likelihood of consecutive autoincrement numbers being used in a batch of inserts.",1,5,workload-specific,mysql
5440,ndb_batch_size,This sets the size in bytes that is used for NDB transaction batches.,1,1,resource,mysql
5441,ndb_blob_read_batch_bytes,"This option can be used to set the size (in bytes) for batching of BLOB data reads in NDB Cluster applications. When this batch size is exceeded by the amount of BLOB data to be read within the current transaction, any pending BLOB read operations are immediately executed.",1,1,resource,mysql
5442,ndb_blob_write_batch_bytes,"This option can be used to set the size (in bytes) for batching of BLOB data writes in NDB Cluster applications. When this batch size is exceeded by the amount of BLOB data to be written within the current transaction, any pending BLOB write operations are immediately executed.",1,1,resource,mysql
5443,ndb_cache_check_time,The number of milliseconds that elapse between checks of NDB Cluster SQL nodes by the MySQL query cache. Setting this to 0 (the default and minimum value) means that the query cache checks for validation on every query.,1,6,function-tradeoff,mysql
5445,ndb_cluster_connection_pool,"By setting this option to a value greater than 1 (the default), a mysqld process can use multiple connections to the cluster, effectively mimicking several SQL nodes. Each connection requires its own [api] or [mysqld] section in the cluster configuration (config.ini) file, and counts against the maximum number of API connections supported by the cluster.",1,5,workload-specific,mysql
5446,ndb_cluster_connection_pool_nodeids,Specifies a comma-separated list of node IDs for connections to the cluster used by an SQL node. The number of nodes in this list must be the same as the value set for the --ndb-cluster-connection-pool option.,0,0,others,mysql
5449,Ndb_config_from_port,"If the server is part of an NDB Cluster, the value of this variable is the number of the port through which it is connected to the Cluster management server from which it gets its configuration data.",0,0,others,mysql
5450,Ndb_config_generation,Shows the generation number of the cluster's current configuration. This can be used as an indicator to determine whether the configuration of the cluster has changed since this SQL node last connected to the cluster.,0,0,others,mysql
5451,Ndb_conflict_fn_epoch,"Used in NDB Cluster Replication conflict resolution, this variable shows the number of rows found to be in conflict using NDB$EPOCH() conflict resolution on a given mysqld since the last time it was restarted.",0,0,others,mysql
5452,Ndb_conflict_fn_epoch_trans,"Used in NDB Cluster Replication conflict resolution, this variable shows the number of rows found to be in conflict using NDB$EPOCH_TRANS() conflict resolution on a given mysqld since the last time it was restarted.",0,0,others,mysql
5453,Ndb_conflict_fn_epoch2,"Shows the number of rows found to be in conflict in NDB Cluster Replication conflict resolution, when using NDB$EPOCH2(), on the source designated as the primary since the last time it was restarted.",0,0,others,mysql
5454,Ndb_conflict_fn_epoch2_trans,"Used in NDB Cluster Replication conflict resolution, this variable shows the number of rows found to be in conflict using NDB$EPOCH_TRANS2() conflict resolution on a given mysqld since the last time it was restarted.",0,0,others,mysql
5455,Ndb_conflict_fn_max,"Used in NDB Cluster Replication conflict resolution, this variable shows the number of times that a row was not applied on the current SQL node due to “greatest timestamp wins” conflict resolution since the last time that this mysqld was started.",0,0,others,mysql
5456,Ndb_conflict_fn_old,"Used in NDB Cluster Replication conflict resolution, this variable shows the number of times that a row was not applied as the result of “same timestamp wins” conflict resolution on a given mysqld since the last time it was restarted.",0,0,others,mysql
5457,Ndb_conflict_last_conflict_epoch,"The most recent epoch in which a conflict was detected on this replica. You can compare this value with Ndb_replica_max_replicated_epoch; if Ndb_replica_max_replicated_epoch is greater than Ndb_conflict_last_conflict_epoch, no conflicts have yet been detected.",0,0,others,mysql
5458,Ndb_conflict_last_stable_epoch,Number of rows found to be in conflict by a transactional conflict function,0,0,others,mysql
5459,Ndb_conflict_reflected_op_discard_count,"When using NDB Cluster Replication conflict resolution, this is the number of reflected operations that were not applied on the secondary, due to encountering an error during execution.",1,5,workload-specific,mysql
5460,Ndb_conflict_reflected_op_prepare_count,"When using conflict resolution with NDB Cluster Replication, this status variable contains the number of reflected operations that have been defined (that is, prepared for execution on the secondary).",1,1,resource,mysql
5461,Ndb_conflict_refresh_op_count,"When using conflict resolution with NDB Cluster Replication, this gives the number of refresh operations that have been prepared for execution on the secondary.",0,0,others,mysql
5462,ndb_conflict_role,"Determines the role of this SQL node (and NDB Cluster) in a circular (“active-active”) replication setup. ndb_slave_conflict_role can take any one of the values PRIMARY, SECONDARY, PASS, or NULL (the default). The replica SQL thread must be stopped before you can change ndb_slave_conflict_role. In addition, it is not possible to change directly between PASS and either of PRIMARY or SECONDARY directly; in such cases, you must ensure that the SQL thread is stopped, then execute SET @@GLOBAL.ndb_slave_conflict_role = 'NONE' first.",1,6,function-tradeoff,mysql
5463,Ndb_conflict_trans_conflict_commit_count,"Used in NDB Cluster Replication conflict resolution, this shows the number of epoch transactions committed after they required transactional conflict handling.",0,0,others,mysql
5464,Ndb_conflict_trans_detect_iter_count,"Used in NDB Cluster Replication conflict resolution, this shows the number of internal iterations required to commit an epoch transaction. Should be (slightly) greater than or equal to Ndb_conflict_trans_conflict_commit_count.",0,0,others,mysql
5465,Ndb_conflict_trans_reject_count,"Used in NDB Cluster Replication conflict resolution, this status variable shows the number of transactions found to be in conflict by a transactional conflict detection function.",0,0,others,mysql
5467,Ndb_conflict_trans_row_reject_count,"Used in NDB Cluster Replication conflict resolution, this status variable shows the total number of rows realigned due to being determined as conflicting by a transactional conflict detection function. This includes not only Ndb_conflict_trans_row_conflict_count, but any rows in or dependent on conflicting transactions.",0,0,others,mysql
5468,ndb_data_node_neighbour,"Sets the ID of a “nearest” data node—that is, a preferred nonlocal data node is chosen to execute the transaction, rather than one running on the same host as the SQL or API node. This used to ensure that when a fully replicated table is accessed, we access it on this data node, to ensure that the local copy of the table is always used whenever possible. This can also be used for providing hints for transactions.",0,0,others,mysql
5472,ndb_distribution,Controls the default distribution method for NDB tables. Can be set to either of KEYHASH (key hashing) or LINHASH (linear hashing). KEYHASH is the default.,0,0,others,mysql
5473,Ndb_epoch_delete_delete_count,"When using delete-delete conflict detection, this is the number of delete-delete conflicts detected, where a delete operation is applied, but the indicated row does not exist.",0,0,others,mysql
5474,ndb_eventbuffer_free_percent,"Sets the percentage of the maximum memory allocated to the event buffer (ndb_eventbuffer_max_alloc) that should be available in event buffer after reaching the maximum, before starting to buffer again.",1,1,resource,mysql
5475,ndb_eventbuffer_max_alloc,"Sets the maximum amount memory (in bytes) that can be allocated for buffering events by the NDB API. 0 means that no limit is imposed, and is the default.",1,1,resource,mysql
5476,Ndb_execute_count,Provides the number of round trips to the NDB kernel made by operations.,0,0,others,mysql
5477,ndb_extra_logging,This variable enables recording in the MySQL error log of information specific to the NDB storage engine.,0,0,others,mysql
5478,ndb_force_send,"Forces sending of buffers to NDB immediately, without waiting for other threads. Defaults to ON.",0,0,others,mysql
5479,ndb_fully_replicated,"Determines whether new NDB tables are fully replicated. This setting can be overridden for an individual table using COMMENT=""NDB_TABLE=FULLY_REPLICATED=..."" in a CREATE TABLE or ALTER TABLE statement; see Section13.1.20.11, “Setting NDB_TABLE Options”, for syntax and other information.",1,4,limited-side-effect,mysql
5480,ndb_index_stat_enable,Use NDB index statistics in query optimization. The default is ON.,1,6,function-tradeoff,mysql
5481,ndb_index_stat_option,"This variable is used for providing tuning options for NDB index statistics generation. The list consist of comma-separated name-value pairs of option names and values, and this list must not contain any space characters.",0,0,others,mysql
5482,ndb_join_pushdown,"This variable controls whether joins on NDB tables are pushed down to the NDB kernel (data nodes). Previously, a join was handled using multiple accesses of NDB by the SQL node; however, when ndb_join_pushdown is enabled, a pushable join is sent in its entirety to the data nodes, where it can be distributed among the data nodes and executed in parallel on multiple copies of the data, with a single, merged result being returned to mysqld. This can reduce greatly the number of round trips between an SQL node and the data nodes required to handle such a join.",0,0,others,mysql
5483,Ndb_last_commit_epoch_server,The epoch most recently committed by NDB.,0,0,others,mysql
5484,Ndb_last_commit_epoch_session,The epoch most recently committed by this NDB client.,0,0,others,mysql
5485,ndb_log_apply_status,A read-only variable which shows whether the server was started with the --ndb-log-apply-status option.,0,0,others,mysql
5486,ndb_log_bin,"Causes updates to NDB tables to be written to the binary log. The setting for this variable has no effect if binary logging is not already enabled on the server using log_bin. In NDB 8.0, ndb_log_bin defaults to 0 (FALSE).",0,0,others,mysql
5487,ndb_log_binlog_index,"Causes a mapping of epochs to positions in the binary log to be inserted into the ndb_binlog_index table. Setting this variable has no effect if binary logging is not already enabled for the server using log_bin. (In addition, ndb_log_bin must not be disabled.) ndb_log_binlog_index defaults to 1 (ON); normally, there is never any need to change this value in a production environment.",0,0,others,mysql
5489,ndb_log_empty_update,"When this variable is set to ON (1), update transactions with no changes are written to the binary log, even when log_replica_updates or log_slave_updates is enabled.",0,0,others,mysql
5490,ndb_log_exclusive_reads,"This variable determines whether primary key reads are logged with exclusive locks, which allows for NDB Cluster Replication conflict detection and resolution based on read conflicts. To enable these locks, set the value of ndb_log_exclusive_reads to 1. 0, which disables such locking, is the default.",0,0,others,mysql
5491,ndb_log_fail_terminate,"When this option is specified, and complete logging of all found row events is not possible, the mysqld process is terminated.",1,3,reliability-tradeoff,mysql
5492,ndb_log_orig,Shows whether the originating server ID and epoch are logged in the ndb_binlog_index table. Set using the --ndb-log-orig server option.,0,0,others,mysql
5493,ndb_log_transaction_id,"This read-only, Boolean system variable shows whether a replica mysqld writes NDB transaction IDs in the binary log (required to use “active-active” NDB Cluster Replication with NDB$EPOCH_TRANS() conflict detection). To change the setting, use the --ndb-log-transaction-id option.",1,4,limited-side-effect,mysql
5494,ndb_log_update_as_write,"Whether updates on the source are written to the binary log as updates (OFF) or writes (ON). Used in NDB Replication conflict resolution; for more information, see Logging Changed Data as Updates.",1,6,function-tradeoff,mysql
5495,ndb_log_update_minimal,"Log updates in a minimal fashion, by writing only the primary key values in the before image, and only the changed columns in the after image. This may cause compatibility problems if replicating to storage engines other than NDB.",1,6,function-tradeoff,mysql
5497,Ndb_metadata_blacklist_size,The number of metadata objects that the NDB binlog thread has been unable to synchronize on this SQL node since it was last restarted.,0,0,others,mysql
5498,ndb_metadata_check,NDB uses a background thread to check for metadata changes each ndb_metadata_check_interval seconds as compared with the MySQL data dictionary. This metadata change detection thread can be disabled by setting ndb_metadata_check to OFF. The thread is enabled by default.,0,0,others,mysql
5499,ndb_metadata_check_interval,"NDB runs a metadata change detection thread in the background to determine when the NDB dictionary has changed with respect to the MySQL data dictionary. By default,the interval between such checks is 60 seconds; this can be adjusted by setting the value of ndb_metadata_check_interval. To enable or disable the thread, use ndb_metadata_check.",0,0,others,mysql
5500,Ndb_metadata_detected_count,The number of times since this server was last started that the NDB metadata change detection thread has discovered changes with respect to the MySQL data dictionary.,0,0,others,mysql
5501,Ndb_metadata_excluded_count,The number of metadata objects that the NDB binlog thread has been unable to synchronize on this SQL node since it was last restarted.,0,0,others,mysql
5503,Ndb_metadata_synced_count,The number of NDB metadata objects which have been synchronized on this SQL node since it was last restarted.,0,0,others,mysql
5505,Ndb_number_of_data_nodes,"If the server is part of an NDB Cluster, the value of this variable is the number of data nodes in the cluster.",0,0,others,mysql
5506,ndb_optimization_delay,Set the number of milliseconds to wait between sets of rows by OPTIMIZE TABLE statements on NDB tables. The default is 10.,0,0,others,mysql
5507,ndb_optimized_node_selection,"There are two forms of optimized node selection, described here:",0,0,others,mysql
5508,Ndb_pruned_scan_count,This variable holds a count of the number of scans executed by NDBCLUSTER since the NDB Cluster was last started where NDBCLUSTER was able to use partition pruning.,0,0,others,mysql
5509,Ndb_pushed_queries_defined,The total number of joins pushed down to the NDB kernel for distributed handling on the data nodes.,0,0,others,mysql
5510,Ndb_pushed_queries_dropped,The number of joins that were pushed down to the NDB kernel but that could not be handled there.,0,0,others,mysql
5511,Ndb_pushed_queries_executed,The number of joins successfully pushed down to NDB and executed there.,0,0,others,mysql
5512,Ndb_pushed_reads,The number of rows returned to mysqld from the NDB kernel by joins that were pushed down.,0,0,others,mysql
5513,ndb_read_backup,Enable read from any fragment replica for any NDB table subsequently created; doing so greatly improves the table read performance at a relatively small cost to writes.,1,4,limited-side-effect,mysql
5515,ndb_recv_thread_cpu_mask,"CPU mask for locking receiver threads to specific CPUs. This is specified as a hexadecimal bitmask. For example, 0x33 means that one CPU is used per receiver thread. An empty string is the default; setting ndb_recv_thread_cpu_mask to this value removes any receiver thread locks previously set.",1,1,resource,mysql
5516,Ndb_replica_max_replicated_epoch,"The most recently committed epoch on this replica. You can compare this value with Ndb_conflict_last_conflict_epoch; if Ndb_replica_max_replicated_epoch is the greater of the two, no conflicts have yet been detected.",0,0,others,mysql
5517,ndb_report_thresh_binlog_epoch_slip,"This represents the threshold for the number of epochs completely buffered in the event buffer, but not yet consumed by the binlog injector thread. When this degree of slippage (lag) is exceeded, an event buffer status message is reported, with BUFFERED_EPOCHS_OVER_THRESHOLD supplied as the reason (see Section23.6.2.3, “Event Buffer Reporting in the Cluster Log”). Slip is increased when an epoch is received from data nodes and buffered completely in the event buffer; it is decreased when an epoch is consumed by the binlog injector thread, it is reduced. Empty epochs are buffered and queued, and so included in this calculation only when this is enabled using the Ndb::setEventBufferQueueEmptyEpoch() method from the NDB API.",1,5,workload-specific,mysql
5518,ndb_report_thresh_binlog_mem_usage,"This is a threshold on the percentage of free memory remaining before reporting binary log status. For example, a value of 10 (the default) means that if the amount of available memory for receiving binary log data from the data nodes falls below 10%, a status message is sent to the cluster log.",1,1,resource,mysql
5519,ndb_row_checksum,"Traditionally, NDB has created tables with row checksums, which checks for hardware issues at the expense of performance. Setting ndb_row_checksum to 0 means that row checksums are not used for new or altered tables, which has a significant impact on performance for all types of queries. This variable is set to 1 by default, to provide backward-compatible behavior.",1,2,security-tradeoff,mysql
5520,Ndb_scan_count,This variable holds a count of the total number of scans executed by NDBCLUSTER since the NDB Cluster was last started.,0,0,others,mysql
5521,ndb_schema_dist_lock_wait_timeout,"Number of seconds to wait during schema distribution for the metadata lock taken on each SQL node in order to change its local data dictionary to reflect the DDL statement change. After this time has elapsed, a warning is returned to the effect that a given SQL node's data dictionary was not updated with the change. This avoids having the binary logging thread wait an excessive length of time while handling schema operations.",0,0,others,mysql
5522,ndb_schema_dist_timeout,"Number of seconds to wait before detecting a timeout during schema distribution. This can indicate that other SQL nodes are experiencing excessive activity, or that they are somehow being prevented from acquiring necessary resources at this time.",0,0,others,mysql
5523,ndb_schema_dist_upgrade_allowed,"Allow upgrading of the schema distribution table when connecting to NDB. When true (the default), this change is deferred until all SQL nodes have been upgraded to the same version of the NDB Cluster software.",0,0,others,mysql
5524,ndb_show_foreign_key_mock_tables,"Show the mock tables used by NDB to support foreign_key_checks=0. When this is enabled, extra warnings are shown when creating and dropping the tables. The real (internal) name of the table can be seen in the output of SHOW CREATE TABLE.",0,0,others,mysql
5527,Ndb_system_name,"If this MySQL Server is connected to an NDB cluster, this read-only variable shows the cluster system name. Otherwise, the value is an empty string.",0,0,others,mysql
5528,ndb_table_no_logging,"When this variable is set to ON or 1, it causes NDB tables not to be checkpointed to disk. More specifically, this setting applies to tables which are created or altered using ENGINE NDB when ndb_table_no_logging is enabled, and continues to apply for the lifetime of the table, even if ndb_table_no_logging is later changed. Suppose that A, B, C, and D are tables that we create (and perhaps also alter), and that we also change the setting for ndb_table_no_logging as shown here:",0,0,others,mysql
5529,ndb_table_temporary,"When set to ON or 1, this variable causes NDB tables not to be written to disk: This means that no table schema files are created, and that the tables are not logged.",0,0,others,mysql
5530,Ndb_trans_hint_count_session,The number of transactions using hints that have been started in the current session. Compare with Ndb_api_trans_start_count_session to obtain the proportion of all NDB transactions able to use hints.,1,1,resource,mysql
5531,ndb_use_copying_alter_table,Forces NDB to use copying of tables in the event of problems with online ALTER TABLE operations. The default value is OFF.,0,0,others,mysql
5532,ndb_use_exact_count,"Forces NDB to use a count of records during SELECT COUNT(*) query planning to speed up this type of query. The default value is OFF, which allows for faster queries overall.",0,0,others,mysql
5533,ndb_use_transactions,You can disable NDB transaction support by setting this variable's values to OFF (not recommended). The default is ON.,1,6,function-tradeoff,mysql
5534,ndb_version,"NDB engine version, as a composite integer.",0,0,others,mysql
5535,ndb_version_string,NDB engine version in ndb-x.y.z format.,0,0,others,mysql
5536,ndb_wait_connected,This option sets the period of time that the MySQL server waits for connections to NDB Cluster management and data nodes to be established before accepting MySQL client connections. The time is specified in seconds. The default value is 30.,0,0,others,mysql
5537,ndb_wait_setup,This variable shows the period of time that the MySQL server waits for the NDB storage engine to complete setup before timing out and treating NDB as unavailable. The time is specified in seconds. The default value is 30.,0,0,others,mysql
5538,ndbcluster,"The NDBCLUSTER storage engine is necessary for using NDB Cluster. If a mysqld binary includes support for the NDBCLUSTER storage engine, the engine is disabled by default. Use the --ndbcluster option to enable it. Use --skip-ndbcluster to explicitly disable the engine.",0,0,others,mysql
5539,ndb-connectstring,"When using the NDBCLUSTER storage engine, this option specifies the management server that distributes cluster configuration data. See Section23.4.3.3, “NDB Cluster Connection Strings”, for syntax.",0,0,others,mysql
5540,ndbinfo,Enables the plugin for the ndbinfo information database. By default this is ON whenever NDBCLUSTER is enabled.,0,0,others,mysql
5542,ndbinfo_max_rows,Used in testing and debugging only.,0,0,others,mysql
5543,ndbinfo_offline,"Place the ndbinfo database into offline mode, in which tables and views can be opened even when they do not actually exist, or when they exist but have different definitions in NDB. No rows are returned from such tables (or views).",1,4,limited-side-effect,mysql
5544,ndbinfo_show_hidden,Whether or not the ndbinfo database's underlying internal tables are shown in the mysql client. The default is OFF.,0,0,others,mysql
5545,ndbinfo_table_prefix,"The prefix used in naming the ndbinfo database's base tables (normally hidden, unless exposed by setting ndbinfo_show_hidden). This is a read-only variable whose default value is ndb$; the prefix itself is determined at compile time.",0,0,others,mysql
5546,ndbinfo_version,Shows the version of the ndbinfo engine in use; read-only.,0,0,others,mysql
5548,ndb-optimized-node-selection,Enable optimizations for selection of nodes for transactions. Enabled by default; use --skip-ndb-optimized-node-selection to disable.,0,0,others,mysql
5550,no-dd-upgrade,"Prevent automatic upgrade of the data dictionary tables during the MySQL server startup process. This option is typically used when starting the MySQL server following an in-place upgrade of an existing installation to a newer MySQL version, which may include changes to data dictionary table definitions.",0,0,others,mysql
5551,no-defaults,"Do not read any option files. If program startup fails due to reading unknown options from an option file, --no-defaults can be used to prevent them from being read. This must be the first option on the command line if it is used.",0,0,others,mysql
5552,no-monitor,"(Windows only). This option suppresses the forking that is used to implement the RESTART statement: Forking enables one process to act as a monitor to the other, which acts as the server. For a server started with this option, RESTART simply exits and does not restart.",0,0,others,mysql
5553,old-style-user-limits,"Enable old-style user limits. (Before MySQL 5.0.3, account resource limits were counted separately for each host from which a user connected rather than per account row in the user table.) See Section6.2.20, “Setting Account Resource Limits”.",0,0,others,mysql
5554,original_commit_timestamp,"For internal use by replication. When re-executing a transaction on a replica, this is set to the time when the transaction was committed on the original source, measured in microseconds since the epoch. This allows the original commit timestamp to be propagated throughout a replication topology.",0,0,others,mysql
5555,original_server_version,"For internal use by replication. This session system variable holds the MySQL Server release number of the server where a transaction was originally committed (for example, 80014 for a MySQL 8.0.14 server instance). If this original server is at a release that does not support the session system variable, the value of the variable is set to 0 (UNKNOWN_SERVER_VERSION). Note that when a release number is set by the original server, the value of the variable is reset to 0 if the immediate server or any other intervening server in the replication topology does not support the session system variable, and so does not replicate its value.",0,0,others,mysql
5557,performance_schema_accounts_size,"The number of rows in the accounts table. If this variable is 0, the Performance Schema does not maintain connection statistics in the accounts table or status variable information in the status_by_account table.",0,0,others,mysql
5558,performance_schema_digests_size,"The maximum number of rows in the events_statements_summary_by_digest table. If this maximum is exceeded such that a digest cannot be instrumented, the Performance Schema increments the Performance_schema_digest_lost status variable.",0,0,others,mysql
5559,performance_schema_error_size,"The number of instrumented server error codes. The default value is the actual number of server error codes. Although the value can be set anywhere from 0 to its maximum, the intended use is to set it to either its default (to instrument all errors) or 0 (to instrument no errors).",0,0,others,mysql
5560,performance_schema_events_stages_history_long_size,The number of rows in the events_stages_history_long table.,0,0,others,mysql
5561,performance_schema_events_stages_history_size,The number of rows per thread in the events_stages_history table.,0,0,others,mysql
5562,performance_schema_events_statements_history_long_size,The number of rows in the events_statements_history_long table.,0,0,others,mysql
5563,performance_schema_events_statements_history_size,The number of rows per thread in the events_statements_history table.,0,0,others,mysql
5565,performance_schema_events_transactions_history_size,The number of rows per thread in the events_transactions_history table.,0,0,others,mysql
5566,performance_schema_events_waits_history_long_size,The number of rows in the events_waits_history_long table.,0,0,others,mysql
5567,performance_schema_events_waits_history_size,The number of rows per thread in the events_waits_history table.,0,0,others,mysql
5568,performance_schema_hosts_size,"The number of rows in the hosts table. If this variable is 0, the Performance Schema does not maintain connection statistics in the hosts table or status variable information in the status_by_host table.",0,0,others,mysql
5569,performance_schema_max_cond_classes,The maximum number of condition instruments.,0,0,others,mysql
5570,performance_schema_max_cond_instances,The maximum number of instrumented condition objects.,0,0,others,mysql
5571,performance_schema_max_digest_length,The maximum number of bytes of memory reserved per statement for computation of normalized statement digest values in the Performance Schema.,0,0,others,mysql
5572,performance_schema_max_digest_sample_age,"This variable affects statement sampling for the events_statements_summary_by_digest table. When a new table row is inserted, the statement that produced the row digest value is stored as the current sample statement associated with the digest. Thereafter, when the server sees other statements with the same digest value, it determines whether to use the new statement to replace the current sample statement (that is, whether to resample). Resampling policy is based on the comparative wait times of the current sample statement and new statement and, optionally, the age of the current sample statement:",1,5,workload-specific,mysql
5573,performance_schema_max_file_classes,The maximum number of file instruments.,0,0,others,mysql
5574,performance_schema_max_file_handles,The maximum number of opened file objects.,0,0,others,mysql
5575,performance_schema_max_file_instances,The maximum number of instrumented file objects.,0,0,others,mysql
5576,performance_schema_max_index_stat,"The maximum number of indexes for which the Performance Schema maintains statistics. If this maximum is exceeded such that index statistics are lost, the Performance Schema increments the Performance_schema_index_stat_lost status variable. The default value is autosized using the value of performance_schema_max_table_instances.",0,0,others,mysql
5579,performance_schema_max_mutex_classes,The maximum number of mutex instruments.,0,0,others,mysql
5580,performance_schema_max_mutex_instances,The maximum number of instrumented mutex objects.,0,0,others,mysql
5581,performance_schema_max_prepared_statements_instances,"The maximum number of rows in the prepared_statements_instances table. If this maximum is exceeded such that a prepared statement cannot be instrumented, the Performance Schema increments the Performance_schema_prepared_statements_lost status variable.",0,0,others,mysql
5582,performance_schema_max_program_instances,"The maximum number of stored programs for which the Performance Schema maintains statistics. If this maximum is exceeded, the Performance Schema increments the Performance_schema_program_lost status variable.",0,0,others,mysql
5583,performance_schema_max_rwlock_classes,The maximum number of rwlock instruments.,0,0,others,mysql
5584,performance_schema_max_rwlock_instances,The maximum number of instrumented rwlock objects.,0,0,others,mysql
5586,performance_schema_max_socket_instances,The maximum number of instrumented socket objects.,1,1,resource,mysql
5587,performance_schema_max_sql_text_length,The maximum number of bytes used to store SQL statements.,1,1,resource,mysql
5590,performance_schema_max_statement_stack,"The maximum depth of nested stored program calls for which the Performance Schema maintains statistics. When this maximum is exceeded, the Performance Schema increments the Performance_schema_nested_statement_lost status variable for each stored program statement executed.",1,5,workload-specific,mysql
5591,performance_schema_max_table_handles,"The maximum number of opened table objects. This value controls the size of the table_handles table. If this maximum is exceeded such that a table handle cannot be instrumented, the Performance Schema increments the Performance_schema_table_handles_lost status variable.",0,0,others,mysql
5592,performance_schema_max_table_instances,The maximum number of instrumented table objects.,0,0,others,mysql
5593,performance_schema_max_table_lock_stat,"The maximum number of tables for which the Performance Schema maintains lock statistics. If this maximum is exceeded such that table lock statistics are lost, the Performance Schema increments the Performance_schema_table_lock_stat_lost status variable.",0,0,others,mysql
5594,performance_schema_max_thread_classes,The maximum number of thread instruments.,1,1,resource,mysql
5595,performance_schema_max_thread_instances,"The maximum number of instrumented thread objects. The value controls the size of the threads table. If this maximum is exceeded such that a thread cannot be instrumented, the Performance Schema increments the Performance_schema_thread_instances_lost status variable.",1,1,resource,mysql
5596,performance_schema_session_connect_attrs_size,"The amount of preallocated memory per thread reserved to hold connection attribute key-value pairs. If the aggregate size of connection attribute data sent by a client is larger than this amount, the Performance Schema truncates the attribute data, increments the Performance_schema_session_connect_attrs_lost status variable, and writes a message to the error log indicating that truncation occurred if the log_error_verbosity system variable is greater than 1. A _truncated attribute is also added to the session attributes with a value indicating how many bytes were lost, if the attribute buffer has sufficient space. This enables the Performance Schema to expose per-connection truncation information in the connection attribute tables. This information can be examined without having to check the error log.",1,1,resource,mysql
5597,performance_schema_setup_actors_size,The number of rows in the setup_actors table.,1,1,resource,mysql
5598,performance_schema_setup_objects_size,The number of rows in the setup_objects table.,0,0,others,mysql
5600,performance_schema_users_size,"The number of rows in the users table. If this variable is 0, the Performance Schema does not maintain connection statistics in the users table or status variable information in the status_by_user table.",0,0,others,mysql
5601,performance-schema-consumer-events-stages-current,Configure the events-stages-current consumer.,0,0,others,mysql
5602,performance-schema-consumer-events-stages-history,Configure the events-stages-history consumer.,0,0,others,mysql
5605,performance-schema-consumer-events-statements-history,Configure the events-statements-history consumer.,0,0,others,mysql
5607,performance-schema-consumer-events-transactions-current,Configure the Performance Schema events-transactions-current consumer.,0,0,others,mysql
5609,performance-schema-consumer-events-transactions-history-long,Configure the Performance Schema events-transactions-history-long consumer.,0,0,others,mysql
5610,performance-schema-consumer-events-waits-current,Configure the events-waits-current consumer.,0,0,others,mysql
5611,performance-schema-consumer-events-waits-history,Configure the events-waits-history consumer.,0,0,others,mysql
5612,performance-schema-consumer-events-waits-history-long,Configure the events-waits-history-long consumer.,0,0,others,mysql
5613,performance-schema-consumer-global-instrumentation,Configure the global-instrumentation consumer.,0,0,others,mysql
5614,performance-schema-consumer-statements-digest,Configure the statements-digest consumer.,0,0,others,mysql
5615,performance-schema-consumer-thread-instrumentation,Configure the thread-instrumentation consumer.,0,0,others,mysql
5616,performance-schema-instrument,Configure a Performance Schema instrument. The name may be given as a pattern to configure instruments that match the pattern.,0,0,others,mysql
5617,plugin_load,"This option tells the server to load the named plugins at startup. If multiple --plugin-load options are given, only the last one applies. Additional plugins to load may be specified using --plugin-load-add options.",0,0,others,mysql
5619,plugin-xxx,"Specifies an option that pertains to a server plugin. For example, many storage engines can be built as plugins, and for such engines, options for them can be specified with a --plugin prefix. Thus, the --innodb-file-per-table option for InnoDB can be specified as --plugin-innodb-file-per-table.",0,0,others,mysql
5620,port,"The port number to use when listening for TCP/IP connections. On Unix and Unix-like systems, the port number must be 1024 or higher unless the server is started by the root operating system user. Setting this option to 0 causes the default value to be used.",0,0,others,mysql
5621,port-open-timeout,"On some systems, when the server is stopped, the TCP/IP port might not become available immediately. If the server is restarted quickly afterward, its attempt to reopen the port can fail. This option indicates how many seconds the server should wait for the TCP/IP port to become free if it cannot be opened. The default is not to wait.",0,0,others,mysql
5622,print-defaults,"Print the program name and all options that it gets from option files. Password values are masked. This must be the first option on the command line if it is used, except that it may be used immediately after --defaults-file or --defaults-extra-file.",0,0,others,mysql
5625,relay_log_index,"The name for the relay log index file. The maximum variable length is 256. If you do not specify this variable, but the relay_log system variable is specified, its value is used as the default base name for the relay log index file. If relay_log is also not specified, then for the default replication channel, the default name is host_name-relay-bin.index, using the name of the host machine. For non-default replication channels, the default name is host_name-relay-bin-channel.index, where channel is the name of the replication channel recorded in this relay log index.",0,0,others,mysql
5626,relay_log_info_file,"The use of this system variable is now deprecated. It was used to set the file name for the replica's applier metadata repository if relay_log_info_repository=FILE was set. relay_log_info_file and the use of the relay_log_info_repository system variable are deprecated because the use of a file for the applier metadata repository has been superseded by crash-safe tables. For information about the applier metadata repository, see Section17.2.4.2, “Replication Metadata Repositories”.",0,0,others,mysql
5627,relay_log_info_repository,"The use of this system variable is now deprecated. The setting TABLE is the default, and is required when multiple replication channels are configured. The TABLE setting for the replica's applier metadata repository is also required to make replication resilient to unexpected halts. See Section17.4.2, “Handling an Unexpected Halt of a Replica” for more information. The alternative setting FILE was previously deprecated.",0,0,others,mysql
5628,relay_log_purge,Disables or enables automatic purging of relay log files as soon as they are not needed any more. The default value is 1 (ON).This is a global variable that can be changed dynamically with SET GLOBAL relay_log_purge = N. Disabling purging of relay logs when enabling the --relay-log-recovery option puts data consistency at risk.,1,3,reliability-tradeoff,mysql
5629,relay_log_recovery,"If enabled, this variable enables automatic relay log recovery immediately following server startup. The recovery process creates a new relay log file, initializes the SQL thread position to this new relay log, and initializes the I/O thread to the SQL thread position. Reading of the relay log from the source then continues.",0,0,others,mysql
5630,relay_log_space_limit,The maximum amount of space to use for all relay logs.,0,0,others,mysql
5631,remove,"(Windows only) Remove a MySQL Windows service. The default service name is MySQL if no service_name value is given. For more information, see Section2.3.4.8, “Starting MySQL as a Windows Service”.",0,0,others,mysql
5632,replica_allow_batching,"Whether or not batched updates are enabled on NDB Cluster replicas. From MySQL 8.0.26, use replica_allow_batching in place of slave_allow_batching, which is deprecated from that release. In releases before MySQL 8.0.26, use slave_allow_batching.",1,6,function-tradeoff,mysql
5633,replica_checkpoint_group,replica_checkpoint_group sets the maximum number of transactions that can be processed by a multithreaded replica before a checkpoint operation is called to update its status as shown by SHOW REPLICA STATUS. Setting this variable has no effect on replicas for which multithreading is not enabled. Setting this variable has no immediate effect. The state of the variable applies on all subsequent START REPLICA commands.,0,0,others,mysql
5634,replica_checkpoint_period,"replica_checkpoint_period sets the maximum time (in milliseconds) that is allowed to pass before a checkpoint operation is called to update the status of a multithreaded replica as shown by SHOW REPLICA STATUS. Setting this variable has no effect on replicas for which multithreading is not enabled. Setting this variable takes effect for all replication channels immediately, including running channels.",1,5,workload-specific,mysql
5636,replica_exec_mode,replica_exec_mode controls how a replication thread resolves conflicts and errors during replication. IDEMPOTENT mode causes suppression of duplicate-key and no-key-found errors; STRICT means no such suppression takes place.,0,0,others,mysql
5637,replica_load_tmpdir,"replica_load_tmpdir specifies the name of the directory where the replica creates temporary files. Setting this variable takes effect for all replication channels immediately, including running channels. The variable value is by default equal to the value of the tmpdir system variable, or the default that applies when that system variable is not specified.",0,0,others,mysql
5638,replica_max_allowed_packet,"replica_max_allowed_packet sets the maximum packet size in bytes that the replication SQL and I/O threads can handle. Setting this variable takes effect for all replication channels immediately, including running channels. It is possible for a source to write binary log events longer than its max_allowed_packet setting once the event header is added. The setting for replica_max_allowed_packet must be larger than the max_allowed_packet setting on the source, so that large updates using row-based replication do not cause replication to fail.",0,0,others,mysql
5639,replica_net_timeout,"replica_net_timeout specifies the number of seconds to wait for more data or a heartbeat signal from the source before the replica considers the connection broken, aborts the read, and tries to reconnect. Setting this variable has no immediate effect. The state of the variable applies on all subsequent START REPLICA commands.",0,0,others,mysql
5640,replica_parallel_type,"For multithreaded replicas (replicas on which replica_parallel_workers or slave_parallel_workers is set to a value greater than 0), replica_parallel_type specifies the policy used to decide which transactions are allowed to execute in parallel on the replica. The variable has no effect on replicas for which multithreading is not enabled.",1,5,workload-specific,mysql
5641,replica_parallel_workers,"replica_parallel_workers enables multithreading on the replica and sets the number of applier threads for executing replication transactions in parallel. When the value is a number greater than 0, the replica is a multithreaded replica with the specified number of applier threads, plus a coordinator thread to manage them. If you are using multiple replication channels, each channel has this number of threads.",1,1,resource,mysql
5644,replica_skip_errors,"Normally, replication stops when an error occurs on the replica, which gives you the opportunity to resolve the inconsistency in the data manually. This variable causes the replication SQL thread to continue replication when a statement returns any of the errors listed in the variable value.",1,3,reliability-tradeoff,mysql
5645,replica_sql_verify_checksum,"slave_sql_verify_checksum causes the replication SQL thread to verify data using the checksums read from the relay log. In the event of a mismatch, the replica stops with an error. Setting this variable takes effect for all replication channels immediately, including running channels.",1,3,reliability-tradeoff,mysql
5646,replica_transaction_retries,"replica_transaction_retries sets the maximum number of times for replication SQL threads on a single-threaded or multithreaded replica to automatically retry failed transactions before stopping. Setting this variable takes effect for all replication channels immediately, including running channels. The default value is 10. Setting the variable to 0 disables automatic retrying of transactions.",0,0,others,mysql
5647,replica_type_conversions,"replica_type_conversions controls the type conversion mode in effect on the replica when using row-based replication. Its value is a comma-delimited set of zero or more elements from the list: ALL_LOSSY, ALL_NON_LOSSY, ALL_SIGNED, ALL_UNSIGNED. Set this variable to an empty string to disallow type conversions between the source and the replica. Setting this variable takes effect for all replication channels immediately, including running channels.",0,0,others,mysql
5648,replicate-do-db,Creates a replication filter using the name of a database. Such filters can also be created using CHANGE REPLICATION FILTER REPLICATE_DO_DB.,0,0,others,mysql
5649,replicate-do-table,"Creates a replication filter by telling the replication SQL thread to restrict replication to a given table. To specify more than one table, use this option multiple times, once for each table. This works for both cross-database updates and default database updates, in contrast to --replicate-do-db. See Section17.2.5, “How Servers Evaluate Replication Filtering Rules”. You can also create such a filter by issuing a CHANGE REPLICATION FILTER REPLICATE_DO_TABLE statement.",0,0,others,mysql
5650,replicate-ignore-db,Creates a replication filter using the name of a database. Such filters can also be created using CHANGE REPLICATION FILTER REPLICATE_IGNORE_DB.,0,0,others,mysql
5651,replicate-ignore-table,"Creates a replication filter by telling the replication SQL thread not to replicate any statement that updates the specified table, even if any other tables might be updated by the same statement. To specify more than one table to ignore, use this option multiple times, once for each table. This works for cross-database updates, in contrast to --replicate-ignore-db. See Section17.2.5, “How Servers Evaluate Replication Filtering Rules”. You can also create such a filter by issuing a CHANGE REPLICATION FILTER REPLICATE_IGNORE_TABLE statement.",0,0,others,mysql
5652,replicate-rewrite-db,"Tells the replica to create a replication filter that translates the specified database to to_name if it was from_name on the source. Only statements involving tables are affected, not statements such as CREATE DATABASE, DROP DATABASE, and ALTER DATABASE.",1,6,function-tradeoff,mysql
5653,replicate-same-server-id,"This option is for use on replicas. The default is 0 (FALSE). With this option set to 1 (TRUE), the replica does not skip events that have its own server ID. This setting is normally useful only in rare configurations.",0,0,others,mysql
5656,replication_optimize_for_static_plugin_config,"Use shared locks, and avoid unnecessary lock acquisitions, to improve performance for semisynchronous replication. This setting and replication_sender_observe_commit_only help as the number of replicas increases, because contention for locks can slow down performance. While this system variable is enabled, the semisynchronous replication plugin cannot be uninstalled, so you must disable the system variable before the uninstall can complete.",1,4,limited-side-effect,mysql
5657,replication_sender_observe_commit_only,"Limit callbacks to improve performance for semisynchronous replication. This setting and replication_optimize_for_static_plugin_config help as the number of replicas increases, because contention for locks can slow down performance.",1,4,limited-side-effect,mysql
5658,report_host,The host name or IP address of the replica to be reported to the source during replica registration. This value appears in the output of SHOW REPLICAS on the source server. Leave the value unset if you do not want the replica to register itself with the source.,0,0,others,mysql
5659,report_password,The account password of the replica to be reported to the source during replica registration. This value appears in the output of SHOW REPLICAS on the source server if the source was started with --show-replica-auth-info or --show-slave-auth-info.,0,0,others,mysql
5660,report_port,"The TCP/IP port number for connecting to the replica, to be reported to the source during replica registration. Set this only if the replica is listening on a nondefault port or if you have a special tunnel from the source or other clients to the replica. If you are not sure, do not use this option.",0,0,others,mysql
5661,report_user,The account user name of the replica to be reported to the source during replica registration. This value appears in the output of SHOW REPLICAS on the source server if the source was started with --show-replica-auth-info or --show-slave-auth-info.,0,0,others,mysql
5662,rewriter_enabled,Whether the Rewriter query rewrite plugin is enabled.,1,5,workload-specific,mysql
5663,Rewriter_number_loaded_rules,The number of rewrite plugin rewrite rules successfully loaded from the rewrite_rules table into memory for use by the Rewriter plugin.,0,0,others,mysql
5665,Rewriter_number_rewritten_queries,The number of queries rewritten by the Rewriter query rewrite plugin since it was loaded.,1,1,resource,mysql
5666,Rewriter_reload_error,"Whether an error occurred the most recent time that the rewrite_rules table was loaded into the in-memory cache used by the Rewriter plugin. If the value is OFF, no error occurred. If the value is ON, an error occurred; check the message column of the rewriter_rules table for error messages.",0,0,others,mysql
5668,rpl_read_size,"The rpl_read_size system variable controls the minimum amount of data in bytes that is read from the binary log files and relay log files. If heavy disk I/O activity for these files is impeding performance for the database, increasing the read size might reduce file reads and I/O stalls when the file data is not currently cached by the operating system.",1,5,workload-specific,mysql
5669,rpl_semi_sync_master_enabled,"Controls whether semisynchronous replication is enabled on the source server. To enable or disable the plugin, set this variable to ON or OFF (or 1 or 0), respectively. The default is OFF.",0,0,others,mysql
5671,rpl_semi_sync_master_trace_level,The semisynchronous replication debug trace level on the source server. Four levels are defined:,0,0,others,mysql
5672,rpl_semi_sync_master_wait_for_slave_count,"The number of replica acknowledgments the source must receive per transaction before proceeding. By default rpl_semi_sync_master_wait_for_slave_count is 1, meaning that semisynchronous replication proceeds after receiving a single replica acknowledgment. Performance is best for small values of this variable.",1,1,resource,mysql
5673,rpl_semi_sync_master_wait_no_slave,"Controls whether the source waits for the timeout period configured by rpl_semi_sync_master_timeout to expire, even if the replica count drops to less than the number of replicas configured by rpl_semi_sync_master_wait_for_slave_count during the timeout period.",0,0,others,mysql
5674,rpl_semi_sync_master_wait_point,This variable controls the point at which a semisynchronous replication source server waits for replica acknowledgment of transaction receipt before returning a status to the client that committed the transaction. These values are permitted:,0,0,others,mysql
5675,rpl_semi_sync_replica_enabled,"rpl_semi_sync_replica_enabled is available when the rpl_semi_sync_replica (semisync_replica.so library) plugin was installed on the replica to set up semisynchronous replication. If the rpl_semi_sync_slave plugin (semisync_slave.so library) was installed, rpl_semi_sync_slave_enabled is available instead.",0,0,others,mysql
5677,rpl_semi_sync_slave_enabled,"rpl_semi_sync_slave_enabled is available when the rpl_semi_sync_slave (semisync_slave.so library) plugin was installed on the replica to set up semisynchronous replication. If the rpl_semi_sync_replica plugin (semisync_replica.so library) was installed, rpl_semi_sync_replica_enabled is available instead.",0,0,others,mysql
5678,rpl_semi_sync_slave_trace_level,"rpl_semi_sync_slave_trace_level is available when the rpl_semi_sync_slave (semisync_slave.so library) plugin was installed on the replica to set up semisynchronous replication. If the rpl_semi_sync_replica plugin (semisync_replica.so library) was installed, rpl_semi_sync_replica_trace_level is available instead.",0,0,others,mysql
5681,rpl_semi_sync_source_trace_level,"rpl_semi_sync_source_trace_level is available when the rpl_semi_sync_source (semisync_source.so library) plugin was installed on the replica to set up semisynchronous replication. If the rpl_semi_sync_master plugin (semisync_master.so library) was installed, rpl_semi_sync_master_trace_level is available instead.",0,0,others,mysql
5682,rpl_semi_sync_source_wait_for_replica_count,"rpl_semi_sync_source_wait_for_replica_count is available when the rpl_semi_sync_source (semisync_source.so library) plugin was installed on the replica to set up semisynchronous replication. If the rpl_semi_sync_master plugin (semisync_master.so library) was installed, rpl_semi_sync_master_wait_for_slave_count is available instead.",0,0,others,mysql
5683,rpl_semi_sync_source_wait_no_replica,"rpl_semi_sync_source_wait_no_replica is available when the rpl_semi_sync_source (semisync_source.so library) plugin was installed on the replica to set up semisynchronous replication. If the rpl_semi_sync_master plugin (semisync_master.so library) was installed, rpl_semi_sync_source_wait_no_replica is available instead.",0,0,others,mysql
5685,rpl_stop_replica_timeout,You can control the length of time (in seconds) that STOP REPLICA waits before timing out by setting this variable. This can be used to avoid deadlocks between STOP REPLICA and other SQL statements using different client connections to the replica.,0,0,others,mysql
5686,rpl_stop_slave_timeout,You can control the length of time (in seconds) that STOP REPLICA waits before timing out by setting this variable. This can be used to avoid deadlocks between STOP REPLICA and other SQL statements using different client connections to the replica.,0,0,others,mysql
5687,safe-user-create,"If this option is enabled, a user cannot create new MySQL users by using the GRANT statement unless the user has the INSERT privilege for the mysql.user system table or any column in the table. If you want a user to have the ability to create new users that have those privileges that the user has the right to grant, you should grant the user the following privilege:",0,0,others,mysql
5690,server_uuid,"The following sections contain information about mysqld options and server variables that are used in replication and for controlling the binary log. Options and variables for use on sources and replicas are covered separately, as are options and variables relating to binary logging and global transaction identifiers (GTIDs). A set of quick-reference tables providing basic information about these options and variables is also included.",0,0,others,mysql
5691,show-replica-auth-info,"The options display replication user names and passwords in the output of SHOW REPLICAS (or before MySQL 8.0.22, SHOW SLAVE HOSTS) on the source for replicas started with the --report-user and --report-password options.",0,0,others,mysql
5693,skip_replica_start,"--skip-replica-start tells the replica server not to start the replication I/O (receiver) and SQL (applier) threads when the server starts. To start the threads later, use a START REPLICA statement. You can use the skip_replica_start system variable in place of the command line option to allow access to this feature using MySQL Server’s privilege structure, so that database administrators do not need any privileged access to the operating system.",0,0,others,mysql
5694,skip_show_database,"This option sets the skip_show_database system variable that controls who is permitted to use the SHOW DATABASES statement. See Section5.1.8, “Server System Variables”.",0,0,others,mysql
5696,skip-character-set-client-handshake,"Do not ignore character set information sent by the client. To ignore client information and use the default server character set, use --skip-character-set-client-handshake; this makes MySQL behave like MySQL 4.0.",0,0,others,mysql
5698,skip-host-cache,"Disable use of the internal host cache for faster name-to-IP resolution. With the cache disabled, the server performs a DNS lookup every time a client connects.",1,6,function-tradeoff,mysql
5699,skip-ndbcluster,"Disable the NDBCLUSTER storage engine. This is the default for binaries that were built with NDBCLUSTER storage engine support; the server allocates memory and other resources for this storage engine only if the --ndbcluster option is given explicitly. See Section23.4.1, “Quick Test Setup of NDB Cluster”, for an example.",0,0,others,mysql
5700,skip-new,"This option disables (what used to be considered) new, possibly unsafe behaviors. It results in these settings: delay_key_write=OFF, concurrent_insert=NEVER, automatic_sp_privileges=OFF. It also causes OPTIMIZE TABLE to be mapped to ALTER TABLE for storage engines for which OPTIMIZE TABLE is not supported.",1,2,security-tradeoff,mysql
5701,skip-ssl,The --ssl option specifies that the server permits but does not require encrypted connections on the main connection interface. This option is enabled by default.,1,2,security-tradeoff,mysql
5702,skip-stack-trace,"Do not write stack traces. This option is useful when you are running mysqld under a debugger. On some systems, you also must use this option to get a core file.",1,4,limited-side-effect,mysql
5703,slave_allow_batching,"Whether or not batched updates are enabled on NDB Cluster replicas. From MySQL 8.0.26, slave_allow_batching is deprecated and the alias replica_allow_batching should be used instead. In releases before MySQL 8.0.26, use slave_allow_batching.",1,6,function-tradeoff,mysql
5704,slave_checkpoint_group,slave_checkpoint_group sets the maximum number of transactions that can be processed by a multithreaded replica before a checkpoint operation is called to update its status as shown by SHOW REPLICA STATUS. Setting this variable has no effect on replicas for which multithreading is not enabled. Setting this variable has no immediate effect. The state of the variable applies on all subsequent START REPLICA commands.,0,0,others,mysql
5705,slave_checkpoint_period,"slave_checkpoint_period sets the maximum time (in milliseconds) that is allowed to pass before a checkpoint operation is called to update the status of a multithreaded replica as shown by SHOW REPLICA STATUS. Setting this variable has no effect on replicas for which multithreading is not enabled. Setting this variable takes effect for all replication channels immediately, including running channels.",0,0,others,mysql
5706,slave_compressed_protocol,"slave_compressed_protocol is deprecated, and from MySQL 8.0.26, the alias replica_compressed_protocol should be used instead. In releases before MySQL 8.0.26, use slave_compressed_protocol.",0,0,others,mysql
5707,slave_exec_mode,slave_exec_mode controls how a replication thread resolves conflicts and errors during replication. IDEMPOTENT mode causes suppression of duplicate-key and no-key-found errors; STRICT means no such suppression takes place.,0,0,others,mysql
5708,slave_load_tmpdir,"slave_load_tmpdir specifies the name of the directory where the replica creates temporary files. Setting this variable takes effect for all replication channels immediately, including running channels. The variable value is by default equal to the value of the tmpdir system variable, or the default that applies when that system variable is not specified.",0,0,others,mysql
5709,slave_max_allowed_packet,"slave_max_allowed_packet sets the maximum packet size in bytes that the replication SQL and I/O threads can handle. Setting this variable takes effect for all replication channels immediately, including running channels. It is possible for a source to write binary log events longer than its max_allowed_packet setting once the event header is added. The setting for slave_max_allowed_packet must be larger than the max_allowed_packet setting on the source, so that large updates using row-based replication do not cause replication to fail.",0,0,others,mysql
5710,slave_net_timeout,"slave_net_timeout specifies the number of seconds to wait for more data or a heartbeat signal from the source before the replica considers the connection broken, aborts the read, and tries to reconnect. Setting this variable has no immediate effect. The state of the variable applies on all subsequent START REPLICA commands.",0,0,others,mysql
5711,slave_parallel_type,"For multithreaded replicas (replicas on which replica_parallel_workers or slave_parallel_workers is set to a value greater than 0), slave_parallel_type specifies the policy used to decide which transactions are allowed to execute in parallel on the replica. The variable has no effect on replicas for which multithreading is not enabled.",0,0,others,mysql
5712,slave_parallel_workers,"slave_parallel_workers enables multithreading on the replica and sets the number of applier threads for executing replication transactions in parallel. When the value is a number greater than 0, the replica is a multithreaded replica with the specified number of applier threads, plus a coordinator thread to manage them. If you are using multiple replication channels, each channel has this number of threads.",1,1,resource,mysql
5713,slave_pending_jobs_size_max,"For multithreaded replicas, this variable sets the maximum amount of memory (in bytes) available to applier queues holding events not yet applied. Setting this variable has no effect on replicas for which multithreading is not enabled. Setting this variable has no immediate effect. The state of the variable applies on all subsequent START REPLICA commands.",0,0,others,mysql
5714,slave_preserve_commit_order,"For multithreaded replicas (replicas on which replica_parallel_workers or slave_parallel_workers is set to a value greater than 0), setting slave_preserve_commit_order=1 ensures that transactions are executed and committed on the replica in the same order as they appear in the replica's relay log. This prevents gaps in the sequence of transactions that have been executed from the replica's relay log, and preserves the same transaction history on the replica as on the source (with the limitations listed below). This variable has no effect on replicas for which multithreading is not enabled.",1,3,reliability-tradeoff,mysql
5715,slave_rows_search_algorithms,"When preparing batches of rows for row-based logging and replication, this system variable controls how the rows are searched for matches, in particular whether hash scans are used. The use of this system variable is now deprecated. The default setting INDEX_SCAN,HASH_SCAN is optimal for performance and works correctly in all scenarios.",0,0,others,mysql
5716,slave_skip_errors,"Normally, replication stops when an error occurs on the replica, which gives you the opportunity to resolve the inconsistency in the data manually. This option causes the replication SQL thread to continue replication when a statement returns any of the errors listed in the option value.",1,3,reliability-tradeoff,mysql
5717,slave_sql_verify_checksum,"slave_sql_verify_checksum causes the replication SQL thread to verify data using the checksums read from the relay log. In the event of a mismatch, the replica stops with an error. Setting this variable takes effect for all replication channels immediately, including running channels.",1,2,security-tradeoff,mysql
5718,slave_transaction_retries,"slave_transaction_retries sets the maximum number of times for replication SQL threads on a single-threaded or multithreaded replica to automatically retry failed transactions before stopping. Setting this variable takes effect for all replication channels immediately, including running channels. The default value is 10. Setting the variable to 0 disables automatic retrying of transactions.",0,0,others,mysql
5719,slave_type_conversions,"slave_type_conversions controls the type conversion mode in effect on the replica when using row-based replication. Its value is a comma-delimited set of zero or more elements from the list: ALL_LOSSY, ALL_NON_LOSSY, ALL_SIGNED, ALL_UNSIGNED. Set this variable to an empty string to disallow type conversions between the source and the replica. Setting this variable takes effect for all replication channels immediately, including running channels.",0,0,others,mysql
5720,slave-sql-verify-checksum,"When this option is enabled, the replica examines checksums read from the relay log. In the event of a mismatch, the replica stops with an error.",1,2,security-tradeoff,mysql
5721,slow-start-timeout,"This option controls the Windows service control manager's service start timeout. The value is the maximum number of milliseconds that the service control manager waits before trying to kill the windows service during startup. The default value is 15000 (15 seconds). If the MySQL service takes too long to start, you may need to increase this value. A value of 0 means there is no timeout.",0,0,others,mysql
5722,socket,"On Unix, this option specifies the Unix socket file to use when listening for local connections. The default value is /tmp/mysql.sock. If this option is given, the server creates the file in the data directory unless an absolute path name is given to specify a different directory. On Windows, the option specifies the pipe name to use when listening for local connections that use a named pipe. The default value is MySQL (not case-sensitive).",0,0,others,mysql
5723,source_verify_checksum,"Enabling source_verify_checksum causes the source to verify events read from the binary log by examining checksums, and to stop with an error in the event of a mismatch. source_verify_checksum is disabled by default; in this case, the source uses the event length from the binary log to verify events, so that only complete events are read from the binary log.",1,2,security-tradeoff,mysql
5725,sql_log_bin,"This variable controls whether logging to the binary log is enabled for the current session (assuming that the binary log itself is enabled). The default value is ON. To disable or enable binary logging for the current session, set the session sql_log_bin variable to OFF or ON.",0,0,others,mysql
5726,sql_mode,Set the SQL mode.,0,0,others,mysql
5727,sql_replica_skip_counter,"sql_replica_skip_counter specifies the number of events from the source that a replica should skip. Setting the option has no immediate effect. The variable applies to the next START REPLICA statement; the next START REPLICA statement also changes the value back to 0. When this variable is set to a nonzero value and there are multiple replication channels configured, the START REPLICA statement can only be used with the FOR CHANNEL channel clause.",0,0,others,mysql
5730,standalone,Available on Windows only; instructs the MySQL server not to run as a service.,0,0,others,mysql
5731,super-large-pages,"Standard use of large pages in MySQL attempts to use the largest size supported, up to 4MB. Under Solaris, a “super large pages” feature enables uses of pages up to 256MB. This feature is available for recent SPARC platforms. It can be enabled or disabled by using the --super-large-pages or --skip-super-large-pages option.",1,4,limited-side-effect,mysql
5732,symbolic-links,"Enable or disable symbolic link support. On Unix, enabling symbolic links means that you can link a MyISAM index file or data file to another directory with the INDEX DIRECTORY or DATA DIRECTORY option of the CREATE TABLE statement. If you delete or rename the table, the files that its symbolic links point to also are deleted or renamed. See Section8.12.2.2, “Using Symbolic Links for MyISAM Tables on Unix”.",0,0,others,mysql
5734,sync_master_info,"sync_master_info specifies the number of events after which the replica updates the connection metadata repository. When the connection metadata repository is stored as an InnoDB table, which is the default from MySQL 8.0, it is updated after this number of events. If the connection metadata repository is stored as a file, which is deprecated from MySQL 8.0, the replica synchronizes its master.info file to disk (using fdatasync()) after this number of events. The default value is 10000, and a zero value means that the repository is never updated. Setting this variable takes effect for all replication channels immediately, including running channels.",1,3,reliability-tradeoff,mysql
5735,sync_relay_log,"If the value of this variable is greater than 0, the MySQL server synchronizes its relay log to disk (using fdatasync()) after every sync_relay_log events are written to the relay log. Setting this variable takes effect for all replication channels immediately, including running channels.",1,3,reliability-tradeoff,mysql
5736,sync_relay_log_info,"The number of transactions after which the replica updates the applier metadata repository. When the applier metadata repository is stored as an InnoDB table, which is the default from MySQL 8.0, it is updated after every transaction and this system variable is ignored. If the applier metadata repository is stored as a file, which is deprecated from MySQL 8.0, the replica synchronizes its relay-log.info file to disk (using fdatasync()) after this number of transactions. The default value for sync_relay_log_info is 10000, and a zero value means that the file contents are only flushed by the operating system. Setting this variable takes effect for all replication channels immediately, including running channels.",1,3,reliability-tradeoff,mysql
5737,sync_source_info,"sync_source_info specifies the number of events after which the replica updates the connection metadata repository. When the connection metadata repository is stored as an InnoDB table, which is the default from MySQL 8.0, it is updated after this number of events. If the connection metadata repository is stored as a file, which is deprecated from MySQL 8.0, the replica synchronizes its master.info file to disk (using fdatasync()) after this number of events. The default value is 10000, and a zero value means that the repository is never updated. Setting this variable takes effect for all replication channels immediately, including running channels.",1,3,reliability-tradeoff,mysql
5739,tc-heuristic-recover,The decision to use in a manual heuristic recovery.,1,2,security-tradeoff,mysql
5740,terminology_use_previous,"In MySQL 8.0.26, incompatible changes were made to instrumentation names containing the terms “master”, which is changed to “source”, “slave”, which is changed to “replica”, and “mts” (for “multithreaded slave”), which is changed to “mta” (for “multithreaded applier”). Monitoring tools that work with these instrumentation names might be impacted. If the incompatible changes have an impact for you, set the terminology_use_previous system variable to BEFORE_8_0_26 to make MySQL Server use the old versions of the names for the objects specified in the previous list. This enables monitoring tools that rely on the old names to continue working until they can be updated to use the new names.",0,0,others,mysql
5741,tmpdir,The path of the directory to use for creating temporary files. It might be useful if your default /tmp directory resides on a partition that is too small to hold temporary tables. This option accepts several paths that are used in round-robin fashion. Paths should be separated by colon characters (:) on Unix and semicolon characters (;) on Windows.,0,0,others,mysql
5742,transaction_allow_batching,"When set to 1 or ON, this variable enables batching of statements within the same transaction. To use this variable, autocommit must first be disabled by setting it to 0 or OFF; otherwise, setting transaction_allow_batching has no effect.",1,4,limited-side-effect,mysql
5743,transaction_isolation,"Sets the default transaction isolation level. The level value can be READ-UNCOMMITTED, READ-COMMITTED, REPEATABLE-READ, or SERIALIZABLE.",1,3,reliability-tradeoff,mysql
5744,transaction_read_only,"Sets the default transaction access mode. By default, read-only mode is disabled, so the mode is read/write.",0,0,others,mysql
5745,transaction_write_set_extraction,"This system variable specifies the algorithm used to hash the writes extracted during a transaction. The default in MySQL 8.0 is that transaction_write_set_extraction is set to XXHASH64. OFF means that write sets are not collected. transaction_write_set_extraction is deprecated from MySQL 8.0.26, and will be removed in a future MySQL release.",1,6,function-tradeoff,mysql
5746,upgrade,This option controls whether and how the server performs an automatic upgrade at startup. Automatic upgrade involves two steps:,0,0,others,mysql
5747,user,"Run the mysqld server as the user having the name user_name or the numeric user ID user_id. (“User” in this context refers to a system login account, not a MySQL user listed in the grant tables.)",0,0,others,mysql
5748,validate_password.check_user_name,Whether validate_password compares passwords to the user name part of the effective user account for the current session and rejects them if they match. This variable is unavailable unless validate_password is installed.,0,0,others,mysql
5750,validate_password.dictionary_file_last_parsed,When the dictionary file was last parsed. This variable is unavailable unless validate_password is installed.,0,0,others,mysql
5751,validate_password.dictionary_file_words_count,The number of words read from the dictionary file. This variable is unavailable unless validate_password is installed.,0,0,others,mysql
5752,validate_password.length,The minimum number of characters that validate_password requires passwords to have. This variable is unavailable unless validate_password is installed.,0,0,others,mysql
5753,validate_password.mixed_case_count,The minimum number of lowercase and uppercase characters that validate_password requires passwords to have if the password policy is MEDIUM or stronger. This variable is unavailable unless validate_password is installed.,0,0,others,mysql
5754,validate_password.number_count,The minimum number of numeric (digit) characters that validate_password requires passwords to have if the password policy is MEDIUM or stronger. This variable is unavailable unless validate_password is installed.,0,0,others,mysql
5756,validate_password.special_char_count,The minimum number of nonalphanumeric characters that validate_password requires passwords to have if the password policy is MEDIUM or stronger. This variable is unavailable unless validate_password is installed.,0,0,others,mysql
5760,validate_password_dictionary_file_words_count,This validate_password plugin status variable is deprecated; expect it to be removed in a future version of MySQL. Use the corresponding validate_password.dictionary_file_words_count status variable of the validate_password component instead.,0,0,others,mysql
5761,validate_password_length,This validate_password plugin system variable is deprecated; expect it to be removed in a future version of MySQL. Use the corresponding validate_password.length system variable of the validate_password component instead.,0,0,others,mysql
5762,validate_password_mixed_case_count,This validate_password plugin system variable is deprecated; expect it to be removed in a future version of MySQL. Use the corresponding validate_password.mixed_case_count system variable of the validate_password component instead.,0,0,others,mysql
5764,validate_password_policy,This validate_password plugin system variable is deprecated; expect it to be removed in a future version of MySQL. Use the corresponding validate_password.policy system variable of the validate_password component instead.,0,0,others,mysql
5765,validate_password_special_char_count,This validate_password plugin system variable is deprecated; expect it to be removed in a future version of MySQL. Use the corresponding validate_password.special_char_count system variable of the validate_password component instead.,0,0,others,mysql
5766,validate-config,"Validate the server startup configuration. If no errors are found, the server terminates with an exit code of 0. If an error is found, the server displays a diagnostic message and terminates with an exit code of 1. Warning and information messages may also be displayed, depending on the log_error_verbosity value, but do not produce immediate validation termination or an exit code of 1.",0,0,others,mysql
5768,validate-user-plugins,"If this option is enabled (the default), the server checks each user account and produces a warning if conditions are found that would make the account unusable. Enabling --validate-user-plugins slows down server initialization and FLUSH PRIVILEGES. If you do not require the additional checking, you can disable this option at startup to avoid the performance decrement.",1,2,security-tradeoff,mysql
5769,verbose,Use this option with the --help option for detailed help.,0,0,others,mysql
5771,version_tokens_session_number,This variable is for internal use.,0,0,others,mysql
5774,accept_mutex_delay,"If accept_mutex is enabled, specifies the maximum time during which a worker process will try to restart accepting new connections if another worker process is currently accepting new connections.",0,0,others,nginx
5775,access_log.B,"Sets the path, format, and configuration for a buffered log write. Several logs can be specified on the same configuration level. Logging to syslog can be configured by specifying the syslog: prefix in the first parameter. The special value off cancels all access_log directives on the current level. If the format is not specified then the predefined combined format is used.",0,0,others,nginx
5776,access_log,"Sets the path, format, and configuration for a buffered log write. Several logs can be specified on the same configuration level. Logging to syslog can be configured by specifying the syslog: prefix in the first parameter. The special value off cancels all access_log directives on the current level.",0,0,others,nginx
5778,add_before_body,"Adds the text returned as a result of processing a given subrequest before the response body. An empty string ("""") as a parameter cancels addition inherited from the previous configuration level.",0,0,others,nginx
5779,add_header,"Adds the specified field to a response header provided that the response code equals 200, 201 (1.3.10), 204, 206, 301, 302, 303, 304, 307 (1.1.16, 1.0.13), or 308 (1.13.0). Parameter value can contain variables.",0,0,others,nginx
5781,addition_types,"Allows adding text in responses with the specified MIME types, in addition to text/html. The special value * matches any MIME type (0.8.29).",0,0,others,nginx
5782,aio,Enables or disables the use of asynchronous file I/O (AIO) on FreeBSD and Linux:,1,6,function-tradeoff,nginx
5784,alias,"Defines a replacement for the specified location. For example, with the following configuration",0,0,others,nginx
5785,allow.B,"Allows access for the specified network or address. If the special value unix: is specified (1.5.1), allows access for all UNIX-domain sockets.",0,0,others,nginx
5786,allow,"Allows access for the specified network or address. If the special value unix: is specified, allows access for all UNIX-domain sockets.",0,0,others,nginx
5787,ancient_browser,"If any of the specified substrings is found in the User-Agent request header field, the browser will be considered ancient. The special string netscape4 corresponds to the regular expression ^Mozilla/[1-4].",0,0,others,nginx
5788,ancient_browser_value,Sets a value for the $ancient_browser variables.,0,0,others,nginx
5789,api,Turns on the REST API interface in the surrounding location. Access to this location should be limited.,0,0,others,nginx
5793,auth_http,Sets the URL of the HTTP authentication server. The protocol is described below.,0,0,others,nginx
5794,auth_http_header,Appends the specified header to requests sent to the authentication server. This header can be used as the shared secret to verify that the request comes from nginx. For example:,0,0,others,nginx
5795,auth_http_pass_client_cert,Appends the Auth-SSL-Cert header with the client certificate in the PEM format (urlencoded) to requests sent to the authentication server.,0,0,others,nginx
5797,auth_jwt,Enables validation of JSON Web Token. The specified string is used as a realm. Parameter value can contain variables.,1,2,security-tradeoff,nginx
5798,auth_jwt_claim_set,"Sets the variable to a JWT claim parameter identified by key names. Name matching starts from the top level of the JSON tree. For arrays, the variable keeps a list of array elements separated by commas.",0,0,others,nginx
5800,auth_jwt_key_file,Specifies a file in JSON Web Key Set format for validating JWT signature. Parameter value can contain variables.,0,0,others,nginx
5801,auth_jwt_key_request,"Allows retrieving a JSON Web Key Set file from a subrequest for validating JWT signature and sets the URI where the subrequest will be sent to. Parameter value can contain variables. To avoid validation overhead, it is recommended to cache the key file:",0,0,others,nginx
5803,auth_jwt_type,Specifies which type of JSON Web Token to expect: JWS (signed) or JWE (encrypted).,1,2,security-tradeoff,nginx
5805,auth_request_set,"Sets the request variable to the given value after the authorization request completes. The value may contain variables from the authorization request, such as $upstream_http_*.",0,0,others,nginx
5806,autoindex,Enables or disables the directory listing output.,0,0,others,nginx
5807,autoindex_exact_size,"For the HTML format, specifies whether exact file sizes should be output in the directory listing, or rather rounded to kilobytes, megabytes, and gigabytes.",0,0,others,nginx
5808,autoindex_format,Sets the format of a directory listing.,0,0,others,nginx
5809,autoindex_localtime,"For the HTML format, specifies whether times in the directory listing should be output in the local time zone or UTC.",0,0,others,nginx
5810,break,Stops processing the current set of ngx_http_rewrite_module directives.,0,0,others,nginx
5811,charset,"Adds the specified charset to the Content-Type response header field. If this charset is different from the charset specified in the source_charset directive, a conversion is performed.",0,0,others,nginx
5812,charset_map,"Describes the conversion table from one charset to another. A reverse conversion table is built using the same data. Character codes are given in hexadecimal. Missing characters in the range 80-FF are replaced with ?. When converting from UTF-8, characters missing in a one-byte charset are replaced with &#XXXX;.",0,0,others,nginx
5813,charset_types,Enables module processing in responses with the specified MIME types in addition to text/html. The special value * matches any MIME type (0.8.29).,0,0,others,nginx
5814,chunked_transfer_encoding,Allows disabling chunked transfer encoding in HTTP/1.1. It may come in handy when using a software failing to support chunked encoding despite the standards requirement.,0,0,others,nginx
5816,client_body_in_file_only,"Determines whether nginx should save the entire client request body into a file. This directive can be used during debugging, or when using the $request_body_file variable, or the $r->request_body_file method of the module ngx_http_perl_module.",0,0,others,nginx
5817,client_body_in_single_buffer,"Determines whether nginx should save the entire client request body in a single buffer. The directive is recommended when using the $request_body variable, to save the number of copy operations involved.",1,6,function-tradeoff,nginx
5819,client_body_timeout,"Defines a timeout for reading client request body. The timeout is set only for a period between two successive read operations, not for the transmission of the whole request body. If a client does not transmit anything within this time, the request is terminated with the 408 (Request Time-out) error.",0,0,others,nginx
5822,client_max_body_size,"Sets the maximum allowed size of the client request body. If the size in a request exceeds the configured value, the 413 (Request Entity Too Large) error is returned to the client. Please be aware that browsers cannot correctly display this error. Setting size to 0 disables checking of client request body size.",1,1,resource,nginx
5823,connection_pool_size,"Allows accurate tuning of per-connection memory allocations. This directive has minimal impact on performance and should not generally be used. By default, the size is equal to 256 bytes on 32-bit platforms and 512 bytes on 64-bit platforms.",1,5,workload-specific,nginx
5824,create_full_put_path,The WebDAV specification only allows creating files in already existing directories. This directive allows creating all needed intermediate directories.,0,0,others,nginx
5825,daemon,Determines whether nginx should become a daemon. Mainly used during development.,0,0,others,nginx
5827,dav_methods,"Allows the specified HTTP and WebDAV methods. The parameter off denies all methods processed by this module. The following methods are supported: PUT, DELETE, MKCOL, COPY, and MOVE.",0,0,others,nginx
5828,debug_connection,"Enables debugging log for selected client connections. Other connections will use logging level set by the error_log directive. Debugged connections are specified by IPv4 or IPv6 (1.3.0, 1.2.1) address or network. A connection may also be specified using a hostname. For connections using UNIX-domain sockets (1.3.0, 1.2.1), debugging log is enabled by the unix: parameter.",0,0,others,nginx
5829,debug_points,This directive is used for debugging.,0,0,others,nginx
5830,default_type,Defines the default MIME type of a response. Mapping of file name extensions to MIME types can be set with the types directive.,0,0,others,nginx
5833,directio,"Enables the use of the O_DIRECT flag (FreeBSD, Linux), the F_NOCACHE flag (macOS), or the directio() function (Solaris), when reading files that are larger than or equal to the specified size. The directive automatically disables (0.7.15) the use of sendfile for a given request. It can be useful for serving large files:",0,0,others,nginx
5834,directio_alignment,"Sets the alignment for directio. In most cases, a 512-byte alignment is enough. However, when using XFS under Linux, it needs to be increased to 4K.",0,0,others,nginx
5835,disable_symlinks,Determines how symbolic links should be treated when opening files:,0,0,others,nginx
5837,env,"By default, nginx removes all environment variables inherited from its parent process except the TZ variable. This directive allows preserving some of the inherited variables, changing their values, or creating new environment variables. These variables are then:",0,0,others,nginx
5838,error_log,"Configures logging. Several logs can be specified on the same configuration level (1.5.2). If on the main configuration level writing a log to a file is not explicitly defined, the default file will be used.",0,0,others,nginx
5839,error_page,Defines the URI that will be shown for the specified errors. A uri value can contain variables.,0,0,others,nginx
5840,etag,Enables or disables automatic generation of the ETag response header field for static resources.,0,0,others,nginx
5842,expires,"Enables or disables adding or modifying the Expires and Cache-Control response header fields provided that the response code equals 200, 201 (1.3.10), 204, 206, 301, 302, 303, 304, 307 (1.1.16, 1.0.13), or 308 (1.13.0). The parameter can be a positive or negative time.",0,0,others,nginx
5843,f4f,Turns on module processing in the surrounding location.,1,6,function-tradeoff,nginx
5844,f4f_buffer_size,Sets the size of the buffer used for reading the .f4x index file.,1,1,resource,nginx
5845,fastcgi_bind,"Makes outgoing connections to a FastCGI server originate from the specified local IP address with an optional port (1.11.2). Parameter value can contain variables (1.3.12). The special value off (1.3.12) cancels the effect of the fastcgi_bind directive inherited from the previous configuration level, which allows the system to auto-assign the local IP address and port.",1,4,limited-side-effect,nginx
5846,fastcgi_buffer_size,"Sets the size of the buffer used for reading the first part of the response received from the FastCGI server. This part usually contains a small response header. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform. It can be made smaller, however.",1,1,resource,nginx
5847,fastcgi_buffering,Enables or disables buffering of responses from the FastCGI server.,1,6,function-tradeoff,nginx
5848,fastcgi_buffers,"Sets the number and size of the buffers used for reading a response from the FastCGI server, for a single connection. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform.",1,1,resource,nginx
5849,fastcgi_busy_buffers_size,"When buffering of responses from the FastCGI server is enabled, limits the total size of buffers that can be busy sending a response to the client while the response is not yet fully read. In the meantime, the rest of the buffers can be used for reading the response and, if needed, buffering part of the response to a temporary file. By default, size is limited by the size of two buffers set by the fastcgi_buffer_size and fastcgi_buffers directives.",1,1,resource,nginx
5851,fastcgi_cache_background_update,"Allows starting a background subrequest to update an expired cache item, while a stale cached response is returned to the client. Note that it is necessary to allow the usage of a stale cached response when it is being updated.",1,6,function-tradeoff,nginx
5852,fastcgi_cache_bypass,Defines conditions under which the response will not be taken from a cache. If at least one value of the string parameters is not empty and is not equal to 0 then the response will not be taken from the cache:,0,0,others,nginx
5853,fastcgi_cache_key,"Defines a key for caching, for example",0,0,others,nginx
5854,fastcgi_cache_lock,"When enabled, only one request at a time will be allowed to populate a new cache element identified according to the fastcgi_cache_key directive by passing a request to a FastCGI server. Other requests of the same cache element will either wait for a response to appear in the cache or the cache lock for this element to be released, up to the time set by the fastcgi_cache_lock_timeout directive.",1,4,limited-side-effect,nginx
5855,fastcgi_cache_lock_age,"If the last request passed to the FastCGI server for populating a new cache element has not completed for the specified time, one more request may be passed to the FastCGI server.",0,0,others,nginx
5856,fastcgi_cache_lock_timeout,"Sets a timeout for fastcgi_cache_lock. When the time expires, the request will be passed to the FastCGI server, however, the response will not be cached.",0,0,others,nginx
5858,fastcgi_cache_methods,"If the client request method is listed in this directive then the response will be cached. GET and HEAD methods are always added to the list, though it is recommended to specify them explicitly. See also the fastcgi_no_cache directive.",0,0,others,nginx
5859,fastcgi_cache_min_uses,Sets the number of requests after which the response will be cached.,1,5,workload-specific,nginx
5860,fastcgi_cache_path,"Sets the path and other parameters of a cache. Cache data are stored in files. Both the key and file name in a cache are a result of applying the MD5 function to the proxied URL. The levels parameter defines hierarchy levels of a cache: from 1 to 3, each level accepts values 1 or 2. For example, in the following configuration",0,0,others,nginx
5861,fastcgi_cache_purge,Defines conditions under which the request will be considered a cache purge request. If at least one value of the string parameters is not empty and is not equal to 0 then the cache entry with a corresponding cache key is removed. The result of successful operation is indicated by returning the 204 (No Content) response.,0,0,others,nginx
5863,fastcgi_cache_use_stale,Determines in which cases a stale cached response can be used when an error occurs during communication with the FastCGI server. The directives parameters match the parameters of the fastcgi_next_upstream directive.,0,0,others,nginx
5865,fastcgi_catch_stderr,"Sets a string to search for in the error stream of a response received from a FastCGI server. If the string is found then it is considered that the FastCGI server has returned an invalid response. This allows handling application errors in nginx, for example:",0,0,others,nginx
5866,fastcgi_connect_timeout,Defines a timeout for establishing a connection with a FastCGI server. It should be noted that this timeout cannot usually exceed 75 seconds.,0,0,others,nginx
5867,fastcgi_force_ranges,Enables byte-range support for both cached and uncached responses from the FastCGI server regardless of the Accept-Ranges field in these responses.,1,4,limited-side-effect,nginx
5868,fastcgi_hide_header,"By default, nginx does not pass the header fields Status and X-Accel-... from the response of a FastCGI server to a client. The fastcgi_hide_header directive sets additional fields that will not be passed. If, on the contrary, the passing of fields needs to be permitted, the fastcgi_pass_header directive can be used.",0,0,others,nginx
5869,fastcgi_ignore_client_abort,Determines whether the connection with a FastCGI server should be closed when a client closes the connection without waiting for a response.,0,0,others,nginx
5870,fastcgi_ignore_headers,"Disables processing of certain response header fields from the FastCGI server. The following fields can be ignored: X-Accel-Redirect, X-Accel-Expires, X-Accel-Limit-Rate (1.1.6), X-Accel-Buffering (1.1.6), X-Accel-Charset (1.1.6), Expires, Cache-Control, Set-Cookie (0.8.44), and Vary (1.7.7).",0,0,others,nginx
5871,fastcgi_index,"Sets a file name that will be appended after a URI that ends with a slash, in the value of the $fastcgi_script_name variable. For example, with these settings",0,0,others,nginx
5872,fastcgi_intercept_errors,Determines whether FastCGI server responses with codes greater than or equal to 300 should be passed to a client or be intercepted and redirected to nginx for processing with the error_page directive.,0,0,others,nginx
5873,fastcgi_keep_conn,"By default, a FastCGI server will close a connection right after sending the response. However, when this directive is set to the value on, nginx will instruct a FastCGI server to keep connections open. This is necessary, in particular, for keepalive connections to FastCGI servers to function.",0,0,others,nginx
5875,fastcgi_max_temp_file_size,"When buffering of responses from the FastCGI server is enabled, and the whole response does not fit into the buffers set by the fastcgi_buffer_size and fastcgi_buffers directives, a part of the response can be saved to a temporary file. This directive sets the maximum size of the temporary file. The size of data written to the temporary file at a time is set by the fastcgi_temp_file_write_size directive.",1,5,workload-specific,nginx
5877,fastcgi_next_upstream_timeout,Limits the time during which a request can be passed to the next server. The 0 value turns off this limitation.,0,0,others,nginx
5878,fastcgi_next_upstream_tries,Limits the number of possible tries for passing a request to the next server. The 0 value turns off this limitation.,0,0,others,nginx
5880,fastcgi_param,"Sets a parameter that should be passed to the FastCGI server. The value can contain text, variables, and their combination. These directives are inherited from the previous configuration level if and only if there are no fastcgi_param directives defined on the current level.",0,0,others,nginx
5881,fastcgi_pass,"Sets the address of a FastCGI server. The address can be specified as a domain name or IP address, and a port:",0,0,others,nginx
5882,fastcgi_pass_header,Permits passing otherwise disabled header fields from a FastCGI server to a client.,0,0,others,nginx
5883,fastcgi_pass_request_body,Indicates whether the original request body is passed to the FastCGI server. See also the fastcgi_pass_request_headers directive.,0,0,others,nginx
5884,fastcgi_pass_request_headers,Indicates whether the header fields of the original request are passed to the FastCGI server. See also the fastcgi_pass_request_body directive.,0,0,others,nginx
5885,fastcgi_read_timeout,"Defines a timeout for reading a response from the FastCGI server. The timeout is set only between two successive read operations, not for the transmission of the whole response. If the FastCGI server does not transmit anything within this time, the connection is closed.",0,0,others,nginx
5886,fastcgi_request_buffering,Enables or disables buffering of a client request body.,1,6,function-tradeoff,nginx
5887,fastcgi_send_lowat,"If the directive is set to a non-zero value, nginx will try to minimize the number of send operations on outgoing connections to a FastCGI server by using either NOTE_LOWAT flag of the kqueue method, or the SO_SNDLOWAT socket option, with the specified size.",1,4,limited-side-effect,nginx
5888,fastcgi_send_timeout,"Sets a timeout for transmitting a request to the FastCGI server. The timeout is set only between two successive write operations, not for the transmission of the whole request. If the FastCGI server does not receive anything within this time, the connection is closed.",0,0,others,nginx
5890,fastcgi_split_path_info,"Defines a regular expression that captures a value for the $fastcgi_path_info variable. The regular expression should have two captures: the first becomes a value of the $fastcgi_script_name variable, the second becomes a value of the $fastcgi_path_info variable. For example, with these settings",0,0,others,nginx
5891,fastcgi_store,"Enables saving of files to a disk. The on parameter saves files with paths corresponding to the directives alias or root. The off parameter disables saving of files. In addition, the file name can be set explicitly using the string with variables:",1,6,function-tradeoff,nginx
5894,fastcgi_temp_path,"Defines a directory for storing temporary files with data received from FastCGI servers. Up to three-level subdirectory hierarchy can be used underneath the specified directory. For example, in the following configuration",0,0,others,nginx
5895,flv,Turns on module processing in a surrounding location.,1,6,function-tradeoff,nginx
5896,geo.B,"Describes the dependency of values of the specified variable on the client IP address. By default, the address is taken from the $remote_addr variable, but it can also be taken from another variable (0.7.27), for example:",0,0,others,nginx
5897,geo,"Describes the dependency of values of the specified variable on the client IP address. By default, the address is taken from the $remote_addr variable, but it can also be taken from another variable, for example:",0,0,others,nginx
5898,geoip_city,"Specifies a database used to determine the country, region, and city depending on the client IP address. The following variables are available when using this database:",0,0,others,nginx
5899,geoip_country,Specifies a database used to determine the country depending on the client IP address. The following variables are available when using this database:,0,0,others,nginx
5900,geoip_org,Specifies a database used to determine the organization depending on the client IP address. The following variable is available when using this database:,0,0,others,nginx
5901,geoip_proxy,"Defines trusted addresses. When a request comes from a trusted address, an address from the X-Forwarded-For request header field will be used instead.",0,0,others,nginx
5903,google_perftools_profiles,"Sets a file name that keeps profiling information of nginx worker process. The ID of the worker process is always a part of the file name and is appended to the end of the file name, after a dot.",0,0,others,nginx
5904,grpc_bind,"Makes outgoing connections to a gRPC server originate from the specified local IP address with an optional port. Parameter value can contain variables. The special value off cancels the effect of the grpc_bind directive inherited from the previous configuration level, which allows the system to auto-assign the local IP address and port.",0,0,others,nginx
5905,grpc_buffer_size,"Sets the size of the buffer used for reading the response received from the gRPC server. The response is passed to the client synchronously, as soon as it is received.",1,1,resource,nginx
5906,grpc_connect_timeout,Defines a timeout for establishing a connection with a gRPC server. It should be noted that this timeout cannot usually exceed 75 seconds.,0,0,others,nginx
5907,grpc_hide_header,"By default, nginx does not pass the header fields Date, Server, and X-Accel-... from the response of a gRPC server to a client. The grpc_hide_header directive sets additional fields that will not be passed. If, on the contrary, the passing of fields needs to be permitted, the grpc_pass_header directive can be used.",0,0,others,nginx
5908,grpc_ignore_headers,Disables processing of certain response header fields from the gRPC server. The following fields can be ignored: X-Accel-Redirect and X-Accel-Charset.,0,0,others,nginx
5909,grpc_intercept_errors,Determines whether gRPC server responses with codes greater than or equal to 300 should be passed to a client or be intercepted and redirected to nginx for processing with the error_page directive.,0,0,others,nginx
5910,grpc_next_upstream,Specifies in which cases a request should be passed to the next server:,0,0,others,nginx
5911,grpc_next_upstream_timeout,Limits the time during which a request can be passed to the next server. The 0 value turns off this limitation.,0,0,others,nginx
5912,grpc_next_upstream_tries,Limits the number of possible tries for passing a request to the next server. The 0 value turns off this limitation.,0,0,others,nginx
5913,grpc_pass,"Sets the gRPC server address. The address can be specified as a domain name or IP address, and a port:",0,0,others,nginx
5914,grpc_pass_header,Permits passing otherwise disabled header fields from a gRPC server to a client.,0,0,others,nginx
5916,grpc_send_timeout,"Sets a timeout for transmitting a request to the gRPC server. The timeout is set only between two successive write operations, not for the transmission of the whole request. If the gRPC server does not receive anything within this time, the connection is closed.",0,0,others,nginx
5918,grpc_socket_keepalive,"Configures the TCP keepalive behavior for outgoing connections to a gRPC server. By default, the operating systems settings are in effect for the socket. If the directive is set to the value on, the SO_KEEPALIVE socket option is turned on for the socket.",0,0,others,nginx
5920,grpc_ssl_certificate_key,Specifies a file with the secret key in the PEM format used for authentication to a gRPC SSL server.,0,0,others,nginx
5921,grpc_ssl_ciphers,Specifies the enabled ciphers for requests to a gRPC SSL server. The ciphers are specified in the format understood by the OpenSSL library.,0,0,others,nginx
5922,grpc_ssl_conf_command,Sets arbitrary OpenSSL configuration commands when establishing a connection with the gRPC SSL server.,0,0,others,nginx
5923,grpc_ssl_crl,Specifies a file with revoked certificates (CRL) in the PEM format used to verify the certificate of the gRPC SSL server.,0,0,others,nginx
5924,grpc_ssl_name,Allows overriding the server name used to verify the certificate of the gRPC SSL server and to be passed through SNI when establishing a connection with the gRPC SSL server.,0,0,others,nginx
5925,grpc_ssl_password_file,Specifies a file with passphrases for secret keys where each passphrase is specified on a separate line. Passphrases are tried in turn when loading the key.,0,0,others,nginx
5928,grpc_ssl_session_reuse,"Determines whether SSL sessions can be reused when working with the gRPC server. If the errors SSL3_GET_FINISHED:digest check failed appear in the logs, try disabling session reuse.",1,4,limited-side-effect,nginx
5929,grpc_ssl_trusted_certificate,Specifies a file with trusted CA certificates in the PEM format used to verify the certificate of the gRPC SSL server.,0,0,others,nginx
5930,grpc_ssl_verify,Enables or disables verification of the gRPC SSL server certificate.,1,2,security-tradeoff,nginx
5931,grpc_ssl_verify_depth,Sets the verification depth in the gRPC SSL server certificates chain.,1,2,security-tradeoff,nginx
5932,gunzip,"Enables or disables decompression of gzipped responses for clients that lack gzip support. If enabled, the following directives are also taken into account when determining if clients support gzip: gzip_http_version, gzip_proxied, and gzip_disable. See also the gzip_vary directive.",1,6,function-tradeoff,nginx
5933,gunzip_buffers,"Sets the number and size of buffers used to decompress a response. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform.",1,1,resource,nginx
5935,gzip_buffers,"Sets the number and size of buffers used to compress a response. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform.",1,1,resource,nginx
5936,gzip_comp_level,Sets a gzip compression level of a response. Acceptable values are in the range from 1 to 9.,1,6,function-tradeoff,nginx
5937,gzip_disable,Disables gzipping of responses for requests with User-Agent header fields matching any of the specified regular expressions.,0,0,others,nginx
5938,gzip_http_version,Sets the minimum HTTP version of a request required to compress a response.,0,0,others,nginx
5939,gzip_min_length,Sets the minimum length of a response that will be gzipped. The length is determined only from the Content-Length response header field.,0,0,others,nginx
5941,gzip_static,"Enables (on) or disables (off) checking the existence of precompressed files. The following directives are also taken into account: gzip_http_version, gzip_proxied, gzip_disable, and gzip_vary.",0,0,others,nginx
5942,gzip_types,Enables gzipping of responses for the specified MIME types in addition to text/html. The special value * matches any MIME type (0.8.29). Responses with the text/html type are always compressed.,0,0,others,nginx
5943,gzip_vary,"Enables or disables inserting the Vary: Accept-Encoding response header field if the directives gzip, gzip_static, or gunzip are active.",0,0,others,nginx
5944,hash.B,"Specifies a load balancing method for a server group where the client-server mapping is based on the hashed key value. The key can contain text, variables, and their combinations (1.11.2). Usage example:",0,0,others,nginx
5945,hash,"Specifies a load balancing method for a server group where the client-server mapping is based on the hashed key value. The key can contain text, variables, and their combinations. Note that adding or removing a server from the group may result in remapping most of the keys to different servers. The method is compatible with the Cache::Memcached Perl library.",0,0,others,nginx
5946,health_check.B,Enables periodic health checks of the servers in a group referenced in the surrounding location.,0,0,others,nginx
5947,health_check,Enables periodic health checks of the servers in a group.,0,0,others,nginx
5949,hls,Turns on HLS streaming in the surrounding location.,0,0,others,nginx
5950,hls_buffers,Sets the maximum number and size of buffers that are used for reading and writing data frames.,1,1,resource,nginx
5951,hls_forward_args,"Adds arguments from a playlist request to URIs of fragments. This may be useful for performing client authorization at the moment of requesting a fragment, or when protecting an HLS stream with the ngx_http_secure_link_module module.",0,0,others,nginx
5952,hls_fragment,Defines the default fragment length for playlist URIs requested without the len argument.,0,0,others,nginx
5953,hls_mp4_buffer_size,Sets the initial size of the buffer used for processing MP4 and MOV files.,1,1,resource,nginx
5954,hls_mp4_max_buffer_size,"During metadata processing, a larger buffer may become necessary. Its size cannot exceed the specified size, or else nginx will return the server error 500 (Internal Server Error), and log the following message:",1,1,resource,nginx
5955,http,Provides the configuration file context in which the HTTP server directives are specified.,0,0,others,nginx
5957,http2_chunk_size,Sets the maximum size of chunks into which the response body is sliced. A too low value results in higher overhead. A too high value impairs prioritization due to HOL blocking.,1,5,workload-specific,nginx
5959,http2_max_concurrent_pushes,Limits the maximum number of concurrent push requests in a connection.,1,5,workload-specific,nginx
5961,http2_max_field_size,"Limits the maximum size of an HPACK-compressed request header field. The limit applies equally to both name and value. Note that if Huffman encoding is applied, the actual size of decompressed name and value strings may be larger. For most requests, the default limit should be enough.",1,5,workload-specific,nginx
5962,http2_max_header_size,"Limits the maximum size of the entire request header list after HPACK decompression. For most requests, the default limit should be enough.",0,0,others,nginx
5963,http2_max_requests,"Sets the maximum number of requests (including push requests) that can be served through one HTTP/2 connection, after which the next client request will lead to connection closing and the need of establishing a new connection. Closing connections periodically is necessary to free per-connection memory allocations. Therefore, using too high maximum number of requests could result in excessive memory usage and not recommended.",1,1,resource,nginx
5965,http2_push_preload,Enables automatic conversion of preload links specified in the Link response header fields into push requests.,0,0,others,nginx
5969,if_modified_since,Specifies how to compare modification time of a response with the time in the If-Modified-Since request header field:,0,0,others,nginx
5970,ignore_invalid_headers,"Controls whether header fields with invalid names should be ignored. Valid names are composed of English letters, digits, hyphens, and possibly underscores (as controlled by the underscores_in_headers directive).",0,0,others,nginx
5971,image_filter,Sets the type of transformation to perform on images:,1,6,function-tradeoff,nginx
5972,image_filter_buffer,Sets the maximum size of the buffer used for reading images. When the size is exceeded the server returns error 415 (Unsupported Media Type).,1,1,resource,nginx
5973,image_filter_interlace,"If enabled, final images will be interlaced. For JPEG, final images will be in progressive JPEG format.",1,4,limited-side-effect,nginx
5974,image_filter_jpeg_quality,Sets the desired quality of the transformed JPEG images. Acceptable values are in the range from 1 to 100. Lesser values usually imply both lower image quality and less data to transfer. The maximum recommended value is 95. Parameter value can contain variables.,0,0,others,nginx
5975,image_filter_sharpen,Increases sharpness of the final image. The sharpness percentage can exceed 100. The zero value disables sharpening. Parameter value can contain variables.,1,5,workload-specific,nginx
5976,image_filter_transparency,Defines whether transparency should be preserved when transforming GIF images or PNG images with colors specified by a palette. The loss of transparency results in images of a better quality. The alpha channel transparency in PNG is always preserved.,1,6,function-tradeoff,nginx
5977,image_filter_webp_quality,Sets the desired quality of the transformed WebP images. Acceptable values are in the range from 1 to 100. Lesser values usually imply both lower image quality and less data to transfer. Parameter value can contain variables.,1,6,function-tradeoff,nginx
5978,imap_auth,Sets permitted methods of authentication for IMAP clients. Supported methods are:,1,2,security-tradeoff,nginx
5980,imap_client_buffer,"Sets the size of the buffer used for reading IMAP commands. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform.",1,1,resource,nginx
5981,include,"Includes another file, or files matching the specified mask, into configuration. Included files should consist of syntactically correct directives and blocks.",0,0,others,nginx
5982,index,Defines files that will be used as an index. The file name can contain variables. Files are checked in the specified order. The last element of the list can be a file with an absolute path. Example:,0,0,others,nginx
5984,ip_hash,"Specifies that a group should use a load balancing method where requests are distributed between servers based on client IP addresses. The first three octets of the client IPv4 address, or the entire IPv6 address, are used as a hashing key. The method ensures that requests from the same client will always be passed to the same server except when this server is unavailable. In the latter case client requests will be passed to another server. Most probably, it will always be the same server as well.",0,0,others,nginx
5985,js_access,"Sets an njs function which will be called at the access phase. Since 0.4.0, a module function can be referenced.",0,0,others,nginx
5986,js_body_filter,Sets an njs function as a response body filter. The filter function is called for each data chunk of a response body with the following arguments:,0,0,others,nginx
5987,js_content,"Sets an njs function as a location content handler. Since 0.4.0, a module function can be referenced.",0,0,others,nginx
5988,js_filter,"Sets a data filter. Since 0.4.0, a module function can be referenced.",0,0,others,nginx
5989,js_header_filter,Sets an njs function as a response header filter. The directive allows changing arbitrary header fields of a response header.,0,0,others,nginx
5991,js_include.B,Specifies a file that implements location and variable handlers in njs:,0,0,others,nginx
5992,js_include,Specifies a file that implements server and variable handlers in njs:,0,0,others,nginx
5993,js_path,Sets an additional path for njs modules.,0,0,others,nginx
5994,js_preread,"Sets an njs function which will be called at the preread phase. Since 0.4.0, a module function can be referenced.",0,0,others,nginx
5995,js_set,"Sets an njs function for the specified variable. Since 0.4.0, a module function can be referenced.",0,0,others,nginx
5996,js_var.B,"Declares a writable variable. The value can contain text, variables, and their combination.",0,0,others,nginx
5997,js_var,"Declares a writable variable. The value can contain text, variables, and their combination. The variable is not overwritten after a redirect unlike variables created with the set directive.",0,0,others,nginx
5998,keepalive,Activates the cache for connections to upstream servers.,0,0,others,nginx
6000,keepalive_requests.B,"Sets the maximum number of requests that can be served through one keep-alive connection. After the maximum number of requests are made, the connection is closed.",1,5,workload-specific,nginx
6001,keepalive_requests,"Sets the maximum number of requests that can be served through one keepalive connection. After the maximum number of requests is made, the connection is closed.",1,5,workload-specific,nginx
6002,keepalive_time.B,"Limits the maximum time during which requests can be processed through one keepalive connection. After this time is reached, the connection is closed following the subsequent request processing.",0,0,others,nginx
6003,keepalive_time,"Limits the maximum time during which requests can be processed through one keep-alive connection. After this time is reached, the connection is closed following the subsequent request processing.",0,0,others,nginx
6005,keepalive_timeout,The first parameter sets a timeout during which a keep-alive client connection will stay open on the server side. The zero value disables keep-alive client connections. The optional second parameter sets a value in the Keep-Alive: timeout=time response header field. Two parameters may differ.,0,0,others,nginx
6006,keyval,Creates a new $variable whose value is looked up by the key in the key-value database. Matching rules are defined by the type parameter of the keyval_zone directive. The database is stored in a shared memory zone specified by the zone parameter.,0,0,others,nginx
6007,keyval_zone,Sets the name and size of the shared memory zone that keeps the key-value database. Key-value pairs are managed by the API.,1,5,workload-specific,nginx
6010,least_conn,"Specifies that a group should use a load balancing method where a request is passed to the server with the least number of active connections, taking into account weights of servers. If there are several such servers, they are tried in turn using a weighted round-robin balancing method.",0,0,others,nginx
6011,least_time.B,"Specifies that a group should use a load balancing method where a connection is passed to the server with the least average time and least number of active connections, taking into account weights of servers. If there are several such servers, they are tried in turn using a weighted round-robin balancing method.",0,0,others,nginx
6012,least_time,"Specifies that a group should use a load balancing method where a request is passed to the server with the least average response time and least number of active connections, taking into account weights of servers. If there are several such servers, they are tried in turn using a weighted round-robin balancing method.",0,0,others,nginx
6014,limit_conn,"Sets the shared memory zone and the maximum allowed number of connections for a given key value. When this limit is exceeded, the server will return the error in reply to a request. For example, the directives",1,1,resource,nginx
6015,limit_conn_dry_run,"Enables the dry run mode. In this mode, the number of connections is not limited, however, in the shared memory zone, the number of excessive connections is accounted as usual.",1,6,function-tradeoff,nginx
6016,limit_conn_log_level,Sets the desired logging level for cases when the server limits the number of connections.,1,6,function-tradeoff,nginx
6017,limit_conn_status,Sets the status code to return in response to rejected requests.,0,0,others,nginx
6018,limit_conn_zone.B,"Sets parameters for a shared memory zone that will keep states for various keys. In particular, the state includes the current number of connections. The key can contain text, variables, and their combination. Requests with an empty key value are not accounted.",1,1,resource,nginx
6019,limit_conn_zone,"Sets parameters for a shared memory zone that will keep states for various keys. In particular, the state includes the current number of connections. The key can contain text, variables, and their combinations (1.11.2). Connections with an empty key value are not accounted. Usage example:",1,1,resource,nginx
6021,limit_rate,"Limits the rate of response transmission to a client. The rate is specified in bytes per second. The zero value disables rate limiting. The limit is set per a request, and so if a client simultaneously opens two connections, the overall rate will be twice as much as the specified limit.",0,0,others,nginx
6022,limit_rate_after,Sets the initial amount after which the further transmission of a response to a client will be rate limited. Parameter value can contain variables (1.17.0).,0,0,others,nginx
6023,limit_req,"Sets the shared memory zone and the maximum burst size of requests. If the requests rate exceeds the rate configured for a zone, their processing is delayed such that requests are processed at a defined rate. Excessive requests are delayed until their number exceeds the maximum burst size in which case the request is terminated with an error. By default, the maximum burst size is equal to zero. For example, the directives",1,1,resource,nginx
6024,limit_req_dry_run,"Enables the dry run mode. In this mode, requests processing rate is not limited, however, in the shared memory zone, the number of excessive requests is accounted as usual.",1,6,function-tradeoff,nginx
6025,limit_req_log_level,"Sets the desired logging level for cases when the server refuses to process requests due to rate exceeding, or delays request processing. Logging level for delays is one point less than for refusals; for example, if limit_req_log_level notice is specified, delays are logged with the info level.",1,6,function-tradeoff,nginx
6026,limit_req_status,Sets the status code to return in response to rejected requests.,0,0,others,nginx
6027,limit_req_zone,"Sets parameters for a shared memory zone that will keep states for various keys. In particular, the state stores the current number of excessive requests. The key can contain text, variables, and their combination. Requests with an empty key value are not accounted.",1,1,resource,nginx
6028,limit_zone,This directive was made obsolete in version 1.1.8 and was removed in version 1.7.6. An equivalent limit_conn_zone directive with a changed syntax should be used instead:,0,0,others,nginx
6031,lingering_timeout,"When lingering_close is in effect, this directive specifies the maximum waiting time for more client data to arrive. If data are not received during this time, the connection is closed. Otherwise, the data are read and ignored, and nginx starts waiting for more data again. The wait-read-ignore cycle is repeated, but no longer than specified by the lingering_time directive.",0,0,others,nginx
6033,listen.C,"Sets the address and port for the socket on which the server will accept connections. It is possible to specify just the port. The address can also be a hostname, for example:",0,0,others,nginx
6034,listen,"Sets the address and port for the socket on which the server will accept requests. It is possible to specify just the port. The address can also be a hostname, for example:",0,0,others,nginx
6035,load_module,Loads a dynamic module.,0,0,others,nginx
6036,location,Sets configuration depending on a request URI.,0,0,others,nginx
6037,lock_file,"nginx uses the locking mechanism to implement accept_mutex and serialize access to shared memory. On most systems the locks are implemented using atomic operations, and this directive is ignored. On other systems the lock file mechanism is used. This directive specifies a prefix for the names of lock files.",0,0,others,nginx
6038,log_format.B,Specifies log format.,0,0,others,nginx
6039,log_format,"Specifies the log format, for example:",1,6,function-tradeoff,nginx
6040,log_not_found,Enables or disables logging of errors about not found files into error_log.,1,6,function-tradeoff,nginx
6042,mail,Provides the configuration file context in which the mail server directives are specified.,0,0,others,nginx
6043,map,Creates a new variable whose value depends on values of one or more of the source variables specified in the first parameter.,0,0,others,nginx
6044,map_hash_bucket_size,Sets the bucket size for the map variables hash tables. Default value depends on the processors cache line size. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx
6045,map_hash_max_size,Sets the maximum size of the map variables hash tables. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx
6046,master_process,Determines whether worker processes are started. This directive is intended for nginx developers.,0,0,others,nginx
6048,match,Defines the named test set used to verify server responses to health checks.,0,0,others,nginx
6049,max_errors,Sets the number of protocol errors after which the connection is closed.,0,0,others,nginx
6050,max_ranges,"Limits the maximum allowed number of ranges in byte-range requests. Requests that exceed the limit are processed as if there were no byte ranges specified. By default, the number of ranges is not limited. The zero value disables the byte-range support completely.",1,1,resource,nginx
6051,memcached_bind,"Makes outgoing connections to a memcached server originate from the specified local IP address with an optional port (1.11.2). Parameter value can contain variables (1.3.12). The special value off (1.3.12) cancels the effect of the memcached_bind directive inherited from the previous configuration level, which allows the system to auto-assign the local IP address and port.",0,0,others,nginx
6053,memcached_connect_timeout,Defines a timeout for establishing a connection with a memcached server. It should be noted that this timeout cannot usually exceed 75 seconds.,0,0,others,nginx
6054,memcached_force_ranges,Enables byte-range support for both cached and uncached responses from the memcached server regardless of the Accept-Ranges field in these responses.,1,4,limited-side-effect,nginx
6055,memcached_gzip_flag,Enables the test for the flag presence in the memcached server response and sets the Content-Encoding response header field to gzip if the flag is set.,0,0,others,nginx
6057,memcached_next_upstream_timeout,Limits the time during which a request can be passed to the next server. The 0 value turns off this limitation.,0,0,others,nginx
6058,memcached_next_upstream_tries,Limits the number of possible tries for passing a request to the next server. The 0 value turns off this limitation.,0,0,others,nginx
6059,memcached_pass,"Sets the memcached server address. The address can be specified as a domain name or IP address, and a port:",0,0,others,nginx
6060,memcached_read_timeout,"Defines a timeout for reading a response from the memcached server. The timeout is set only between two successive read operations, not for the transmission of the whole response. If the memcached server does not transmit anything within this time, the connection is closed.",0,0,others,nginx
6061,memcached_send_timeout,"Sets a timeout for transmitting a request to the memcached server. The timeout is set only between two successive write operations, not for the transmission of the whole request. If the memcached server does not receive anything within this time, the connection is closed.",0,0,others,nginx
6062,memcached_socket_keepalive,"Configures the TCP keepalive behavior for outgoing connections to a memcached server. By default, the operating systems settings are in effect for the socket. If the directive is set to the value on, the SO_KEEPALIVE socket option is turned on for the socket.",0,0,others,nginx
6063,merge_slashes,Enables or disables compression of two or more adjacent slashes in a URI into a single slash.,1,4,limited-side-effect,nginx
6064,min_delete_depth,"Allows the DELETE method to remove files provided that the number of elements in a request path is not less than the specified number. For example, the directive",1,5,workload-specific,nginx
6067,modern_browser,"Specifies a version starting from which a browser is considered modern. A browser can be any one of the following: msie, gecko (browsers based on Mozilla), opera, safari, or konqueror.",0,0,others,nginx
6068,modern_browser_value,Sets a value for the $modern_browser variables.,0,0,others,nginx
6069,mp4,Turns on module processing in a surrounding location.,0,0,others,nginx
6072,mp4_limit_rate_after,Sets the initial amount of media data (measured in playback time) after which the further transmission of the response to a client will be rate limited.,0,0,others,nginx
6073,mp4_max_buffer_size,"During metadata processing, a larger buffer may become necessary. Its size cannot exceed the specified size, or else nginx will return the 500 (Internal Server Error) server error, and log the following message:",1,1,resource,nginx
6074,msie_padding,Enables or disables adding comments to responses for MSIE clients with status greater than 400 to increase the response size to 512 bytes.,1,6,function-tradeoff,nginx
6075,msie_refresh,Enables or disables issuing refreshes instead of redirects for MSIE clients.,1,4,limited-side-effect,nginx
6076,multi_accept,"If multi_accept is disabled, a worker process will accept one new connection at a time. Otherwise, a worker process will accept all new connections at a time.",1,4,limited-side-effect,nginx
6078,open_file_cache,Configures a cache that can store:,1,6,function-tradeoff,nginx
6079,open_file_cache_errors,Enables or disables caching of file lookup errors by open_file_cache.,1,4,limited-side-effect,nginx
6082,open_log_file_cache,Defines a cache that stores the file descriptors of frequently used logs whose names contain variables. The directive has the following parameters:,1,4,limited-side-effect,nginx
6083,output_buffers,Sets the number and size of the buffers used for reading a response from a disk.,1,1,resource,nginx
6084,override_charset,"Determines whether a conversion should be performed for answers received from a proxied or a FastCGI/uwsgi/SCGI/gRPC server when the answers already carry a charset in the Content-Type response header field. If conversion is enabled, a charset specified in the received response is used as a source charset.",0,0,others,nginx
6086,perl,Sets a Perl handler for the given location.,0,0,others,nginx
6087,perl_modules,Sets an additional path for Perl modules.,0,0,others,nginx
6088,perl_require,Defines the name of a module that will be loaded during each reconfiguration. Several perl_require directives can be present.,0,0,others,nginx
6089,perl_set,Installs a Perl handler for the specified variable.,0,0,others,nginx
6090,pid,Defines a file that will store the process ID of the main process.,0,0,others,nginx
6092,pop3_capabilities,Sets the POP3 protocol extensions list that is passed to the client in response to the CAPA command. The authentication methods specified in the pop3_auth directive (SASL extension) and STLS are automatically added to this list depending on the starttls directive value.,1,2,security-tradeoff,nginx
6094,postpone_output,"If possible, the transmission of client data will be postponed until nginx has at least size bytes of data to send. The zero value disables postponing data transmission.",0,0,others,nginx
6096,preread_timeout,Specifies a timeout of the preread phase.,0,0,others,nginx
6097,protocol,"Sets the protocol for a proxied server. Supported protocols are IMAP, POP3, and SMTP.",0,0,others,nginx
6098,proxy_bind.B,"Makes outgoing connections to a proxied server originate from the specified local IP address with an optional port (1.11.2). Parameter value can contain variables (1.3.12). The special value off (1.3.12) cancels the effect of the proxy_bind directive inherited from the previous configuration level, which allows the system to auto-assign the local IP address and port.",0,0,others,nginx
6099,proxy_bind,"Makes outgoing connections to a proxied server originate from the specified local IP address. Parameter value can contain variables (1.11.2). The special value off cancels the effect of the proxy_bind directive inherited from the previous configuration level, which allows the system to auto-assign the local IP address.",0,0,others,nginx
6100,proxy_buffer,"Sets the size of the buffer used for proxying. By default, the buffer size is equal to one memory page. Depending on a platform, it is either 4K or 8K.",1,1,resource,nginx
6102,proxy_buffer_size,"Sets the size of the buffer used for reading the first part of the response received from the proxied server. This part usually contains a small response header. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform. It can be made smaller, however.",1,1,resource,nginx
6103,proxy_buffering,Enables or disables buffering of responses from the proxied server.,1,6,function-tradeoff,nginx
6104,proxy_buffers,"Sets the number and size of the buffers used for reading a response from the proxied server, for a single connection. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform.",1,1,resource,nginx
6105,proxy_busy_buffers_size,"When buffering of responses from the proxied server is enabled, limits the total size of buffers that can be busy sending a response to the client while the response is not yet fully read. In the meantime, the rest of the buffers can be used for reading the response and, if needed, buffering part of the response to a temporary file. By default, size is limited by the size of two buffers set by the proxy_buffer_size and proxy_buffers directives.",1,1,resource,nginx
6106,proxy_cache,Defines a shared memory zone used for caching. The same zone can be used in several places. Parameter value can contain variables (1.7.9). The off parameter disables caching inherited from the previous configuration level.,1,4,limited-side-effect,nginx
6107,proxy_cache_background_update,"Allows starting a background subrequest to update an expired cache item, while a stale cached response is returned to the client. Note that it is necessary to allow the usage of a stale cached response when it is being updated.",1,6,function-tradeoff,nginx
6108,proxy_cache_bypass,Defines conditions under which the response will not be taken from a cache. If at least one value of the string parameters is not empty and is not equal to 0 then the response will not be taken from the cache:,0,0,others,nginx
6109,proxy_cache_convert_head,"Enables or disables the conversion of the HEAD method to GET for caching. When the conversion is disabled, the cache key should be configured to include the $request_method.",1,5,workload-specific,nginx
6110,proxy_cache_key,"Defines a key for caching, for example",0,0,others,nginx
6111,proxy_cache_lock,"When enabled, only one request at a time will be allowed to populate a new cache element identified according to the proxy_cache_key directive by passing a request to a proxied server. Other requests of the same cache element will either wait for a response to appear in the cache or the cache lock for this element to be released, up to the time set by the proxy_cache_lock_timeout directive.",1,6,function-tradeoff,nginx
6113,proxy_cache_lock_timeout,"Sets a timeout for proxy_cache_lock. When the time expires, the request will be passed to the proxied server, however, the response will not be cached.",0,0,others,nginx
6115,proxy_cache_methods,"If the client request method is listed in this directive then the response will be cached. GET and HEAD methods are always added to the list, though it is recommended to specify them explicitly. See also the proxy_no_cache directive.",0,0,others,nginx
6117,proxy_cache_path,"Sets the path and other parameters of a cache. Cache data are stored in files. The file name in a cache is a result of applying the MD5 function to the cache key. The levels parameter defines hierarchy levels of a cache: from 1 to 3, each level accepts values 1 or 2. For example, in the following configuration",0,0,others,nginx
6118,proxy_cache_purge,Defines conditions under which the request will be considered a cache purge request. If at least one value of the string parameters is not empty and is not equal to 0 then the cache entry with a corresponding cache key is removed. The result of successful operation is indicated by returning the 204 (No Content) response.,0,0,others,nginx
6119,proxy_cache_revalidate,Enables revalidation of expired cache items using conditional requests with the If-Modified-Since and If-None-Match header fields.,1,2,security-tradeoff,nginx
6120,proxy_cache_use_stale,Determines in which cases a stale cached response can be used during communication with the proxied server. The directives parameters match the parameters of the proxy_next_upstream directive.,0,0,others,nginx
6121,proxy_cache_valid,"Sets caching time for different response codes. For example, the following directives",0,0,others,nginx
6123,proxy_connect_timeout,Defines a timeout for establishing a connection with a proxied server. It should be noted that this timeout cannot usually exceed 75 seconds.,0,0,others,nginx
6125,proxy_cookie_flags,"Sets one or more flags for the cookie. The cookie can contain text, variables, and their combinations. The flag can contain text, variables, and their combinations (1.19.8). The secure, httponly, samesite=strict, samesite=lax, samesite=none parameters add the corresponding flags. The nosecure, nohttponly, nosamesite parameters remove the corresponding flags.",0,0,others,nginx
6127,proxy_download_rate,"Limits the speed of reading the data from the proxied server. The rate is specified in bytes per second. The zero value disables rate limiting. The limit is set per a connection, so if nginx simultaneously opens two connections to the proxied server, the overall rate will be twice as much as the specified limit.",0,0,others,nginx
6128,proxy_force_ranges,Enables byte-range support for both cached and uncached responses from the proxied server regardless of the Accept-Ranges field in these responses.,1,4,limited-side-effect,nginx
6129,proxy_headers_hash_bucket_size,Sets the bucket size for hash tables used by the proxy_hide_header and proxy_set_header directives. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx
6131,proxy_hide_header,"By default, nginx does not pass the header fields Date, Server, X-Pad, and X-Accel-... from the response of a proxied server to a client. The proxy_hide_header directive sets additional fields that will not be passed. If, on the contrary, the passing of fields needs to be permitted, the proxy_pass_header directive can be used.",1,2,security-tradeoff,nginx
6132,proxy_http_version,"Sets the HTTP protocol version for proxying. By default, version 1.0 is used. Version 1.1 is recommended for use with keepalive connections and NTLM authentication.",0,0,others,nginx
6133,proxy_ignore_client_abort,Determines whether the connection with a proxied server should be closed when a client closes the connection without waiting for a response.,0,0,others,nginx
6134,proxy_ignore_headers,"Disables processing of certain response header fields from the proxied server. The following fields can be ignored: X-Accel-Redirect, X-Accel-Expires, X-Accel-Limit-Rate (1.1.6), X-Accel-Buffering (1.1.6), X-Accel-Charset (1.1.6), Expires, Cache-Control, Set-Cookie (0.8.44), and Vary (1.7.7).",0,0,others,nginx
6135,proxy_intercept_errors,Determines whether proxied responses with codes greater than or equal to 300 should be passed to a client or be intercepted and redirected to nginx for processing with the error_page directive.,1,6,function-tradeoff,nginx
6136,proxy_limit_rate,"Limits the speed of reading the response from the proxied server. The rate is specified in bytes per second. The zero value disables rate limiting. The limit is set per a request, and so if nginx simultaneously opens two connections to the proxied server, the overall rate will be twice as much as the specified limit. The limitation works only if buffering of responses from the proxied server is enabled.",0,0,others,nginx
6139,proxy_next_upstream.B,Specifies in which cases a request should be passed to the next server:,0,0,others,nginx
6140,proxy_next_upstream,"When a connection to the proxied server cannot be established, determines whether a client connection will be passed to the next server.",0,0,others,nginx
6142,proxy_next_upstream_timeout,Limits the time during which a request can be passed to the next server. The 0 value turns off this limitation.,0,0,others,nginx
6143,proxy_next_upstream_tries.B,Limits the number of possible tries for passing a connection to the next server. The 0 value turns off this limitation.,0,0,others,nginx
6144,proxy_next_upstream_tries,Limits the number of possible tries for passing a request to the next server. The 0 value turns off this limitation.,0,0,others,nginx
6145,proxy_no_cache,Defines conditions under which the response will not be saved to a cache. If at least one value of the string parameters is not empty and is not equal to 0 then the response will not be saved:,0,0,others,nginx
6146,proxy_pass.B,"Sets the address of a proxied server. The address can be specified as a domain name or IP address, and a port:",0,0,others,nginx
6147,proxy_pass,"Sets the protocol and address of a proxied server and an optional URI to which a location should be mapped. As a protocol, http or https can be specified. The address can be specified as a domain name or IP address, and an optional port:",0,0,others,nginx
6148,proxy_pass_error_message,Indicates whether to pass the error message obtained during the authentication on the backend to the client.,1,6,function-tradeoff,nginx
6149,proxy_pass_header,Permits passing otherwise disabled header fields from a proxied server to a client.,0,0,others,nginx
6150,proxy_pass_request_body,Indicates whether the original request body is passed to the proxied server.,0,0,others,nginx
6151,proxy_pass_request_headers,Indicates whether the header fields of the original request are passed to the proxied server.,0,0,others,nginx
6152,proxy_protocol.B,Enables the PROXY protocol for connections to a backend.,1,6,function-tradeoff,nginx
6153,proxy_protocol,Enables the PROXY protocol for connections to a proxied server.,0,0,others,nginx
6155,proxy_read_timeout,"Defines a timeout for reading a response from the proxied server. The timeout is set only between two successive read operations, not for the transmission of the whole response. If the proxied server does not transmit anything within this time, the connection is closed.",0,0,others,nginx
6156,proxy_redirect,Sets the text that should be changed in the Location and Refresh header fields of a proxied server response. Suppose a proxied server returned the header field Location: http://localhost:8000/two/some/uri/. The directive,0,0,others,nginx
6158,proxy_requests,"Sets the number of client datagrams at which binding between a client and existing UDP stream session is dropped. After receiving the specified number of datagrams, next datagram from the same client starts a new session. The session terminates when all client datagrams are transmitted to a proxied server and the expected number of responses is received, or when it reaches a timeout.",1,5,workload-specific,nginx
6159,proxy_responses,"Sets the number of datagrams expected from the proxied server in response to a client datagram if the UDP protocol is used. The number serves as a hint for session termination. By default, the number of datagrams is not limited.",1,5,workload-specific,nginx
6160,proxy_send_lowat,"If the directive is set to a non-zero value, nginx will try to minimize the number of send operations on outgoing connections to a proxied server by using either NOTE_LOWAT flag of the kqueue method, or the SO_SNDLOWAT socket option, with the specified size.",1,5,workload-specific,nginx
6162,proxy_session_drop,Enables terminating all sessions to a proxied server after it was removed from the group or marked as permanently unavailable. This can occur because of re-resolve or with the API DELETE command. A server can be marked as permanently unavailable if it is considered unhealthy or with the API PATCH command. Each session is terminated when the next read or write event is processed for the client or proxied server.,0,0,others,nginx
6164,proxy_set_header,"Allows redefining or appending fields to the request header passed to the proxied server. The value can contain text, variables, and their combinations. These directives are inherited from the previous configuration level if and only if there are no proxy_set_header directives defined on the current level. By default, only two fields are redefined:",0,0,others,nginx
6165,proxy_smtp_auth,Enables or disables user authentication on the SMTP backend using the AUTH command.,1,2,security-tradeoff,nginx
6166,proxy_socket_keepalive,"Configures the TCP keepalive behavior for outgoing connections to a proxied server. By default, the operating systems settings are in effect for the socket. If the directive is set to the value on, the SO_KEEPALIVE socket option is turned on for the socket.",0,0,others,nginx
6169,proxy_ssl_certificate,Specifies a file with the certificate in the PEM format used for authentication to a proxied server.,0,0,others,nginx
6170,proxy_ssl_certificate_key.B,Specifies a file with the secret key in the PEM format used for authentication to a proxied HTTPS server.,0,0,others,nginx
6171,proxy_ssl_certificate_key,Specifies a file with the secret key in the PEM format used for authentication to a proxied server.,0,0,others,nginx
6173,proxy_ssl_ciphers,Specifies the enabled ciphers for requests to a proxied HTTPS server. The ciphers are specified in the format understood by the OpenSSL library.,0,0,others,nginx
6174,proxy_ssl_conf_command.B,Sets arbitrary OpenSSL configuration commands when establishing a connection with the proxied HTTPS server.,0,0,others,nginx
6175,proxy_ssl_conf_command,Sets arbitrary OpenSSL configuration commands when establishing a connection with the proxied server.,0,0,others,nginx
6176,proxy_ssl_crl.B,Specifies a file with revoked certificates (CRL) in the PEM format used to verify the certificate of the proxied HTTPS server.,0,0,others,nginx
6179,proxy_ssl_name,Allows overriding the server name used to verify the certificate of the proxied server and to be passed through SNI when establishing a connection with the proxied server. The server name can also be specified using variables (1.11.3).,0,0,others,nginx
6180,proxy_ssl_password_file,Specifies a file with passphrases for secret keys where each passphrase is specified on a separate line. Passphrases are tried in turn when loading the key.,0,0,others,nginx
6181,proxy_ssl_protocols.B,Enables the specified protocols for connections to a proxied server.,0,0,others,nginx
6182,proxy_ssl_protocols,Enables the specified protocols for requests to a proxied HTTPS server.,1,2,security-tradeoff,nginx
6183,proxy_ssl_server_name.B,"Enables or disables passing of the server name through TLS Server Name Indication extension (SNI, RFC 6066) when establishing a connection with the proxied HTTPS server.",0,0,others,nginx
6184,proxy_ssl_server_name,"Enables or disables passing of the server name through TLS Server Name Indication extension (SNI, RFC 6066) when establishing a connection with the proxied server.",0,0,others,nginx
6185,proxy_ssl_session_reuse,"Determines whether SSL sessions can be reused when working with the proxied server. If the errors SSL3_GET_FINISHED:digest check failed appear in the logs, try disabling session reuse.",1,4,limited-side-effect,nginx
6186,proxy_ssl_trusted_certificate.B,Specifies a file with trusted CA certificates in the PEM format used to verify the certificate of the proxied HTTPS server.,0,0,others,nginx
6187,proxy_ssl_trusted_certificate,Specifies a file with trusted CA certificates in the PEM format used to verify the certificate of the proxied server.,0,0,others,nginx
6188,proxy_ssl_verify.B,Enables or disables verification of the proxied HTTPS server certificate.,1,2,security-tradeoff,nginx
6189,proxy_ssl_verify,Enables or disables verification of the proxied server certificate.,1,2,security-tradeoff,nginx
6190,proxy_ssl_verify_depth.B,Sets the verification depth in the proxied HTTPS server certificates chain.,1,2,security-tradeoff,nginx
6191,proxy_ssl_verify_depth,Sets the verification depth in the proxied server certificates chain.,1,2,security-tradeoff,nginx
6192,proxy_store,"Enables saving of files to a disk. The on parameter saves files with paths corresponding to the directives alias or root. The off parameter disables saving of files. In addition, the file name can be set explicitly using the string with variables:",0,0,others,nginx
6193,proxy_store_access,"Sets access permissions for newly created files and directories, e.g.:",0,0,others,nginx
6195,proxy_temp_path,"Defines a directory for storing temporary files with data received from proxied servers. Up to three-level subdirectory hierarchy can be used underneath the specified directory. For example, in the following configuration",0,0,others,nginx
6197,proxy_upload_rate,"Limits the speed of reading the data from the client. The rate is specified in bytes per second. The zero value disables rate limiting. The limit is set per a connection, so if the client simultaneously opens two connections, the overall rate will be twice as much as the specified limit.",0,0,others,nginx
6198,queue,"If an upstream server cannot be selected immediately while processing a request, the request will be placed into the queue. The directive specifies the maximum number of requests that can be in the queue at the same time. If the queue is filled up, or the server to pass the request to cannot be selected within the time period specified in the timeout parameter, the 502 (Bad Gateway) error will be returned to the client.",0,0,others,nginx
6199,random.B,"Specifies that a group should use a load balancing method where a connection is passed to a randomly selected server, taking into account weights of servers.",0,0,others,nginx
6200,random,"Specifies that a group should use a load balancing method where a request is passed to a randomly selected server, taking into account weights of servers.",0,0,others,nginx
6201,random_index,Enables or disables module processing in a surrounding location.,0,0,others,nginx
6202,read_ahead,Sets the amount of pre-reading for the kernel when working with file.,1,5,workload-specific,nginx
6204,real_ip_recursive,"If recursive search is disabled, the original client address that matches one of the trusted addresses is replaced by the last address sent in the request header field defined by the real_ip_header directive. If recursive search is enabled, the original client address that matches one of the trusted addresses is replaced by the last non-trusted address sent in the request header field.",0,0,others,nginx
6205,recursive_error_pages,Enables or disables doing several redirects using the error_page directive. The number of such redirects is limited.,1,4,limited-side-effect,nginx
6206,referer_hash_bucket_size,Sets the bucket size for the valid referers hash tables. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx
6207,referer_hash_max_size,Sets the maximum size of the valid referers hash tables. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx
6208,request_pool_size,Allows accurate tuning of per-request memory allocations. This directive has minimal impact on performance and should not generally be used.,1,5,workload-specific,nginx
6209,reset_timedout_connection,"Enables or disables resetting timed out connections and connections closed with the non-standard code 444 (1.15.2). The reset is performed as follows. Before closing a socket, the SO_LINGER option is set on it with a timeout value of 0. When the socket is closed, TCP RST is sent to the client, and all memory occupied by this socket is released. This helps avoid keeping an already closed socket with filled buffers in a FIN_WAIT1 state for a long time.",0,0,others,nginx
6210,resolver,"Configures name servers used to find the clients hostname to pass it to the authentication server, and in the XCLIENT command when proxying SMTP. For example:",0,0,others,nginx
6211,resolver_timeout.B,"Sets a timeout for DNS operations, for example:",0,0,others,nginx
6213,return.B,"Specifies a value to send to the client. The value can contain text, variables, and their combination.",0,0,others,nginx
6214,return,Stops processing and returns the specified code to a client. The non-standard code 444 closes a connection without sending a response header.,0,0,others,nginx
6215,rewrite,"If the specified regular expression matches a request URI, URI is changed as specified in the replacement string. The rewrite directives are executed sequentially in order of their appearance in the configuration file. It is possible to terminate further processing of the directives using flags. If a replacement string starts with http://, https://, or $scheme, the processing stops and the redirect is returned to a client.",1,2,security-tradeoff,nginx
6216,rewrite_log,Enables or disables logging of ngx_http_rewrite_module module directives processing results into the error_log at the notice level.,1,6,function-tradeoff,nginx
6217,root,"Sets the root directory for requests. For example, with the following configuration",0,0,others,nginx
6219,scgi_bind,"Makes outgoing connections to an SCGI server originate from the specified local IP address with an optional port (1.11.2). Parameter value can contain variables (1.3.12). The special value off (1.3.12) cancels the effect of the scgi_bind directive inherited from the previous configuration level, which allows the system to auto-assign the local IP address and port.",0,0,others,nginx
6220,scgi_buffer_size,"Sets the size of the buffer used for reading the first part of the response received from the SCGI server. This part usually contains a small response header. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform. It can be made smaller, however.",1,1,resource,nginx
6221,scgi_buffering,Enables or disables buffering of responses from the SCGI server.,1,6,function-tradeoff,nginx
6222,scgi_buffers,"Sets the number and size of the buffers used for reading a response from the SCGI server, for a single connection. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform.",1,1,resource,nginx
6223,scgi_busy_buffers_size,"When buffering of responses from the SCGI server is enabled, limits the total size of buffers that can be busy sending a response to the client while the response is not yet fully read. In the meantime, the rest of the buffers can be used for reading the response and, if needed, buffering part of the response to a temporary file. By default, size is limited by the size of two buffers set by the scgi_buffer_size and scgi_buffers directives.",1,1,resource,nginx
6224,scgi_cache,Defines a shared memory zone used for caching. The same zone can be used in several places. Parameter value can contain variables (1.7.9). The off parameter disables caching inherited from the previous configuration level.,1,4,limited-side-effect,nginx
6225,scgi_cache_background_update,"Allows starting a background subrequest to update an expired cache item, while a stale cached response is returned to the client. Note that it is necessary to allow the usage of a stale cached response when it is being updated.",1,6,function-tradeoff,nginx
6226,scgi_cache_bypass,Defines conditions under which the response will not be taken from a cache. If at least one value of the string parameters is not empty and is not equal to 0 then the response will not be taken from the cache:,0,0,others,nginx
6228,scgi_cache_lock,"When enabled, only one request at a time will be allowed to populate a new cache element identified according to the scgi_cache_key directive by passing a request to an SCGI server. Other requests of the same cache element will either wait for a response to appear in the cache or the cache lock for this element to be released, up to the time set by the scgi_cache_lock_timeout directive.",0,0,others,nginx
6231,scgi_cache_max_range_offset,"Sets an offset in bytes for byte-range requests. If the range is beyond the offset, the range request will be passed to the SCGI server and the response will not be cached.",1,5,workload-specific,nginx
6232,scgi_cache_methods,"If the client request method is listed in this directive then the response will be cached. GET and HEAD methods are always added to the list, though it is recommended to specify them explicitly. See also the scgi_no_cache directive.",0,0,others,nginx
6233,scgi_cache_min_uses,Sets the number of requests after which the response will be cached.,1,5,workload-specific,nginx
6235,scgi_cache_purge,Defines conditions under which the request will be considered a cache purge request. If at least one value of the string parameters is not empty and is not equal to 0 then the cache entry with a corresponding cache key is removed. The result of successful operation is indicated by returning the 204 (No Content) response.,0,0,others,nginx
6236,scgi_cache_revalidate,Enables revalidation of expired cache items using conditional requests with the If-Modified-Since and If-None-Match header fields.,1,2,security-tradeoff,nginx
6238,scgi_cache_valid,"Sets caching time for different response codes. For example, the following directives",0,0,others,nginx
6240,scgi_force_ranges,Enables byte-range support for both cached and uncached responses from the SCGI server regardless of the Accept-Ranges field in these responses.,0,0,others,nginx
6241,scgi_hide_header,"By default, nginx does not pass the header fields Status and X-Accel-... from the response of an SCGI server to a client. The scgi_hide_header directive sets additional fields that will not be passed. If, on the contrary, the passing of fields needs to be permitted, the scgi_pass_header directive can be used.",0,0,others,nginx
6242,scgi_ignore_client_abort,Determines whether the connection with an SCGI server should be closed when a client closes the connection without waiting for a response.,0,0,others,nginx
6243,scgi_ignore_headers,"Disables processing of certain response header fields from the SCGI server. The following fields can be ignored: X-Accel-Redirect, X-Accel-Expires, X-Accel-Limit-Rate (1.1.6), X-Accel-Buffering (1.1.6), X-Accel-Charset (1.1.6), Expires, Cache-Control, Set-Cookie (0.8.44), and Vary (1.7.7).",0,0,others,nginx
6245,scgi_limit_rate,"Limits the speed of reading the response from the SCGI server. The rate is specified in bytes per second. The zero value disables rate limiting. The limit is set per a request, and so if nginx simultaneously opens two connections to the SCGI server, the overall rate will be twice as much as the specified limit. The limitation works only if buffering of responses from the SCGI server is enabled.",0,0,others,nginx
6246,scgi_max_temp_file_size,"When buffering of responses from the SCGI server is enabled, and the whole response does not fit into the buffers set by the scgi_buffer_size and scgi_buffers directives, a part of the response can be saved to a temporary file. This directive sets the maximum size of the temporary file. The size of data written to the temporary file at a time is set by the scgi_temp_file_write_size directive.",1,5,workload-specific,nginx
6248,scgi_next_upstream_timeout,Limits the time during which a request can be passed to the next server. The 0 value turns off this limitation.,0,0,others,nginx
6249,scgi_next_upstream_tries,Limits the number of possible tries for passing a request to the next server. The 0 value turns off this limitation.,0,0,others,nginx
6250,scgi_no_cache,Defines conditions under which the response will not be saved to a cache. If at least one value of the string parameters is not empty and is not equal to 0 then the response will not be saved:,0,0,others,nginx
6251,scgi_param,"Sets a parameter that should be passed to the SCGI server. The value can contain text, variables, and their combination. These directives are inherited from the previous configuration level if and only if there are no scgi_param directives defined on the current level.",0,0,others,nginx
6252,scgi_pass,"Sets the address of an SCGI server. The address can be specified as a domain name or IP address, and a port:",0,0,others,nginx
6254,scgi_pass_request_body,Indicates whether the original request body is passed to the SCGI server. See also the scgi_pass_request_headers directive.,0,0,others,nginx
6255,scgi_pass_request_headers,Indicates whether the header fields of the original request are passed to the SCGI server. See also the scgi_pass_request_body directive.,0,0,others,nginx
6256,scgi_read_timeout,"Defines a timeout for reading a response from the SCGI server. The timeout is set only between two successive read operations, not for the transmission of the whole response. If the SCGI server does not transmit anything within this time, the connection is closed.",0,0,others,nginx
6257,scgi_request_buffering,Enables or disables buffering of a client request body.,1,6,function-tradeoff,nginx
6258,scgi_send_timeout,"Sets a timeout for transmitting a request to the SCGI server. The timeout is set only between two successive write operations, not for the transmission of the whole request. If the SCGI server does not receive anything within this time, the connection is closed.",0,0,others,nginx
6259,scgi_socket_keepalive,"Configures the TCP keepalive behavior for outgoing connections to an SCGI server. By default, the operating systems settings are in effect for the socket. If the directive is set to the value on, the SO_KEEPALIVE socket option is turned on for the socket.",0,0,others,nginx
6261,scgi_store_access,"Sets access permissions for newly created files and directories, e.g.:",0,0,others,nginx
6262,scgi_temp_file_write_size,"Limits the size of data written to a temporary file at a time, when buffering of responses from the SCGI server to temporary files is enabled. By default, size is limited by two buffers set by the scgi_buffer_size and scgi_buffers directives. The maximum size of a temporary file is set by the scgi_max_temp_file_size directive.",1,5,workload-specific,nginx
6263,scgi_temp_path,"Defines a directory for storing temporary files with data received from SCGI servers. Up to three-level subdirectory hierarchy can be used underneath the specified directory. For example, in the following configuration",0,0,others,nginx
6264,secure_link,Defines a string with variables from which the checksum value and lifetime of a link will be extracted.,0,0,others,nginx
6265,secure_link_md5,Defines an expression for which the MD5 hash value will be computed and compared with the value passed in a request.,0,0,others,nginx
6266,secure_link_secret,Defines a secret word used to check authenticity of requested links.,0,0,others,nginx
6267,send_lowat,"If the directive is set to a non-zero value, nginx will try to minimize the number of send operations on client sockets by using either NOTE_LOWAT flag of the kqueue method or the SO_SNDLOWAT socket option. In both cases the specified size is used.",1,5,workload-specific,nginx
6268,send_timeout,"Sets a timeout for transmitting a response to the client. The timeout is set only between two successive write operations, not for the transmission of the whole response. If the client does not receive anything within this time, the connection is closed.",0,0,others,nginx
6269,sendfile,Enables or disables the use of sendfile().,0,0,others,nginx
6270,sendfile_max_chunk,"When set to a non-zero value, limits the amount of data that can be transferred in a single sendfile() call. Without the limit, one fast connection may seize the worker process entirely.",1,4,limited-side-effect,nginx
6271,server.B,"Defines the address and other parameters of a server. The address can be specified as a domain name or IP address with an obligatory port, or as a UNIX-domain socket path specified after the unix: prefix. A domain name that resolves to several IP addresses defines multiple servers at once.",0,0,others,nginx
6272,server.C,"Defines the address and other parameters of a server. The address can be specified as a domain name or IP address, with an optional port, or as a UNIX-domain socket path specified after the unix: prefix. If a port is not specified, the port 80 is used. A domain name that resolves to several IP addresses defines multiple servers at once.",0,0,others,nginx
6274,server,Sets the configuration for a server.,0,0,others,nginx
6275,server_name.B,"Sets names of a virtual server, for example:",0,0,others,nginx
6276,server_name,Sets the server name that is used:,0,0,others,nginx
6279,server_names_hash_max_size,Sets the maximum size of the server names hash tables. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx
6280,server_tokens,Enables or disables emitting nginx version on error pages and in the Server response header field.,1,6,function-tradeoff,nginx
6282,session_log_format,Specifies the output format of a log. The value of the $body_bytes_sent variable is aggregated across all requests in a session. The values of all other variables available for logging correspond to the first request in a session.,0,0,others,nginx
6283,session_log_zone,Sets the path to a log file and configures the shared memory zone that is used to store currently active sessions.,0,0,others,nginx
6284,set,"Sets a value for the specified variable. The value can contain text, variables, and their combination.",0,0,others,nginx
6285,set_real_ip_from.B,"Defines trusted addresses that are known to send correct replacement addresses. If the special value unix: is specified, all UNIX-domain sockets will be trusted.",0,0,others,nginx
6286,set_real_ip_from,"Defines trusted addresses that are known to send correct replacement addresses. If the special value unix: is specified, all UNIX-domain sockets will be trusted. Trusted addresses may also be specified using a hostname (1.13.1).",0,0,others,nginx
6287,slice,Sets the size of the slice. The zero value disables splitting responses into slices. Note that a too low value may result in excessive memory usage and opening a large number of files.,1,5,workload-specific,nginx
6288,smtp_auth,Sets permitted methods of SASL authentication for SMTP clients. Supported methods are:,1,2,security-tradeoff,nginx
6289,smtp_capabilities,Sets the SMTP protocol extensions list that is passed to the client in response to the EHLO command. The authentication methods specified in the smtp_auth directive and STARTTLS are automatically added to this list depending on the starttls directive value.,1,2,security-tradeoff,nginx
6291,smtp_greeting_delay,Allows setting a delay before sending an SMTP greeting in order to reject clients who fail to wait for the greeting before sending SMTP commands.,0,0,others,nginx
6292,source_charset,"Defines the source charset of a response. If this charset is different from the charset specified in the charset directive, a conversion is performed.",0,0,others,nginx
6293,spdy_chunk_size,Sets the maximum size of chunks into which the response body is sliced. A too low value results in higher overhead. A too high value impairs prioritization due to HOL blocking.,1,5,workload-specific,nginx
6294,spdy_headers_comp,"Sets the header compression level of a response in a range from 1 (fastest, less compression) to 9 (slowest, best compression). The special value 0 turns off the header compression.",1,6,function-tradeoff,nginx
6295,split_clients,"Creates a variable for A/B testing, for example:",0,0,others,nginx
6296,ssi,Enables or disables processing of SSI commands in responses.,1,6,function-tradeoff,nginx
6297,ssi_last_modified,Allows preserving the Last-Modified header field from the original response during SSI processing to facilitate response caching.,1,4,limited-side-effect,nginx
6298,ssi_min_file_chunk,"Sets the minimum size for parts of a response stored on disk, starting from which it makes sense to send them using sendfile.",1,5,workload-specific,nginx
6300,ssi_types,Enables processing of SSI commands in responses with the specified MIME types in addition to text/html. The special value * matches any MIME type (0.8.29).,0,0,others,nginx
6301,ssi_value_length,Sets the maximum length of parameter values in SSI commands.,0,0,others,nginx
6302,ssl,This directive was made obsolete in version 1.15.0. The ssl parameter of the listen directive should be used instead.,1,2,security-tradeoff,nginx
6303,ssl_buffer_size,Sets the size of the buffer used for sending data.,1,1,resource,nginx
6304,ssl_certificate.B,"Specifies a file with the certificate in the PEM format for the given server. If intermediate certificates should be specified in addition to a primary certificate, they should be specified in the same file in the following order: the primary certificate comes first, then the intermediate certificates. A secret key in the PEM format may be placed in the same file.",0,0,others,nginx
6305,ssl_certificate,"Specifies a file with the certificate in the PEM format for the given virtual server. If intermediate certificates should be specified in addition to a primary certificate, they should be specified in the same file in the following order: the primary certificate comes first, then the intermediate certificates. A secret key in the PEM format may be placed in the same file.",0,0,others,nginx
6307,ssl_certificate_key,Specifies a file with the secret key in the PEM format for the given virtual server.,0,0,others,nginx
6308,ssl_ciphers,"Specifies the enabled ciphers. The ciphers are specified in the format understood by the OpenSSL library, for example:",0,0,others,nginx
6310,ssl_client_certificate,Specifies a file with trusted CA certificates in the PEM format used to verify client certificates.,0,0,others,nginx
6311,ssl_conf_command,Sets arbitrary OpenSSL configuration commands.,0,0,others,nginx
6312,ssl_crl,Specifies a file with revoked certificates (CRL) in the PEM format used to verify client certificates.,0,0,others,nginx
6313,ssl_dhparam,Specifies a file with DH parameters for DHE ciphers.,0,0,others,nginx
6314,ssl_early_data,Enables or disables TLS 1.3 early data.,1,2,security-tradeoff,nginx
6315,ssl_ecdh_curve,Specifies a curve for ECDHE ciphers.,0,0,others,nginx
6317,ssl_handshake_timeout,Specifies a timeout for the SSL handshake to complete.,0,0,others,nginx
6318,ssl_ocsp,Enables OCSP validation of the client certificate chain. The leaf parameter enables validation of the client certificate only.,1,2,security-tradeoff,nginx
6319,ssl_ocsp_cache,Sets name and size of the cache that stores client certificates status for OCSP validation. The cache is shared between all worker processes. A cache with the same name can be used in several virtual servers.,1,1,resource,nginx
6320,ssl_ocsp_responder,Overrides the URL of the OCSP responder specified in the Authority Information Access certificate extension for validation of client certificates.,0,0,others,nginx
6321,ssl_password_file,Specifies a file with passphrases for secret keys where each passphrase is specified on a separate line. Passphrases are tried in turn when loading the key.,0,0,others,nginx
6322,ssl_prefer_server_ciphers.B,Specifies that server ciphers should be preferred over client ciphers when the SSLv3 and TLS protocols are used.,0,0,others,nginx
6324,ssl_preread,Enables extracting information from the ClientHello message at the preread phase.,1,6,function-tradeoff,nginx
6326,ssl_reject_handshake,"If enabled, SSL handshakes in the server block will be rejected.",0,0,others,nginx
6327,ssl_session_cache.B,Sets the types and sizes of caches that store session parameters.,1,1,resource,nginx
6328,ssl_session_cache,Sets the types and sizes of caches that store session parameters. A cache can be of any of the following types:,0,0,others,nginx
6329,ssl_session_ticket_key,"Sets a file with the secret key used to encrypt and decrypt TLS session tickets. The directive is necessary if the same key has to be shared between multiple servers. By default, a randomly generated key is used.",0,0,others,nginx
6330,ssl_session_tickets,Enables or disables session resumption through TLS session tickets.,0,0,others,nginx
6331,ssl_session_timeout,Specifies a time during which a client may reuse the session parameters.,0,0,others,nginx
6332,ssl_stapling,Enables or disables stapling of OCSP responses by the server.,0,0,others,nginx
6333,ssl_stapling_file,"When set, the stapled OCSP response will be taken from the specified file instead of querying the OCSP responder specified in the server certificate.",0,0,others,nginx
6334,ssl_stapling_responder,Overrides the URL of the OCSP responder specified in the Authority Information Access certificate extension.,0,0,others,nginx
6335,ssl_stapling_verify,Enables or disables verification of OCSP responses by the server.,1,2,security-tradeoff,nginx
6336,ssl_trusted_certificate.B,Specifies a file with trusted CA certificates in the PEM format used to verify client certificates and OCSP responses if ssl_stapling is enabled.,0,0,others,nginx
6337,ssl_trusted_certificate,Specifies a file with trusted CA certificates in the PEM format used to verify client certificates.,0,0,others,nginx
6338,ssl_verify_client.B,Enables verification of client certificates. The verification result is passed in the Auth-SSL-Verify header of the authentication request.,0,0,others,nginx
6340,ssl_verify_client,"Enables verification of client certificates. The verification result is stored in the $ssl_client_verify variable. If an error has occurred during the client certificate verification or a client has not presented the required certificate, the connection is closed.",1,2,security-tradeoff,nginx
6341,ssl_verify_depth,Sets the verification depth in the client certificates chain.,1,2,security-tradeoff,nginx
6342,state,Specifies a file that keeps the state of the dynamically configurable group.,0,0,others,nginx
6343,status,The status information will be accessible from the surrounding location. Access to this location should be limited.,0,0,others,nginx
6345,status_zone.B,Enables collection of virtual http or stream (1.7.11) server status information in the specified zone. Several servers may share the same zone.,0,0,others,nginx
6346,status_zone,Enables collection of virtual http or stream server status information in the specified zone. Several servers may share the same zone.,0,0,others,nginx
6350,stub_status,The basic status information will be accessible from the surrounding location.,0,0,others,nginx
6351,sub_filter,Sets a string to replace and a replacement string. The string to replace is matched ignoring the case. The string to replace (1.9.4) and replacement string can contain variables. Several sub_filter directives can be specified on the same configuration level (1.9.4). These directives are inherited from the previous configuration level if and only if there are no sub_filter directives defined on the current level.,0,0,others,nginx
6352,sub_filter_last_modified,Allows preserving the Last-Modified header field from the original response during replacement to facilitate response caching.,0,0,others,nginx
6353,sub_filter_once,Indicates whether to look for each string to replace once or repeatedly.,0,0,others,nginx
6354,sub_filter_types,Enables string replacement in responses with the specified MIME types in addition to text/html. The special value * matches any MIME type (0.8.29).,0,0,others,nginx
6355,subrequest_output_buffer_size,"Sets the size of the buffer used for storing the response body of a subrequest. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform. It can be made smaller, however.",1,1,resource,nginx
6356,tcp_nodelay.B,Enables or disables the use of the TCP_NODELAY option. The option is enabled for both client and proxied server connections.,0,0,others,nginx
6357,tcp_nodelay,"Enables or disables the use of the TCP_NODELAY option. The option is enabled when a connection is transitioned into the keep-alive state. Additionally, it is enabled on SSL connections, for unbuffered proxying, and for WebSocket proxying.",1,6,function-tradeoff,nginx
6358,tcp_nopush,Enables or disables the use of the TCP_NOPUSH socket option on FreeBSD or the TCP_CORK socket option on Linux. The options are enabled only when sendfile is used. Enabling the option allows,1,6,function-tradeoff,nginx
6359,thread_pool,Defines the name and parameters of a thread pool used for multi-threaded reading and sending of files without blocking worker processes.,0,0,others,nginx
6361,timer_resolution,"Reduces timer resolution in worker processes, thus reducing the number of gettimeofday() system calls made. By default, gettimeofday() is called each time a kernel event is received. With reduced resolution, gettimeofday() is only called once per specified interval.",0,0,others,nginx
6362,try_files,"Checks the existence of files in the specified order and uses the first found file for request processing; the processing is performed in the current context. The path to a file is constructed from the file parameter according to the root and alias directives. It is possible to check directorys existence by specifying a slash at the end of a name, e.g. $uri/. If none of the files were found, an internal redirect to the uri specified in the last parameter is made. For example:",0,0,others,nginx
6363,types,"Maps file name extensions to MIME types of responses. Extensions are case-insensitive. Several extensions can be mapped to one type, for example:",0,0,others,nginx
6364,types_hash_bucket_size,Sets the bucket size for the types hash tables. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx
6365,types_hash_max_size,Sets the maximum size of the types hash tables. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx
6366,underscores_in_headers,"Enables or disables the use of underscores in client request header fields. When the use of underscores is disabled, request header fields whose names contain underscores are marked as invalid and become subject to the ignore_invalid_headers directive.",0,0,others,nginx
6367,uninitialized_variable_warn,Controls whether warnings about uninitialized variables are logged.,1,6,function-tradeoff,nginx
6368,upstream,"Defines a group of servers. Servers can listen on different ports. In addition, servers listening on TCP and UNIX-domain sockets can be mixed.",0,0,others,nginx
6369,upstream_conf,Turns on the HTTP interface of upstream configuration in the surrounding location. Access to this location should be limited.,0,0,others,nginx
6370,use,"Specifies the connection processing method to use. There is normally no need to specify it explicitly, because nginx will by default use the most efficient method.",0,0,others,nginx
6371,user,"Defines user and group credentials used by worker processes. If group is omitted, a group whose name equals that of user is used.",0,0,others,nginx
6373,userid_domain,Defines a domain for which the cookie is set. The none parameter disables setting of a domain for the cookie.,0,0,others,nginx
6374,userid_expires,Sets a time during which a browser should keep the cookie. The parameter max will cause the cookie to expire on 31 Dec 2037 23:55:55 GMT. The parameter off will cause the cookie to expire at the end of a browser session.,0,0,others,nginx
6375,userid_flags,"If the parameter is not off, defines one or more additional flags for the cookie: secure, httponly, samesite=strict, samesite=lax, samesite=none.",0,0,others,nginx
6376,userid_mark,"If the parameter is not off, enables the cookie marking mechanism and sets the character used as a mark. This mechanism is used to add or change userid_p3p and/or a cookie expiration time while preserving the client identifier. A mark can be any letter of the English alphabet (case-sensitive), digit, or the = character.",0,0,others,nginx
6377,userid_name,Sets the cookie name.,1,6,function-tradeoff,nginx
6379,userid_path,Defines a path for which the cookie is set.,0,0,others,nginx
6380,userid_service,"If identifiers are issued by multiple servers (services), each service should be assigned its own number to ensure that client identifiers are unique. For version 1 cookies, the default value is zero. For version 2 cookies, the default value is the number composed from the last four octets of the servers IP address.",0,0,others,nginx
6382,uwsgi_buffer_size,"Sets the size of the buffer used for reading the first part of the response received from the uwsgi server. This part usually contains a small response header. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform. It can be made smaller, however.",1,1,resource,nginx
6383,uwsgi_buffering,Enables or disables buffering of responses from the uwsgi server.,1,6,function-tradeoff,nginx
6385,uwsgi_busy_buffers_size,"When buffering of responses from the uwsgi server is enabled, limits the total size of buffers that can be busy sending a response to the client while the response is not yet fully read. In the meantime, the rest of the buffers can be used for reading the response and, if needed, buffering part of the response to a temporary file. By default, size is limited by the size of two buffers set by the uwsgi_buffer_size and uwsgi_buffers directives.",1,1,resource,nginx
6386,uwsgi_cache,Defines a shared memory zone used for caching. The same zone can be used in several places. Parameter value can contain variables (1.7.9). The off parameter disables caching inherited from the previous configuration level.,1,4,limited-side-effect,nginx
6387,uwsgi_cache_background_update,"Allows starting a background subrequest to update an expired cache item, while a stale cached response is returned to the client. Note that it is necessary to allow the usage of a stale cached response when it is being updated.",1,6,function-tradeoff,nginx
6388,uwsgi_cache_bypass,Defines conditions under which the response will not be taken from a cache. If at least one value of the string parameters is not empty and is not equal to 0 then the response will not be taken from the cache:,0,0,others,nginx
6390,uwsgi_cache_lock,"When enabled, only one request at a time will be allowed to populate a new cache element identified according to the uwsgi_cache_key directive by passing a request to a uwsgi server. Other requests of the same cache element will either wait for a response to appear in the cache or the cache lock for this element to be released, up to the time set by the uwsgi_cache_lock_timeout directive.",0,0,others,nginx
6391,uwsgi_cache_lock_age,"If the last request passed to the uwsgi server for populating a new cache element has not completed for the specified time, one more request may be passed to the uwsgi server.",0,0,others,nginx
6393,uwsgi_cache_max_range_offset,"Sets an offset in bytes for byte-range requests. If the range is beyond the offset, the range request will be passed to the uwsgi server and the response will not be cached.",1,5,workload-specific,nginx
6395,uwsgi_cache_min_uses,Sets the number of requests after which the response will be cached.,1,5,workload-specific,nginx
6396,uwsgi_cache_path,"Sets the path and other parameters of a cache. Cache data are stored in files. The file name in a cache is a result of applying the MD5 function to the cache key. The levels parameter defines hierarchy levels of a cache: from 1 to 3, each level accepts values 1 or 2. For example, in the following configuration",0,0,others,nginx
6397,uwsgi_cache_purge,Defines conditions under which the request will be considered a cache purge request. If at least one value of the string parameters is not empty and is not equal to 0 then the cache entry with a corresponding cache key is removed. The result of successful operation is indicated by returning the 204 (No Content) response.,0,0,others,nginx
6399,uwsgi_cache_use_stale,Determines in which cases a stale cached response can be used when an error occurs during communication with the uwsgi server. The directives parameters match the parameters of the uwsgi_next_upstream directive.,0,0,others,nginx
6400,uwsgi_cache_valid,"Sets caching time for different response codes. For example, the following directives",0,0,others,nginx
6401,uwsgi_connect_timeout,Defines a timeout for establishing a connection with a uwsgi server. It should be noted that this timeout cannot usually exceed 75 seconds.,0,0,others,nginx
6402,uwsgi_force_ranges,Enables byte-range support for both cached and uncached responses from the uwsgi server regardless of the Accept-Ranges field in these responses.,0,0,others,nginx
6404,uwsgi_ignore_client_abort,Determines whether the connection with a uwsgi server should be closed when a client closes the connection without waiting for a response.,0,0,others,nginx
6405,uwsgi_ignore_headers,"Disables processing of certain response header fields from the uwsgi server. The following fields can be ignored: X-Accel-Redirect, X-Accel-Expires, X-Accel-Limit-Rate (1.1.6), X-Accel-Buffering (1.1.6), X-Accel-Charset (1.1.6), Expires, Cache-Control, Set-Cookie (0.8.44), and Vary (1.7.7).",0,0,others,nginx
6406,uwsgi_intercept_errors,Determines whether a uwsgi server responses with codes greater than or equal to 300 should be passed to a client or be intercepted and redirected to nginx for processing with the error_page directive.,0,0,others,nginx
6407,uwsgi_limit_rate,"Limits the speed of reading the response from the uwsgi server. The rate is specified in bytes per second. The zero value disables rate limiting. The limit is set per a request, and so if nginx simultaneously opens two connections to the uwsgi server, the overall rate will be twice as much as the specified limit. The limitation works only if buffering of responses from the uwsgi server is enabled.",0,0,others,nginx
6408,uwsgi_max_temp_file_size,"When buffering of responses from the uwsgi server is enabled, and the whole response does not fit into the buffers set by the uwsgi_buffer_size and uwsgi_buffers directives, a part of the response can be saved to a temporary file. This directive sets the maximum size of the temporary file. The size of data written to the temporary file at a time is set by the uwsgi_temp_file_write_size directive.",1,5,workload-specific,nginx
6409,uwsgi_modifier1,Sets the value of the modifier1 field in the uwsgi packet header.,0,0,others,nginx
6410,uwsgi_modifier2,Sets the value of the modifier2 field in the uwsgi packet header.,0,0,others,nginx
6411,uwsgi_next_upstream,Specifies in which cases a request should be passed to the next server:,0,0,others,nginx
6412,uwsgi_next_upstream_timeout,Limits the time during which a request can be passed to the next server. The 0 value turns off this limitation.,0,0,others,nginx
6413,uwsgi_next_upstream_tries,Limits the number of possible tries for passing a request to the next server. The 0 value turns off this limitation.,0,0,others,nginx
6414,uwsgi_no_cache,Defines conditions under which the response will not be saved to a cache. If at least one value of the string parameters is not empty and is not equal to 0 then the response will not be saved:,0,0,others,nginx
6416,uwsgi_pass,"Sets the protocol and address of a uwsgi server. As a protocol, uwsgi or suwsgi (secured uwsgi, uwsgi over SSL) can be specified. The address can be specified as a domain name or IP address, and a port:",0,0,others,nginx
6417,uwsgi_pass_header,Permits passing otherwise disabled header fields from a uwsgi server to a client.,0,0,others,nginx
6418,uwsgi_pass_request_body,Indicates whether the original request body is passed to the uwsgi server. See also the uwsgi_pass_request_headers directive.,0,0,others,nginx
6419,uwsgi_pass_request_headers,Indicates whether the header fields of the original request are passed to the uwsgi server. See also the uwsgi_pass_request_body directive.,0,0,others,nginx
6420,uwsgi_read_timeout,"Defines a timeout for reading a response from the uwsgi server. The timeout is set only between two successive read operations, not for the transmission of the whole response. If the uwsgi server does not transmit anything within this time, the connection is closed.",0,0,others,nginx
6421,uwsgi_request_buffering,Enables or disables buffering of a client request body.,1,6,function-tradeoff,nginx
6422,uwsgi_send_timeout,"Sets a timeout for transmitting a request to the uwsgi server. The timeout is set only between two successive write operations, not for the transmission of the whole request. If the uwsgi server does not receive anything within this time, the connection is closed.",0,0,others,nginx
6423,uwsgi_socket_keepalive,"Configures the TCP keepalive behavior for outgoing connections to a uwsgi server. By default, the operating systems settings are in effect for the socket. If the directive is set to the value on, the SO_KEEPALIVE socket option is turned on for the socket.",0,0,others,nginx
6424,uwsgi_ssl_certificate,Specifies a file with the certificate in the PEM format used for authentication to a secured uwsgi server.,0,0,others,nginx
6427,uwsgi_ssl_conf_command,Sets arbitrary OpenSSL configuration commands when establishing a connection with the secured uwsgi server.,0,0,others,nginx
6428,uwsgi_ssl_crl,Specifies a file with revoked certificates (CRL) in the PEM format used to verify the certificate of the secured uwsgi server.,0,0,others,nginx
6430,uwsgi_ssl_password_file,Specifies a file with passphrases for secret keys where each passphrase is specified on a separate line. Passphrases are tried in turn when loading the key.,0,0,others,nginx
6431,uwsgi_ssl_protocols,Enables the specified protocols for requests to a secured uwsgi server.,1,6,function-tradeoff,nginx
6432,uwsgi_ssl_server_name,"Enables or disables passing of the server name through TLS Server Name Indication extension (SNI, RFC 6066) when establishing a connection with the secured uwsgi server.",0,0,others,nginx
6433,uwsgi_ssl_session_reuse,"Determines whether SSL sessions can be reused when working with a secured uwsgi server. If the errors SSL3_GET_FINISHED:digest check failed appear in the logs, try disabling session reuse.",1,6,function-tradeoff,nginx
6435,uwsgi_ssl_verify,Enables or disables verification of the secured uwsgi server certificate.,1,2,security-tradeoff,nginx
6436,uwsgi_ssl_verify_depth,Sets the verification depth in the secured uwsgi server certificates chain.,1,2,security-tradeoff,nginx
6438,uwsgi_store_access,"Sets access permissions for newly created files and directories, e.g.:",0,0,others,nginx
6439,uwsgi_temp_file_write_size,"Limits the size of data written to a temporary file at a time, when buffering of responses from the uwsgi server to temporary files is enabled. By default, size is limited by two buffers set by the uwsgi_buffer_size and uwsgi_buffers directives. The maximum size of a temporary file is set by the uwsgi_max_temp_file_size directive.",1,5,workload-specific,nginx
6440,uwsgi_temp_path,"Defines a directory for storing temporary files with data received from uwsgi servers. Up to three-level subdirectory hierarchy can be used underneath the specified directory. For example, in the following configuration",0,0,others,nginx
6441,valid_referers,"Specifies the Referer request header field values that will cause the embedded $invalid_referer variable to be set to an empty string. Otherwise, the variable will be set to 1. Search for a match is case-insensitive.",0,0,others,nginx
6442,variables_hash_bucket_size,Sets the bucket size for the variables hash table. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx
6443,variables_hash_max_size,Sets the maximum size of the variables hash table. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx
6445,worker_connections,Sets the maximum number of simultaneous connections that can be opened by a worker process.,1,5,workload-specific,nginx
6446,worker_cpu_affinity,"Binds worker processes to the sets of CPUs. Each CPU set is represented by a bitmask of allowed CPUs. There should be a separate set defined for each of the worker processes. By default, worker processes are not bound to any specific CPUs.",0,0,others,nginx
6447,worker_priority,Defines the scheduling priority for worker processes like it is done by the nice command: a negative number means higher priority. Allowed range normally varies from -20 to 20.,0,0,others,nginx
6448,worker_processes,Defines the number of worker processes.,1,5,workload-specific,nginx
6449,worker_rlimit_core,Changes the limit on the largest size of a core file (RLIMIT_CORE) for worker processes. Used to increase the limit without restarting the main process.,1,5,workload-specific,nginx
6451,worker_shutdown_timeout,"Configures a timeout for a graceful shutdown of worker processes. When the time expires, nginx will try to close all the connections currently open to facilitate shutdown.",0,0,others,nginx
6452,working_directory,"Defines the current working directory for a worker process. It is primarily used when writing a core-file, in which case a worker process should have write permission for the specified directory.",0,0,others,nginx
6454,xml_entities,"Specifies the DTD file that declares character entities. This file is compiled at the configuration stage. For technical reasons, the module is unable to use the external subset declared in the processed XML, so it is ignored and a specially defined file is used instead. This file should not describe the XML structure. It is enough to declare just the required character entities, for example:",0,0,others,nginx
6456,xslt_param,"Defines the parameters for XSLT stylesheets. The value is treated as an XPath expression. The value can contain variables. To pass a string value to a stylesheet, the xslt_string_param directive can be used.",0,0,others,nginx
6457,xslt_string_param,Defines the string parameters for XSLT stylesheets. XPath expressions in the value are not interpreted. The value can contain variables.,0,0,others,nginx
6459,xslt_types,"Enables transformations in responses with the specified MIME types in addition to text/xml. The special value * matches any MIME type (0.8.29). If the transformation result is an HTML response, its MIME type is changed to text/html.",0,0,others,nginx
6460,zone,"Defines the name and size of the shared memory zone that keeps the groups configuration and run-time state that are shared between worker processes. Several groups may share the same zone. In this case, it is enough to specify the size only once.",0,0,others,nginx
6461,zone_sync,Enables the synchronization of shared memory zones between cluster nodes. Cluster nodes are defined using zone_sync_server directives.,1,3,reliability-tradeoff,nginx
6462,zone_sync_buffers,"Sets the number and size of the per-zone buffers used for pushing zone contents. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform.",1,1,resource,nginx
6463,zone_sync_connect_retry_interval,Defines an interval between connection attempts to another cluster node.,0,0,others,nginx
6464,zone_sync_connect_timeout,Defines a timeout for establishing a connection with another cluster node.,0,0,others,nginx
6465,zone_sync_interval,Defines an interval for polling updates in a shared memory zone.,0,0,others,nginx
6466,zone_sync_recv_buffer_size,"Sets size of a per-connection receive buffer used to parse incoming stream of synchronization messages. The buffer size must be equal or greater than one of the zone_sync_buffers. By default, the buffer size is equal to zone_sync_buffers size multiplied by number.",1,1,resource,nginx
6467,zone_sync_server,"Defines the address of a cluster node. The address can be specified as a domain name or IP address with a mandatory port, or as a UNIX-domain socket path specified after the unix: prefix. A domain name that resolves to several IP addresses defines multiple nodes at once.",0,0,others,nginx
6469,zone_sync_ssl_certificate,Specifies a file with the certificate in the PEM format used for authentication to another cluster server.,0,0,others,nginx
6470,zone_sync_ssl_certificate_key,Specifies a file with the secret key in the PEM format used for authentication to another cluster server.,0,0,others,nginx
6471,zone_sync_ssl_ciphers,Specifies the enabled ciphers for connections to another cluster server. The ciphers are specified in the format understood by the OpenSSL library.,0,0,others,nginx
6472,zone_sync_ssl_conf_command,Sets arbitrary OpenSSL configuration commands when establishing a connection with another cluster server.,0,0,others,nginx
6474,zone_sync_ssl_name,Allows overriding the server name used to verify the certificate of a cluster server and to be passed through SNI when establishing a connection with the cluster server.,0,0,others,nginx
6475,zone_sync_ssl_password_file,Specifies a file with passphrases for secret keys where each passphrase is specified on a separate line. Passphrases are tried in turn when loading the key.,0,0,others,nginx
6476,zone_sync_ssl_protocols,Enables the specified protocols for connections to another cluster server.,0,0,others,nginx
6477,zone_sync_ssl_server_name,"Enables or disables passing of the server name through TLS Server Name Indication extension (SNI, RFC 6066) when establishing a connection with another cluster server.",0,0,others,nginx
6478,zone_sync_ssl_trusted_certificate,Specifies a file with trusted CA certificates in the PEM format used to verify the certificate of another cluster server.,0,0,others,nginx
6479,zone_sync_ssl_verify,Enables or disables verification of another cluster server certificate.,1,2,security-tradeoff,nginx
6480,zone_sync_ssl_verify_depth,Sets the verification depth in another cluster server certificates chain.,1,2,security-tradeoff,nginx
6481,zone_sync_timeout,"Sets the timeout between two successive read or write operations on connection to another cluster node. If no data is transmitted within this time, the connection is closed.",0,0,others,nginx
6482,addressing_mode,Indicates the addressing mode used by the driver. Permitted values: 'legacy' - use legacy non-routable addressing 'routable' - use routable addresses 'dynamic' - use legacy addresses if the message bus does not support routing otherwise use routable addressing,0,0,others,nova
6483,agent_enabled,Enable the SPICE guest agent support on the instances.,1,6,function-tradeoff,nova
6484,agent_path,Path to locate guest agent on the server.,0,0,others,nova
6485,agent_resetnetwork_timeout,Number of seconds to wait for agent's reply to resetnetwork request.,0,0,others,nova
6486,agent_timeout,Number of seconds to wait for agent's reply to a request.,0,0,others,nova
6487,agent_version_timeout,Number of seconds to wait for agent't reply to version request.,0,0,others,nova
6488,aggregate_image_properties_isolation_namespace,Image property namespace for use in the host aggregate.,0,0,others,nova
6489,aggregate_image_properties_isolation_separator,Separator character(s) for image property namespace and name.,0,0,others,nova
6491,allow_credentials,Indicate that the actual request can include user credentials,0,0,others,nova
6492,allow_headers,Indicate which header field names may be used during the actual request.,0,0,others,nova
6493,allow_methods,Indicate which methods can be used during the actual request.,0,0,others,nova
6494,allow_resize_to_same_host,Allow destination machine to match source for resize. Useful when testing in single-host environments. By default it is not allowed to resize to the same host. Setting this option to true will add the same host to the destination options. Also set to true if you allow the ServerGroupAffinityFilter and need to resize.,0,0,others,nova
6495,allow_same_net_traffic,Determine whether to allow network traffic from same network.,0,0,others,nova
6496,allowed_direct_url_schemes,List of url schemes that can be directly accessed.,0,0,others,nova
6497,allowed_origin,"Indicate whether this resource may be shared with the domain received in the requests 'origin' header. Format: '<protocol>://<host>[:<port>]', no trailing slash. Example: https://horizon.example.com",0,0,others,nova
6498,allowed_origins,Adds list of allowed origins to the console websocket proxy to allow connections from other origin hostnames. Websocket proxy matches the host header with the origin header to prevent cross-site requests. This list specifies if any there are values other than host are allowed in the origin header.,0,0,others,nova
6499,amqp_auto_delete,Auto-delete queues in AMQP.,1,5,workload-specific,nova
6500,amqp_durable_queues,Use durable queues in AMQP.,1,5,workload-specific,nova
6501,anycast_address,Appended to the address prefix when sending to a group of consumers. Used by the message bus to identify messages that should be delivered in a round-robin fashion across consumers.,0,0,others,nova
6502,api_endpoint,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",0,0,others,nova
6503,api_max_retries,"The number of times to retry when a request conflicts. If set to 0, only try once, no retries.",0,0,others,nova
6504,api_paste_config,This option represents a file name for the paste.deploy config for nova-api.,0,0,others,nova
6505,api_retry_count,"Number of times VMware vCenter server API must be retried on connection failures, e.g. socket error, etc.",0,0,others,nova
6506,api_retry_interval,The number of seconds to wait before retrying the request.,0,0,others,nova
6507,api_servers,List of glance api servers endpoints available to nova.,0,0,others,nova
6510,auth_endpoint,Use this endpoint to connect to Keystone,0,0,others,nova
6511,auth_schemes,The authentication schemes to use with the compute node.,1,2,security-tradeoff,nova
6512,auth_section,Config Section from which to load plugin specific options,0,0,others,nova
6513,auth_strategy,Determine the strategy to use for authentication.,1,2,security-tradeoff,nova
6514,auth_type.B,Authentication type to load,1,2,security-tradeoff,nova
6515,auth_type,"The type of authentication credential to create. Possible values are 'token', 'password', 'keystone_token', and 'keystone_password'. Required if no context is passed to the credential factory.",0,0,others,nova
6516,auth_uri,"Complete 'public' Identity API endpoint. This endpoint should not be an 'admin' endpoint, as it should be accessible by all end users. Unauthenticated clients are redirected to this endpoint to authenticate. Although this endpoint should ideally be unversioned, client support in the wild varies. If you're using a versioned v2 endpoint here, then this should not be the same endpoint the service user utilizes for validating tokens, because normal end users may not be able to reach that endpoint. This option is deprecated in favor of www_authenticate_uri and will be removed in the S release.",0,0,others,nova
6517,auth_url.B,Authentication URL,0,0,others,nova
6518,auth_url,Use this endpoint to connect to Keystone.,0,0,others,nova
6519,auth_version,API version of the admin Identity API endpoint.,0,0,others,nova
6520,auto_assign_floating_ip,Autoassigning floating IP to VM,0,0,others,nova
6521,available_filters,Filters that the scheduler can use.,0,0,others,nova
6522,backdoor_port,"Enable eventlet backdoor. Acceptable values are 0, <port>, and <start>:<end>, where 0 results in listening on a random tcp port number; <port> results in listening on the specified port number (and not enabling backdoor if that port is in use); and <start>:<end> results in listening on the smallest unused port number within the specified range of port numbers. The chosen port is displayed in the service's log file.",0,0,others,nova
6523,backdoor_socket,"Enable eventlet backdoor, using the provided path as a unix socket that can receive connections. This option is mutually exclusive with 'backdoor_port' in that only one should be provided. If both are provided then the existence of this option overrides the usage of that option.",0,0,others,nova
6524,backend.B,"Cache backend module. For eventlet-based or environments with hundreds of threaded servers, Memcache with pooling (oslo_cache.memcache_pool) is recommended. For environments with less than 100 threaded servers, Memcached (dogpile.cache.memcached) or Redis (dogpile.cache.redis) is recommended. Test environments with a single instance of the server can use the dogpile.cache.memory backend.",0,0,others,nova
6525,backend.C,Specify the key manager implementation. Options are 'barbican' and 'vault'. Default is 'barbican'. Will support the values earlier set using [key_manager]/api_class for some time.,0,0,others,nova
6526,backend,The back end to use for the database.,0,0,others,nova
6527,backend_argument,Arguments supplied to the backend module. Specify this option once per argument to be passed to the dogpile.cache backend. Example format: '<argname>:<value>'.,0,0,others,nova
6528,backends,Additional backends that can perform health checks and report that information back as part of a request.,0,0,others,nova
6531,barbican_api_version,"Version of the Barbican API, for example: 'v1'",0,0,others,nova
6532,barbican_endpoint,"Use this endpoint to connect to Barbican, for example: 'http://localhost:9311/'",0,0,others,nova
6533,barbican_endpoint_type,"Specifies the type of endpoint. Allowed values are: public, private, and admin",0,0,others,nova
6534,base_url,The URL an end user would use to connect to the nova-serialproxy service.,0,0,others,nova
6536,bdms_in_notifications,"If enabled, include block device information in the versioned notification payload. Sending block device information is disabled by default as providing that information can incur some overhead on the system since the information may need to be loaded from the database.",1,6,function-tradeoff,nova
6537,bindir,The directory where the Nova binaries are installed.,0,0,others,nova
6538,block_device_allocate_retries,"Number of times to retry block device allocation on failures. Starting with Liberty, Cinder can use image volume cache. This may help with block device allocation performance. Look at the cinder image_volume_cache_enabled configuration option.",0,0,others,nova
6539,block_device_allocate_retries_interval,Interval (in seconds) between block device allocation retries on failures.,0,0,others,nova
6540,block_device_creation_timeout,Time in secs to wait for a block device to be created,0,0,others,nova
6541,broadcast_prefix,address prefix used when broadcasting to all servers,0,0,others,nova
6542,build_failure_weight_multiplier,Multiplier used for weighing hosts that have had recent build failures.,0,0,others,nova
6543,ca_file.B,CA certificate file to be verified in httpd server with TLS enabled,0,0,others,nova
6544,ca_file,Specifies the CA bundle file to be used in verifying the vCenter server certificate.,0,0,others,nova
6546,cache_images,Cache glance images locally.,1,4,limited-side-effect,nova
6547,cache_prefix,This option adds a prefix to the folder where cached images are stored,0,0,others,nova
6548,cafile.B,A PEM encoded Certificate Authority to use when verifying HTTPs connections. Defaults to system CAs.,0,0,others,nova
6549,cafile,PEM encoded Certificate Authority to use when verifying HTTPs connections.,0,0,others,nova
6550,call_timeout,Call timeout.,0,0,others,nova
6551,capabilities.B,Cell capabilities.,0,0,others,nova
6552,capabilities,List of Linux capabilities retained by the privsep daemon.,0,0,others,nova
6553,catalog_info,Info to match when looking for cinder in the service catalog.,0,0,others,nova
6555,cells,Cells RPC API version cap.,0,0,others,nova
6556,cells_config,Optional cells configuration.,0,0,others,nova
6557,cert.B,Cert RPC API version cap.,0,0,others,nova
6559,certfile.B,PEM encoded client certificate cert file,0,0,others,nova
6560,certfile,Required if identity server requires client certificate,0,0,others,nova
6561,check_host,Ensure compute service is running on host XenAPI connects to. This option must be set to false if the 'independent_compute' option is set to true.,1,3,reliability-tradeoff,nova
6562,checksum_base_images,Write a checksum for files in _base to disk,1,6,function-tradeoff,nova
6563,checksum_interval_seconds,How frequently to checksum base images,0,0,others,nova
6564,cipher,Cipher-mode string to be used.,0,0,others,nova
6565,client_socket_timeout,This option specifies the timeout for client connections' socket operations. If an incoming connection is idle for this number of seconds it will be closed. It indicates timeout on individual read/writes on the socket connection. To wait forever set to 0.,0,0,others,nova
6566,cloud_connector_url,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",0,0,others,nova
6567,cluster_name,Name of a VMware Cluster ComputeResource.,0,0,others,nova
6568,cnt_vpn_clients,This option represents the number of IP addresses to reserve at the top of the address range for VPN clients. It also will be ignored if the configuration option for network_manager is not set to the default of 'nova.network.manager.VlanManager'.,0,0,others,nova
6570,compute,Compute RPC API version cap.,0,0,others,nova
6571,compute_driver,Defines which driver to use for controlling virtualization.,0,0,others,nova
6572,compute_link_prefix,"This string is prepended to the normal URL that is returned in links to the OpenStack Compute API. If it is empty (the default), the URLs are returned unchanged.",0,0,others,nova
6573,compute_monitors,"A comma-separated list of monitors that can be used for getting compute metrics. You can use the alias/name from the setuptools entry points for nova.compute.monitors.* namespaces. If no namespace is supplied, the 'cpu.' namespace is assumed for backwards-compatibility.",0,0,others,nova
6577,config_drive_inject_password,Configuration drive inject password,0,0,others,nova
6578,config_drive_skip_versions,"When gathering the existing metadata for a config drive, the EC2-style metadata is returned for all versions that don't appear in this option. As of the Liberty release, the available versions are:",0,0,others,nova
6580,conn_pool_min_size,The pool size limit for connections expiration policy,1,1,resource,nova
6581,conn_pool_ttl,The time-to-live in sec of idle connections in the pool,0,0,others,nova
6582,connection.B,The SQLAlchemy connection string to use to connect to the database.,0,0,others,nova
6583,connection,The SQLAlchemy connection string to use to connect to the database. Do not set this for the nova-compute service.,0,0,others,nova
6585,connection_debug,"Verbosity of SQL debugging information: 0=None, 100=Everything.",1,6,function-tradeoff,nova
6586,connection_parameters,Optional URL parameters to append onto the connection URL at connect time; specify as param1=value1&param2=value2&',0,0,others,nova
6587,connection_password,Password for connection to XenServer/Xen Cloud Platform,0,0,others,nova
6588,connection_pool_size,This option sets the http connection pool size,1,1,resource,nova
6590,connection_retry_backoff,Increase the connection_retry_interval by this many seconds after each unsuccessful failover attempt.,0,0,others,nova
6591,connection_retry_interval,Seconds to pause before attempting to re-connect.,0,0,others,nova
6592,connection_retry_interval_max,Maximum limit for connection_retry_interval + connection_retry_backoff,0,0,others,nova
6594,connection_trace,Add Python stack traces to SQL as comment strings.,0,0,others,nova
6595,connection_uri,Overrides the default libvirt URI of the chosen virtualization type.,0,0,others,nova
6596,connection_url,URL for connection to XenServer/Xen Cloud Platform. A special value of unix://local can be used to connect to the local unix socket.,0,0,others,nova
6597,connection_username,Username for connection to XenServer/Xen Cloud Platform,0,0,others,nova
6599,console,Console RPC API version cap.,0,0,others,nova
6600,console_delay_seconds,Set this value if affected by an increased network latency causing repeated characters when typing in a remote console.,0,0,others,nova
6601,console_host,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",0,0,others,nova
6602,console_public_hostname,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",0,0,others,nova
6603,console_xvp_conf,Generated XVP conf file,0,0,others,nova
6604,console_xvp_conf_template,XVP conf template,0,0,others,nova
6605,console_xvp_log,XVP log file,0,0,others,nova
6606,console_xvp_multiplex_port,Port for XVP to multiplex VNC connections on,0,0,others,nova
6607,console_xvp_pid,XVP master process pid file,0,0,others,nova
6609,consumer_group,Group id for Kafka consumer. Consumers in one group will coordinate message consumption,0,0,others,nova
6610,container_name,Name for the AMQP container. must be globally unique. Defaults to a generated UUID,0,0,others,nova
6611,control_exchange,The default exchange under which topics are scoped. May be overridden by an exchange name specified in the transport_url option.,0,0,others,nova
6613,cpu_allocation_ratio,This option helps you specify virtual CPU to physical CPU allocation ratio.,1,5,workload-specific,nova
6614,cpu_mode,Is used to set the CPU mode an instance should have.,1,5,workload-specific,nova
6616,cpu_model_extra_flags,"This allows specifying granular CPU feature flags when configuring CPU models. For example, to explicitly specify the pcid (Process-Context ID, an Intel processor feature -- which is now required to address the guest performance degradation as a result of applying the 'Meltdown' CVE fixes to certain Intel CPU models) flag to the 'IvyBridge' virtual CPU model:",1,6,function-tradeoff,nova
6617,cpu_shared_set,Defines which physical CPUs (pCPUs) will be used for best-effort guest vCPU resources.,0,0,others,nova
6619,create_unique_mac_address_attempts,This option determines how many times nova-network will attempt to create a unique MAC address before giving up and raising a VirtualInterfaceMacAddressException error.,0,0,others,nova
6620,cross_az_attach,Allow attach between instance and volume in different availability zones.,0,0,others,nova
6621,daemon,Run as a background process.,1,4,limited-side-effect,nova
6622,datastore_regex,Regular expression pattern to match the name of datastore.,0,0,others,nova
6624,db_inc_retry_interval,"If True, increases the interval between retries of a database operation up to db_max_retry_interval.",0,0,others,nova
6625,db_max_retries,Maximum retries in case of connection error or deadlock error before error is raised. Set to -1 to specify an infinite retry count.,0,0,others,nova
6627,db_retry_interval,Seconds between retries of a database transaction.,0,0,others,nova
6628,debug.B,Enable or disable debug logging with glanceclient.,1,6,function-tradeoff,nova
6629,debug.C,Enable/disables guestfs logging.,1,6,function-tradeoff,nova
6632,default_access_ip_network_name,"Name of the network to be used to set access IPs for instances. If there are multiple IPs to choose from, an arbitrary one will be chosen.",0,0,others,nova
6633,default_availability_zone,Default availability zone for compute services.,0,0,others,nova
6634,default_domain_id,Optional domain ID to use with v3 and v2 parameters. It will be used for both the user and project domain in v3 and ignored in v2 authentication.,0,0,others,nova
6636,default_ephemeral_format,The default format an ephemeral_volume will be formatted with on creation.,0,0,others,nova
6637,default_flavor,Default flavor to use for the EC2 API only. The Nova API does not support a default flavor.,0,0,others,nova
6638,default_floating_pool,Default pool for floating IPs.,0,0,others,nova
6639,default_level,Default notification level for outgoing notifications.,0,0,others,nova
6640,default_log_levels,List of package logging levels in logger=LEVEL pairs. This option is ignored if log_config_append is set.,1,6,function-tradeoff,nova
6641,default_notification_exchange,Exchange name used in notification addresses. Exchange name resolution precedence: Target.exchange if set else default_notification_exchange if set else control_exchange if set else 'notify',0,0,others,nova
6642,default_notify_timeout,The deadline for a sent notification message delivery. Only used when caller does not provide a timeout expiry.,0,0,others,nova
6644,default_pool_size,This option specifies the size of the pool of greenthreads used by wsgi. It is possible to limit the number of concurrent connections using this option.,1,1,resource,nova
6645,default_reply_retry,The maximum number of attempts to re-send a reply message which failed due to a recoverable error.,0,0,others,nova
6646,default_reply_timeout,The deadline for an rpc reply message delivery.,0,0,others,nova
6647,default_rpc_exchange,Exchange name used in RPC addresses. Exchange name resolution precedence: Target.exchange if set else default_rpc_exchange if set else control_exchange if set else 'rpc',0,0,others,nova
6649,default_send_timeout,The deadline for an rpc cast or call message delivery. Only used when caller does not provide a timeout expiry.,0,0,others,nova
6650,default_sender_link_timeout,The duration to schedule a purge of idle sender links. Detach link after expiry.,0,0,others,nova
6651,default_trusted_certificate_ids,List of certificate IDs for certificates that should be trusted.,0,0,others,nova
6652,defer_iptables_apply,Defer application of IPTables rules until after init phase.,0,0,others,nova
6653,delay_auth_decision,"Do not handle authorization requests within the middleware, but delegate the authorization decision to downstream WSGI components.",1,2,security-tradeoff,nova
6655,dhcp_domain,This option allows you to specify the domain for the DHCP server.,0,0,others,nova
6656,dhcp_lease_time,"The lifetime of a DHCP lease, in seconds. The default is 86400 (one day).",0,0,others,nova
6657,dhcpbridge,The location of the binary nova-dhcpbridge. By default it is the binary named 'nova-dhcpbridge' that is installed with all the other nova binaries.,0,0,others,nova
6658,dhcpbridge_flagfile,"This option is a list of full paths to one or more configuration files for dhcpbridge. In most cases the default path of '/etc/nova/nova-dhcpbridge.conf' should be sufficient, but if you have special needs for configuring dhcpbridge, you can change or add to this list.",0,0,others,nova
6659,disable_agent,Disables the use of XenAPI agent.,1,6,function-tradeoff,nova
6660,disable_by_file_path,Check the presence of a file to determine if an application is running on a port. Used by DisableByFileHealthcheck plugin.,0,0,others,nova
6661,disable_by_file_paths,Check the presence of a file based on a port to determine if an application is running on a port. Expects a 'port:path' list of strings. Used by DisableByFilesPortsHealthcheck plugin.,0,0,others,nova
6662,disable_group_policy_check_upcall,Disable the server group policy check upcall in compute.,1,6,function-tradeoff,nova
6663,disable_libvirt_livesnapshot,Disable live snapshots when using the libvirt driver.,1,6,function-tradeoff,nova
6664,disable_process_locking,Enables or disables inter-process locks.,0,0,others,nova
6665,disable_rootwrap,Use sudo instead of rootwrap.,1,6,function-tradeoff,nova
6666,discover_hosts_in_cells_interval,Periodic task interval.,0,0,others,nova
6667,disk_allocation_ratio,This option helps you specify virtual disk to physical disk allocation ratio.,1,5,workload-specific,nova
6668,disk_cachemodes,Specific cache modes to use for different disk types.,1,5,workload-specific,nova
6669,disk_driver,The disk driver to use for PowerVM disks. PowerVM provides support for localdisk and PowerVM Shared Storage Pool disk drivers.,0,0,others,nova
6670,disk_prefix,Override the default disk prefix for the devices attached to an instance.,0,0,others,nova
6671,disk_weight_multiplier,Disk weight multipler ratio.,1,5,workload-specific,nova
6672,dmz_cidr,This option is a list of zero or more IP address ranges in your network's DMZ that should be accepted.,0,0,others,nova
6673,dns_server,"Despite the singular form of the name of this option, it is actually a list of zero or more server addresses that dnsmasq will use for DNS nameservers. If this is not empty, dnsmasq will not read /etc/resolv.conf, but will only use the servers specified in this option. If the option use_network_dns_servers is True, the dns1 and dns2 servers from the network will be appended to this list, and will be used as DNS servers, too.",0,0,others,nova
6674,dns_update_periodic_interval,"This option determines the time, in seconds, to wait between refreshing DNS entries for the network.",0,0,others,nova
6675,dnsmasq_config_file,"The path to the custom dnsmasq configuration file, if any.",0,0,others,nova
6676,domain_id.B,Domain ID for domain scoping. Optional for 'keystone_token' and 'keystone_password' auth_type.,0,0,others,nova
6677,domain_id,Domain ID to scope to,0,0,others,nova
6678,domain_name.B,Domain name for domain scoping. Optional for 'keystone_token' and 'keystone_password' auth_type.,0,0,others,nova
6679,domain_name,Domain name to scope to,0,0,others,nova
6680,driver.B,Provides abstraction for quota checks. Users can configure a specific driver to use for quota checks.,0,0,others,nova
6681,driver.C,"The Drivers(s) to handle sending notifications. Possible values are messaging, messagingv2, routing, log, test, noop",0,0,others,nova
6682,driver,"The class of the driver used by the scheduler. This should be chosen from one of the entrypoints under the namespace 'nova.scheduler.driver' of file 'setup.cfg'. If nothing is specified in this option, the 'filter_scheduler' is used.",0,0,others,nova
6683,dynamic_memory_ratio,Dynamic memory ratio,1,5,workload-specific,nova
6684,ebtables_exec_attempts,This option determines the number of times to retry ebtables commands before giving up. The minimum number of retries is 1.,0,0,others,nova
6685,ebtables_retry_interval,"This option determines the time, in seconds, that the system will sleep in between ebtables retries. Note that each successive retry waits a multiple of this value, so for example, if this is set to the default of 1.0 seconds, and ebtables_exec_attempts is 4, after the first failure, the system will sleep for 1 * 1.0 seconds, after the second failure it will sleep 2 * 1.0 seconds, and after the third failure it will sleep 3 * 1.0 seconds.",0,0,others,nova
6687,enable_auto_commit,Enable asynchronous consumer commits,1,4,limited-side-effect,nova
6688,enable_certificate_validation,Enable certificate validation for image signature verification.,1,6,function-tradeoff,nova
6689,enable_consoleauth,Enable the consoleauth service to avoid resetting unexpired consoles.,1,6,function-tradeoff,nova
6690,enable_instance_metrics_collection,Enable instance metrics collection,1,6,function-tradeoff,nova
6691,enable_instance_password,"Enables returning of the instance password by the relevant server API calls such as create, rebuild, evacuate, or rescue. If the hypervisor does not support password injection, then the password returned will not be correct, so if your hypervisor does not support password injection, set this to False.",0,0,others,nova
6692,enable_network_quota,This option is used to enable or disable quota checking for tenant networks.,1,6,function-tradeoff,nova
6693,enable_new_services,Enable new nova-compute services on this host automatically.,1,6,function-tradeoff,nova
6694,enable_numa_live_migration,Enable live migration of instances with NUMA topologies.,1,6,function-tradeoff,nova
6695,enable_proxy_headers_parsing,Whether the application is behind a proxy or not. This determines if the middleware should parse the headers or not.,0,0,others,nova
6696,enable_remotefx,Enable RemoteFX feature,1,6,function-tradeoff,nova
6697,enabled.B,Enable Remote Desktop Protocol (RDP) related features.,1,6,function-tradeoff,nova
6698,enabled.C,Enable SPICE related features.,1,6,function-tradeoff,nova
6699,enabled.D,Enable the profiling for all services on this node.,1,6,function-tradeoff,nova
6701,enabled.F,Enable VNC related features.,1,6,function-tradeoff,nova
6702,enabled.G,Enables graphical console access for virtual machines.,0,0,others,nova
6705,enabled_apis,List of APIs to be enabled by default.,0,0,others,nova
6706,enabled_filters,Filters that the scheduler will use.,0,0,others,nova
6707,enabled_perf_events,"This will allow you to specify a list of events to monitor low-level performance of guests, and collect related statsitics via the libvirt driver, which in turn uses the Linux kernel's perf infrastructure. With this config attribute set, Nova will generate libvirt guest XML to monitor the specified events. For more information, refer to the 'Performance monitoring events' section here: https://libvirt.org/formatdomain.html#elementsPerf. And here: https://libvirt.org/html/libvirt-libvirt-domain.html -- look for VIR_PERF_PARAM_*",0,0,others,nova
6708,enabled_ssl_apis,List of APIs with enabled SSL.,0,0,others,nova
6710,endpoint_override,"Always use this endpoint URL for requests for this client. NOTE: The unversioned endpoint should be specified here; to request a particular API version, use the version, min-version, and/or max-version options.",0,0,others,nova
6711,endpoint_template,If this option is set then it will override service catalog lookup with this template for cinder endpoint,0,0,others,nova
6712,enforce_scope,"This option controls whether or not to enforce scope when evaluating policies. If True, the scope of the token used in the request is compared to the scope_types of the policy being enforced. If the scopes do not match, an InvalidScope exception will be raised. If False, a message will be logged informing operators that policies are being invoked with mismatching scope.",0,0,others,nova
6714,ensure_libvirt_rbd_instance_dir_cleanup,Ensure the instance directory is removed during clean up when using rbd.,0,0,others,nova
6716,es_scroll_size,Elasticsearch splits large requests in batches. This parameter defines maximum size of each batch (for example: es_scroll_size=10000).,1,5,workload-specific,nova
6718,executor_thread_pool_size,Size of executor thread pool when executor is threading or eventlet.,1,1,resource,nova
6719,expiration_time,"Default TTL, in seconds, for any cached item in the dogpile.cache region. This applies to any cached method that doesn't have an explicit cache expiration time defined for it.",0,0,others,nova
6720,expose_headers,Indicate which headers are safe to expose to the API. Defaults to HTTP Simple Headers.,0,0,others,nova
6721,extension_sync_interval,Integer value representing the number of seconds to wait before querying Neutron for extensions. After this number of seconds the next time Nova needs to create a resource in Neutron it will requery Neutron for the extensions that it has loaded. Setting value to 0 will refresh the extensions with no wait.,1,3,reliability-tradeoff,nova
6722,fake_network,This option is used mainly in testing to avoid calls to the underlying network utilities.,0,0,others,nova
6725,filter_error_trace,Enable filter traces that contain error/exception to a separated place.,1,6,function-tradeoff,nova
6726,firewall_driver,Firewall driver to use with nova-network service.,0,0,others,nova
6727,fixed_ip_disassociate_timeout,"This is the number of seconds to wait before disassociating a deallocated fixed IP address. This is only used with the nova-network service, and has no effect when using neutron for networking.",0,0,others,nova
6728,fixed_ips,The number of fixed IPs allowed per project.,0,0,others,nova
6729,fixed_key,"Fixed key returned by key manager, specified in hex.",0,0,others,nova
6730,fixed_range_v6,This option determines the fixed IPv6 address block when creating a network.,0,0,others,nova
6731,flat_injected,"This option determines whether the network setup information is injected into the VM before it is booted. While it was originally designed to be used only by nova-network, it is also used by the vmware and xenapi virt drivers to control whether network information is injected into a VM. The libvirt virt driver also uses it when we use config_drive to configure network to control whether network information is injected into a VM.",0,0,others,nova
6732,flat_interface,"This option is the name of the virtual interface of the VM on which the bridge will be built. While it was originally designed to be used only by nova-network, it is also used by libvirt for the bridge interface name.",0,0,others,nova
6733,flat_network_bridge,This option determines the bridge used for simple network interfaces when no bridge is specified in the VM creation request.,0,0,others,nova
6734,flat_network_dns,"This is the address of the DNS server for a simple network. If this option is not specified, the default of '8.8.4.4' is used.",0,0,others,nova
6735,floating_ip_dns_manager,Full class name for the DNS Manager for floating IPs.,0,0,others,nova
6738,force_dhcp_release,"When this option is True, a call is made to release the DHCP for the instance when that instance is terminated.",0,0,others,nova
6739,force_raw_images,Force conversion of backing images to raw format.,1,6,function-tradeoff,nova
6742,gateway,This is the default IPv4 gateway. It is used only in the testing suite.,0,0,others,nova
6743,gateway_v6,This is the default IPv6 gateway. It is used only in the testing suite.,0,0,others,nova
6744,gid_maps,List of guid targets and ranges.Syntax is guest-gid:host-gid:count. Maximum of 5 allowed.,0,0,others,nova
6745,glance_link_prefix,"This string is prepended to the normal URL that is returned in links to Glance resources. If it is empty (the default), the URLs are returned unchanged.",0,0,others,nova
6746,graceful_shutdown_timeout,Specify a timeout after which a gracefully shutdown server will exit. Zero value means endless wait.,0,0,others,nova
6748,group_request_prefix,address prefix when sending to any server in group,0,0,others,nova
6750,hash_algorithms,"Hash algorithms to use for hashing PKI tokens. This may be a single algorithm or multiple. The algorithms are those supported by Python standard hashlib.new(). The hashes will be tried in the order given, so put the preferred one first for performance. The result of the first hash will be stored in the cache. This will typically be set to multiple values only while migrating from a less secure algorithm to a more secure one. Once all the old tokens are expired this option should be set to a single value for better performance.",1,4,limited-side-effect,nova
6752,heartbeat_rate,How often times during the heartbeat_timeout_threshold we check the heartbeat.,0,0,others,nova
6755,hmac_keys,Secret key(s) to use for encrypting context data for performance profiling.,0,0,others,nova
6756,host,Debug host (IP or name) to connect to. This command line parameter is used when you want to connect to a nova service via a debugger running on a different host.,0,0,others,nova
6757,host_ip,Hostname or IP address for connection to VMware vCenter host.,0,0,others,nova
6758,host_password,Password for connection to VMware vCenter host.,0,0,others,nova
6759,host_port,Port for connection to VMware vCenter host.,0,0,others,nova
6760,host_subset_size,Size of subset of best hosts selected by scheduler.,0,0,others,nova
6761,host_username,Username for connection to VMware vCenter host.,0,0,others,nova
6762,html5_proxy_base_url,The URL an end user would use to connect to the RDP HTML5 console proxy. The console proxy service is called with this token-embedded URL and establishes the connection to the proper instance.,0,0,others,nova
6763,html5proxy_base_url,Location of the SPICE HTML5 console proxy.,0,0,others,nova
6764,html5proxy_host,IP address or a hostname on which the nova-spicehtml5proxy service listens for incoming requests.,0,0,others,nova
6765,html5proxy_port,Port on which the nova-spicehtml5proxy service listens for incoming requests.,0,0,others,nova
6766,http_connect_timeout,Request timeout value for communicating with Identity API server.,0,0,others,nova
6767,http_request_max_retries,How many times are we trying to reconnect when communicating with Identity API Server.,0,0,others,nova
6768,http_retries.B,Number of times cinderclient should retry on any failed http call. 0 means connection is attempted only once. Setting it to any positive integer means that on failure connection is retried that many times e.g. setting it to 3 means total attempts to connect will be 4.,0,0,others,nova
6769,http_retries,Number of times neutronclient should retry on any failed http call.,0,0,others,nova
6770,hw_disk_discard,Discard option for nova managed disks.,0,0,others,nova
6771,hw_machine_type,"For qemu or KVM guests, set this option to specify a default machine type per host architecture. You can find a list of supported machine types in your environment by checking the output of the 'virsh capabilities' command. The format of the value for this config option is host-arch=machine-type. For example: x86_64=machinetype1,armv7l=machinetype2",0,0,others,nova
6773,image_cache_manager_interval,Number of seconds to wait between runs of the image cache manager.,0,0,others,nova
6774,image_cache_subdirectory_name,Location of cached images.,0,0,others,nova
6775,image_compression_level,Compression level for images.,1,6,function-tradeoff,nova
6776,image_handler,The plugin used to handle image uploads and downloads.,0,0,others,nova
6777,image_info_filename_pattern,Allows image information files to be stored in non-standard locations,0,0,others,nova
6778,image_properties_default_architecture,The default architecture to be used when using the image properties filter.,0,0,others,nova
6780,image_upload_handler,Dom0 plugin driver used to handle image uploads.,0,0,others,nova
6781,images_rbd_ceph_conf,Path to the ceph configuration file to use,0,0,others,nova
6782,images_rbd_pool,The RADOS pool in which rbd volumes are stored,0,0,others,nova
6785,include_service_catalog,"(Optional) Indicate whether to set the X-Service-Catalog header. If False, middleware will not ask for service catalog on token validation and will not set the X-Service-Catalog header.",0,0,others,nova
6787,incomplete_consumer_user_id,"Early API microversions (<1.8) allowed creating allocations and not specifying a project or user identifier for the consumer. In cleaning up the data modeling, we no longer allow missing project and user information. If an older client makes an allocation, we'll use this in place of the information it doesn't provide.",0,0,others,nova
6788,independent_compute,"Used to prevent attempts to attach VBDs locally, so Nova can be run in a VM on a different host.",0,0,others,nova
6789,initial_cpu_allocation_ratio,This option helps you specify initial virtual CPU to physical CPU allocation ratio.,1,5,workload-specific,nova
6790,initial_disk_allocation_ratio,This option helps you specify initial virtual disk to physical disk allocation ratio.,1,5,workload-specific,nova
6791,initial_ram_allocation_ratio,This option helps you specify initial virtual RAM to physical RAM allocation ratio.,1,5,workload-specific,nova
6792,inject_key,Allow the injection of an SSH key at boot time.,1,2,security-tradeoff,nova
6793,inject_partition,Determines the way how the file system is chosen to inject data into it.,0,0,others,nova
6794,inject_password,Allow the injection of an admin password for instance only at create and rebuild process.,1,6,function-tradeoff,nova
6795,injected_file_content_bytes,The number of bytes allowed per injected file.,1,1,resource,nova
6796,injected_file_path_length,The maximum allowed injected file path length.,0,0,others,nova
6797,injected_files,The number of injected files allowed.,0,0,others,nova
6798,injected_network_template,Path to '/etc/network/interfaces' template.,0,0,others,nova
6799,insecure.B,"If true, the vCenter server certificate is not verified. If false, then the default CA truststore is used for verification.",1,2,security-tradeoff,nova
6800,insecure,Verify HTTPS connections.,1,2,security-tradeoff,nova
6801,instance_build_timeout,Maximum time in seconds that an instance can take to build.,0,0,others,nova
6802,instance_delete_interval,Interval for retrying failed instance file deletes.,0,0,others,nova
6803,instance_dns_domain,"If specified, Nova checks if the availability_zone of every instance matches what the database says the availability_zone should be for the specified dns_domain.",0,0,others,nova
6804,instance_dns_manager,Full class name for the DNS Manager for instance IPs.,0,0,others,nova
6805,instance_format,The format for an instance that is passed with the log message.,0,0,others,nova
6806,instance_list_cells_batch_fixed_size,"This controls the batch size of instances requested from each cell database if instance_list_cells_batch_strategy` is set to fixed. This integral value will define the limit issued to each cell every time a batch of instances is requested, regardless of the number of cells in the system or any other factors. Per the general logic called out in the documentation for instance_list_cells_batch_strategy, the minimum value for this is 100 records per batch.",1,5,workload-specific,nova
6808,instance_list_per_project_cells,"When enabled, this will cause the API to only query cell databases in which the tenant has mapped instances. This requires an additional (fast) query in the API database before each list, but also (potentially) limits the number of cell databases that must be queried to provide the result. If you have a small number of cells, or tenants are likely to have instances in all cells, then this should be False. If you have many cells, especially if you confine tenants to a small subset of those cells, this should be True.",0,0,others,nova
6809,instance_name_template,Template string to be used to generate instance names.,0,0,others,nova
6810,instance_update_num_instances,Instance update num instances,1,5,workload-specific,nova
6811,instance_update_sync_database_limit,Instance update sync database limit.,1,3,reliability-tradeoff,nova
6812,instance_updated_at_threshold,Number of seconds after an instance was updated or deleted to continue to update cells. This option lets cells manager to only attempt to sync instances that have been updated recently.,1,5,workload-specific,nova
6813,instance_usage_audit,This option enables periodic compute.instance.exists notifications. Each compute node must be configured to generate system usage data. These notifications are consumed by OpenStack Telemetry service.,1,6,function-tradeoff,nova
6815,instance_uuid_format,The format for an instance UUID that is passed with the log message.,0,0,others,nova
6817,instances_path,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",0,0,others,nova
6818,instances_path_share,Instances path share,0,0,others,nova
6819,integration_bridge,This option should be configured only when using the NSX-MH Neutron plugin. This is the name of the integration bridge on the ESXi server or host. This should not be set for any other Neutron plugin. Hence the default value is not set.,0,0,others,nova
6820,intercell,Intercell RPC API version cap.,0,0,others,nova
6821,internal_service_availability_zone,Availability zone for internal services.,0,0,others,nova
6822,introduce_vdi_retry_wait,Number of seconds to wait for SR to settle if the VDI does not exist when first introduced.,0,0,others,nova
6823,io_ops_weight_multiplier,IO operations weight multipler ratio.,1,5,workload-specific,nova
6824,iptables_bottom_regex,"This expression, if defined, will select any matching iptables rules and place them at the bottom when applying metadata changes to the rules.",0,0,others,nova
6825,iptables_drop_action,"By default, packets that do not pass the firewall are DROPped. In many cases, though, an operator may find it more useful to change this from DROP to REJECT, so that the user issuing those packets may have a better idea as to what's going on, or LOGDROP in order to record the blocked traffic before DROPping.",0,0,others,nova
6826,iptables_top_regex,"This expression, if defined, will select any matching iptables rules and place them at the top when applying metadata changes to the rules.",0,0,others,nova
6827,ipv6_backend,Abstracts out IPv6 address generation to pluggable backends.,0,0,others,nova
6828,ipxe_boot_menu_url,URL to the iPXE boot menu.,0,0,others,nova
6829,ipxe_mkisofs_cmd,Name and optionally path of the tool used for ISO image creation.,0,0,others,nova
6830,ipxe_network_name,Name of network to use for booting iPXE ISOs.,0,0,others,nova
6831,iscsi_iface,The iSCSI transport iface to use to connect to target in case offload support is desired.,0,0,others,nova
6832,iscsi_initiator_list,List of iSCSI initiators that will be used for estabilishing iSCSI sessions.,0,0,others,nova
6833,iser_use_multipath,Use multipath connection of the iSER volume.,0,0,others,nova
6835,isolated_images,List of UUIDs for images that can only be run on certain hosts.,0,0,others,nova
6836,kafka_consumer_timeout,Default timeout(s) for Kafka consumers,0,0,others,nova
6837,kafka_max_fetch_bytes,Max fetch bytes of Kafka consumer,1,1,resource,nova
6839,key,SSL key file (if separate from cert).,0,0,others,nova
6840,key_pairs,The maximum number of key pairs allowed per user.,0,0,others,nova
6841,key_size,Encryption key length in bits.,1,2,security-tradeoff,nova
6842,keyfile.B,PEM encoded client certificate key file,0,0,others,nova
6843,keyfile,Required if identity server requires client certificate,0,0,others,nova
6844,keymap.B,A keyboard layout which is supported by the underlying hypervisor on this node.,0,0,others,nova
6846,kombu_compression,"EXPERIMENTAL: Possible values are: gzip, bz2. If not set compression will not be used. This option may not be available in future versions.",1,6,function-tradeoff,nova
6847,kombu_failover_strategy,Determines how the next RabbitMQ node is chosen in case the one we are currently connected to becomes unavailable. Takes effect only if more than one RabbitMQ node is provided in config.,0,0,others,nova
6848,kombu_missing_consumer_retry_timeout,How long to wait a missing client before abandoning to send it its replies. This value should not be longer than rpc_response_timeout.,0,0,others,nova
6850,kv_mountpoint,"Mountpoint of KV store in Vault to use, for example: secret",0,0,others,nova
6852,ldap_dns_base_dn,Base distinguished name for the LDAP search query,0,0,others,nova
6853,ldap_dns_password,Bind user's password for LDAP server,0,0,others,nova
6854,ldap_dns_servers,DNS Servers for LDAP DNS driver,0,0,others,nova
6855,ldap_dns_soa_expiry,Expiry interval (in seconds) for LDAP DNS driver Start of Authority,0,0,others,nova
6856,ldap_dns_soa_hostmaster,Hostmaster for LDAP DNS driver Statement of Authority,0,0,others,nova
6857,ldap_dns_soa_minimum,Minimum interval (in seconds) for LDAP DNS driver Start of Authority,0,0,others,nova
6858,ldap_dns_soa_refresh,Refresh interval (in seconds) for LDAP DNS driver Start of Authority,0,0,others,nova
6859,ldap_dns_soa_retry,Retry interval (in seconds) for LDAP DNS driver Start of Authority,0,0,others,nova
6860,ldap_dns_url,URL for LDAP server which will store DNS entries,0,0,others,nova
6861,ldap_dns_user,Bind user for LDAP server,0,0,others,nova
6862,limit_cpu_features,Limit CPU features,1,6,function-tradeoff,nova
6863,limit_tenants_to_placement_aggregate,"This setting causes the scheduler to look up a host aggregate with the metadata key of filter_tenant_id set to the project of an incoming request, and request results from placement be limited to that aggregate. Multiple tenants may be added to a single aggregate by appending a serial number to the key, such as filter_tenant_id:123.",0,0,others,nova
6865,linuxnet_interface_driver,"This is the class used as the ethernet device driver for linuxnet bridge operations. The default value should be all you need for most cases, but if you wish to use a customized class, set this option to the full dot-separated import path for that class.",0,0,others,nova
6866,linuxnet_ovs_integration_bridge,The name of the Open vSwitch bridge that is used with linuxnet when connecting with Open vSwitch.',0,0,others,nova
6868,live_migration_bandwidth,Maximum bandwidth(in MiB/s) to be used during migration.,1,1,resource,nova
6869,live_migration_completion_timeout,"Time to wait, in seconds, for migration to successfully complete transferring data before aborting the operation.",0,0,others,nova
6871,live_migration_downtime_delay,"Time to wait, in seconds, between each step increase of the migration downtime.",0,0,others,nova
6872,live_migration_downtime_steps,Number of incremental steps to reach max downtime value.,0,0,others,nova
6873,live_migration_inbound_addr,Target used for live migration traffic.,0,0,others,nova
6874,live_migration_permit_auto_converge,This option allows nova to start live migration with auto converge on.,0,0,others,nova
6875,live_migration_permit_post_copy,"This option allows nova to switch an on-going live migration to post-copy mode, i.e., switch the active VM to the one on the destination node before the migration is complete, therefore ensuring an upper bound on the memory that needs to be transferred. Post-copy requires libvirt>=1.3.3 and QEMU>=2.5.0.",0,0,others,nova
6876,live_migration_retry_count,Maximum number of 1 second retries in live_migration. It specifies number of retries to iptables when it complains. It happens when an user continuously sends live-migration request to same host leading to concurrent request to iptables.,0,0,others,nova
6877,live_migration_scheme,URI scheme used for live migration.,0,0,others,nova
6878,live_migration_timeout_action,"This option will be used to determine what action will be taken against a VM after live_migration_completion_timeout expires. By default, the live migrate operation will be aborted after completion timeout. If it is set to force_complete, the compute service will either pause the VM or trigger post-copy depending on if post copy is enabled and available (live_migration_permit_post_copy is set to True).",0,0,others,nova
6879,live_migration_tunnelled,Enable tunnelled migration.,1,6,function-tradeoff,nova
6880,live_migration_uri,Live migration target URI to use.,0,0,others,nova
6881,live_migration_wait_for_vif_plug,Determine if the source compute host should wait for a network-vif-plugged event from the (neutron) networking service before starting the actual transfer of the guest to the destination compute host.,0,0,others,nova
6882,live_migration_with_native_tls,Use QEMU-native TLS encryption when live migrating.,1,2,security-tradeoff,nova
6883,local_metadata_per_cell,"Indicates that the nova-metadata API service has been deployed per-cell, so that we can have better performance and data isolation in a multi-cell deployment. Users should consider the use of this configuration depending on how neutron is setup. If you have networks that span cells, you might need to run nova-metadata API service globally. If your networks are segmented along cell boundaries, then you can run nova-metadata API service per cell. When running nova-metadata API service per cell, you should also configure each Neutron metadata-agent to point to the corresponding nova-metadata API service.",0,0,others,nova
6885,log_config_append,"The name of a logging configuration file. This file is appended to any existing logging configuration files. For details about logging configuration files, see the Python logging module documentation. Note that when logging configuration files are used then all logging configuration is set in the configuration file and other logging configuration options are ignored (for example, log-date-format).",0,0,others,nova
6886,log_date_format,Defines the format string for %(asctime)s in log records. Default: the value above . This option is ignored if log_config_append is set.,0,0,others,nova
6887,log_dir,(Optional) The base directory used for relative log_file paths. This option is ignored if log_config_append is set.,0,0,others,nova
6888,log_file,"(Optional) Name of log file to send logging output to. If no default is set, logging will go to stderr as defined by use_stderr. This option is ignored if log_config_append is set.",0,0,others,nova
6889,log_options,Enables or disables logging values of all registered options when starting a service (at DEBUG level).,1,6,function-tradeoff,nova
6890,log_rotate_interval,The amount of time before the log files are rotated. This option is ignored unless log_rotation_type is setto 'interval'.,0,0,others,nova
6891,log_rotate_interval_type,Rotation interval type. The time of the last file change (or the time when the service was started) is used when scheduling the next rotation.,0,0,others,nova
6892,log_rotation_type,Log rotation type.,0,0,others,nova
6893,logging_context_format_string,Format string to use for log messages with context. Used by oslo_log.formatters.ContextFormatter,0,0,others,nova
6894,logging_debug_format_suffix,Additional data to append to log message when logging level for the message is DEBUG. Used by oslo_log.formatters.ContextFormatter,1,6,function-tradeoff,nova
6895,logging_default_format_string,Format string to use for log messages when context is undefined. Used by oslo_log.formatters.ContextFormatter,0,0,others,nova
6896,logging_exception_prefix,Prefix each line of exception output with this format. Used by oslo_log.formatters.ContextFormatter,0,0,others,nova
6898,login_timeout,Timeout in seconds for XenAPI login.,0,0,others,nova
6900,max_age,Maximum cache age of CORS preflight requests.,1,5,workload-specific,nova
6901,max_attempts,"This is the maximum number of attempts that will be made for a given instance build/move operation. It limits the number of alternate hosts returned by the scheduler. When that list of hosts is exhausted, a MaxRetriesExceeded exception is raised and the instance is set to an error state.",0,0,others,nova
6902,max_concurrent_builds,"Limits the maximum number of instance builds to run concurrently by nova-compute. Compute service can attempt to build an infinite number of instances, if asked to do so. This limit is enforced to avoid building unlimited instance concurrently on a compute node. This value can be set per compute node.",1,1,resource,nova
6903,max_concurrent_disk_ops,"Number of concurrent disk-IO-intensive operations (glance image downloads, image format conversions, etc.) that we will do in parallel. If this is set too high then response time suffers. The default value of 0 means no limit.",1,1,resource,nova
6904,max_concurrent_live_migrations,Maximum number of live migrations to run concurrently. This limit is enforced to avoid outbound live migrations overwhelming the host/network and causing failures. It is not recommended that you change this unless you are very sure that doing so is safe and stable in your environment.,1,1,resource,nova
6905,max_disk_devices_to_attach,"Maximum number of disk devices allowed to attach to a single server. Note that the number of disks supported by an server depends on the bus used. For example, the ide disk bus is limited to 4 attached devices. The configured maximum is enforced during server create, rebuild, evacuate, unshelve, live migrate, and attach volume.",1,1,resource,nova
6906,max_header_line,This option specifies the maximum line size of message headers to be accepted. max_header_line may need to be increased when using large tokens (typically those generated by the Keystone v3 API with big service catalogs).,0,0,others,nova
6908,max_instances_per_host,Maximum number of instances that be active on a host.,1,1,resource,nova
6909,max_io_ops_per_host,The number of instances that can be actively performing IO on a host.,1,1,resource,nova
6911,max_limit,"As a query can potentially return many thousands of items, you can limit the maximum number of items in a single response by setting this option.",1,5,workload-specific,nova
6912,max_local_block_devices,Maximum number of devices that will result in a local image being created on the hypervisor node.,1,1,resource,nova
6913,max_logfile_count,Maximum number of rotated log files.,0,0,others,nova
6914,max_logfile_size_mb,Log file maximum size in MB. This option is ignored if 'log_rotation_type' is not set to 'size'.,0,0,others,nova
6915,max_overflow,"If set, use this value for max_overflow with SQLAlchemy.",0,0,others,nova
6917,max_poll_records,The maximum number of records returned in a poll call,1,1,resource,nova
6918,max_pool_size,Maximum number of SQL connections to keep open in a pool. Setting a value of 0 indicates no limit.,1,1,resource,nova
6919,max_request_body_size,"The maximum body size for each request, in bytes.",1,1,resource,nova
6920,max_retries,Maximum number of database connection retries during startup. Set to -1 to specify an infinite retry count.,0,0,others,nova
6921,maximum_instance_delete_attempts,The number of times to attempt to reap an instance's files.,0,0,others,nova
6922,maximum_objects,This option specifies the limit on the maximum number of objects to return in a single result.,0,0,others,nova
6923,mem_stats_period_seconds,A number of seconds to memory usage statistics period. Zero or negative value mean to disable memory usage statistics.,0,0,others,nova
6926,memcache_pool_connection_get_timeout,Number of seconds that an operation will wait to get a memcache client connection.,0,0,others,nova
6927,memcache_pool_dead_retry,(Optional) Number of seconds memcached server is considered dead before it is tried again.,0,0,others,nova
6928,memcache_pool_maxsize.B,(Optional) Maximum total number of open connections to every memcached server.,1,1,resource,nova
6929,memcache_pool_maxsize,Max total number of open connections to every memcached server. (oslo_cache.memcache_pool backend only).,1,1,resource,nova
6930,memcache_pool_socket_timeout,(Optional) Socket timeout in seconds for communicating with a memcached server.,0,0,others,nova
6931,memcache_pool_unused_timeout.B,(Optional) Number of seconds a connection to memcached is held unused in the pool before it is closed.,0,0,others,nova
6932,memcache_pool_unused_timeout,Number of seconds a connection to memcached is held unused in the pool before it is closed. (oslo_cache.memcache_pool backend only).,0,0,others,nova
6933,memcache_secret_key,"(Optional, mandatory if memcache_security_strategy is defined) This string is used for key derivation.",0,0,others,nova
6934,memcache_security_strategy,"(Optional) If defined, indicate whether token data should be authenticated or authenticated and encrypted. If MAC, token data is authenticated (with HMAC) in the cache. If ENCRYPT, token data is encrypted and authenticated in the cache. If the value is not one of these options or empty, auth_token will raise an exception on initialization.",1,2,security-tradeoff,nova
6936,memcache_socket_timeout,Timeout in seconds for every call to a server. (dogpile.cache.memcache and oslo_cache.memcache_pool backends only).,0,0,others,nova
6937,memcache_use_advanced_pool,(Optional) Use the advanced (eventlet safe) memcached client pool. The advanced pool will only work under python 2.x.,0,0,others,nova
6938,memcached_servers,"Optionally specify a list of memcached server(s) to use for caching. If left undefined, tokens will instead be cached in-process.",0,0,others,nova
6939,metadata_cache_expiration,"This option is the time (in seconds) to cache metadata. When set to 0, metadata caching is disabled entirely; this is generally not recommended for performance reasons. Increasing this setting should improve response times of the metadata API when under heavy load. Higher values may increase memory usage, and result in longer times for host metadata changes to take effect.",1,5,workload-specific,nova
6940,metadata_host,This option determines the IP address for the network metadata API server.,0,0,others,nova
6941,metadata_items,The number of metadata items allowed per instance.,0,0,others,nova
6942,metadata_listen,IP address on which the metadata API will listen.,0,0,others,nova
6943,metadata_listen_port,Port on which the metadata API will listen.,0,0,others,nova
6944,metadata_port,This option determines the port used for the metadata API server.,0,0,others,nova
6945,metadata_proxy_shared_secret,"This option holds the shared secret string used to validate proxy requests to Neutron metadata requests. In order to be used, the 'X-Metadata-Provider-Signature' header must be supplied in the request.",1,2,security-tradeoff,nova
6946,metadata_workers,Number of workers for metadata service. If not specified the number of available CPUs will be used.,1,1,resource,nova
6948,min_pool_size,Minimum number of SQL connections to keep open in a pool.,1,1,resource,nova
6949,mkisofs_cmd,Name or path of the tool used for ISO image creation,0,0,others,nova
6950,mksproxy_base_url,Location of MKS web console proxy,0,0,others,nova
6951,mounted_disk_query_retry_count,Mounted disk query retry count,0,0,others,nova
6952,mounted_disk_query_retry_interval,Mounted disk query retry interval,0,0,others,nova
6953,multi_host,Default value for multi_host in networks.,0,0,others,nova
6954,multicast_address,Appended to the address prefix when sending a fanout message. Used by the message bus to identify fanout messages.,0,0,others,nova
6955,mute_child_interval,Mute child interval.,0,0,others,nova
6956,mute_weight_multiplier,Mute weight multiplier.,0,0,others,nova
6957,my_block_storage_ip,The IP address which is used to connect to the block storage network.,0,0,others,nova
6959,mysql_enable_ndb,"If True, transparently enables support for handling MySQL Cluster (NDB).",0,0,others,nova
6960,mysql_sql_mode,"The SQL mode to be used for MySQL sessions. This option, including the default, overrides any server-set SQL mode. To use whatever SQL mode is set by the server configuration, set this to no value. Example: mysql_sql_mode=",1,6,function-tradeoff,nova
6961,name,Name of the current cell.,0,0,others,nova
6962,network,Network RPC API version cap.,0,0,others,nova
6963,network_allocate_retries,Number of times to retry network allocation. It is required to attempt network allocation retries if the virtual interface plug fails.,0,0,others,nova
6964,network_driver,Driver to use for network creation.,0,0,others,nova
6965,network_manager,Full class name for the Manager for network,0,0,others,nova
6967,networks_path,The location where the network configuration files will be kept. The default is the 'networks' directory off of the location where nova's Python module is installed.,0,0,others,nova
6968,neutron_default_tenant_id,Tenant ID for getting the default network from Neutron API (also referred in some places as the 'project ID') to use.,0,0,others,nova
6969,nfs_mount_options,Mount options passed to the NFS client. See section of the nfs man page for details.,0,0,others,nova
6970,nfs_mount_point_base,Directory where the NFS volume is mounted on the compute node. The default is 'mnt' directory of the location where nova's Python module is installed.,0,0,others,nova
6972,notification_format,Specifies which notification format shall be used by nova.,0,0,others,nova
6973,notify_address_prefix,Address prefix for all generated Notification addresses,0,0,others,nova
6974,notify_on_state_change,"If set, send compute.instance.update notifications on instance state changes.",0,0,others,nova
6975,notify_server_credit,Window size for incoming Notification messages,1,1,resource,nova
6976,novncproxy_base_url,Public address of noVNC VNC console proxy.,0,0,others,nova
6977,novncproxy_host,IP address that the noVNC console proxy should bind to.,0,0,others,nova
6978,novncproxy_port,Port that the noVNC console proxy should bind to.,0,0,others,nova
6979,num_aoe_discover_tries,Number of times to rediscover AoE target to find volume.,0,0,others,nova
6980,num_iser_scan_tries,Number of times to scan iSER target to find volume.,0,0,others,nova
6981,num_networks,"This option represents the number of networks to create if not explicitly specified when the network is created. The only time this is used is if a CIDR is specified, but an explicit network_size is not. In that case, the subnets are created by diving the IP address space of the CIDR by num_networks. The resulting subnet sizes cannot be larger than the configuration option network_size; in that event, they are reduced to network_size, and a warning is logged.",0,0,others,nova
6982,num_nvme_discover_tries,Number of times to rediscover NVMe target to find volume,0,0,others,nova
6983,num_pcie_ports,The number of PCIe ports an instance will get.,0,0,others,nova
6985,num_vbd_unplug_retries,"Maximum number of retries to unplug VBD. If set to 0, should try once, no retries.",0,0,others,nova
6986,num_volume_scan_tries,Number of times to scan given storage protocol to find volume.,0,0,others,nova
6989,os_region_name,Region name of this node. This is used when picking the URL in the service catalog.,0,0,others,nova
6990,osapi_compute_listen,IP address on which the OpenStack API will listen.,0,0,others,nova
6991,osapi_compute_listen_port,Port on which the OpenStack API will listen.,0,0,others,nova
6992,osapi_compute_unique_server_name_scope,Sets the scope of the check for unique instance names.,1,6,function-tradeoff,nova
6993,osapi_compute_workers,Number of workers for OpenStack API service. The default will be the number of CPUs available.,1,1,resource,nova
6994,ovs_bridge,Default name for the Open vSwitch integration bridge.,0,0,others,nova
6995,ovs_integration_bridge,The name of the integration Bridge that is used with xenapi when connecting with Open vSwitch.,0,0,others,nova
6996,ovs_vsctl_timeout,"This option represents the period of time, in seconds, that the ovs_vsctl calls will wait for a response from the database before timing out. A setting of 0 means that the utility should wait forever for a response.",0,0,others,nova
6998,passthrough_whitelist,White list of PCI devices available to VMs.,0,0,others,nova
6999,password.B,Password for authentication. Required for 'password' and 'keystone_password' auth_type.,0,0,others,nova
7000,password,User's password,0,0,others,nova
7001,password_length,Length of generated instance admin passwords.,0,0,others,nova
7002,path,The path to respond to healtcheck requests on.,0,0,others,nova
7003,pbm_default_policy,This option specifies the default policy to be used.,1,6,function-tradeoff,nova
7004,pbm_enabled,This option enables or disables storage policy based placement of instances.,1,6,function-tradeoff,nova
7005,pbm_wsdl_location,This option specifies the PBM service WSDL file location URL.,0,0,others,nova
7006,pci_weight_multiplier,PCI device affinity weight multiplier.,0,0,others,nova
7007,peer_list,"List of hostnames for all nova-compute services (including this host) with this partition_key config value. Nodes matching the partition_key value will be distributed between all services specified here. If partition_key is unset, this option is ignored.",0,0,others,nova
7008,periodic_enable,Enable periodic tasks.,1,6,function-tradeoff,nova
7009,periodic_fuzzy_delay,Number of seconds to randomly delay when starting the periodic task scheduler to reduce stampeding.,1,5,workload-specific,nova
7010,periodic_task_interval,Periodic task interval.,1,3,reliability-tradeoff,nova
7011,physnets,List of physnets present on this host.,0,0,others,nova
7012,placement_aggregate_required_for_tenants,"This setting, when limit_tenants_to_placement_aggregate=True, will control whether or not a tenant with no aggregate affinity will be allowed to schedule to any available node. If aggregates are used to limit some tenants but not all, then this should be False. If all tenants should be confined via aggregate, then this should be True to prevent them from receiving unrestricted scheduling to any available node.",1,4,limited-side-effect,nova
7013,pointer_model,Generic property to specify the pointer type.,0,0,others,nova
7014,policy_default_rule,Default rule. Enforced when a requested rule is not found.,0,0,others,nova
7015,policy_dirs,"Directories where policy configuration files are stored. They can be relative to any directory in the search path defined by the config_dir option, or absolute paths. The file defined by policy_file must exist for these directories to be searched. Missing or empty directories are ignored.",0,0,others,nova
7016,policy_file.B,The file that defines placement policies. This can be an absolute path or relative to the configuration file.,0,0,others,nova
7017,policy_file,The file that defines policies.,0,0,others,nova
7018,pool_size,Pool Size for Kafka Consumers,1,1,resource,nova
7019,pool_timeout,"If set, use this value for pool_timeout with SQLAlchemy.",0,0,others,nova
7020,port,Debug port to connect to. This command line parameter allows you to specify the port you want to use to connect to a nova service via a debugger running on different host.,0,0,others,nova
7021,port_range,A range of TCP ports a guest can use for its backend.,0,0,others,nova
7022,power_state_check_timeframe,Power state check timeframe,0,0,others,nova
7023,power_state_event_polling_interval,Power state event polling interval,0,0,others,nova
7026,proc_units_factor,"Factor used to calculate the amount of physical processor compute power given to each vCPU. E.g. A value of 1.0 means a whole physical processor, whereas 0.05 means 1/20th of a physical processor.",1,1,resource,nova
7027,producer_batch_size,Size of batch for the producer async send,1,5,workload-specific,nova
7028,producer_batch_timeout,Upper bound on the delay for KafkaProducer batching in seconds,0,0,others,nova
7029,project_domain_id.B,Domain ID containing project,0,0,others,nova
7030,project_domain_id,Project's domain ID for project. Optional for 'keystone_token' and 'keystone_password' auth_type.,0,0,others,nova
7031,project_domain_name.B,Domain name containing project,0,0,others,nova
7032,project_domain_name,Project's domain name for project. Optional for 'keystone_token' and 'keystone_password' auth_type.,0,0,others,nova
7033,project_id.B,Project ID for project scoping. Optional for 'keystone_token' and 'keystone_password' auth_type.,0,0,others,nova
7034,project_id,Project ID to scope to,0,0,others,nova
7035,project_id_regex,"This option is a string representing a regular expression (regex) that matches the project_id as contained in URLs. If not set, it will match normal UUIDs created by keystone.",0,0,others,nova
7036,project_name.B,Project name for project scoping. Optional for 'keystone_token' and 'keystone_password' auth_type.,0,0,others,nova
7037,project_name,Project name to scope to,0,0,others,nova
7039,proxyclient_address,The IP address to which proxy clients (like nova-serialproxy) should connect to get the serial console of an instance.,0,0,others,nova
7040,pseudo_vhost,"Enable virtual host support for those message buses that do not natively support virtual hosting (such as qpidd). When set to true the virtual host name will be added to all message bus addresses, effectively creating a private 'subnet' per virtual host. Set to False if the message bus supports virtual hosting using the 'hostname' field in the AMQP 1.0 Open performative as the name of the virtual host.",0,0,others,nova
7041,public_interface,This is the name of the network interface for public IP addresses. The default is 'eth0'.,0,0,others,nova
7042,publish_errors,Enables or disables publication of error events.,1,6,function-tradeoff,nova
7045,query_placement_for_availability_zone,"This setting causes the scheduler to look up a host aggregate with the metadata key of availability_zone set to the value provided by an incoming request, and request results from placement be limited to that aggregate.",0,0,others,nova
7046,quobyte_client_cfg,Path to a Quobyte Client configuration file.,0,0,others,nova
7047,quobyte_mount_point_base,Directory where the Quobyte volume is mounted on the compute node.,0,0,others,nova
7048,quota_networks,This option controls the number of private networks that can be created per project (or per tenant).,0,0,others,nova
7049,rabbit_ha_queues,"Try to use HA queues in RabbitMQ (x-ha-policy: all). If you change this option, you must wipe the RabbitMQ database. In RabbitMQ 3.0, queue mirroring is no longer controlled by the x-ha-policy argument when declaring a queue. If you just want to make sure that all queues (except those with auto-generated names) are mirrored across all nodes, run: 'rabbitmqctl set_policy HA '^(?!amq.).*' '{'ha-mode': 'all'}' '",0,0,others,nova
7051,rabbit_login_method,The RabbitMQ login method.,0,0,others,nova
7053,rabbit_retry_backoff,How long to backoff for between retries when connecting to RabbitMQ.,0,0,others,nova
7054,rabbit_retry_interval,How frequently to retry connecting with RabbitMQ.,0,0,others,nova
7055,rabbit_transient_queues_ttl,Positive integer representing duration in seconds for queue TTL (x-expires). Queues which are unused for the duration of the TTL are automatically deleted. The parameter affects only reply and fanout queues.,0,0,others,nova
7056,ram,The number of megabytes of instance RAM allowed per project.,1,1,resource,nova
7057,ram_allocation_ratio,This option helps you specify virtual RAM to physical RAM allocation ratio.,1,5,workload-specific,nova
7058,ram_weight_multiplier.B,RAM weight multipler ratio.,1,5,workload-specific,nova
7059,ram_weight_multiplier,Ram weight multiplier.,0,0,others,nova
7060,randomize_allocation_candidates,"If True, when limiting allocation candidate results, the results will be a random sampling of the full result set. If False, allocation candidates are returned in a deterministic but undefined order. That is, all things being equal, two requests for allocation candidates will return the same results in the same order; but no guarantees are made as to how that order is determined.",0,0,others,nova
7062,rate_limit_except_level,"Log level name used by rate limiting: CRITICAL, ERROR, INFO, WARNING, DEBUG or empty string. Logs with level greater or equal to rate_limit_except_level are not filtered. An empty string means that all levels are filtered.",1,6,function-tradeoff,nova
7064,rbd_secret_uuid,The libvirt UUID of the secret for the rbd_user volumes.,0,0,others,nova
7065,rbd_user,The RADOS client name for accessing rbd(RADOS Block Devices) volumes.,0,0,others,nova
7066,reachable_timeout,Timeout (seconds) to wait for an instance to start.,0,0,others,nova
7068,reauthenticate,Allow fetching a new token if the current one is going to expire. Optional for 'keystone_token' and 'keystone_password' auth_type.,1,6,function-tradeoff,nova
7070,recheck_quota,Recheck quota after resource creation to prevent allowing quota to be exceeded.,1,6,function-tradeoff,nova
7071,reclaim_instance_interval,Interval for reclaiming deleted instances.,0,0,others,nova
7072,record,"Filename that will be used for storing websocket frames received and sent by a proxy service (like VNC, spice, serial) running on this host. If this is not set, no recording will be done.",0,0,others,nova
7073,region_name.B,The default region_name for endpoint URL discovery.,0,0,others,nova
7074,region_name,The region in which the identity server can be found.,0,0,others,nova
7075,remote_content_type,Content Type to send and receive data for REST based policy check,0,0,others,nova
7076,remote_filesystem_transport,libvirt's transport method for remote file operations.,0,0,others,nova
7077,remote_ssl_ca_crt_file,Absolute path to ca cert file for REST based policy check,0,0,others,nova
7078,remote_ssl_client_crt_file,Absolute path to client cert for REST based policy check,0,0,others,nova
7079,remote_ssl_client_key_file,Absolute path client key file REST based policy check,0,0,others,nova
7080,remote_ssl_verify_server_crt,server identity verification for REST based policy check,1,2,security-tradeoff,nova
7081,remove_unused_base_images,Should unused base images be removed?,1,4,limited-side-effect,nova
7082,remove_unused_original_minimum_age_seconds,Unused unresized base images younger than this will not be removed.,0,0,others,nova
7083,remove_unused_resized_minimum_age_seconds,Unused resized base images younger than this will not be removed,0,0,others,nova
7084,reply_link_credit,Window size for incoming RPC Reply messages.,1,1,resource,nova
7085,report_interval,Number of seconds indicating how frequently the state of services on a given hypervisor is reported. Nova needs to know this to determine the overall health of the deployment.,0,0,others,nova
7086,required,"This setting determines how any unavailable metrics are treated. If this option is set to True, any hosts for which a metric is unavailable will raise an exception, so it is recommended to also use the MetricFilter to filter out those hosts before weighing.",0,0,others,nova
7087,rescue_image_id,The ID of the image to boot from to rescue data from a corrupted instance.,0,0,others,nova
7088,rescue_kernel_id,The ID of the kernel (AKI) image to use with the rescue image.,0,0,others,nova
7089,rescue_ramdisk_id,The ID of the RAM disk (ARI) image to use with the rescue image.,0,0,others,nova
7090,rescue_timeout,Interval to wait before un-rescuing an instance stuck in RESCUE.,0,0,others,nova
7091,reserve_percent,Reserve percentage,1,5,workload-specific,nova
7092,reserved_host_cpus,"Number of physical CPUs to reserve for the host. The host resources usage is reported back to the scheduler continuously from nova-compute running on the compute node. To prevent the host CPU from being considered as available, this option is used to reserve random pCPU(s) for the host.",1,1,resource,nova
7094,reserved_host_memory_mb,"Amount of memory in MB to reserve for the host so that it is always available to host processes. The host resources usage is reported back to the scheduler continuously from nova-compute running on the compute node. To prevent the host memory from being considered as available, this option is used to reserve memory for the host.",1,1,resource,nova
7095,reserved_huge_pages,Number of huge/large memory pages to reserved per NUMA host cell.,1,1,resource,nova
7096,resize_confirm_window,Automatically confirm resizes after N seconds.,0,0,others,nova
7097,resize_fs_using_block_device,Enable resizing of filesystems via a block device.,1,5,workload-specific,nova
7098,resource_provider_association_refresh,"Interval for updating nova-compute-side cache of the compute node resource provider's inventories, aggregates, and traits.",1,5,workload-specific,nova
7099,restrict_isolated_hosts_to_isolated_images,Prevent non-isolated images from being built on isolated hosts.,0,0,others,nova
7100,resume_guests_state_on_host_boot,This option specifies whether to start guests that were running before the host rebooted. It ensures that all of the instances on a Nova compute node resume their state each time the compute node boots or restarts.,1,6,function-tradeoff,nova
7101,retry,"The maximum number of attempts to re-send a notification message which failed to be delivered due to a recoverable error. 0 - No retry, -1 - indefinite",0,0,others,nova
7102,retry_delay,Number of seconds to wait before retrying poll for key creation completion,0,0,others,nova
7103,retry_interval,Interval between retries of opening a SQL connection.,0,0,others,nova
7104,rng_dev_path,"The path to an RNG (Random Number Generator) device that will be used as the source of entropy on the host. Since libvirt 1.3.4, any path (that returns random numbers when read) is accepted. The recommended source of entropy is /dev/urandom -- it is non-blocking, therefore relatively fast; and avoids the limitations of /dev/random, which is a legacy interface. For more details (and comparision between different RNG sources), refer to the 'Usage' section in the Linux kernel API documentation for [u]random: http://man7.org/linux/man-pages/man4/urandom.4.html and http://man7.org/linux/man-pages/man7/random.7.html.",0,0,others,nova
7105,root_token_id,root token for vault,0,0,others,nova
7106,rootwrap_config,Path to the rootwrap configuration file.,0,0,others,nova
7107,routing_source_ip,The public IP address of the network host.,0,0,others,nova
7108,rpc_address_prefix,Address prefix for all generated RPC addresses,0,0,others,nova
7109,rpc_conn_pool_size,Size of RPC connection pool.,1,1,resource,nova
7110,rpc_driver_queue_base,RPC driver queue base.,0,0,others,nova
7111,rpc_response_timeout,Seconds to wait for a response from a call.,0,0,others,nova
7112,rpc_server_credit,Window size for incoming RPC Request messages,1,1,resource,nova
7113,run_external_periodic_tasks,Some periodic tasks can be run in a separate process. Should we run them here,0,0,others,nova
7115,running_deleted_instance_poll_interval,"Time interval in seconds to wait between runs for the clean up action. If set to 0, above check will be disabled. If 'running_deleted_instance _action' is set to 'log' or 'reap', a value greater than 0 must be set.",0,0,others,nova
7116,running_deleted_instance_timeout,Time interval in seconds to wait for the instances that have been marked as deleted in database to be eligible for cleanup.,1,3,reliability-tradeoff,nova
7118,rx_queue_size,Configure virtio rx queue size.,1,1,resource,nova
7119,sasl_config_dir,Path to directory that contains the SASL configuration,0,0,others,nova
7120,sasl_config_name,Name of configuration file (without .conf suffix),0,0,others,nova
7121,sasl_default_realm,SASL realm to use if no realm present in username,0,0,others,nova
7123,sasl_mechanisms,Space separated list of acceptable SASL mechanisms,0,0,others,nova
7124,scheduler.B,Cells scheduler.,0,0,others,nova
7125,scheduler,Scheduler RPC API version cap.,0,0,others,nova
7126,scheduler_filter_classes,Scheduler filter classes.,0,0,others,nova
7127,scheduler_instance_sync_interval,Interval between sending the scheduler a list of current instance UUIDs to verify that its view of instances is in sync with nova.,1,3,reliability-tradeoff,nova
7129,scheduler_retry_delay,Scheduler retry delay.,0,0,others,nova
7130,scheduler_weight_classes,Scheduler weight classes.,0,0,others,nova
7131,secure_proxy_ssl_header.B,"The HTTP Header that will be used to determine what the original request protocol scheme was, even if it was hidden by a SSL termination proxy.",0,0,others,nova
7132,secure_proxy_ssl_header,"This option specifies the HTTP header used to determine the protocol scheme for the original request, even if it was removed by a SSL terminating proxy.",0,0,others,nova
7133,security_group_rules,The number of security rules per security group.,0,0,others,nova
7134,security_groups,The number of security groups per project.,0,0,others,nova
7135,security_protocol,Protocol used to communicate with brokers,0,0,others,nova
7137,send_arp_for_ha_count,"When arp messages are configured to be sent, they will be sent with the count set to the value of this option. Of course, if this is set to zero, no arp messages will be sent.",0,0,others,nova
7139,sentinel_service_name,Redissentinel uses a service name to identify a master redis service. This parameter defines the name (for example: sentinal_service_name=mymaster).,0,0,others,nova
7141,serial_log_dir,Specifies the directory where the Virtual Serial Port Concentrator is storing console log files. It should match the 'serial_log_dir' config value of VSPC.,0,0,others,nova
7142,serial_port_proxy_uri,Identifies a proxy service that provides network access to the serial_port_service_uri.,0,0,others,nova
7143,serial_port_service_uri,Identifies the remote system where the serial port traffic will be sent.,0,0,others,nova
7144,serialproxy_host,The IP address which is used by the nova-serialproxy service to listen for incoming requests.,0,0,others,nova
7145,serialproxy_port,The port number which is used by the nova-serialproxy service to listen for incoming requests.,0,0,others,nova
7146,server_group_members,The maximum number of servers per server group.,0,0,others,nova
7148,server_listen.B,The address where the SPICE server running on the instances should listen.,0,0,others,nova
7149,server_listen,The IP address or hostname on which an instance should listen to for incoming VNC connection requests on this node.,0,0,others,nova
7150,server_proxyclient_address.B,"Private, internal IP address or hostname of VNC console proxy.",0,0,others,nova
7151,server_proxyclient_address,The address used by nova-spicehtml5proxy client to connect to instance console.,0,0,others,nova
7152,server_request_prefix,address prefix used when sending to a specific server,0,0,others,nova
7153,service_down_time,Maximum time in seconds since last check-in for up service,0,0,others,nova
7154,service_metadata_proxy,"When set to True, this option indicates that Neutron will be used to proxy metadata requests and resolve instance ids. Otherwise, the instance ID must be passed to the metadata request in the 'X-Instance-ID' header.",0,0,others,nova
7155,service_name,The default service_name for endpoint URL discovery.,0,0,others,nova
7156,service_token_roles,A choice of roles that must be present in a service token. Service tokens are allowed to request that an expired token can be used and so this check should tightly control that only actual services should be sending this token. Roles here are applied as an ANY check so any role in this list must be present. For backwards compatibility reasons this currently only affects the allow_expired check.,0,0,others,nova
7157,service_token_roles_required,For backwards compatibility reasons we must let valid service tokens pass that don't pass the service_token_roles check as valid. Setting this true will become the default in a future release and should be enabled if possible.,0,0,others,nova
7158,service_type,The default service_type for endpoint URL discovery.,0,0,others,nova
7159,servicegroup_driver,This option specifies the driver to be used for the servicegroup service.,0,0,others,nova
7160,share_dhcp_address,THIS VALUE SHOULD BE SET WHEN CREATING THE NETWORK.,0,0,others,nova
7161,shelved_offload_time,Time before a shelved instance is eligible for removal from a host.,0,0,others,nova
7162,shelved_poll_interval,Interval for polling shelved instances to offload.,0,0,others,nova
7163,shuffle_best_same_weighed_hosts,Enable spreading the instances between hosts with the same best weight.,0,0,others,nova
7164,shutdown_retry_interval,Time to wait in seconds before resending an ACPI shutdown signal to instances.,0,0,others,nova
7166,signing_dir,Directory used to cache files related to PKI tokens. This option has been deprecated in the Ocata release and will be removed in the P release.,0,0,others,nova
7167,slave_connection,The SQLAlchemy connection string to use to connect to the slave database.,0,0,others,nova
7168,smbfs_mount_options,Mount options passed to the SMBFS client.,0,0,others,nova
7169,smbfs_mount_point_base,Directory where the SMBFS shares are mounted on the compute node.,0,0,others,nova
7170,snapshot_compression,Enable snapshot compression for qcow2 images.,1,4,limited-side-effect,nova
7171,snapshot_image_format,Determine the snapshot image format when sending to the image service.,0,0,others,nova
7172,snapshots_directory,Location where libvirt driver will store snapshots before uploading them to image service,0,0,others,nova
7173,socket_timeout,Redissentinel provides a timeout option on the connections. This parameter defines that timeout (for example: socket_timeout=0.1).,0,0,others,nova
7174,soft_affinity_weight_multiplier,Multiplier used for weighing hosts for group soft-affinity.,0,0,others,nova
7175,soft_anti_affinity_weight_multiplier,Multiplier used for weighing hosts for group soft-anti-affinity.,0,0,others,nova
7176,source_is_ipv6,Set to True if source host is addressed with IPv6.,0,0,others,nova
7177,sparse_copy,Whether to use sparse_copy for copying data on a resize down. (False will use standard dd). This speeds up resizes down considerably since large runs of zeros won't have to be rsynced.,1,4,limited-side-effect,nova
7178,sparse_logical_volumes,Create sparse logical volumes (with virtualsize) if this flag is set to True.,1,4,limited-side-effect,nova
7181,sr_base_path,Base path to the storage repository on the XenServer host.,0,0,others,nova
7185,ssl_ca_crt_file,Absolute path to ca cert file,0,0,others,nova
7186,ssl_ca_file.B,CA certificate PEM file used to verify the server's certificate,0,0,others,nova
7187,ssl_ca_file.C,SSL certification authority file (valid only if SSL enabled).,0,0,others,nova
7188,ssl_ca_file,This option allows setting path to the CA certificate file that should be used to verify connecting clients.,0,0,others,nova
7190,ssl_cert_file.B,Self-identifying certificate PEM file for client authentication,0,0,others,nova
7191,ssl_cert_file.C,SSL cert file (valid only if SSL enabled).,0,0,others,nova
7192,ssl_cert_file,This option allows setting path to the SSL certificate of API server.,0,0,others,nova
7193,ssl_key_file.B,SSL key file (valid only if SSL enabled).,0,0,others,nova
7194,ssl_key_file.C,This option specifies the path to the file where SSL private key of API server is stored when SSL is in effect.,0,0,others,nova
7195,ssl_key_file,Private key PEM file used to sign ssl_cert_file certificate (optional),0,0,others,nova
7196,ssl_key_password,Password for decrypting ssl_key_file (if encrypted),0,0,others,nova
7197,ssl_only,Disallow non-encrypted connections.,1,2,security-tradeoff,nova
7199,ssl_version,"SSL version to use (valid only if SSL enabled). Valid values are TLSv1 and SSLv23. SSLv2, SSLv3, TLSv1_1, and TLSv1_2 may be available on some distributions.",0,0,others,nova
7200,state_path,The top-level directory for maintaining Nova's state.,0,0,others,nova
7202,sync_power_state_pool_size,Number of greenthreads available for use to sync power states.,1,1,resource,nova
7203,sysinfo_serial,"The data source used to the populate the host 'serial' UUID exposed to guest in the virtual BIOS. All choices except unique will change the serial when migrating the instance to another host. Changing the choice of this option will also affect existing instances on this host once they are stopped and started again. It is recommended to use the default choice (unique) since that will not change when an instance is migrated. However, if you have a need for per-host serials in addition to per-instance serial numbers, then consider restricting flavors via host aggregates.",0,0,others,nova
7204,syslog_log_facility,Syslog facility to receive log lines. This option is ignored if log_config_append is set.,0,0,others,nova
7205,system_scope,Scope for system operations,0,0,others,nova
7206,target_host,The iSCSI Target Host.,0,0,others,nova
7207,target_port,The iSCSI Target Port.,0,0,others,nova
7208,task_poll_interval,Time interval in seconds to poll remote tasks invoked on VMware VC server.,0,0,others,nova
7209,tcp_keepidle,This option sets the value of TCP_KEEPIDLE in seconds for each server socket. It specifies the duration of time to keep connection active. TCP generates a KEEPALIVE transmission for an application that requests to keep connection active. Not supported on OS X.,0,0,others,nova
7210,teardown_unused_network_gateway,"Determines whether unused gateway devices, both VLAN and bridge, are deleted if the network is in nova-network VLAN mode and is multi-hosted.",1,4,limited-side-effect,nova
7211,tempdir,Explicitly specify the temporary working directory.,0,0,others,nova
7212,tenant_id,Tenant ID,0,0,others,nova
7213,tenant_name,Tenant Name,0,0,others,nova
7214,thread_pool_size,The number of threads available for privsep to concurrently run processes. Defaults to the number of CPU cores in the system.,1,1,resource,nova
7215,timeout,Timeout value for http requests,0,0,others,nova
7216,timeout_nbd,"Amount of time, in seconds, to wait for NBD device start up.",0,0,others,nova
7217,token,Token for authentication. Required for 'token' and 'keystone_token' auth_type if no context is passed to the credential factory.,0,0,others,nova
7220,topics,AMQP topic used for OpenStack notifications.,0,0,others,nova
7222,trace_sqlalchemy,Enable SQL requests profiling in services.,1,6,function-tradeoff,nova
7223,track_instance_changes,Enable querying of individual hosts for instance information.,1,6,function-tradeoff,nova
7224,transport_url.B,"A URL representing the messaging driver to use for notifications. If not set, we fall back to the same configuration used for RPC.",0,0,others,nova
7225,transport_url,"The network address and optional user credentials for connecting to the messaging backend, in URL format. The expected format is:",0,0,others,nova
7226,trust_id.B,Trust ID,0,0,others,nova
7227,trust_id,Trust ID for trust scoping. Optional for 'keystone_token' and 'keystone_password' auth_type.,0,0,others,nova
7228,tx_queue_size,Configure virtio tx queue size.,1,1,resource,nova
7229,uid_maps,List of uid targets and ranges.Syntax is guest-uid:host-uid:count. Maximum of 5 allowed.,0,0,others,nova
7230,unicast_address,Appended to the address prefix when sending to a particular RPC/Notification server. Used by the message bus to identify messages sent to a single destination.,0,0,others,nova
7231,update_dns_entries,"When this option is True, whenever a DNS entry must be updated, a fanout cast message is sent to all network hosts to update their DNS entries in multi-host mode.",0,0,others,nova
7232,update_resources_interval,Interval for updating compute resources.,1,3,reliability-tradeoff,nova
7233,url,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",0,0,others,nova
7234,use_agent_default,Whether or not to use the agent by default when its usage is enabled but not indicated by the image.,0,0,others,nova
7235,use_cow_images,Enable use of copy-on-write (cow) images.,1,4,limited-side-effect,nova
7236,use_db_reconnect,Enable the experimental use of database reconnect on connection lost.,0,0,others,nova
7237,use_eventlog,Log output to Windows Event Log.,1,6,function-tradeoff,nova
7238,use_forwarded_for,"When True, the 'X-Forwarded-For' header is treated as the canonical remote address. When False (the default), the 'remote_address' header is used.",0,0,others,nova
7239,use_ipv6,Assign IPv6 and IPv4 addresses when creating instances.,0,0,others,nova
7240,use_join_force,"When adding new host to a pool, this will append a --force flag to the command, forcing hosts to join a pool, even if they have different CPUs.",0,0,others,nova
7241,use_journal,Enable journald for logging. If running in a systemd environment you may wish to enable journal support. Doing so will use the journal native protocol which includes structured metadata in addition to log messages.This option is ignored if log_config_append is set.,1,6,function-tradeoff,nova
7242,use_json,Use JSON formatting for logging. This option is ignored if log_config_append is set.,1,6,function-tradeoff,nova
7243,use_linked_clone,This option enables/disables the use of linked clone.,1,6,function-tradeoff,nova
7244,use_multipath_io,Use multipath connections when attaching iSCSI or FC disks.,1,4,limited-side-effect,nova
7245,use_network_dns_servers,"When this option is set to True, the dns1 and dns2 servers for the network specified by the user on boot will be used for DNS, as well as any specified in the dns_server option.",0,0,others,nova
7246,use_neutron,Enable neutron as the backend for networking.,0,0,others,nova
7247,use_neutron_default_nets,"When True, the TenantNetworkController will query the Neutron API to get the default networks to use.",0,0,others,nova
7249,use_single_default_gateway,"When set to True, only the firt nic of a VM will get its default gateway from the DHCP server. This option is deprecated for removal since 16.0.0. Its value may be silently ignored in the future.",0,0,others,nova
7250,use_ssl,SSL Enabled/Disabled,1,2,security-tradeoff,nova
7251,use_stderr,Log output to standard error. This option is ignored if log_config_append is set.,1,6,function-tradeoff,nova
7252,use_syslog,Use syslog for logging. Existing syslog format is DEPRECATED and will be changed later to honor RFC5424. This option is ignored if log_config_append is set.,1,6,function-tradeoff,nova
7253,use_tpool,Enable the experimental use of thread pooling for all DB API calls,1,4,limited-side-effect,nova
7254,use_usb_tablet,Enable a mouse cursor within a graphical VNC or SPICE sessions.,0,0,others,nova
7255,use_virtio_for_bridges,Use virtio for bridge interfaces with KVM/QEMU,0,0,others,nova
7256,user,User that the privsep daemon should run as.,0,0,others,nova
7257,user_domain_id.B,User's domain ID for authentication. Optional for 'keystone_token' and 'keystone_password' auth_type.,0,0,others,nova
7258,user_domain_id,User's domain id,0,0,others,nova
7259,user_domain_name.B,User's domain name for authentication. Optional for 'keystone_token' and 'keystone_password' auth_type.,0,0,others,nova
7260,user_domain_name,User's domain name,0,0,others,nova
7261,user_id.B,User ID for authentication. Optional for 'keystone_token' and 'keystone_password' auth_type.,0,0,others,nova
7262,user_id,User ID,0,0,others,nova
7263,username.B,Username,0,0,others,nova
7264,username,Username for authentication. Required for 'password' auth_type. Optional for the 'keystone_password' auth_type.,0,0,others,nova
7265,valid_interfaces,"List of interfaces, in order of preference, for endpoint URL.",0,0,others,nova
7266,vault_url,"Use this endpoint to connect to Vault, for example: 'http://127.0.0.1:8200'",0,0,others,nova
7268,vencrypt_ca_certs,The path to the CA certificate PEM file,0,0,others,nova
7269,vencrypt_client_cert,The path to the client key file (for x509),0,0,others,nova
7270,vencrypt_client_key,The path to the client certificate PEM file (for x509),0,0,others,nova
7271,vendordata_dynamic_connect_timeout,Maximum wait time for an external REST service to connect.,0,0,others,nova
7272,vendordata_dynamic_failure_fatal,Should failures to fetch dynamic vendordata be fatal to instance boot?,0,0,others,nova
7273,vendordata_dynamic_read_timeout,Maximum wait time for an external REST service to return data once connected.,0,0,others,nova
7274,vendordata_dynamic_ssl_certfile,Path to an optional certificate file or CA bundle to verify dynamic vendordata REST services ssl certificates against.,0,0,others,nova
7275,vendordata_dynamic_targets,A list of targets for the dynamic vendordata provider. These targets are of the form <name>@<url>.,0,0,others,nova
7276,vendordata_jsonfile_path,"Cloud providers may store custom data in vendor data file that will then be available to the instances via the metadata service, and to the rendering of config-drive. The default class for this, JsonFileVendorData, loads this information from a JSON file, whose path is configured by this option. If there is no path set by this option, the class returns an empty dictionary.",0,0,others,nova
7277,vendordata_providers,A list of vendordata providers.,0,0,others,nova
7278,verify_glance_signatures,Enable image signature verification.,1,6,function-tradeoff,nova
7279,verify_ssl,"Specifies if insecure TLS (https) requests. If False, the server's certificate will not be validated",1,2,security-tradeoff,nova
7280,versioned_notifications_topics,Specifies the topics for the versioned notifications issued by nova.,0,0,others,nova
7281,vhd_coalesce_max_attempts,Max number of times to poll for VHD to coalesce.,0,0,others,nova
7282,vhd_coalesce_poll_interval,The interval used for polling of coalescing vhds.,0,0,others,nova
7283,vif_plugging_is_fatal,Determine if instance should boot or fail on VIF plugging timeout.,0,0,others,nova
7285,virt_mkfs,Name of the mkfs commands for ephemeral device.,0,0,others,nova
7286,virt_type,Describes the virtualization type (or so called domain type) libvirt should use.,0,0,others,nova
7287,vlan_interface.B,"This option is the name of the virtual interface of the VM on which the VLAN bridge will be built. While it was originally designed to be used only by nova-network, it is also used by libvirt and xenapi for the bridge interface name.",0,0,others,nova
7288,vlan_interface,This option specifies the physical ethernet adapter name for VLAN networking.,0,0,others,nova
7289,vlan_start,"This is the VLAN number used for private networks. Note that the when creating the networks, if the specified number has already been assigned, nova-network will increment this number until it finds an available VLAN.",1,1,resource,nova
7290,vnc_keymap,Keymap for VNC.,0,0,others,nova
7291,vnc_port,This option specifies VNC starting port.,0,0,others,nova
7292,vnc_port_total,Total number of VNC ports.,0,0,others,nova
7294,volume_attach_retry_interval,Volume attach retry interval,0,0,others,nova
7295,volume_clear,Method used to wipe ephemeral disks when they are deleted. Only takes effect if LVM is set as backing storage.,0,0,others,nova
7297,volume_group_name,"Volume Group to use for block device operations. If disk_driver is localdisk, then this attribute must be specified. It is strongly recommended NOT to use rootvg since that is used by the management partition and filling it will cause failures.",0,0,others,nova
7299,volume_use_multipath,Use multipath connection of the iSCSI or FC volume,0,0,others,nova
7300,vpn_ip,This option is no longer used since the /os-cloudpipe API was removed in the 16.0.0 Pike release. This is the public IP address for the cloudpipe VPN servers. It defaults to the IP address of the host.,0,0,others,nova
7302,vswitch_name,External virtual switch name,0,0,others,nova
7303,vzstorage_cache_path,Path to the SSD cache file.,0,0,others,nova
7304,vzstorage_log_path,Path to vzstorage client log.,0,0,others,nova
7305,vzstorage_mount_group,Mount owner group name.,0,0,others,nova
7306,vzstorage_mount_opts,Extra mount options for pstorage-mount,0,0,others,nova
7308,vzstorage_mount_point_base,Directory where the Virtuozzo Storage clusters are mounted on the compute node.,0,0,others,nova
7309,vzstorage_mount_user,Mount owner user name.,0,0,others,nova
7310,wait_soft_reboot_seconds.B,Number of seconds to wait for instance to shut down after soft reboot request is made. We fall back to hard reboot if instance does not shutdown within this window.,0,0,others,nova
7311,wait_soft_reboot_seconds,Wait soft reboot seconds,0,0,others,nova
7312,watch_log_file,Uses logging handler designed to watch file system. When log file is moved or removed this handler will open a new log file with specified path instantaneously. It makes sense only if log_file option is specified and Linux platform is used. This option is ignored if log_config_append is set.,1,6,function-tradeoff,nova
7313,web,Path to directory with content which will be served by a web server.,0,0,others,nova
7314,weight_classes,Weighers that the scheduler will use.,0,0,others,nova
7315,weight_multiplier,"When using metrics to weight the suitability of a host, you can use this option to change how the calculated weight influences the weight assigned to a host as follows:",0,0,others,nova
7316,weight_of_unavailable,"When any of the following conditions are met, this value will be used in place of any actual metric value:",0,0,others,nova
7317,weight_setting,"This setting specifies the metrics to be weighed and the relative ratios for each metric. This should be a single string value, consisting of a series of one or more 'name=ratio' pairs, separated by commas, where 'name' is the name of the metric to be weighed, and 'ratio' is the relative weight for that metric.",0,0,others,nova
7318,workers.B,Number of workers for OpenStack Conductor service. The default will be the number of CPUs available.,1,1,resource,nova
7319,workers,"Number of workers for the nova-scheduler service. The default will be the number of CPUs available if using the 'filter_scheduler' scheduler driver, otherwise the default will be 1.",1,1,resource,nova
7320,wsgi_log_format,"It represents a python format string that is used as the template to generate log lines. The following values can be formatted into it: client_ip, date_time, request_line, status_code, body_length, wall_seconds.",0,0,others,nova
7322,xen_hvmloader_path,Location where the Xen hvmloader is kept,0,0,others,nova
7323,xvpvncproxy_base_url,Public URL address of XVP VNC console proxy.,0,0,others,nova
7324,xvpvncproxy_host,IP address or hostname that the XVP VNC console proxy should bind to.,0,0,others,nova
7325,xvpvncproxy_port,Port that the XVP VNC console proxy should bind to.,0,0,others,nova
7326,allow_system_table_mods,Allows modification of the structure of system tables. This is used by initdb. This parameter can only be set at server start.,1,5,workload-specific,postgresql
7327,allow_system_table_mods,Allows modifications of the structure of system tables,1,5,workload-specific,postgresql
7328,application_name,Sets the application name to be reported in statistics and logs,0,0,others,postgresql
7329,application_name,The application_name can be any string of less than NAMEDATALEN characters (64 characters in a standard build). It is typically set by an application upon connection to the server. The name will be displayed in the pg_stat_activity view and included in CSV log entries. It can also be included in regular log entries via the log_line_prefix parameter. Only printable ASCII characters may be used in the application_name value. Other characters will be replaced with question marks (?).,0,0,others,postgresql
7330,archive_command,"The local shell command to execute to archive a completed WAL file segment. Any %p in the string is replaced by the path name of the file to archive, and any %f is replaced by only the file name. (The path name is relative to the working directory of the server, i.e., the cluster's data directory.) Use %% to embed an actual % character in the command. It is important for the command to return a zero exit status only if it succeeds. For more information see backup-archiving-wal.",0,0,others,postgresql
7331,archive_command,Sets the shell command that will be called to archive a WAL file,0,0,others,postgresql
7332,archive_mode,Allows archiving of WAL files using archive_command,1,3,reliability-tradeoff,postgresql
7333,archive_mode,"When archive_mode is enabled, completed WAL segments are sent to archive storage by setting archive_command. In addition to off, to disable, there are two modes: on, and always. During normal operation, there is no difference between the two modes, but when set to always the WAL archiver is enabled also during archive recovery or standby mode. In always mode, all files restored from the archive or streamed with streaming replication will be archived (again). See continuous-archiving-in-standby for details.",1,3,reliability-tradeoff,postgresql
7334,archive_timeout,"The archive_command is only invoked for completed WAL segments. Hence, if your server generates little WAL traffic (or has slack periods where it does so), there could be a long delay between the completion of a transaction and its safe recording in archive storage. To limit how old unarchived data can be, you can set archive_timeout to force the server to switch to a new WAL segment file periodically. When this parameter is greater than zero, the server will switch to a new segment file whenever this many seconds have elapsed since the last segment file switch, and there has been any database activity, including a single checkpoint (checkpoints are skipped if there is no database activity). Note that archived files that are closed early due to a forced switch are still the same length as completely full files. Therefore, it is unwise to use a very short archive_timeout-- it will bloat your archive storage. archive_timeout settings of a minute or so are usually reasonable. You should consider using streaming replication, instead of archiving, if you want data to be copied off the master server more quickly than that. This parameter can only be set in the postgresql.conf file or on the server command line.",0,0,others,postgresql
7335,archive_timeout,Forces a switch to the next WAL file if a new file has not been started within N seconds,0,0,others,postgresql
7336,array_nulls,Enable input of NULL elements in arrays,0,0,others,postgresql
7337,array_nulls,"This controls whether the array input parser recognizes unquoted NULL as specifying a null array element. By default, this is on, allowing array values containing null values to be entered. However, PostgreSQL versions before 8.2 did not support null values in arrays, and therefore would treat NULL as specifying a normal array element with the string value NULL. For backward compatibility with applications that require the old behavior, this variable can be turned off.",0,0,others,postgresql
7338,authentication_timeout,"Maximum time to complete client authentication, in seconds. If a would-be client has not completed the authentication protocol in this much time, the server closes the connection. This prevents hung clients from occupying a connection indefinitely. The default is one minute (1m). This parameter can only be set in the postgresql.conf file or on the server command line.",0,0,others,postgresql
7339,authentication_timeout,Sets the maximum allowed time to complete client authentication,0,0,others,postgresql
7340,autovacuum,"Controls whether the server should run the autovacuum launcher daemon. This is on by default; however, track_counts must also be enabled for autovacuum to work. This parameter can only be set in the postgresql.conf file or on the server command line; however, autovacuuming can be disabled for individual tables by changing table storage parameters.",1,6,function-tradeoff,postgresql
7341,autovacuum,Starts the autovacuum subprocess,1,6,function-tradeoff,postgresql
7342,autovacuum_analyze_scale_factor,"Number of tuple inserts, updates, or deletes prior to analyze as a fraction of reltuples",1,5,workload-specific,postgresql
7343,autovacuum_analyze_scale_factor,Specifies a fraction of the table size to add to autovacuum_analyze_threshold when deciding whether to trigger an ANALYZE. The default is 0.1 (10% of table size). This parameter can only be set in the postgresql.conf file or on the server command line; but the setting can be overridden for individual tables by changing table storage parameters.,1,5,workload-specific,postgresql
7344,autovacuum_analyze_threshold,"Minimum number of tuple inserts, updates, or deletes prior to analyze",1,5,workload-specific,postgresql
7345,autovacuum_analyze_threshold,"Specifies the minimum number of inserted, updated or deleted tuples needed to trigger an ANALYZE in any one table. The default is 50 tuples. This parameter can only be set in the postgresql.conf file or on the server command line; but the setting can be overridden for individual tables by changing table storage parameters.",1,5,workload-specific,postgresql
7346,autovacuum_freeze_max_age,Age at which to autovacuum a table to prevent transaction ID wraparound,0,0,others,postgresql
7347,autovacuum_freeze_max_age,Specifies the maximum age (in transactions) that a table's pg_class.relfrozenxid field can attain before a VACUUM operation is forced to prevent transaction ID wraparound within the table. Note that the system will launch autovacuum processes to prevent wraparound even when autovacuum is otherwise disabled.,0,0,others,postgresql
7348,autovacuum_max_workers,Sets the maximum number of simultaneously running autovacuum worker processes,1,1,resource,postgresql
7349,autovacuum_max_workers,Specifies the maximum number of autovacuum processes (other than the autovacuum launcher) that may be running at any one time. The default is three. This parameter can only be set at server start.,1,1,resource,postgresql
7350,autovacuum_multixact_freeze_max_age,Multixact age at which to autovacuum a table to prevent multixact wraparound,0,0,others,postgresql
7351,autovacuum_multixact_freeze_max_age,Specifies the maximum age (in multixacts) that a table's pg_class.relminmxid field can attain before a VACUUM operation is forced to prevent multixact ID wraparound within the table. Note that the system will launch autovacuum processes to prevent wraparound even when autovacuum is otherwise disabled.,0,0,others,postgresql
7352,autovacuum_naptime,Time to sleep between autovacuum runs,0,0,others,postgresql
7353,autovacuum_naptime,"Specifies the minimum delay between autovacuum runs on any given database. In each round the daemon examines the database and issues VACUUM and ANALYZE commands as needed for tables in that database. The delay is measured in seconds, and the default is one minute (1min). This parameter can only be set in the postgresql.conf file or on the server command line.",0,0,others,postgresql
7354,autovacuum_vacuum_cost_delay,"Vacuum cost delay in milliseconds, for autovacuum",0,0,others,postgresql
7355,autovacuum_vacuum_cost_delay,"Specifies the cost delay value that will be used in automatic VACUUM operations. If -1 is specified, the regular vacuum_cost_delay value will be used. The default value is 20 milliseconds. This parameter can only be set in the postgresql.conf file or on the server command line; but the setting can be overridden for individual tables by changing table storage parameters.",0,0,others,postgresql
7356,autovacuum_vacuum_cost_limit,"Specifies the cost limit value that will be used in automatic VACUUM operations. If -1 is specified (which is the default), the regular vacuum_cost_limit value will be used. Note that the value is distributed proportionally among the running autovacuum workers, if there is more than one, so that the sum of the limits for each worker does not exceed the value of this variable. This parameter can only be set in the postgresql.conf file or on the server command line; but the setting can be overridden for individual tables by changing table storage parameters.",1,5,workload-specific,postgresql
7357,autovacuum_vacuum_cost_limit,"Vacuum cost amount available before napping, for autovacuum",1,5,workload-specific,postgresql
7358,autovacuum_vacuum_scale_factor,Number of tuple updates or deletes prior to vacuum as a fraction of reltuples,1,5,workload-specific,postgresql
7359,autovacuum_vacuum_scale_factor,Specifies a fraction of the table size to add to autovacuum_vacuum_threshold when deciding whether to trigger a VACUUM. The default is 0.2 (20% of table size). This parameter can only be set in the postgresql.conf file or on the server command line; but the setting can be overridden for individual tables by changing table storage parameters.,1,5,workload-specific,postgresql
7360,autovacuum_vacuum_threshold,Minimum number of tuple updates or deletes prior to vacuum,1,5,workload-specific,postgresql
7361,autovacuum_vacuum_threshold,Specifies the minimum number of updated or deleted tuples needed to trigger a VACUUM in any one table. The default is 50 tuples. This parameter can only be set in the postgresql.conf file or on the server command line; but the setting can be overridden for individual tables by changing table storage parameters.,1,5,workload-specific,postgresql
7362,autovacuum_work_mem,Sets the maximum memory to be used by each autovacuum worker process,1,1,resource,postgresql
7363,autovacuum_work_mem,"Specifies the maximum amount of memory to be used by each autovacuum worker process. It defaults to -1, indicating that the value of maintenance_work_mem should be used instead. The setting has no effect on the behavior of VACUUM when run in other contexts.",1,1,resource,postgresql
7364,backend_flush_after,Number of pages after which previously performed writes are flushed to disk,1,5,workload-specific,postgresql
7365,backend_flush_after,"Whenever more than backend_flush_after bytes have been written by a single backend, attempt to force the OS to issue these writes to the underlying storage. Doing so will limit the amount of dirty data in the kernel's page cache, reducing the likelihood of stalls when an fsync is issued at the end of a checkpoint, or when the OS writes data back in larger batches in the background. Often that will result in greatly reduced transaction latency, but there also are some cases, especially with workloads that are bigger than shared_buffers, but smaller than the OS's page cache, where performance might degrade. This setting may have no effect on some platforms. The valid range is between 0, which disables forced writeback, and 2MB. The default is 0, i.e., no forced writeback. (If BLCKSZ is not 8kB, the maximum value scales proportionally to it.)",1,5,workload-specific,postgresql
7366,backslash_quote,"Sets whether ""\'"" is allowed in string literals",0,0,others,postgresql
7367,backslash_quote,"This controls whether a quote mark can be represented by \' in a string literal. The preferred, SQL-standard way to represent a quote mark is by doubling it ('') but PostgreSQL has historically also accepted \'. However, use of \' creates security risks because in some client character set encodings, there are multibyte characters in which the last byte is numerically equivalent to ASCII \. If client-side code does escaping incorrectly then a SQL-injection attack is possible. This risk can be prevented by making the server reject queries in which a quote mark appears to be escaped by a backslash. The allowed values of backslash_quote are on (allow \' always), off (reject always), and safe_encoding (allow only if client encoding does not allow ASCII \ within a multibyte character). safe_encoding is the default setting.",0,0,others,postgresql
7368,bgwriter_delay,Background writer sleep time between rounds,0,0,others,postgresql
7369,bgwriter_delay,"Specifies the delay between activity rounds for the background writer. In each round the writer issues writes for some number of dirty buffers (controllable by the following parameters). It then sleeps for bgwriter_delay milliseconds, and repeats. When there are no dirty buffers in the buffer pool, though, it goes into a longer sleep regardless of bgwriter_delay. The default value is 200 milliseconds (200ms). Note that on many systems, the effective resolution of sleep delays is 10 milliseconds; setting bgwriter_delay to a value that is not a multiple of 10 might have the same results as setting it to the next higher multiple of 10. This parameter can only be set in the postgresql.conf file or on the server command line.",0,0,others,postgresql
7370,bgwriter_flush_after,Number of pages after which previously performed writes are flushed to disk,1,5,workload-specific,postgresql
7371,bgwriter_flush_after,"Whenever more than bgwriter_flush_after bytes have been written by the background writer, attempt to force the OS to issue these writes to the underlying storage. Doing so will limit the amount of dirty data in the kernel's page cache, reducing the likelihood of stalls when an fsync is issued at the end of a checkpoint, or when the OS writes data back in larger batches in the background. Often that will result in greatly reduced transaction latency, but there also are some cases, especially with workloads that are bigger than shared_buffers, but smaller than the OS's page cache, where performance might degrade. This setting may have no effect on some platforms. The valid range is between 0, which disables forced writeback, and 2MB. The default is 512kB on Linux, 0 elsewhere. (If BLCKSZ is not 8kB, the default and maximum values scale proportionally to it.) This parameter can only be set in the postgresql.conf file or on the server command line.",1,5,workload-specific,postgresql
7372,bgwriter_lru_maxpages,Background writer maximum number of LRU pages to flush per round,1,5,workload-specific,postgresql
7373,bgwriter_lru_maxpages,"In each round, no more than this many buffers will be written by the background writer. Setting this to zero disables background writing. (Note that checkpoints, which are managed by a separate, dedicated auxiliary process, are unaffected.) The default value is 100 buffers. This parameter can only be set in the postgresql.conf file or on the server command line.",1,5,workload-specific,postgresql
7374,bgwriter_lru_multiplier,Multiple of the average buffer usage to free per round,1,1,resource,postgresql
7375,bgwriter_lru_multiplier,"The number of dirty buffers written in each round is based on the number of new buffers that have been needed by server processes during recent rounds. The average recent need is multiplied by bgwriter_lru_multiplier to arrive at an estimate of the number of buffers that will be needed during the next round. Dirty buffers are written until there are that many clean, reusable buffers available. (However, no more than bgwriter_lru_maxpages buffers will be written per round.) Thus, a setting of 1.0 represents a just in time policy of writing exactly the number of buffers predicted to be needed. Larger values provide some cushion against spikes in demand, while smaller values intentionally leave writes to be done by server processes. The default is 2.0. This parameter can only be set in the postgresql.conf file or on the server command line.",1,1,resource,postgresql
7376,bonjour,Enables advertising the server via Bonjour,0,0,others,postgresql
7377,bonjour,Enables advertising the server's existence via Bonjour. The default is off. This parameter can only be set at server start.,0,0,others,postgresql
7378,bonjour_name,Sets the Bonjour service name,0,0,others,postgresql
7379,bonjour_name,Specifies the Bonjour service name. The computer name is used if this parameter is set to the empty string '' (which is the default). This parameter is ignored if the server was not compiled with Bonjour support. This parameter can only be set at server start.,0,0,others,postgresql
7380,bytea_output,Sets the output format for bytea,0,0,others,postgresql
7381,bytea_output,"Sets the output format for values of type bytea. Valid values are hex (the default) and escape (the traditional PostgreSQL format). See datatype-binary for more information. The bytea type always accepts both formats on input, regardless of this setting.",0,0,others,postgresql
7382,check_function_bodies,Check function bodies during CREATE FUNCTION,1,6,function-tradeoff,postgresql
7383,check_function_bodies,"This parameter is normally on. When set to off, it disables validation of the function body string during sql-createfunction. Disabling validation avoids side effects of the validation process and avoids false positives due to problems such as forward references. Set this parameter to off before loading functions on behalf of other users; pg_dump does so automatically.",1,6,function-tradeoff,postgresql
7384,checkpoint_completion_target,"Specifies the target of checkpoint completion, as a fraction of total time between checkpoints. The default is 0.5. This parameter can only be set in the postgresql.conf file or on the server command line.",1,5,workload-specific,postgresql
7385,checkpoint_completion_target,"Time spent flushing dirty buffers during checkpoint, as fraction of checkpoint interval",1,5,workload-specific,postgresql
7386,checkpoint_flush_after,Number of pages after which previously performed writes are flushed to disk,1,5,workload-specific,postgresql
7387,checkpoint_flush_after,"Whenever more than checkpoint_flush_after bytes have been written while performing a checkpoint, attempt to force the OS to issue these writes to the underlying storage. Doing so will limit the amount of dirty data in the kernel's page cache, reducing the likelihood of stalls when an fsync is issued at the end of the checkpoint, or when the OS writes data back in larger batches in the background. Often that will result in greatly reduced transaction latency, but there also are some cases, especially with workloads that are bigger than shared_buffers, but smaller than the OS's page cache, where performance might degrade. This setting may have no effect on some platforms. The valid range is between 0, which disables forced writeback, and 2MB. The default is 256kB on Linux, 0 elsewhere. (If BLCKSZ is not 8kB, the default and maximum values scale proportionally to it.) This parameter can only be set in the postgresql.conf file or on the server command line.",1,5,workload-specific,postgresql
7388,checkpoint_timeout,"Maximum time between automatic WAL checkpoints, in seconds. The valid range is between 30 seconds and one day. The default is five minutes (5min). Increasing this parameter can increase the amount of time needed for crash recovery. This parameter can only be set in the postgresql.conf file or on the server command line.",0,0,others,postgresql
7389,checkpoint_timeout,Sets the maximum time between automatic WAL checkpoints,0,0,others,postgresql
7390,checkpoint_warning,Enables warnings if checkpoint segments are filled more frequently than this,0,0,others,postgresql
7391,checkpoint_warning,Write a message to the server log if checkpoints caused by the filling of checkpoint segment files happen closer together than this many seconds (which suggests that max_wal_size ought to be raised). The default is 30 seconds (30s). Zero disables the warning. No warnings will be generated if checkpoint_timeout is less than checkpoint_warning. This parameter can only be set in the postgresql.conf file or on the server command line.,0,0,others,postgresql
7392,client_encoding,Sets the client's character set encoding,0,0,others,postgresql
7393,client_encoding,Sets the client-side encoding (character set). The default is to use the database encoding. The character sets supported by the PostgreSQL server are described in multibyte-charset-supported.,0,0,others,postgresql
7394,client_min_messages,"Controls which message levels are sent to the client. Valid values are DEBUG5, DEBUG4, DEBUG3, DEBUG2, DEBUG1, LOG, NOTICE, WARNING, and ERROR. Each level includes all the levels that follow it. The later the level, the fewer messages are sent. The default is NOTICE. Note that LOG has a different rank here than in log_min_messages.",1,6,function-tradeoff,postgresql
7395,client_min_messages,Sets the message levels that are sent to the client,1,6,function-tradeoff,postgresql
7396,cluster_name,Sets the cluster name that appears in the process title for all server processes in this cluster. The name can be any string of less than NAMEDATALEN characters (64 characters in a standard build). Only printable ASCII characters may be used in the cluster_name value. Other characters will be replaced with question marks (?). No name is shown if this parameter is set to the empty string '' (which is the default). This parameter can only be set at server start.,0,0,others,postgresql
7397,cluster_name,"Sets the name of the cluster, which is included in the process title",0,0,others,postgresql
7398,commit_delay,"commit_delay adds a time delay, measured in microseconds, before a WAL flush is initiated. This can improve group commit throughput by allowing a larger number of transactions to commit via a single WAL flush, if system load is high enough that additional transactions become ready to commit within the given interval. However, it also increases latency by up to commit_delay microseconds for each WAL flush. Because the delay is just wasted if no other transactions become ready to commit, a delay is only performed if at least commit_siblings other transactions are active when a flush is about to be initiated. Also, no delays are performed if fsync is disabled. The default commit_delay is zero (no delay). Only superusers can change this setting.",1,5,workload-specific,postgresql
7399,commit_delay,Sets the delay in microseconds between transaction commit and flushing WAL to disk,1,5,workload-specific,postgresql
7400,commit_siblings,Minimum number of concurrent open transactions to require before performing the commit_delay delay. A larger value makes it more probable that at least one other transaction will become ready to commit during the delay interval. The default is five transactions.,1,1,resource,postgresql
7401,commit_siblings,Sets the minimum concurrent open transactions before performing commit_delay,1,1,resource,postgresql
7402,config_file,Sets the server's main configuration file,0,0,others,postgresql
7403,config_file,Specifies the main server configuration file (customarily called postgresql.conf). This parameter can only be set on the postgres command line.,0,0,others,postgresql
7404,constraint_exclusion,"Controls the query planner's use of table constraints to optimize queries. The allowed values of constraint_exclusion are on (examine constraints for all tables), off (never examine constraints), and partition (examine constraints only for inheritance child tables and UNION ALL subqueries). partition is the default setting. It is often used with inheritance and partitioned tables to improve performance.",1,4,limited-side-effect,postgresql
7405,constraint_exclusion,Enables the planner to use constraints to optimize queries,1,4,limited-side-effect,postgresql
7406,cpu_index_tuple_cost,Sets the planner's estimate of the cost of processing each index entry during an index scan,1,5,workload-specific,postgresql
7407,cpu_index_tuple_cost,Sets the planner's estimate of the cost of processing each index entry during an index scan. The default is 0.005.,1,5,workload-specific,postgresql
7408,cpu_operator_cost,Sets the planner's estimate of the cost of processing each operator or function call,1,5,workload-specific,postgresql
7409,cpu_operator_cost,Sets the planner's estimate of the cost of processing each operator or function executed during a query. The default is 0.0025.,1,5,workload-specific,postgresql
7410,cpu_tuple_cost,Sets the planner's estimate of the cost of processing each row during a query. The default is 0.01.,1,5,workload-specific,postgresql
7411,cpu_tuple_cost,Sets the planner's estimate of the cost of processing each tuple (row),1,5,workload-specific,postgresql
7412,cursor_tuple_fraction,Sets the planner's estimate of the fraction of a cursor's rows that will be retrieved,0,0,others,postgresql
7413,cursor_tuple_fraction,"Sets the planner's estimate of the fraction of a cursor's rows that will be retrieved. The default is 0.1. Smaller values of this setting bias the planner towards using fast start plans for cursors, which will retrieve the first few rows quickly while perhaps taking a long time to fetch all rows. Larger values put more emphasis on the total estimated time. At the maximum setting of 1.0, cursors are planned exactly like regular queries, considering only the total estimated time and not how soon the first rows might be delivered.",0,0,others,postgresql
7414,data_directory,Sets the server's data directory,0,0,others,postgresql
7415,data_directory,Specifies the directory to use for data storage. This parameter can only be set at server start.,0,0,others,postgresql
7416,data_sync_retry,"When set to false, which is the default, PostgreSQL will raise a PANIC-level error on failure to flush modified data files to the filesystem. This causes the database server to crash. This parameter can only be set at server start.",1,3,reliability-tradeoff,postgresql
7417,data_sync_retry,Whether to continue running after a failure to sync data files,1,3,reliability-tradeoff,postgresql
7418,DateStyle,Sets the display format for date and time values,0,0,others,postgresql
7419,DateStyle,"Sets the display format for date and time values, as well as the rules for interpreting ambiguous date input values. For historical reasons, this variable contains two independent components: the output format specification (ISO, Postgres, SQL, or German) and the input/output specification for year/month/day ordering (DMY, MDY, or YMD). These can be set separately or together. The keywords Euro and European are synonyms for DMY; the keywords US, NonEuro, and NonEuropean are synonyms for MDY. See datatype-datetime for more information. The built-in default is ISO, MDY, but initdb will initialize the configuration file with a setting that corresponds to the behavior of the chosen lc_time locale.",0,0,others,postgresql
7420,db_user_namespace,Enables per-database user names,0,0,others,postgresql
7421,db_user_namespace,This parameter enables per-database user names. It is off by default. This parameter can only be set in the postgresql.conf file or on the server command line.,0,0,others,postgresql
7422,deadlock_timeout,Sets the time to wait on a lock before checking for deadlock,0,0,others,postgresql
7423,deadlock_timeout,"This is the amount of time, in milliseconds, to wait on a lock before checking to see if there is a deadlock condition. The check for deadlock is relatively expensive, so the server doesn't run it every time it waits for a lock. We optimistically assume that deadlocks are not common in production applications and just wait on the lock for a while before checking for a deadlock. Increasing this value reduces the amount of time wasted in needless deadlock checks, but slows down reporting of real deadlock errors. The default is one second (1s), which is probably about the smallest value you would want in practice. On a heavily loaded server you might want to raise it. Ideally the setting should exceed your typical transaction time, so as to improve the odds that a lock will be released before the waiter decides to check for deadlock. Only superusers can change this setting.",0,0,others,postgresql
7424,debug_pretty_print,Indents parse and plan tree displays,1,6,function-tradeoff,postgresql
7425,debug_pretty_print,"When set, debug_pretty_print indents the messages produced by debug_print_parse, debug_print_rewritten, or debug_print_plan. This results in more readable but much longer output than the compact format used when it is off. It is on by default.",1,6,function-tradeoff,postgresql
7426,debug_print_parse,Logs each query's parse tree,1,6,function-tradeoff,postgresql
7427,debug_print_parse,"These parameters enable various debugging output to be emitted. When set, they print the resulting parse tree, the query rewriter output, or the execution plan for each executed query. These messages are emitted at LOG message level, so by default they will appear in the server log but will not be sent to the client. You can change that by adjusting client_min_messages and/or log_min_messages. These parameters are off by default.",1,6,function-tradeoff,postgresql
7428,debug_print_plan,Logs each query's execution plan,1,6,function-tradeoff,postgresql
7429,debug_print_plan,"These parameters enable various debugging output to be emitted. When set, they print the resulting parse tree, the query rewriter output, or the execution plan for each executed query. These messages are emitted at LOG message level, so by default they will appear in the server log but will not be sent to the client. You can change that by adjusting client_min_messages and/or log_min_messages. These parameters are off by default.",1,6,function-tradeoff,postgresql
7430,debug_print_rewritten,Logs each query's rewritten parse tree,1,6,function-tradeoff,postgresql
7431,debug_print_rewritten,"These parameters enable various debugging output to be emitted. When set, they print the resulting parse tree, the query rewriter output, or the execution plan for each executed query. These messages are emitted at LOG message level, so by default they will appear in the server log but will not be sent to the client. You can change that by adjusting client_min_messages and/or log_min_messages. These parameters are off by default.",1,6,function-tradeoff,postgresql
7432,default_statistics_target,Sets the default statistics target,1,5,workload-specific,postgresql
7433,default_statistics_target,"Sets the default statistics target for table columns without a column-specific target set via ALTER TABLE SET STATISTICS. Larger values increase the time needed to do ANALYZE, but might improve the quality of the planner's estimates. The default is 100. For more information on the use of statistics by the PostgreSQL query planner, refer to planner-stats.",1,5,workload-specific,postgresql
7434,default_tablespace,Sets the default tablespace to create tables and indexes in,0,0,others,postgresql
7435,default_tablespace,This variable specifies the default tablespace in which to create objects (tables and indexes) when a CREATE command does not explicitly specify a tablespace.,0,0,others,postgresql
7436,default_text_search_config,"Selects the text search configuration that is used by those variants of the text search functions that do not have an explicit argument specifying the configuration. See textsearch for further information. The built-in default is pg_catalog.simple, but initdb will initialize the configuration file with a setting that corresponds to the chosen lc_ctype locale, if a configuration matching that locale can be identified.",0,0,others,postgresql
7437,default_text_search_config,Sets default text search configuration,0,0,others,postgresql
7438,default_transaction_deferrable,Sets the default deferrable status of new transactions,0,0,others,postgresql
7439,default_transaction_deferrable,"When running at the serializable isolation level, a deferrable read-only SQL transaction may be delayed before it is allowed to proceed. However, once it begins executing it does not incur any of the overhead required to ensure serializability; so serialization code will have no reason to force it to abort because of concurrent updates, making this option suitable for long-running read-only transactions.",0,0,others,postgresql
7440,default_transaction_isolation,"Each SQL transaction has an isolation level, which can be either read uncommitted, read committed, repeatable read, or serializable. This parameter controls the default isolation level of each new transaction. The default is read committed.",1,3,reliability-tradeoff,postgresql
7441,default_transaction_isolation,Sets the transaction isolation level of each new transaction,1,3,reliability-tradeoff,postgresql
7442,default_transaction_read_only,A read-only SQL transaction cannot alter non-temporary tables. This parameter controls the default read-only status of each new transaction. The default is off (read/write).,0,0,others,postgresql
7443,default_transaction_read_only,Sets the default read-only status of new transactions,0,0,others,postgresql
7444,default_with_oids,Create new tables with OIDs by default,0,0,others,postgresql
7445,default_with_oids,"This controls whether CREATE TABLE and CREATE TABLE AS include an OID column in newly-created tables, if neither WITH OIDS nor WITHOUT OIDS is specified. It also determines whether OIDs will be included in tables created by SELECT INTO. The parameter is off by default; in PostgreSQL 8.0 and earlier, it was on by default.",0,0,others,postgresql
7446,dynamic_library_path,"If a dynamically loadable module needs to be opened and the file name specified in the CREATE FUNCTION or LOAD command does not have a directory component (i.e., the name does not contain a slash), the system will search this path for the required file.",0,0,others,postgresql
7447,dynamic_library_path,Sets the path for dynamically loadable modules,0,0,others,postgresql
7448,dynamic_shared_memory_type,Selects the dynamic shared memory implementation used,1,4,limited-side-effect,postgresql
7449,dynamic_shared_memory_type,"Specifies the dynamic shared memory implementation that the server should use. Possible values are posix (for POSIX shared memory allocated using shm_open), sysv (for System V shared memory allocated via shmget), windows (for Windows shared memory), mmap (to simulate shared memory using memory-mapped files stored in the data directory), and none (to disable this feature). Not all values are supported on all platforms; the first supported option is the default for that platform. The use of the mmap option, which is not the default on any platform, is generally discouraged because the operating system may write modified pages back to disk repeatedly, increasing system I/O load; however, it may be useful for debugging, when the pg_dynshmem directory is stored on a RAM disk, or when other shared memory facilities are not available.",1,4,limited-side-effect,postgresql
7450,effective_cache_size,"Sets the planner's assumption about the effective size of the disk cache that is available to a single query. This is factored into estimates of the cost of using an index; a higher value makes it more likely index scans will be used, a lower value makes it more likely sequential scans will be used. When setting this parameter you should consider both PostgreSQL's shared buffers and the portion of the kernel's disk cache that will be used for PostgreSQL data files, though some data might exist in both places. Also, take into account the expected number of concurrent queries on different tables, since they will have to share the available space. This parameter has no effect on the size of shared memory allocated by PostgreSQL, nor does it reserve kernel disk cache; it is used only for estimation purposes. The system also does not assume data remains in the disk cache between queries. The default is 4 gigabytes (4GB).",1,1,resource,postgresql
7451,effective_cache_size,Sets the planner's assumption about the total size of the data caches,1,1,resource,postgresql
7452,effective_io_concurrency,Number of simultaneous requests that can be handled efficiently by the disk subsystem,1,1,resource,postgresql
7453,effective_io_concurrency,"Sets the number of concurrent disk I/O operations that PostgreSQL expects can be executed simultaneously. Raising this value will increase the number of I/O operations that any individual PostgreSQL session attempts to initiate in parallel. The allowed range is 1 to 1000, or zero to disable issuance of asynchronous I/O requests. Currently, this setting only affects bitmap heap scans.",1,1,resource,postgresql
7454,enable_bitmapscan,Enables or disables the query planner's use of bitmap-scan plan types. The default is on.,1,4,limited-side-effect,postgresql
7455,enable_bitmapscan,Enables the planner's use of bitmap-scan plans,1,4,limited-side-effect,postgresql
7456,enable_gathermerge,Enables or disables the query planner's use of gather merge plan types. The default is on.,1,4,limited-side-effect,postgresql
7457,enable_gathermerge,Enables the planner's use of gather merge plans,1,4,limited-side-effect,postgresql
7458,enable_hashagg,Enables or disables the query planner's use of hashed aggregation plan types. The default is on.,1,4,limited-side-effect,postgresql
7459,enable_hashagg,Enables the planner's use of hashed aggregation plans,1,4,limited-side-effect,postgresql
7460,enable_hashjoin,Enables or disables the query planner's use of hash-join plan types. The default is on.,1,4,limited-side-effect,postgresql
7461,enable_hashjoin,Enables the planner's use of hash join plans,1,4,limited-side-effect,postgresql
7462,enable_indexonlyscan,Enables or disables the query planner's use of index-only-scan plan types (see indexes-index-only-scans). The default is on.,1,4,limited-side-effect,postgresql
7463,enable_indexonlyscan,Enables the planner's use of index-only-scan plans,1,4,limited-side-effect,postgresql
7464,enable_indexscan,Enables or disables the query planner's use of index-scan plan types. The default is on.,1,4,limited-side-effect,postgresql
7465,enable_indexscan,Enables the planner's use of index-scan plans,1,4,limited-side-effect,postgresql
7466,enable_material,"Enables or disables the query planner's use of materialization. It is impossible to suppress materialization entirely, but turning this variable off prevents the planner from inserting materialize nodes except in cases where it is required for correctness. The default is on.",1,4,limited-side-effect,postgresql
7467,enable_material,Enables the planner's use of materialization,1,4,limited-side-effect,postgresql
7468,enable_mergejoin,Enables or disables the query planner's use of merge-join plan types. The default is on.,1,4,limited-side-effect,postgresql
7469,enable_mergejoin,Enables the planner's use of merge join plans,1,4,limited-side-effect,postgresql
7470,enable_nestloop,"Enables or disables the query planner's use of nested-loop join plans. It is impossible to suppress nested-loop joins entirely, but turning this variable off discourages the planner from using one if there are other methods available. The default is on.",1,4,limited-side-effect,postgresql
7471,enable_nestloop,Enables the planner's use of nested-loop join plans,1,4,limited-side-effect,postgresql
7472,enable_parallel_hash,Enables or disables the query planner's use of hash-join plan types with parallel hash. Has no effect if hash-join plans are not also enabled.,1,4,limited-side-effect,postgresql
7473,enable_partition_pruning,Enables or disables the query planner's ability to eliminate a partitioned table's partitions from query plans. This also controls the planner's ability to generate query plans which allow the query executor to remove (ignore) partitions during query execution.,1,4,limited-side-effect,postgresql
7474,enable_partitionwise_aggregate,"Enables or disables the query planner's use of partitionwise grouping or aggregation, which allows grouping or aggregation on a partitioned tables performed separately for each partition. If the GROUP BY clause does not include the partition keys, only partial aggregation can be performed on a per-partition basis, and finalization must be performed later. Because partitionwise grouping or aggregation can use significantly more CPU time and memory during planning.
",1,1,resource,postgresql
7475,enable_seqscan,"Enables or disables the query planner's use of sequential scan plan types. It is impossible to suppress sequential scans entirely, but turning this variable off discourages the planner from using one if there are other methods available. The default is on.",1,4,limited-side-effect,postgresql
7476,enable_seqscan,Enables the planner's use of sequential-scan plans,1,4,limited-side-effect,postgresql
7477,enable_sort,"Enables or disables the query planner's use of explicit sort steps. It is impossible to suppress explicit sorts entirely, but turning this variable off discourages the planner from using one if there are other methods available. The default is on.",1,4,limited-side-effect,postgresql
7478,enable_sort,Enables the planner's use of explicit sort steps,1,4,limited-side-effect,postgresql
7479,enable_tidscan,Enables or disables the query planner's use of TID scan plan types. The default is on.,1,4,limited-side-effect,postgresql
7480,enable_tidscan,Enables the planner's use of TID scan plans,1,4,limited-side-effect,postgresql
7481,escape_string_warning,Warn about backslash escapes in ordinary string literals,1,4,limited-side-effect,postgresql
7482,escape_string_warning,"When on, a warning is issued if a backslash (\) appears in an ordinary string literal ('...' syntax) and standard_conforming_strings is off. The default is on.",1,4,limited-side-effect,postgresql
7483,event_source,Sets the application name used to identify PostgreSQL messages in the event log,0,0,others,postgresql
7484,event_source,"When logging to event log is enabled, this parameter determines the program name used to identify PostgreSQL messages in the log. The default is PostgreSQL. This parameter can only be set in the postgresql.conf file or on the server command line.",0,0,others,postgresql
7485,exit_on_error,"If true, any error will terminate the current session. By default, this is set to false, so that only FATAL errors will terminate the session.",0,0,others,postgresql
7486,exit_on_error,Terminate session on any error,0,0,others,postgresql
7487,external_pid_file,Specifies the name of an additional process-ID (PID) file that the server should create for use by server administration programs. This parameter can only be set at server start.,0,0,others,postgresql
7488,external_pid_file,Writes the postmaster PID to the specified file,0,0,others,postgresql
7489,extra_float_digits,Sets the number of digits displayed for floating-point values,0,0,others,postgresql
7490,extra_float_digits,"This parameter adjusts the number of digits displayed for floating-point values, including float4, float8, and geometric data types. The parameter value is added to the standard number of digits (FLT_DIG or DBL_DIG as appropriate). The value can be set as high as 3, to include partially-significant digits; this is especially useful for dumping float data that needs to be restored exactly. Or it can be set negative to suppress unwanted digits. See also datatype-float.",0,0,others,postgresql
7491,force_parallel_mode,"Allows the use of parallel queries for testing purposes even in cases where no performance benefit is expected. The allowed values of force_parallel_mode are off (use parallel mode only when it is expected to improve performance), on (force parallel query for all queries for which it is thought to be safe), and regress (like on, but with additional behavior changes as explained below).",1,4,limited-side-effect,postgresql
7492,force_parallel_mode,Forces use of parallel query facilities,1,4,limited-side-effect,postgresql
7493,from_collapse_limit,Sets the FROM-list size beyond which subqueries are not collapsed,1,5,workload-specific,postgresql
7494,from_collapse_limit,The planner will merge sub-queries into upper queries if the resulting FROM list would have no more than this many items. Smaller values reduce planning time but might yield inferior query plans. The default is eight. For more information see explicit-joins.,1,6,function-tradeoff,postgresql
7495,fsync,Forces synchronization of updates to disk,1,3,reliability-tradeoff,postgresql
7496,fsync,"If this parameter is on, the PostgreSQL server will try to make sure that updates are physically written to disk, by issuing fsync() system calls or various equivalent methods (see wal_sync_method). This ensures that the database cluster can recover to a consistent state after an operating system or hardware crash.",1,3,reliability-tradeoff,postgresql
7497,full_page_writes,"When this parameter is on, the PostgreSQL server writes the entire content of each disk page to WAL during the first modification of that page after a checkpoint. This is needed because a page write that is in process during an operating system crash might be only partially completed, leading to an on-disk page that contains a mix of old and new data. The row-level change data normally stored in WAL will not be enough to completely restore such a page during post-crash recovery. Storing the full page image guarantees that the page can be correctly restored, but at the price of increasing the amount of data that must be written to WAL. (Because WAL replay always starts from a checkpoint, it is sufficient to do this during the first change of each page after a checkpoint. Therefore, one way to reduce the cost of full-page writes is to increase the checkpoint interval parameters.)",1,3,reliability-tradeoff,postgresql
7498,full_page_writes,Writes full pages to WAL when first modified after a checkpoint,1,3,reliability-tradeoff,postgresql
7499,geqo,Enables genetic query optimization,1,4,limited-side-effect,postgresql
7500,geqo,Enables or disables genetic query optimization. This is on by default. It is usually best not to turn it off in production; the geqo_threshold variable provides more granular control of GEQO.,1,4,limited-side-effect,postgresql
7501,geqo_effort,"Controls the trade-off between planning time and query plan quality in GEQO. This variable must be an integer in the range from 1 to 10. The default value is five. Larger values increase the time spent doing query planning, but also increase the likelihood that an efficient query plan will be chosen.",1,5,workload-specific,postgresql
7502,geqo_effort,GEQO: effort is used to set the default for other GEQO parameters,1,6,function-tradeoff,postgresql
7503,geqo_generations,"Controls the number of generations used by GEQO, that is the number of iterations of the algorithm. It must be at least one, and useful values are in the same range as the pool size. If it is set to zero (the default setting) then a suitable value is chosen based on geqo_pool_size.",1,5,workload-specific,postgresql
7504,geqo_generations,GEQO: number of iterations of the algorithm,1,5,workload-specific,postgresql
7505,geqo_pool_size,"Controls the pool size used by GEQO, that is the number of individuals in the genetic population. It must be at least two, and useful values are typically 100 to 1000. If it is set to zero (the default setting) then a suitable value is chosen based on geqo_effort and the number of tables in the query.",1,1,resource,postgresql
7506,geqo_pool_size,GEQO: number of individuals in the population,1,5,workload-specific,postgresql
7507,geqo_seed,"Controls the initial value of the random number generator used by GEQO to select random paths through the join order search space. The value can range from zero (the default) to one. Varying the value changes the set of join paths explored, and may result in a better or worse best path being found.",0,0,others,postgresql
7508,geqo_seed,GEQO: seed for random path selection,0,0,others,postgresql
7509,geqo_selection_bias,Controls the selection bias used by GEQO. The selection bias is the selective pressure within the population. Values can be from 1.50 to 2.00; the latter is the default.,0,0,others,postgresql
7510,geqo_selection_bias,GEQO: selective pressure within the population,0,0,others,postgresql
7511,geqo_threshold,Sets the threshold of FROM items beyond which GEQO is used,1,5,workload-specific,postgresql
7512,geqo_threshold,"Use genetic query optimization to plan queries with at least this many FROM items involved. (Note that a FULL OUTER JOIN construct counts as only one FROM item.) The default is 12. For simpler queries it is usually best to use the regular, exhaustive-search planner, but for queries with many tables the exhaustive search takes too long, often longer than the penalty of executing a suboptimal plan. Thus, a threshold on the size of the query is a convenient way to manage use of GEQO.",1,5,workload-specific,postgresql
7513,gin_fuzzy_search_limit,Sets the maximum allowed result for exact search by GIN,0,0,others,postgresql
7514,gin_fuzzy_search_limit,Soft upper limit of the size of the set returned by GIN index scans. For more information see gin-tips.,0,0,others,postgresql
7515,gin_pending_list_limit,"Sets the maximum size of the GIN pending list which is used when fastupdate is enabled. If the list grows larger than this maximum size, it is cleaned up by moving the entries in it to the main GIN data structure in bulk. The default is four megabytes (4MB). This setting can be overridden for individual GIN indexes by changing index storage parameters. See gin-fast-update and gin-tips for more information.",1,1,resource,postgresql
7516,gin_pending_list_limit,Sets the maximum size of the pending list for GIN index,1,1,resource,postgresql
7517,hba_file,Specifies the configuration file for host-based authentication (customarily called pg_hba.conf). This parameter can only be set at server start.,0,0,others,postgresql
7518,hba_file,"Sets the server's ""hba"" configuration file",0,0,others,postgresql
7519,hot_standby,Allows connections and queries during recovery,0,0,others,postgresql
7520,hot_standby,"Specifies whether or not you can connect and run queries during recovery, as described in hot-standby. The default value is on. This parameter can only be set at server start. It only has effect during archive recovery or in standby mode.",0,0,others,postgresql
7521,hot_standby_feedback,Allows feedback from a hot standby to the primary that will avoid query conflicts,1,6,function-tradeoff,postgresql
7522,hot_standby_feedback,"Specifies whether or not a hot standby will send feedback to the primary or upstream standby about queries currently executing on the standby. This parameter can be used to eliminate query cancels caused by cleanup records, but can cause database bloat on the primary for some workloads. Feedback messages will not be sent more frequently than once per wal_receiver_status_interval. The default value is off. This parameter can only be set in the postgresql.conf file or on the server command line.",1,6,function-tradeoff,postgresql
7523,huge_pages,"Enables/disables the use of huge memory pages. Valid values are try (the default), on, and off.",1,4,limited-side-effect,postgresql
7524,huge_pages,Use of huge pages on Linux,1,4,limited-side-effect,postgresql
7525,ident_file,Specifies the configuration file for user name mapping (customarily called pg_ident.conf). This parameter can only be set at server start. See also auth-username-maps.,0,0,others,postgresql
7526,ident_file,"Sets the server's ""ident"" configuration file",0,0,others,postgresql
7527,idle_in_transaction_session_timeout,Sets the maximum allowed duration of any idling transaction,0,0,others,postgresql
7528,idle_in_transaction_session_timeout,Terminate any session with an open transaction that has been idle for longer than the specified duration in milliseconds. This allows any locks held by that session to be released and the connection slot to be reused; it also allows tuples visible only to this transaction to be vacuumed. See routine-vacuuming for more details about this.,0,0,others,postgresql
7529,ignore_checksum_failure,Continues processing after a checksum failure,0,0,others,postgresql
7530,ignore_checksum_failure,Only has effect if app-initdb-data-checksums are enabled.,0,0,others,postgresql
7531,ignore_system_indexes,Disables reading from system indexes,0,0,others,postgresql
7532,ignore_system_indexes,Ignore system indexes when reading system tables (but still update the indexes when modifying the tables). This is useful when recovering from damaged system indexes. This parameter cannot be changed after session start.,0,0,others,postgresql
7533,IntervalStyle,Sets the display format for interval values,0,0,others,postgresql
7534,IntervalStyle,Sets the display format for interval values. The value sql_standard will produce output matching SQL standard interval literals. The value postgres (which is the default) will produce output matching PostgreSQL releases prior to 8.4 when the DateStyle parameter was set to ISO. The value postgres_verbose will produce output matching PostgreSQL releases prior to 8.4 when the DateStyle parameter was set to non-ISO output. The value iso_8601 will produce output matching the time interval format with designators defined in section 4.4.3.2 of ISO 8601.,0,0,others,postgresql
7535,jit,"Determines whether JIT compilation may be used by PostgreSQL, if available. Just-in-Time (JIT) compilation is the process of turning some form of interpreted program evaluation into a native program, and doing so at run time. For example, instead of using general-purpose code that can evaluate arbitrary SQL expressions to evaluate a particular SQL predicate like WHERE a.col = 3, it is possible to generate a function that is specific to that expression and can be natively executed by the CPU, yielding a speedup.",1,4,limited-side-effect,postgresql
7536,join_collapse_limit,Sets the FROM-list size beyond which JOIN constructs are not flattened,1,6,function-tradeoff,postgresql
7537,join_collapse_limit,The planner will rewrite explicit JOIN constructs (except FULL JOINs) into lists of FROM items whenever a list of no more than this many items would result. Smaller values reduce planning time but might yield inferior query plans.,1,6,function-tradeoff,postgresql
7538,krb_caseins_users,Sets whether Kerberos and GSSAPI user names should be treated as case-insensitive,0,0,others,postgresql
7539,krb_caseins_users,Sets whether GSSAPI user names should be treated case-insensitively. The default is off (case sensitive). This parameter can only be set in the postgresql.conf file or on the server command line.,0,0,others,postgresql
7540,krb_server_keyfile,Sets the location of the Kerberos server key file. See gssapi-auth for details. This parameter can only be set in the postgresql.conf file or on the server command line.,0,0,others,postgresql
7541,krb_server_keyfile,Sets the location of the Kerberos server key file,0,0,others,postgresql
7542,lc_messages,Sets the language in which messages are displayed,0,0,others,postgresql
7543,lc_messages,Sets the language in which messages are displayed. Acceptable values are system-dependent; see locale for more information. If this variable is set to the empty string (which is the default) then the value is inherited from the execution environment of the server in a system-dependent way.,0,0,others,postgresql
7544,lc_monetary,Sets the locale for formatting monetary amounts,0,0,others,postgresql
7545,lc_monetary,"Sets the locale to use for formatting monetary amounts, for example with the to_char family of functions. Acceptable values are system-dependent; see locale for more information. If this variable is set to the empty string (which is the default) then the value is inherited from the execution environment of the server in a system-dependent way.",0,0,others,postgresql
7546,lc_numeric,Sets the locale for formatting numbers,0,0,others,postgresql
7547,lc_numeric,"Sets the locale to use for formatting numbers, for example with the to_char family of functions. Acceptable values are system-dependent; see locale for more information. If this variable is set to the empty string (which is the default) then the value is inherited from the execution environment of the server in a system-dependent way.",0,0,others,postgresql
7548,lc_time,Sets the locale for formatting date and time values,0,0,others,postgresql
7549,lc_time,"Sets the locale to use for formatting dates and times, for example with the to_char family of functions. Acceptable values are system-dependent; see locale for more information. If this variable is set to the empty string (which is the default) then the value is inherited from the execution environment of the server in a system-dependent way.",0,0,others,postgresql
7550,listen_addresses,"Specifies the TCP/IP address(es) on which the server is to listen for connections from client applications. The value takes the form of a comma-separated list of host names and/or numeric IP addresses. The special entry * corresponds to all available IP interfaces. The entry 0.0.0.0 allows listening for all IPv4 addresses and :: allows listening for all IPv6 addresses. If the list is empty, the server does not listen on any IP interface at all, in which case only Unix-domain sockets can be used to connect to it. The default value is localhost, which allows only local TCP/IP loopback connections to be made. While client authentication (client-authentication) allows fine-grained control over who can access the server, listen_addresses controls which interfaces accept connection attempts, which can help prevent repeated malicious connection requests on insecure network interfaces. This parameter can only be set at server start.",0,0,others,postgresql
7551,listen_addresses,Sets the host name or IP address(es) to listen to,0,0,others,postgresql
7552,lo_compat_privileges,Enables backward compatibility mode for privilege checks on large objects,0,0,others,postgresql
7553,lo_compat_privileges,"In PostgreSQL releases prior to 9.0, large objects did not have access privileges and were, therefore, always readable and writable by all users. Setting this variable to on disables the new privilege checks, for compatibility with prior releases. The default is off. Only superusers can change this setting.",0,0,others,postgresql
7554,local_preload_libraries,Lists unprivileged shared libraries to preload into each backend,0,0,others,postgresql
7555,local_preload_libraries,"This variable specifies one or more shared libraries that are to be preloaded at connection start. It contains a comma-separated list of library names, where each name is interpreted as for the SQL-LOAD command. Whitespace between entries is ignored; surround a library name with double quotes if you need to include whitespace or commas in the name. The parameter value only takes effect at the start of the connection. Subsequent changes have no effect. If a specified library is not found, the connection attempt will fail.",0,0,others,postgresql
7556,lock_timeout,"Abort any statement that waits longer than the specified number of milliseconds while attempting to acquire a lock on a table, index, row, or other database object. The time limit applies separately to each lock acquisition attempt. The limit applies both to explicit locking requests (such as LOCK TABLE, or SELECT FOR UPDATE without NOWAIT) and to implicitly-acquired locks. A value of zero (the default) turns this off.",0,0,others,postgresql
7557,lock_timeout,Sets the maximum allowed duration of any wait for a lock,0,0,others,postgresql
7558,log_autovacuum_min_duration,"Causes each action executed by autovacuum to be logged if it ran for at least the specified number of milliseconds. Setting this to zero logs all autovacuum actions. Minus-one (the default) disables logging autovacuum actions. For example, if you set this to 250ms then all automatic vacuums and analyzes that run 250ms or longer will be logged. In addition, when this parameter is set to any value other than -1, a message will be logged if an autovacuum action is skipped due to the existence of a conflicting lock. Enabling this parameter can be helpful in tracking autovacuum activity. This parameter can only be set in the postgresql.conf file or on the server command line; but the setting can be overridden for individual tables by changing table storage parameters.",0,0,others,postgresql
7559,log_autovacuum_min_duration,Sets the minimum execution time above which autovacuum actions will be logged,0,0,others,postgresql
7560,log_checkpoints,"Causes checkpoints and restartpoints to be logged in the server log. Some statistics are included in the log messages, including the number of buffers written and the time spent writing them. This parameter can only be set in the postgresql.conf file or on the server command line. The default is off.",1,6,function-tradeoff,postgresql
7561,log_checkpoints,Logs each checkpoint,1,6,function-tradeoff,postgresql
7562,log_connections,"Causes each attempted connection to the server to be logged, as well as successful completion of client authentication. Only superusers can change this parameter at session start, and it cannot be changed at all within a session. The default is off.",1,6,function-tradeoff,postgresql
7563,log_connections,Logs each successful connection,1,6,function-tradeoff,postgresql
7564,log_destination,"PostgreSQL supports several methods for logging server messages, including stderr, csvlog and syslog. On Windows, eventlog is also supported. Set this parameter to a list of desired log destinations separated by commas. The default is to log to stderr only. This parameter can only be set in the postgresql.conf file or on the server command line.",0,0,others,postgresql
7565,log_destination,Sets the destination for server log output,0,0,others,postgresql
7566,log_directory,Sets the destination directory for log files,0,0,others,postgresql
7567,log_directory,"When logging_collector is enabled, this parameter determines the directory in which log files will be created. It can be specified as an absolute path, or relative to the cluster data directory. This parameter can only be set in the postgresql.conf file or on the server command line. The default is log.",0,0,others,postgresql
7568,log_disconnections,"Causes session terminations to be logged. The log output provides information similar to log_connections, plus the duration of the session. Only superusers can change this parameter at session start, and it cannot be changed at all within a session. The default is off.",1,6,function-tradeoff,postgresql
7569,log_disconnections,"Logs end of a session, including duration",0,0,others,postgresql
7570,log_duration,Causes the duration of every completed statement to be logged. The default is off. Only superusers can change this setting.,0,0,others,postgresql
7571,log_duration,Logs the duration of each completed SQL statement,0,0,others,postgresql
7572,log_error_verbosity,"Controls the amount of detail written in the server log for each message that is logged. Valid values are TERSE, DEFAULT, and VERBOSE, each adding more fields to displayed messages. TERSE excludes the logging of DETAIL, HINT, QUERY, and CONTEXT error information. VERBOSE output includes the SQLSTATE error code (see also errcodes-appendix) and the source code file name, function name, and line number that generated the error. Only superusers can change this setting.",1,6,function-tradeoff,postgresql
7573,log_error_verbosity,Sets the verbosity of logged messages,1,6,function-tradeoff,postgresql
7574,log_executor_stats,"For each query, output performance statistics of the respective module to the server log. This is a crude profiling instrument, similar to the Unix getrusage() operating system facility. log_statement_stats reports total statement statistics, while the others report per-module statistics. log_statement_stats cannot be enabled together with any of the per-module options. All of these options are disabled by default. Only superusers can change these settings.",1,6,function-tradeoff,postgresql
7575,log_executor_stats,Writes executor performance statistics to the server log,1,6,function-tradeoff,postgresql
7576,log_file_mode,On Unix systems this parameter sets the permissions for log files when logging_collector is enabled. (On Microsoft Windows this parameter is ignored.) The parameter value is expected to be a numeric mode specified in the format accepted by the chmod and umask system calls. (To use the customary octal format the number must start with a 0 (zero).),0,0,others,postgresql
7577,log_file_mode,Sets the file permissions for log files,0,0,others,postgresql
7578,log_filename,Sets the file name pattern for log files,0,0,others,postgresql
7579,log_filename,"When logging_collector is enabled, this parameter sets the file names of the created log files. The value is treated as a strftime pattern, so %-escapes can be used to specify time-varying file names. (Note that if there are any time-zone-dependent %-escapes, the computation is done in the zone specified by log_timezone.) The supported %-escapes are similar to those listed in the Open Group's strftime",0,0,others,postgresql
7580,log_hostname,"By default, connection log messages only show the IP address of the connecting host. Turning this parameter on causes logging of the host name as well. Note that depending on your host name resolution setup this might impose a non-negligible performance penalty. This parameter can only be set in the postgresql.conf file or on the server command line.",1,6,function-tradeoff,postgresql
7581,log_hostname,Logs the host name in the connection logs,1,6,function-tradeoff,postgresql
7582,log_line_prefix,Controls information prefixed to each log line,0,0,others,postgresql
7583,log_line_prefix,"This is a printf-style string that is output at the beginning of each log line. % characters begin escape sequences that are replaced with status information as outlined below. Unrecognized escapes are ignored. Other characters are copied straight to the log line. Some escapes are only recognized by session processes, and will be treated as empty by background processes such as the main server process. Status information may be aligned either left or right by specifying a numeric literal after the % and before the option. A negative value will cause the status information to be padded on the right with spaces to give it a minimum width, whereas a positive value will pad on the left. Padding can be useful to aid human readability in log files. This parameter can only be set in the postgresql.conf file or on the server command line. The default is '%m [%p] ' which logs a time stamp and the process ID. EscapeEffectSession only%aApplication nameyes%uUser nameyes%dDatabase nameyes%rRemote host name or IP address, and remote portyes%hRemote host name or IP addressyes%pProcess IDno%tTime stamp without millisecondsno%mTime stamp with millisecondsno%nTime stamp with milliseconds (as a Unix epoch)no%iCommand tag: type of session's current commandyes%eSQLSTATE error codeno%cSession ID: see belowno%lNumber of the log line for each session or process, starting at 1no%sProcess start time stampno%vVirtual transaction ID (backendID/localXID)no%xTransaction ID (0 if none is assigned)no%qProduces no output, but tells non-session processes to stop at this point in the string; ignored by session processesno%%Literal %no The %c escape prints a quasi-unique session identifier, consisting of two 4-byte hexadecimal numbers (without leading zeros) separated by a dot. The numbers are the process start time and the process ID, so %c can also be used as a space saving way of printing those items. For example, to generate the session identifier from pg_stat_activity, use this query:SELECT to_hex(trunc(EXTRACT(EPOCH FROM backend_start))::integer) || '.' || to_hex(pid)FROM pg_stat_activity;",0,0,others,postgresql
7584,log_lock_waits,Controls whether a log message is produced when a session waits longer than deadlock_timeout to acquire a lock. This is useful in determining if lock waits are causing poor performance. The default is off. Only superusers can change this setting.,0,0,others,postgresql
7585,log_lock_waits,Logs long lock waits,0,0,others,postgresql
7586,log_min_duration_statement,"Causes the duration of each completed statement to be logged if the statement ran for at least the specified number of milliseconds. Setting this to zero prints all statement durations. Minus-one (the default) disables logging statement durations. For example, if you set it to 250ms then all SQL statements that run 250ms or longer will be logged. Enabling this parameter can be helpful in tracking down unoptimized queries in your applications. Only superusers can change this setting.",0,0,others,postgresql
7587,log_min_duration_statement,Sets the minimum execution time above which statements will be logged,0,0,others,postgresql
7588,log_min_error_statement,Causes all statements generating error at or above this level to be logged,0,0,others,postgresql
7589,log_min_error_statement,"Controls which SQL statements that cause an error condition are recorded in the server log. The current SQL statement is included in the log entry for any message of the specified severity or higher. Valid values are DEBUG5, DEBUG4, DEBUG3, DEBUG2, DEBUG1, INFO, NOTICE, WARNING, ERROR, LOG, FATAL, and PANIC. The default is ERROR, which means statements causing errors, log messages, fatal errors, or panics will be logged. To effectively turn off logging of failing statements, set this parameter to PANIC. Only superusers can change this setting.",0,0,others,postgresql
7590,log_min_messages,"Controls which message levels are written to the server log. Valid values are DEBUG5, DEBUG4, DEBUG3, DEBUG2, DEBUG1, INFO, NOTICE, WARNING, ERROR, LOG, FATAL, and PANIC. Each level includes all the levels that follow it. The later the level, the fewer messages are sent to the log. The default is WARNING. Note that LOG has a different rank here than in client_min_messages. Only superusers can change this setting.",0,0,others,postgresql
7591,log_min_messages,Sets the message levels that are logged,0,0,others,postgresql
7592,log_parser_stats,"For each query, output performance statistics of the respective module to the server log. This is a crude profiling instrument, similar to the Unix getrusage() operating system facility. log_statement_stats reports total statement statistics, while the others report per-module statistics. log_statement_stats cannot be enabled together with any of the per-module options. All of these options are disabled by default. Only superusers can change these settings.",1,6,function-tradeoff,postgresql
7593,log_parser_stats,Writes parser performance statistics to the server log,1,6,function-tradeoff,postgresql
7594,log_planner_stats,"For each query, output performance statistics of the respective module to the server log. This is a crude profiling instrument, similar to the Unix getrusage() operating system facility. log_statement_stats reports total statement statistics, while the others report per-module statistics. log_statement_stats cannot be enabled together with any of the per-module options. All of these options are disabled by default. Only superusers can change these settings.",1,6,function-tradeoff,postgresql
7595,log_planner_stats,Writes planner performance statistics to the server log,1,6,function-tradeoff,postgresql
7596,log_replication_commands,Causes each replication command to be logged in the server log. See protocol-replication for more information about replication command. The default value is off. Only superusers can change this setting.,1,6,function-tradeoff,postgresql
7597,log_replication_commands,Logs each replication command,1,6,function-tradeoff,postgresql
7598,log_rotation_age,Automatic log file rotation will occur after N minutes,0,0,others,postgresql
7599,log_rotation_age,"When logging_collector is enabled, this parameter determines the maximum lifetime of an individual log file. After this many minutes have elapsed, a new log file will be created. Set to zero to disable time-based creation of new log files. This parameter can only be set in the postgresql.conf file or on the server command line.",0,0,others,postgresql
7600,log_rotation_size,Automatic log file rotation will occur after N kilobytes,0,0,others,postgresql
7601,log_rotation_size,"When logging_collector is enabled, this parameter determines the maximum size of an individual log file. After this many kilobytes have been emitted into a log file, a new log file will be created. Set to zero to disable size-based creation of new log files. This parameter can only be set in the postgresql.conf file or on the server command line.",0,0,others,postgresql
7602,log_statement,"Controls which SQL statements are logged. Valid values are none (off), ddl, mod, and all (all statements). ddl logs all data definition statements, such as CREATE, ALTER, and DROP statements. mod logs all ddl statements, plus data-modifying statements such as INSERT, UPDATE, DELETE, TRUNCATE, and COPY FROM. PREPARE, EXECUTE, and EXPLAIN ANALYZE statements are also logged if their contained command is of an appropriate type. For clients using extended query protocol, logging occurs when an Execute message is received, and values of the Bind parameters are included (with any embedded single-quote marks doubled).",1,6,function-tradeoff,postgresql
7603,log_statement,Sets the type of statements logged,1,6,function-tradeoff,postgresql
7604,log_statement_stats,"For each query, output performance statistics of the respective module to the server log. This is a crude profiling instrument, similar to the Unix getrusage() operating system facility. log_statement_stats reports total statement statistics, while the others report per-module statistics. log_statement_stats cannot be enabled together with any of the per-module options. All of these options are disabled by default. Only superusers can change these settings.",1,6,function-tradeoff,postgresql
7605,log_statement_stats,Writes cumulative performance statistics to the server log,1,6,function-tradeoff,postgresql
7606,log_temp_files,"Controls logging of temporary file names and sizes. Temporary files can be created for sorts, hashes, and temporary query results. A log entry is made for each temporary file when it is deleted. A value of zero logs all temporary file information, while positive values log only files whose size is greater than or equal to the specified number of kilobytes. The default setting is -1, which disables such logging. Only superusers can change this setting.",0,0,others,postgresql
7607,log_temp_files,Log the use of temporary files larger than this number of kilobytes,0,0,others,postgresql
7608,log_timezone,Sets the time zone to use in log messages,0,0,others,postgresql
7609,log_timezone,"Sets the time zone used for timestamps written in the server log. Unlike TimeZone, this value is cluster-wide, so that all sessions will report timestamps consistently. The built-in default is GMT, but that is typically overridden in postgresql.conf; initdb will install a setting there corresponding to its system environment. See datatype-timezones for more information. This parameter can only be set in the postgresql.conf file or on the server command line.",0,0,others,postgresql
7610,log_truncate_on_rotation,Truncate existing log files of same name during log rotation,0,0,others,postgresql
7611,log_truncate_on_rotation,"When logging_collector is enabled, this parameter will cause PostgreSQL to truncate (overwrite), rather than append to, any existing log file of the same name. However, truncation will occur only when a new file is being opened due to time-based rotation, not during server startup or size-based rotation. When off, pre-existing files will be appended to in all cases. For example, using this setting in combination with a log_filename like postgresql-%H.log would result in generating twenty-four hourly log files and then cyclically overwriting them. This parameter can only be set in the postgresql.conf file or on the server command line.",0,0,others,postgresql
7612,logging_collector,Start a subprocess to capture stderr output and/or csvlogs into log files,1,6,function-tradeoff,postgresql
7613,logging_collector,"This parameter enables the logging collector, which is a background process that captures log messages sent to stderr and redirects them into log files. This approach is often more useful than logging to syslog, since some types of messages might not appear in syslog output. (One common example is dynamic-linker failure messages; another is error messages produced by scripts such as archive_command.) This parameter can only be set at server start.",1,6,function-tradeoff,postgresql
7614,maintenance_work_mem,Sets the maximum memory to be used for maintenance operations,1,1,resource,postgresql
7615,maintenance_work_mem,"Specifies the maximum amount of memory to be used by maintenance operations, such as VACUUM, CREATE INDEX, and ALTER TABLE ADD FOREIGN KEY. It defaults to 64 megabytes (64MB). Since only one of these operations can be executed at a time by a database session, and an installation normally doesn't have many of them running concurrently, it's safe to set this value significantly larger than work_mem. Larger settings might improve performance for vacuuming and for restoring database dumps.",1,1,resource,postgresql
7616,max_connections,"Determines the maximum number of concurrent connections to the database server. The default is typically 100 connections, but might be less if your kernel settings will not support it (as determined during initdb). This parameter can only be set at server start.",1,1,resource,postgresql
7617,max_connections,Sets the maximum number of concurrent connections,1,1,resource,postgresql
7618,max_files_per_process,"Sets the maximum number of simultaneously open files allowed to each server subprocess. The default is one thousand files. If the kernel is enforcing a safe per-process limit, you don't need to worry about this setting. But on some platforms (notably, most BSD systems), the kernel will allow individual processes to open many more files than the system can actually support if many processes all try to open that many files. If you find yourself seeing Too many open files failures, try reducing this setting. This parameter can only be set at server start.",1,1,resource,postgresql
7619,max_files_per_process,Sets the maximum number of simultaneously open files for each server process,1,1,resource,postgresql
7620,max_locks_per_transaction,Sets the maximum number of locks per transaction,0,0,others,postgresql
7621,max_locks_per_transaction,"The shared lock table tracks locks on max_locks_per_transaction * (max_connections + max_prepared_transactions) objects (e.g., tables); hence, no more than this many distinct objects can be locked at any one time. This parameter controls the average number of object locks allocated for each transaction; individual transactions can lock more objects as long as the locks of all transactions fit in the lock table. This is not the number of rows that can be locked; that value is unlimited. The default, 64, has historically proven sufficient, but you might need to raise this value if you have queries that touch many different tables in a single transaction, e.g. query of a parent table with many children. This parameter can only be set at server start.",0,0,others,postgresql
7622,max_logical_replication_workers,Maximum number of logical replication worker processes,1,1,resource,postgresql
7623,max_logical_replication_workers,Specifies maximum number of logical replication workers. This includes both apply workers and table synchronization workers.,1,1,resource,postgresql
7624,max_parallel_workers,Sets the maximum number of parallel workers that can be active at one time,1,1,resource,postgresql
7625,max_parallel_workers,"Sets the maximum number of workers that the system can support for parallel queries. The default value is 8. When increasing or decreasing this value, consider also adjusting max_parallel_workers_per_gather. Also, note that a setting for this value which is higher than max_worker_processes will have no effect, since parallel workers are taken from the pool of worker processes established by that setting.",1,1,resource,postgresql
7626,max_parallel_workers_per_gather,Sets the maximum number of parallel processes per executor node,1,1,resource,postgresql
7627,max_parallel_workers_per_gather,"Sets the maximum number of workers that can be started by a single Gather or Gather Merge node. Parallel workers are taken from the pool of processes established by max_worker_processes, limited by guc-max-parallel-workers. Note that the requested number of workers may not actually be available at run time. If this occurs, the plan will run with fewer workers than expected, which may be inefficient. The default value is 2. Setting this value to 0 disables parallel query execution.",1,1,resource,postgresql
7628,max_pred_locks_per_page,Sets the maximum number of predicate-locked tuples per page,0,0,others,postgresql
7629,max_pred_locks_per_page,This controls how many rows on a single page can be predicate-locked before the lock is promoted to covering the whole page. The default is 2. This parameter can only be set in the postgresql.conf file or on the server command line.,0,0,others,postgresql
7630,max_pred_locks_per_relation,Sets the maximum number of predicate-locked pages and tuples per relation,0,0,others,postgresql
7631,max_pred_locks_per_relation,"This controls how many pages or tuples of a single relation can be predicate-locked before the lock is promoted to covering the whole relation. Values greater than or equal to zero mean an absolute limit, while negative values mean max_pred_locks_per_transaction divided by the absolute value of this setting. The default is -2, which keeps the behavior from previous versions of PostgreSQL. This parameter can only be set in the postgresql.conf file or on the server command line.",0,0,others,postgresql
7632,max_pred_locks_per_transaction,Sets the maximum number of predicate locks per transaction,0,0,others,postgresql
7633,max_pred_locks_per_transaction,"The shared predicate lock table tracks locks on max_pred_locks_per_transaction * (max_connections + max_prepared_transactions) objects (e.g., tables); hence, no more than this many distinct objects can be locked at any one time. This parameter controls the average number of object locks allocated for each transaction; individual transactions can lock more objects as long as the locks of all transactions fit in the lock table. This is not the number of rows that can be locked; that value is unlimited. The default, 64, has generally been sufficient in testing, but you might need to raise this value if you have clients that touch many different tables in a single serializable transaction. This parameter can only be set at server start.",0,0,others,postgresql
7634,max_prepared_transactions,Sets the maximum number of simultaneously prepared transactions,1,1,resource,postgresql
7635,max_prepared_transactions,Sets the maximum number of transactions that can be in the prepared state simultaneously (see sql-prepare-transaction). Setting this parameter to zero (which is the default) disables the prepared-transaction feature. This parameter can only be set at server start.,1,1,resource,postgresql
7636,max_replication_slots,Sets the maximum number of simultaneously defined replication slots,0,0,others,postgresql
7637,max_replication_slots,Specifies the maximum number of replication slots (see streaming-replication-slots) that the server can support. The default is 10. This parameter can only be set at server start. wal_level must be set to replica or higher to allow replication slots to be used. Setting it to a lower value than the number of currently existing replication slots will prevent the server from starting.,0,0,others,postgresql
7638,max_stack_depth,"Sets the maximum stack depth, in kilobytes",1,3,reliability-tradeoff,postgresql
7639,max_stack_depth,"Specifies the maximum safe depth of the server's execution stack. The ideal setting for this parameter is the actual stack size limit enforced by the kernel (as set by ulimit -s or local equivalent), less a safety margin of a megabyte or so. The safety margin is needed because the stack depth is not checked in every routine in the server, but only in key potentially-recursive routines such as expression evaluation. The default setting is two megabytes (2MB), which is conservatively small and unlikely to risk crashes. However, it might be too small to allow execution of complex functions. Only superusers can change this setting.",1,3,reliability-tradeoff,postgresql
7640,max_standby_archive_delay,Sets the maximum delay before canceling queries when a hot standby server is processing archived WAL data,0,0,others,postgresql
7641,max_standby_archive_delay,"When Hot Standby is active, this parameter determines how long the standby server should wait before canceling standby queries that conflict with about-to-be-applied WAL entries, as described in hot-standby-conflict. max_standby_archive_delay applies when WAL data is being read from WAL archive (and is therefore not current). The default is 30 seconds. Units are milliseconds if not specified. A value of -1 allows the standby to wait forever for conflicting queries to complete. This parameter can only be set in the postgresql.conf file or on the server command line.",0,0,others,postgresql
7642,max_standby_streaming_delay,Sets the maximum delay before canceling queries when a hot standby server is processing streamed WAL data,0,0,others,postgresql
7643,max_standby_streaming_delay,"When Hot Standby is active, this parameter determines how long the standby server should wait before canceling standby queries that conflict with about-to-be-applied WAL entries, as described in hot-standby-conflict. max_standby_streaming_delay applies when WAL data is being received via streaming replication. The default is 30 seconds. Units are milliseconds if not specified. A value of -1 allows the standby to wait forever for conflicting queries to complete. This parameter can only be set in the postgresql.conf file or on the server command line.",0,0,others,postgresql
7644,max_sync_workers_per_subscription,Maximum number of synchronization workers per subscription. This parameter controls the amount of parallelism of the initial data copy during the subscription initialization or when new tables are added.,1,1,resource,postgresql
7645,max_sync_workers_per_subscription,Maximum number of table synchronization workers per subscription,1,1,resource,postgresql
7646,max_wal_senders,Sets the maximum number of simultaneously running WAL sender processes,1,1,resource,postgresql
7647,max_wal_senders,"Specifies the maximum number of concurrent connections from standby servers or streaming base backup clients (i.e., the maximum number of simultaneously running WAL sender processes). The default is 10. The value 0 means replication is disabled. WAL sender processes count towards the total number of connections, so the parameter cannot be set higher than max_connections. Abrupt streaming client disconnection might cause an orphaned connection slot until a timeout is reached, so this parameter should be set slightly higher than the maximum number of expected clients so disconnected clients can immediately reconnect. This parameter can only be set at server start. wal_level must be set to replica or higher to allow connections from standby servers.",1,1,resource,postgresql
7648,max_wal_size,"Maximum size to let the WAL grow to between automatic WAL checkpoints. This is a soft limit; WAL size can exceed max_wal_size under special circumstances, like under heavy load, a failing archive_command, or a high wal_keep_segments setting. The default is 1 GB. Increasing this parameter can increase the amount of time needed for crash recovery. This parameter can only be set in the postgresql.conf file or on the server command line.",1,5,workload-specific,postgresql
7649,max_wal_size,Sets the WAL size that triggers a checkpoint,1,5,workload-specific,postgresql
7650,max_worker_processes,Maximum number of concurrent worker processes,1,1,resource,postgresql
7651,max_worker_processes,Sets the maximum number of background processes that the system can support. This parameter can only be set at server start. The default is 8.,1,1,resource,postgresql
7652,min_parallel_index_scan_size,Sets the minimum amount of index data for a parallel scan,0,0,others,postgresql
7653,min_parallel_index_scan_size,Sets the minimum amount of index data that must be scanned in order for a parallel scan to be considered. Note that a parallel index scan typically won't touch the entire index; it is the number of pages which the planner believes will actually be touched by the scan which is relevant. The default is 512 kilobytes (512kB).,0,0,others,postgresql
7654,min_parallel_table_scan_size,Sets the minimum amount of table data for a parallel scan,0,0,others,postgresql
7655,min_parallel_table_scan_size,"Sets the minimum amount of table data that must be scanned in order for a parallel scan to be considered. For a parallel sequential scan, the amount of table data scanned is always equal to the size of the table, but when indexes are used the amount of table data scanned will normally be less. The default is 8 megabytes (8MB).",0,0,others,postgresql
7656,min_wal_size,"As long as WAL disk usage stays below this setting, old WAL files are always recycled for future use at a checkpoint, rather than removed. This can be used to ensure that enough WAL space is reserved to handle spikes in WAL usage, for example when running large batch jobs. The default is 80 MB. This parameter can only be set in the postgresql.conf file or on the server command line.",0,0,others,postgresql
7657,min_wal_size,Sets the minimum size to shrink the WAL to,0,0,others,postgresql
7658,old_snapshot_threshold,Time before a snapshot is too old to read pages changed after the snapshot was taken,0,0,others,postgresql
7659,old_snapshot_threshold,Sets the minimum time that a snapshot can be used without risk of a snapshot too old error occurring when using the snapshot. This parameter can only be set at server start.,0,0,others,postgresql
7660,operator_precedence_warning,Emit a warning for constructs that changed meaning since PostgreSQL 9.4,0,0,others,postgresql
7661,operator_precedence_warning,"When on, the parser will emit a warning for any construct that might have changed meanings since PostgreSQL 9.4 as a result of changes in operator precedence. This is useful for auditing applications to see if precedence changes have broken anything; but it is not meant to be kept turned on in production, since it will warn about some perfectly valid, standard-compliant SQL code. The default is off.",0,0,others,postgresql
7662,parallel_leader_participation,"Allows the leader process to execute the query plan under Gather and Gather Merge nodes instead of waiting for worker processes. The default is on. Setting this value to off reduces the likelihood that workers will become blocked because the leader is not reading tuples fast enough, but requires the leader process to wait for worker processes to start up before the first tuples can be produced. The degree to which the leader can help or hinder performance depends on the plan type, number of workers and query duration.",1,5,workload-specific,postgresql
7663,parallel_setup_cost,Sets the planner's estimate of the cost of launching parallel worker processes. The default is 1000.,1,5,workload-specific,postgresql
7664,parallel_setup_cost,Sets the planner's estimate of the cost of starting up worker processes for parallel query,1,5,workload-specific,postgresql
7665,parallel_tuple_cost,Sets the planner's estimate of the cost of passing each tuple (row) from worker to master backend,1,5,workload-specific,postgresql
7666,parallel_tuple_cost,Sets the planner's estimate of the cost of transferring one tuple from a parallel worker process to another process. The default is 0.1.,1,5,workload-specific,postgresql
7667,password_encryption,Chooses the algorithm for encrypting passwords,0,0,others,postgresql
7668,password_encryption,"When a password is specified in sql-createrole or sql-alterrole, this parameter determines the algorithm to use to encrypt the password. The default value is md5, which stores the password as an MD5 hash (on is also accepted, as alias for md5). Setting this parameter to scram-sha-256 will encrypt the password with SCRAM-SHA-256.",0,0,others,postgresql
7669,port,The TCP port the server listens on; 5432 by default. Note that the same port number is used for all IP addresses the server listens on. This parameter can only be set at server start.,0,0,others,postgresql
7670,port,Sets the TCP port the server listens on,0,0,others,postgresql
7671,post_auth_delay,"If nonzero, a delay of this many seconds occurs when a new server process is started, after it conducts the authentication procedure. This is intended to give developers an opportunity to attach to the server process with a debugger. This parameter cannot be changed after session start.",0,0,others,postgresql
7672,post_auth_delay,Waits N seconds on connection startup after authentication,0,0,others,postgresql
7673,pre_auth_delay,"If nonzero, a delay of this many seconds occurs just after a new server process is forked, before it conducts the authentication procedure. This is intended to give developers an opportunity to attach to the server process with a debugger to trace down misbehavior in authentication. This parameter can only be set in the postgresql.conf file or on the server command line.",0,0,others,postgresql
7674,pre_auth_delay,Waits N seconds on connection startup before authentication,0,0,others,postgresql
7675,quote_all_identifiers,"When generating SQL fragments, quote all identifiers",0,0,others,postgresql
7676,quote_all_identifiers,"When the database generates SQL, force all identifiers to be quoted, even if they are not (currently) keywords. This will affect the output of EXPLAIN as well as the results of functions like pg_get_viewdef. See also the --quote-all-identifiers option of app-pgdump and app-pg-dumpall.",0,0,others,postgresql
7677,random_page_cost,Sets the planner's estimate of the cost of a nonsequentially fetched disk page,1,5,workload-specific,postgresql
7678,random_page_cost,Sets the planner's estimate of the cost of a non-sequentially-fetched disk page. The default is 4.0. This value can be overridden for tables and indexes in a particular tablespace by setting the tablespace parameter of the same name (see sql-altertablespace).,1,5,workload-specific,postgresql
7679,replacement_sort_tuples,Sets the maximum number of tuples to be sorted using replacement selection,0,0,others,postgresql
7680,replacement_sort_tuples,"When the number of tuples to be sorted is smaller than this number, a sort will produce its first output run using replacement selection rather than quicksort. This may be useful in memory-constrained environments where tuples that are input into larger sort operations have a strong physical-to-logical correlation. Note that this does not include input tuples with an inverse correlation. It is possible for the replacement selection algorithm to generate one long run that requires no merging, where use of the default strategy would result in many runs that must be merged to produce a final sorted output. This may allow sort operations to complete sooner.",0,0,others,postgresql
7681,restart_after_crash,Reinitialize server after backend crash,0,0,others,postgresql
7682,restart_after_crash,"When set to true, which is the default, PostgreSQL will automatically reinitialize after a backend crash. Leaving this value set to true is normally the best way to maximize the availability of the database. However, in some circumstances, such as when PostgreSQL is being invoked by clusterware, it may be useful to disable the restart so that the clusterware can gain control and take any actions it deems appropriate.",0,0,others,postgresql
7683,row_security,Enable row security,0,0,others,postgresql
7684,row_security,"This variable controls whether to raise an error in lieu of applying a row security policy. When set to on, policies apply normally. When set to off, queries fail which would otherwise apply at least one policy. The default is on. Change to off where limited row visibility could cause incorrect results; for example, pg_dump makes that change by default. This variable has no effect on roles which bypass every row security policy, to wit, superusers and roles with the BYPASSRLS attribute.",0,0,others,postgresql
7685,search_path,Sets the schema search order for names that are not schema-qualified,0,0,others,postgresql
7686,search_path,"This variable specifies the order in which schemas are searched when an object (table, data type, function, etc.) is referenced by a simple name with no schema specified. When there are objects of identical names in different schemas, the one found first in the search path is used. An object that is not in any of the schemas in the search path can only be referenced by specifying its containing schema with a qualified (dotted) name.",0,0,others,postgresql
7687,seq_page_cost,Sets the planner's estimate of the cost of a disk page fetch that is part of a series of sequential fetches. The default is 1.0. This value can be overridden for tables and indexes in a particular tablespace by setting the tablespace parameter of the same name (see sql-altertablespace).,1,5,workload-specific,postgresql
7688,seq_page_cost,Sets the planner's estimate of the cost of a sequentially fetched disk page,1,5,workload-specific,postgresql
7689,session_preload_libraries,Lists shared libraries to preload into each backend,0,0,others,postgresql
7690,session_preload_libraries,"This variable specifies one or more shared libraries that are to be preloaded at connection start. It contains a comma-separated list of library names, where each name is interpreted as for the SQL-LOAD command. Whitespace between entries is ignored; surround a library name with double quotes if you need to include whitespace or commas in the name. The parameter value only takes effect at the start of the connection. Subsequent changes have no effect. If a specified library is not found, the connection attempt will fail. Only superusers can change this setting.",0,0,others,postgresql
7691,session_replication_role,"Controls firing of replication-related triggers and rules for the current session. Setting this variable requires superuser privilege and results in discarding any previously cached query plans. Possible values are origin (the default), replica and local. See sql-altertable for more information.",0,0,others,postgresql
7692,session_replication_role,Sets the session's behavior for triggers and rewrite rules,0,0,others,postgresql
7693,shared_buffers,"Sets the amount of memory the database server uses for shared memory buffers. The default is typically 128 megabytes (128MB), but might be less if your kernel settings will not support it (as determined during initdb). This setting must be at least 128 kilobytes. (Non-default values of BLCKSZ change the minimum.) However, settings significantly higher than the minimum are usually needed for good performance. This parameter can only be set at server start.",1,1,resource,postgresql
7694,shared_buffers,Sets the number of shared memory buffers used by the server,1,1,resource,postgresql
7695,shared_preload_libraries,Lists shared libraries to preload into server,0,0,others,postgresql
7696,shared_preload_libraries,"This variable specifies one or more shared libraries to be preloaded at server start. It contains a comma-separated list of library names, where each name is interpreted as for the SQL-LOAD command. Whitespace between entries is ignored; surround a library name with double quotes if you need to include whitespace or commas in the name. This parameter can only be set at server start. If a specified library is not found, the server will fail to start.",0,0,others,postgresql
7697,ssl,Enables SSL connections,1,2,security-tradeoff,postgresql
7698,ssl,Enables SSL connections. Please read ssl-tcp before using this. This parameter can only be set in the postgresql.conf file or on the server command line. The default is off.,1,2,security-tradeoff,postgresql
7699,ssl_ca_file,Location of the SSL certificate authority file,0,0,others,postgresql
7700,ssl_ca_file,"Specifies the name of the file containing the SSL server certificate authority (CA). Relative paths are relative to the data directory. This parameter can only be set in the postgresql.conf file or on the server command line. The default is empty, meaning no CA file is loaded, and client certificate verification is not performed.",0,0,others,postgresql
7701,ssl_cert_file,Location of the SSL server certificate file,0,0,others,postgresql
7702,ssl_cert_file,Specifies the name of the file containing the SSL server certificate. Relative paths are relative to the data directory. This parameter can only be set in the postgresql.conf file or on the server command line. The default is server.crt.,0,0,others,postgresql
7703,ssl_ciphers,Sets the list of allowed SSL ciphers,0,0,others,postgresql
7704,ssl_ciphers,Specifies a list of SSL cipher suites that are allowed to be used on secure connections. See the ciphers manual page in the OpenSSL package for the syntax of this setting and a list of supported values. This parameter can only be set in the postgresql.conf file or on the server command line. The default value is HIGH:MEDIUM:+3DES:!aNULL. The default is usually a reasonable choice unless you have specific security requirements.,0,0,others,postgresql
7705,ssl_crl_file,Location of the SSL certificate revocation list file,0,0,others,postgresql
7706,ssl_crl_file,"Specifies the name of the file containing the SSL server certificate revocation list (CRL). Relative paths are relative to the data directory. This parameter can only be set in the postgresql.conf file or on the server command line. The default is empty, meaning no CRL file is loaded.",0,0,others,postgresql
7707,ssl_dh_params_file,Location of the SSL DH parameters file,0,0,others,postgresql
7708,ssl_dh_params_file,"Specifies the name of the file containing Diffie-Hellman parameters used for so-called ephemeral DH family of SSL ciphers. The default is empty, in which case compiled-in default DH parameters used. Using custom DH parameters reduces the exposure if an attacker manages to crack the well-known compiled-in DH parameters. You can create your own DH parameters file with the command openssl dhparam -out dhparams.pem 2048.",0,0,others,postgresql
7709,ssl_ecdh_curve,Sets the curve to use for ECDH,1,2,security-tradeoff,postgresql
7710,ssl_ecdh_curve,Specifies the name of the curve to use in ECDH key exchange. It needs to be supported by all clients that connect. It does not need to be the same curve used by the server's Elliptic Curve key. This parameter can only be set in the postgresql.conf file or on the server command line. The default is prime256v1.,0,0,others,postgresql
7711,ssl_key_file,Location of the SSL server private key file,0,0,others,postgresql
7712,ssl_key_file,Specifies the name of the file containing the SSL server private key. Relative paths are relative to the data directory. This parameter can only be set in the postgresql.conf file or on the server command line. The default is server.key.,0,0,others,postgresql
7713,ssl_prefer_server_ciphers,"Specifies whether to use the server's SSL cipher preferences, rather than the client's. This parameter can only be set in the postgresql.conf file or on the server command line. The default is true.",0,0,others,postgresql
7714,ssl_prefer_server_ciphers,Give priority to server ciphersuite order,0,0,others,postgresql
7715,standard_conforming_strings,Causes '...' strings to treat backslashes literally,0,0,others,postgresql
7716,standard_conforming_strings,"This controls whether ordinary string literals ('...') treat backslashes literally, as specified in the SQL standard. Beginning in PostgreSQL 9.1, the default is on (prior releases defaulted to off). Applications can check this parameter to determine how string literals will be processed. The presence of this parameter can also be taken as an indication that the escape string syntax (E'...') is supported. Escape string syntax (sql-syntax-strings-escape) should be used if an application desires backslashes to be treated as escape characters.",0,0,others,postgresql
7717,statement_timeout,"Abort any statement that takes more than the specified number of milliseconds, starting from the time the command arrives at the server from the client. If log_min_error_statement is set to ERROR or lower, the statement that timed out will also be logged. A value of zero (the default) turns this off.",0,0,others,postgresql
7718,statement_timeout,Sets the maximum allowed duration of any statement,0,0,others,postgresql
7719,stats_temp_directory,Sets the directory to store temporary statistics data in. This can be a path relative to the data directory or an absolute path. The default is pg_stat_tmp. Pointing this at a RAM-based file system will decrease physical I/O requirements and can lead to improved performance. This parameter can only be set in the postgresql.conf file or on the server command line.,0,0,others,postgresql
7720,stats_temp_directory,Writes temporary statistics files to the specified directory,0,0,others,postgresql
7721,superuser_reserved_connections,"Determines the number of connection slots that are reserved for connections by PostgreSQL superusers. At most max_connections connections can ever be active simultaneously. Whenever the number of active concurrent connections is at least max_connections minus superuser_reserved_connections, new connections will be accepted only for superusers, and no new replication connections will be accepted.",1,1,resource,postgresql
7722,superuser_reserved_connections,Sets the number of connection slots reserved for superusers,1,1,resource,postgresql
7723,synchronize_seqscans,Enable synchronized sequential scans,1,3,reliability-tradeoff,postgresql
7724,synchronize_seqscans,"This allows sequential scans of large tables to synchronize with each other, so that concurrent scans read the same block at about the same time and hence share the I/O workload. When this is enabled, a scan might start in the middle of the table and then wrap around the end to cover all rows, so as to synchronize with the activity of scans already in progress. This can result in unpredictable changes in the row ordering returned by queries that have no ORDER BY clause. Setting this parameter to off ensures the pre-8.3 behavior in which a sequential scan always starts from the beginning of the table. The default is on.",1,3,reliability-tradeoff,postgresql
7725,synchronous_commit,Sets the current transaction's synchronization level,1,3,reliability-tradeoff,postgresql
7726,synchronous_commit,"Specifies whether transaction commit will wait for WAL records to be written to disk before the command returns a success indication to the client. Valid values are on, remote_apply, remote_write, local, and off. The default, and safe, setting is on. When off, there can be a delay between when success is reported to the client and when the transaction is really guaranteed to be safe against a server crash. (The maximum delay is three times wal_writer_delay.) Unlike fsync, setting this parameter to off does not create any risk of database inconsistency: an operating system or database crash might result in some recent allegedly-committed transactions being lost, but the database state will be just the same as if those transactions had been aborted cleanly. So, turning synchronous_commit off can be a useful alternative when performance is more important than exact certainty about the durability of a transaction. For more discussion see wal-async-commit.",1,3,reliability-tradeoff,postgresql
7727,synchronous_standby_names,Number of synchronous standbys and list of names of potential synchronous ones,0,0,others,postgresql
7728,synchronous_standby_names,"Specifies a list of standby servers that can support synchronous replication, as described in synchronous-replication. There will be one or more active synchronous standbys; transactions waiting for commit will be allowed to proceed after these standby servers confirm receipt of their data. The synchronous standbys will be those whose names appear in this list, and that are both currently connected and streaming data in real-time (as shown by a state of streaming in the pg_stat_replication view). Specifying more than one synchronous standby can allow for very high availability and protection against data loss.",0,0,others,postgresql
7729,syslog_facility,"Sets the syslog ""facility"" to be used when syslog enabled",0,0,others,postgresql
7730,syslog_facility,"When logging to syslog is enabled, this parameter determines the syslogfacility to be used. You can choose from LOCAL0, LOCAL1, LOCAL2, LOCAL3, LOCAL4, LOCAL5, LOCAL6, LOCAL7; the default is LOCAL0. See also the documentation of your system's syslog daemon. This parameter can only be set in the postgresql.conf file or on the server command line.",0,0,others,postgresql
7731,syslog_ident,"When logging to syslog is enabled, this parameter determines the program name used to identify PostgreSQL messages in syslog logs. The default is postgres. This parameter can only be set in the postgresql.conf file or on the server command line.",0,0,others,postgresql
7732,syslog_ident,Sets the program name used to identify PostgreSQL messages in syslog,0,0,others,postgresql
7733,syslog_sequence_numbers,Add sequence number to syslog messages to avoid duplicate suppression,0,0,others,postgresql
7734,syslog_sequence_numbers,"When logging to syslog and this is on (the default), then each message will be prefixed by an increasing sequence number (such as [2]). This circumvents the --- last message repeated N times --- suppression that many syslog implementations perform by default. In more modern syslog implementations, repeated message suppression can be configured (for example, $RepeatedMsgReduction in rsyslog), so this might not be necessary. Also, you could turn this off if you actually want to suppress repeated messages.",0,0,others,postgresql
7735,syslog_split_messages,Split messages sent to syslog by lines and to fit into 1024 bytes,1,4,limited-side-effect,postgresql
7736,syslog_split_messages,"When logging to syslog is enabled, this parameter determines how messages are delivered to syslog. When on (the default), messages are split by lines, and long lines are split so that they will fit into 1024 bytes, which is a typical size limit for traditional syslog implementations. When off, PostgreSQL server log messages are delivered to the syslog service as is, and it is up to the syslog service to cope with the potentially bulky messages.",1,4,limited-side-effect,postgresql
7737,tcp_keepalives_count,Maximum number of TCP keepalive retransmits,0,0,others,postgresql
7738,tcp_keepalives_count,"Specifies the number of TCP keepalives that can be lost before the server's connection to the client is considered dead. A value of 0 uses the system default. This parameter is supported only on systems that support TCP_KEEPCNT or an equivalent socket option; on other systems, it must be zero. In sessions connected via a Unix-domain socket, this parameter is ignored and always reads as zero.",0,0,others,postgresql
7739,tcp_keepalives_idle,"Specifies the number of seconds of inactivity after which TCP should send a keepalive message to the client. A value of 0 uses the system default. This parameter is supported only on systems that support TCP_KEEPIDLE or an equivalent socket option, and on Windows; on other systems, it must be zero. In sessions connected via a Unix-domain socket, this parameter is ignored and always reads as zero.",0,0,others,postgresql
7740,tcp_keepalives_idle,Time between issuing TCP keepalives,0,0,others,postgresql
7741,tcp_keepalives_interval,"Specifies the number of seconds after which a TCP keepalive message that is not acknowledged by the client should be retransmitted. A value of 0 uses the system default. This parameter is supported only on systems that support TCP_KEEPINTVL or an equivalent socket option, and on Windows; on other systems, it must be zero. In sessions connected via a Unix-domain socket, this parameter is ignored and always reads as zero.",0,0,others,postgresql
7742,tcp_keepalives_interval,Time between TCP keepalive retransmits,0,0,others,postgresql
7743,temp_buffers,"Sets the maximum number of temporary buffers used by each database session. These are session-local buffers used only for access to temporary tables. The default is eight megabytes (8MB). The setting can be changed within individual sessions, but only before the first use of temporary tables within the session; subsequent attempts to change the value will have no effect on that session.",1,1,resource,postgresql
7744,temp_buffers,Sets the maximum number of temporary buffers used by each session,1,1,resource,postgresql
7745,temp_file_limit,Limits the total size of all temporary files used by each process,1,1,resource,postgresql
7746,temp_file_limit,"Specifies the maximum amount of disk space that a process can use for temporary files, such as sort and hash temporary files, or the storage file for a held cursor. A transaction attempting to exceed this limit will be canceled. The value is specified in kilobytes, and -1 (the default) means no limit. Only superusers can change this setting.",1,1,resource,postgresql
7747,temp_tablespaces,Sets the tablespace(s) to use for temporary tables and sort files,0,0,others,postgresql
7748,temp_tablespaces,This variable specifies tablespaces in which to create temporary objects (temp tables and indexes on temp tables) when a CREATE command does not explicitly specify a tablespace. Temporary files for purposes such as sorting large data sets are also created in these tablespaces.,0,0,others,postgresql
7749,TimeZone,Sets the time zone for displaying and interpreting time stamps,0,0,others,postgresql
7750,TimeZone,"Sets the time zone for displaying and interpreting time stamps. The built-in default is GMT, but that is typically overridden in postgresql.conf; initdb will install a setting there corresponding to its system environment. See datatype-timezones for more information.",0,0,others,postgresql
7751,timezone_abbreviations,Selects a file of time zone abbreviations,0,0,others,postgresql
7752,timezone_abbreviations,"Sets the collection of time zone abbreviations that will be accepted by the server for datetime input. The default is 'Default', which is a collection that works in most of the world; there are also 'Australia' and 'India', and other collections can be defined for a particular installation. See datetime-config-files for more information.",0,0,others,postgresql
7753,trace_notify,"Generates a great amount of debugging output for the LISTEN and NOTIFY commands. client_min_messages or log_min_messages must be DEBUG1 or lower to send this output to the client or server logs, respectively.",1,6,function-tradeoff,postgresql
7754,trace_notify,Generates debugging output for LISTEN and NOTIFY,1,6,function-tradeoff,postgresql
7755,trace_recovery_messages,Enables logging of recovery-related debugging information,1,6,function-tradeoff,postgresql
7756,trace_recovery_messages,"Enables logging of recovery-related debugging output that otherwise would not be logged. This parameter allows the user to override the normal setting of log_min_messages, but only for specific messages. This is intended for use in debugging Hot Standby. Valid values are DEBUG5, DEBUG4, DEBUG3, DEBUG2, DEBUG1, and LOG. The default, LOG, does not affect logging decisions at all. The other values cause recovery-related debug messages of that priority or higher to be logged as though they had LOG priority; for common settings of log_min_messages this results in unconditionally sending them to the server log. This parameter can only be set in the postgresql.conf file or on the server command line.",1,6,function-tradeoff,postgresql
7757,trace_sort,Emit information about resource usage in sorting,1,6,function-tradeoff,postgresql
7758,trace_sort,"If on, emit information about resource usage during sort operations. This parameter is only available if the TRACE_SORT macro was defined when PostgreSQL was compiled. (However, TRACE_SORT is currently defined by default.)",1,6,function-tradeoff,postgresql
7759,track_activities,Collects information about executing commands,1,6,function-tradeoff,postgresql
7760,track_activities,"Enables the collection of information on the currently executing command of each session, along with the time when that command began execution. This parameter is on by default. Note that even when enabled, this information is not visible to all users, only to superusers and the user owning the session being reported on, so it should not represent a security risk. Only superusers can change this setting.",1,6,function-tradeoff,postgresql
7761,track_activity_query_size,"Sets the size reserved for pg_stat_activity.query, in bytes",1,1,resource,postgresql
7762,track_activity_query_size,"Specifies the number of bytes reserved to track the currently executing command for each active session, for the pg_stat_activity.query field. The default value is 1024. This parameter can only be set at server start.",1,1,resource,postgresql
7763,track_commit_timestamp,Collects transaction commit time,1,6,function-tradeoff,postgresql
7764,track_commit_timestamp,Record commit time of transactions. This parameter can only be set in postgresql.conf file or on the server command line. The default value is off.,1,6,function-tradeoff,postgresql
7765,track_counts,Collects statistics on database activity,1,6,function-tradeoff,postgresql
7766,track_counts,"Enables collection of statistics on database activity. This parameter is on by default, because the autovacuum daemon needs the collected information. Only superusers can change this setting.",1,6,function-tradeoff,postgresql
7767,track_functions,Collects function-level statistics on database activity,1,6,function-tradeoff,postgresql
7768,track_functions,"Enables tracking of function call counts and time used. Specify pl to track only procedural-language functions, all to also track SQL and C language functions. The default is none, which disables function statistics tracking. Only superusers can change this setting.",1,6,function-tradeoff,postgresql
7769,track_io_timing,Collects timing statistics for database I/O activity,1,6,function-tradeoff,postgresql
7770,track_io_timing,"Enables timing of database I/O calls. This parameter is off by default, because it will repeatedly query the operating system for the current time, which may cause significant overhead on some platforms. You can use the pgtesttiming tool to measure the overhead of timing on your system. I/O timing information is displayed in pg-stat-database-view, in the output of sql-explain when the BUFFERS option is used, and by pgstatstatements. Only superusers can change this setting.",1,6,function-tradeoff,postgresql
7771,transform_null_equals,"Treats ""expr=NULL"" as ""expr IS NULL""",0,0,others,postgresql
7772,transform_null_equals,"When on, expressions of the form expr = NULL (or NULL = expr) are treated as expr IS NULL, that is, they return true if expr evaluates to the null value, and false otherwise. The correct SQL-spec-compliant behavior of expr = NULL is to always return null (unknown). Therefore this parameter defaults to off.",0,0,others,postgresql
7773,unix_socket_directories,Sets the directories where Unix-domain sockets will be created,0,0,others,postgresql
7774,unix_socket_directories,"Specifies the directory of the Unix-domain socket(s) on which the server is to listen for connections from client applications. Multiple sockets can be created by listing multiple directories separated by commas. Whitespace between entries is ignored; surround a directory name with double quotes if you need to include whitespace or commas in the name. An empty value specifies not listening on any Unix-domain sockets, in which case only TCP/IP sockets can be used to connect to the server. The default value is normally /tmp, but that can be changed at build time. This parameter can only be set at server start.",0,0,others,postgresql
7775,unix_socket_group,Sets the owning group of the Unix-domain socket,0,0,others,postgresql
7776,unix_socket_group,"Sets the owning group of the Unix-domain socket(s). (The owning user of the sockets is always the user that starts the server.) In combination with the parameter unix_socket_permissions this can be used as an additional access control mechanism for Unix-domain connections. By default this is the empty string, which uses the default group of the server user. This parameter can only be set at server start.",0,0,others,postgresql
7777,unix_socket_permissions,Sets the access permissions of the Unix-domain socket(s). Unix-domain sockets use the usual Unix file system permission set. The parameter value is expected to be a numeric mode specified in the format accepted by the chmod and umask system calls. (To use the customary octal format the number must start with a 0 (zero).),0,0,others,postgresql
7778,unix_socket_permissions,Sets the access permissions of the Unix-domain socket,0,0,others,postgresql
7779,update_process_title,"Enables updating of the process title every time a new SQL command is received by the server. This setting defaults to on on most platforms, but it defaults to off on Windows due to that platform's larger overhead for updating the process title. Only superusers can change this setting.",0,0,others,postgresql
7780,update_process_title,Updates the process title to show the active SQL command,0,0,others,postgresql
7781,vacuum_cost_delay,"The length of time, in milliseconds, that the process will sleep when the cost limit has been exceeded. The default value is zero, which disables the cost-based vacuum delay feature. Positive values enable cost-based vacuuming. Note that on many systems, the effective resolution of sleep delays is 10 milliseconds; setting vacuum_cost_delay to a value that is not a multiple of 10 might have the same results as setting it to the next higher multiple of 10.",0,0,others,postgresql
7782,vacuum_cost_delay,Vacuum cost delay in milliseconds,0,0,others,postgresql
7783,vacuum_cost_limit,The accumulated cost that will cause the vacuuming process to sleep. The default value is 200.,1,5,workload-specific,postgresql
7784,vacuum_cost_limit,Vacuum cost amount available before napping,1,5,workload-specific,postgresql
7785,vacuum_cost_page_dirty,The estimated cost charged when vacuum modifies a block that was previously clean. It represents the extra I/O required to flush the dirty block out to disk again. The default value is 20.,1,5,workload-specific,postgresql
7786,vacuum_cost_page_dirty,Vacuum cost for a page dirtied by vacuum,1,5,workload-specific,postgresql
7787,vacuum_cost_page_hit,"The estimated cost for vacuuming a buffer found in the shared buffer cache. It represents the cost to lock the buffer pool, lookup the shared hash table and scan the content of the page. The default value is one.",1,5,workload-specific,postgresql
7788,vacuum_cost_page_hit,Vacuum cost for a page found in the buffer cache,1,5,workload-specific,postgresql
7789,vacuum_cost_page_miss,"The estimated cost for vacuuming a buffer that has to be read from disk. This represents the effort to lock the buffer pool, lookup the shared hash table, read the desired block in from the disk and scan its content. The default value is 10.",1,5,workload-specific,postgresql
7790,vacuum_cost_page_miss,Vacuum cost for a page not found in the buffer cache,1,5,workload-specific,postgresql
7791,vacuum_defer_cleanup_age,"Number of transactions by which VACUUM and HOT cleanup should be deferred, if any",1,5,workload-specific,postgresql
7792,vacuum_defer_cleanup_age,"Specifies the number of transactions by which VACUUM and HOT updates will defer cleanup of dead row versions. The default is zero transactions, meaning that dead row versions can be removed as soon as possible, that is, as soon as they are no longer visible to any open transaction. You may wish to set this to a non-zero value on a primary server that is supporting hot standby servers, as described in hot-standby. This allows more time for queries on the standby to complete without incurring conflicts due to early cleanup of rows. However, since the value is measured in terms of number of write transactions occurring on the primary server, it is difficult to predict just how much additional grace time will be made available to standby queries. This parameter can only be set in the postgresql.conf file or on the server command line.",1,5,workload-specific,postgresql
7793,vacuum_freeze_min_age,"Specifies the cutoff age (in transactions) that VACUUM should use to decide whether to freeze row versions while scanning a table. The default is 50 million transactions. Although users can set this value anywhere from zero to one billion, VACUUM will silently limit the effective value to half the value of autovacuum_freeze_max_age, so that there is not an unreasonably short time between forced autovacuums. For more information see vacuum-for-wraparound.",0,0,others,postgresql
7794,vacuum_freeze_min_age,Minimum age at which VACUUM should freeze a table row,0,0,others,postgresql
7795,vacuum_freeze_table_age,Age at which VACUUM should scan whole table to freeze tuples,0,0,others,postgresql
7796,vacuum_freeze_table_age,"VACUUM performs an aggressive scan if the table's pg_class.relfrozenxid field has reached the age specified by this setting. An aggressive scan differs from a regular VACUUM in that it visits every page that might contain unfrozen XIDs or MXIDs, not just those that might contain dead tuples. The default is 150 million transactions. Although users can set this value anywhere from zero to two billions, VACUUM will silently limit the effective value to 95% of autovacuum_freeze_max_age, so that a periodical manual VACUUM has a chance to run before an anti-wraparound autovacuum is launched for the table. For more information see vacuum-for-wraparound.",0,0,others,postgresql
7797,vacuum_multixact_freeze_min_age,"Specifies the cutoff age (in multixacts) that VACUUM should use to decide whether to replace multixact IDs with a newer transaction ID or multixact ID while scanning a table. The default is 5 million multixacts. Although users can set this value anywhere from zero to one billion, VACUUM will silently limit the effective value to half the value of autovacuum_multixact_freeze_max_age, so that there is not an unreasonably short time between forced autovacuums. For more information see vacuum-for-multixact-wraparound.",0,0,others,postgresql
7798,vacuum_multixact_freeze_min_age,Minimum age at which VACUUM should freeze a MultiXactId in a table row,0,0,others,postgresql
7799,vacuum_multixact_freeze_table_age,Multixact age at which VACUUM should scan whole table to freeze tuples,0,0,others,postgresql
7800,vacuum_multixact_freeze_table_age,"VACUUM performs an aggressive scan if the table's pg_class.relminmxid field has reached the age specified by this setting. An aggressive scan differs from a regular VACUUM in that it visits every page that might contain unfrozen XIDs or MXIDs, not just those that might contain dead tuples. The default is 150 million multixacts. Although users can set this value anywhere from zero to two billions, VACUUM will silently limit the effective value to 95% of autovacuum_multixact_freeze_max_age, so that a periodical manual VACUUM has a chance to run before an anti-wraparound is launched for the table. For more information see vacuum-for-multixact-wraparound.",0,0,others,postgresql
7801,wal_buffers,Sets the number of disk-page buffers in shared memory for WAL,1,1,resource,postgresql
7802,wal_buffers,"The amount of shared memory used for WAL data that has not yet been written to disk. The default setting of -1 selects a size equal to 1/32nd (about 3%) of shared_buffers, but not less than 64kB nor more than the size of one WAL segment, typically 16MB. This value can be set manually if the automatic choice is too large or too small, but any positive value less than 32kB will be treated as 32kB. This parameter can only be set at server start.",1,1,resource,postgresql
7803,wal_compression,Compresses full-page writes written in WAL file,1,4,limited-side-effect,postgresql
7804,wal_compression,"When this parameter is on, the PostgreSQL server compresses a full page image written to WAL when full_page_writes is on or during a base backup. A compressed page image will be decompressed during WAL replay. The default value is off. Only superusers can change this setting.",1,4,limited-side-effect,postgresql
7805,wal_consistency_checking,Sets the WAL resource managers for which WAL consistency checks are done,1,3,reliability-tradeoff,postgresql
7806,wal_consistency_checking,"This parameter is intended to be used to check for bugs in the WAL redo routines. When enabled, full-page images of any buffers modified in conjunction with the WAL record are added to the record. If the record is subsequently replayed, the system will first apply each record and then test whether the buffers modified by the record match the stored images. In certain cases (such as hint bits), minor variations are acceptable, and will be ignored. Any unexpected differences will result in a fatal error, terminating recovery.",1,3,reliability-tradeoff,postgresql
7807,wal_keep_segments,Sets the number of WAL files held for standby servers,1,1,resource,postgresql
7808,wal_keep_segments,"Specifies the minimum number of past log file segments kept in the pg_wal directory, in case a standby server needs to fetch them for streaming replication. Each segment is normally 16 megabytes. If a standby server connected to the sending server falls behind by more than wal_keep_segments segments, the sending server might remove a WAL segment still needed by the standby, in which case the replication connection will be terminated. Downstream connections will also eventually fail as a result. (However, the standby server can recover by fetching the segment from archive, if WAL archiving is in use.)",1,1,resource,postgresql
7809,wal_level,Set the level of information written to the WAL,1,6,function-tradeoff,postgresql
7810,wal_level,"wal_level determines how much information is written to the WAL. The default value is replica, which writes enough data to support WAL archiving and replication, including running read-only queries on a standby server. minimal removes all logging except the information required to recover from a crash or immediate shutdown. Finally, logical adds information necessary to support logical decoding. Each level includes the information logged at all lower levels. This parameter can only be set at server start.",1,6,function-tradeoff,postgresql
7811,wal_log_hints,"When this parameter is on, the PostgreSQL server writes the entire content of each disk page to WAL during the first modification of that page after a checkpoint, even for non-critical modifications of so-called hint bits.",1,6,function-tradeoff,postgresql
7812,wal_log_hints,"Writes full pages to WAL when first modified after a checkpoint, even for a non-critical modifications",1,6,function-tradeoff,postgresql
7813,wal_receiver_status_interval,Sets the maximum interval between WAL receiver status reports to the primary,0,0,others,postgresql
7814,wal_receiver_status_interval,"Specifies the minimum frequency for the WAL receiver process on the standby to send information about replication progress to the primary or upstream standby, where it can be seen using the pg_stat_replication view. The standby will report the last write-ahead log location it has written, the last position it has flushed to disk, and the last position it has applied. This parameter's value is the maximum interval, in seconds, between reports. Updates are sent each time the write or flush positions change, or at least as often as specified by this parameter. Thus, the apply position may lag slightly behind the true position. Setting this parameter to zero disables status updates completely. This parameter can only be set in the postgresql.conf file or on the server command line. The default value is 10 seconds.",0,0,others,postgresql
7815,wal_receiver_timeout,Sets the maximum wait time to receive data from the primary,1,5,workload-specific,postgresql
7816,wal_receiver_timeout,Terminate replication connections that are inactive longer than the specified number of milliseconds. This is useful for the receiving standby server to detect a primary node crash or network outage. A value of zero disables the timeout mechanism. This parameter can only be set in the postgresql.conf file or on the server command line. The default value is 60 seconds.,0,0,others,postgresql
7817,wal_retrieve_retry_interval,Sets the time to wait before retrying to retrieve WAL after a failed attempt,0,0,others,postgresql
7818,wal_retrieve_retry_interval,"Specify how long the standby server should wait when WAL data is not available from any sources (streaming replication, local pg_wal or WAL archive) before retrying to retrieve WAL data. This parameter can only be set in the postgresql.conf file or on the server command line. The default value is 5 seconds. Units are milliseconds if not specified.",0,0,others,postgresql
7819,wal_sender_timeout,Sets the maximum time to wait for WAL replication,0,0,others,postgresql
7820,wal_sender_timeout,Terminate replication connections that are inactive longer than the specified number of milliseconds. This is useful for the sending server to detect a standby crash or network outage. A value of zero disables the timeout mechanism. This parameter can only be set in the postgresql.conf file or on the server command line. The default value is 60 seconds.,0,0,others,postgresql
7821,wal_sync_method,"Method used for forcing WAL updates out to disk. If fsync is off then this setting is irrelevant, since WAL file updates will not be forced out at all. Possible values are:",1,3,reliability-tradeoff,postgresql
7822,wal_sync_method,Selects the method used for forcing WAL updates to disk,1,3,reliability-tradeoff,postgresql
7823,wal_writer_delay,"Specifies how often the WAL writer flushes WAL. After flushing WAL it sleeps for wal_writer_delay milliseconds, unless woken up by an asynchronously committing transaction. If the last flush happened less than wal_writer_delay milliseconds ago and less than wal_writer_flush_after bytes of WAL have been produced since, then WAL is only written to the operating system, not flushed to disk. The default value is 200 milliseconds (200ms). Note that on many systems, the effective resolution of sleep delays is 10 milliseconds; setting wal_writer_delay to a value that is not a multiple of 10 might have the same results as setting it to the next higher multiple of 10. This parameter can only be set in the postgresql.conf file or on the server command line.",1,5,workload-specific,postgresql
7824,wal_writer_delay,Time between WAL flushes performed in the WAL writer,0,0,others,postgresql
7825,wal_writer_flush_after,Amount of WAL written out by WAL writer that triggers a flush,1,5,workload-specific,postgresql
7826,wal_writer_flush_after,"Specifies how often the WAL writer flushes WAL. If the last flush happened less than wal_writer_delay milliseconds ago and less than wal_writer_flush_after bytes of WAL have been produced since, then WAL is only written to the operating system, not flushed to disk. If wal_writer_flush_after is set to 0 then WAL data is flushed immediately. The default is 1MB. This parameter can only be set in the postgresql.conf file or on the server command line.",0,0,others,postgresql
7827,work_mem,Sets the maximum memory to be used for query workspaces,1,1,resource,postgresql
7828,work_mem,"Specifies the amount of memory to be used by internal sort operations and hash tables before writing to temporary disk files. The value defaults to four megabytes (4MB). Note that for a complex query, several sort or hash operations might be running in parallel; each operation will be allowed to use as much memory as this value specifies before it starts to write data into temporary files. Also, several running sessions could be doing such operations concurrently. Therefore, the total memory used could be many times the value of work_mem; it is necessary to keep this fact in mind when choosing the value. Sort operations are used for ORDER BY, DISTINCT, and merge joins. Hash tables are used in hash joins, hash-based aggregation, and hash-based processing of IN subqueries.",1,1,resource,postgresql
7829,xmlbinary,Sets how binary values are to be encoded in XML,0,0,others,postgresql
7830,xmlbinary,"Sets how binary values are to be encoded in XML. This applies for example when bytea values are converted to XML by the functions xmlelement or xmlforest. Possible values are base64 and hex, which are both defined in the XML Schema standard. The default is base64. For further information about XML-related functions, see functions-xml.",0,0,others,postgresql
7831,xmloption,Sets whether DOCUMENT or CONTENT is implicit when converting between XML and character string values. See datatype-xml for a description of this. Valid values are DOCUMENT and CONTENT. The default is CONTENT.,0,0,others,postgresql
7832,xmloption,Sets whether XML data in implicit parsing and serialization operations is to be considered as documents or content fragments,0,0,others,postgresql
7833,zero_damaged_pages,Continues processing past damaged page headers,0,0,others,postgresql
7834,zero_damaged_pages,"Detection of a damaged page header normally causes PostgreSQL to report an error, aborting the current transaction. Setting zero_damaged_pages to on causes the system to instead report a warning, zero out the damaged page in memory, and continue processing. This behavior will destroy data, namely all the rows on the damaged page. However, it does allow you to get past the error and retrieve rows from any undamaged pages that might be present in the table. It is useful for recovering data if corruption has occurred due to a hardware or software error. You should generally not set this on until you have given up hope of recovering data from the damaged pages of a table. Zeroed-out pages are not forced to disk so it is recommended to recreate the table or the index before turning this parameter off again. The default setting is off, and it can only be changed by a superuser.",0,0,others,postgresql
7835,JAVA_HOME,Location where Java is installed (if it's not on your default PATH).,0,0,others,spark
7836,PYSPARK_DRIVER_PYTHON,Python binary executable to use for PySpark in driver only (default is PYSPARK_PYTHON). Property spark.pyspark.driver.python take precedence if it is set,0,0,others,spark
7837,PYSPARK_PYTHON,"Python binary executable to use for PySpark in both driver and workers (default is python2.7 if available, otherwise python).",0,0,others,spark
7838,spark.app.name,The name of your application. This will appear in the UI and in log data.,0,0,others,spark
7839,spark.blacklist.application.fetchFailure.enabled,"(Experimental) If set to ""true"", Spark will blacklist the executor immediately when a fetch failure happens. If external shuffle service is enabled, then the whole node will be blacklisted.",1,3,reliability-tradeoff,spark
7840,spark.blacklist.application.maxFailedExecutorsPerNode,"(Experimental) How many different executors must be blacklisted for the entire application, before the node is blacklisted for the entire application. Blacklisted nodes will be automatically added back to the pool of available resources after the timeout specified by spark.blacklist.timeout. Note that with dynamic allocation, though, the executors on the node may get marked as idle and be reclaimed by the cluster manager.",1,3,reliability-tradeoff,spark
7841,spark.blacklist.application.maxFailedTasksPerExecutor,"(Experimental) How many different tasks must fail on one executor, in successful task sets, before the executor is blacklisted for the entire application. Blacklisted executors will be automatically added back to the pool of available resources after the timeout specified by spark.blacklist.timeout. Note that with dynamic allocation, though, the executors may get marked as idle and be reclaimed by the cluster manager.",1,3,reliability-tradeoff,spark
7842,spark.blacklist.enabled,"If set to ""true"", prevent Spark from scheduling tasks on executors that have been blacklisted due to too many task failures. The blacklisting algorithm can be further controlled by the other ""spark.blacklist"" configuration options.",1,3,reliability-tradeoff,spark
7843,spark.blacklist.killBlacklistedExecutors,"(Experimental) If set to ""true"", allow Spark to automatically kill the executors when they are blacklisted on fetch failure or blacklisted for the entire application, as controlled by spark.blacklist.application.*. Note that, when an entire node is added to the blacklist, all of the executors on that node will be killed.",1,3,reliability-tradeoff,spark
7844,spark.blacklist.stage.maxFailedExecutorsPerNode,"(Experimental) How many different executors are marked as blacklisted for a given stage, before the entire node is marked as failed for the stage.",0,0,others,spark
7845,spark.blacklist.stage.maxFailedTasksPerExecutor,"(Experimental) How many different tasks must fail on one executor, within one stage, before the executor is blacklisted for that stage.",0,0,others,spark
7846,spark.blacklist.task.maxTaskAttemptsPerExecutor,"(Experimental) For a given task, how many times it can be retried on one executor before the executor is blacklisted for that task.",0,0,others,spark
7847,spark.blacklist.task.maxTaskAttemptsPerNode,"(Experimental) For a given task, how many times it can be retried on one node, before the entire node is blacklisted for that task.",0,0,others,spark
7848,spark.blacklist.timeout,"(Experimental) How long a node or executor is blacklisted for the entire application, before it is unconditionally removed from the blacklist to attempt running new tasks.",0,0,others,spark
7849,spark.blockManager.port,Port for all block managers to listen on. These exist on both the driver and the executors.,0,0,others,spark
7850,spark.broadcast.blockSize,"Size of each piece of a block for TorrentBroadcastFactory, in KiB unless otherwise specified. Too large a value decreases parallelism during broadcast (makes it slower); however, if it is too small, BlockManager might take a performance hit.",1,5,workload-specific,spark
7851,spark.broadcast.checksum,"Whether to enable checksum for broadcast. If enabled, broadcasts will include a checksum, which can help detect corrupted blocks, at the cost of computing and sending a little more data. It's possible to disable it if the network has other mechanisms to guarantee data won't be corrupted during broadcast.",1,3,reliability-tradeoff,spark
7852,spark.broadcast.compress,Whether to compress broadcast variables before sending them. Generally a good idea. Compression will use spark.io.compression.codec.,1,4,limited-side-effect,spark
7853,spark.checkpoint.compress,Whether to compress RDD checkpoints. Generally a good idea. Compression will use spark.io.compression.codec.,1,4,limited-side-effect,spark
7854,spark.cleaner.periodicGC.interval,"Controls how often to trigger a garbage collection. This context cleaner triggers cleanups only when weak references are garbage collected. In long-running applications with large driver JVMs, where there is little memory pressure on the driver, this may happen very occasionally or not at all. Not cleaning at all may lead to executors running out of disk space after a while.",0,0,others,spark
7855,spark.cleaner.referenceTracking,Enables or disables context cleaning.,1,6,function-tradeoff,spark
7856,spark.cleaner.referenceTracking.blocking,"Controls whether the cleaning thread should block on cleanup tasks (other than shuffle, which is controlled by spark.cleaner.referenceTracking.blocking.shuffle Spark property).",1,3,reliability-tradeoff,spark
7857,spark.cleaner.referenceTracking.blocking.shuffle,Controls whether the cleaning thread should block on shuffle cleanup tasks.,1,3,reliability-tradeoff,spark
7858,spark.cleaner.referenceTracking.cleanCheckpoints,Controls whether to clean checkpoint files if the reference is out of scope.,1,3,reliability-tradeoff,spark
7859,spark.core.connection.ack.wait.timeout,"How long for the connection to wait for ack to occur before timing out and giving up. To avoid unwilling timeout caused by long pause like GC, you can set larger value.",0,0,others,spark
7860,spark.cores.max,"When running on a standalone deploy cluster or a Mesos cluster in ""coarse-grained"" sharing mode, the maximum amount of CPU cores to request for the application from across the cluster (not from each machine). If not set, the default will be spark.deploy.defaultCores on Spark's standalone cluster manager, or infinite (all available cores) on Mesos.",1,1,resource,spark
7861,spark.default.parallelism,"Default number of partitions in RDDs returned by transformations like join, reduceByKey, and parallelize when not set by user.",1,1,resource,spark
7862,spark.deploy.recoveryMode,The recovery mode setting to recover submitted Spark jobs with cluster mode when it failed and relaunches. This is only applicable for cluster mode when running with Standalone or Mesos.,1,3,reliability-tradeoff,spark
7863,spark.deploy.zookeeper.dir,"When `spark.deploy.recoveryMode` is set to ZOOKEEPER, this configuration is used to set the zookeeper directory to store recovery state.",0,0,others,spark
7864,spark.deploy.zookeeper.url,"When `spark.deploy.recoveryMode` is set to ZOOKEEPER, this configuration is used to set the zookeeper URL to connect to.",0,0,others,spark
7865,spark.driver.bindAddress,"Hostname or IP address where to bind listening sockets. This config overrides the SPARK_LOCAL_IP environment variable (see below). It also allows a different address from the local one to be advertised to executors or external systems. This is useful, for example, when running containers with bridged networking. For this to properly work, the different ports used by the driver (RPC, block manager and UI) need to be forwarded from the container's host.",0,0,others,spark
7866,spark.driver.blockManager.port,"Driver-specific port for the block manager to listen on, for cases where it cannot use the same configuration as executors.",0,0,others,spark
7867,spark.driver.cores,"Number of cores to use for the driver process, only in cluster mode.",1,1,resource,spark
7868,spark.driver.extraClassPath,"Extra classpath entries to prepend to the classpath of the driver. Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-class-path command line option or in your default properties file.",0,0,others,spark
7869,spark.driver.extraJavaOptions,"A string of extra JVM options to pass to the driver. For instance, GC settings or other logging. Note that it is illegal to set maximum heap size (-Xmx) settings with this option. Maximum heap size settings can be set with spark.driver.memory in the cluster mode and through the --driver-memory command line option in the client mode. Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-java-options command line option or in your default properties file.",0,0,others,spark
7870,spark.driver.extraLibraryPath,"Set a special library path to use when launching the driver JVM. Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-library-path command line option or in your default properties file.",0,0,others,spark
7871,spark.driver.host,Hostname or IP address for the driver. This is used for communicating with the executors and the standalone Master.,0,0,others,spark
7872,spark.driver.maxResultSize,"Limit of total size of serialized results of all partitions for each Spark action (e.g. collect) in bytes. Should be at least 1M, or 0 for unlimited. Jobs will be aborted if the total size is above this limit. Having a high limit may cause out-of-memory errors in driver (depends on spark.driver.memory and memory overhead of objects in JVM). Setting a proper limit can protect the driver from out-of-memory errors.",1,5,workload-specific,spark
7873,spark.driver.memory,"Amount of memory to use for the driver process, i.e. where SparkContext is initialized, in the same format as JVM memory strings with a size unit suffix (""k"", ""m"", ""g"" or ""t"") (e.g. 512m, 2g). Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-memory command line option or in your default properties file.",1,1,resource,spark
7874,spark.driver.memoryOverhead,"The amount of off-heap memory to be allocated per driver in cluster mode, in MiB unless otherwise specified. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the container size (typically 6-10%). This option is currently supported on YARN and Kubernetes.",1,1,resource,spark
7875,spark.driver.port,Port for the driver to listen on. This is used for communicating with the executors and the standalone Master.,0,0,others,spark
7876,spark.driver.supervise,"If true, restarts the driver automatically if it fails with a non-zero exit status. Only has effect in Spark standalone mode or Mesos cluster deploy mode.",0,0,others,spark
7877,spark.driver.userClassPathFirst,(Experimental) Whether to give user-added jars precedence over Spark's own jars when loading classes in the driver. This feature can be used to mitigate conflicts between Spark's dependencies and user dependencies. It is currently an experimental feature. This is used in cluster mode only.,0,0,others,spark
7878,spark.dynamicAllocation.cachedExecutorIdleTimeout,"If dynamic allocation is enabled and an executor which has cached data blocks has been idle for more than this duration, the executor will be removed.",0,0,others,spark
7879,spark.dynamicAllocation.enabled,"Whether to use dynamic resource allocation, which scales the number of executors registered with this application up and down based on the workload. For more detail, see the description here. This requires spark.shuffle.service.enabled to be set. The following configurations are also relevant: spark.dynamicAllocation.minExecutors, spark.dynamicAllocation.maxExecutors, and spark.dynamicAllocation.initialExecutors spark.dynamicAllocation.executorAllocationRatio",1,4,limited-side-effect,spark
7880,spark.dynamicAllocation.executorAllocationRatio,"By default, the dynamic allocation will request enough executors to maximize the parallelism according to the number of tasks to process. While this minimizes the latency of the job, with small tasks this setting can waste a lot of resources due to executor allocation overhead, as some executor might not even do any work. This setting allows to set a ratio that will be used to reduce the number of executors w.r.t. full parallelism. Defaults to 1.0 to give maximum parallelism. 0.5 will divide the target number of executors by 2 The target number of executors computed by the dynamicAllocation can still be overridden by the spark.dynamicAllocation.minExecutors and spark.dynamicAllocation.maxExecutors settings",1,4,limited-side-effect,spark
7881,spark.dynamicAllocation.executorIdleTimeout,"If dynamic allocation is enabled and an executor has been idle for more than this duration, the executor will be removed. For more detail, see this description.",0,0,others,spark
7882,spark.dynamicAllocation.initialExecutors,"Initial number of executors to run if dynamic allocation is enabled. If `--num-executors` (or `spark.executor.instances`) is set and larger than this value, it will be used as the initial number of executors.",1,1,resource,spark
7883,spark.dynamicAllocation.maxExecutors,Upper bound for the number of executors if dynamic allocation is enabled.,1,1,resource,spark
7884,spark.dynamicAllocation.minExecutors,Lower bound for the number of executors if dynamic allocation is enabled.,1,1,resource,spark
7885,spark.dynamicAllocation.schedulerBacklogTimeout,"If dynamic allocation is enabled and there have been pending tasks backlogged for more than this duration, new executors will be requested. For more detail, see this description.",0,0,others,spark
7886,spark.dynamicAllocation.sustainedSchedulerBacklogTimeout,"Same as spark.dynamicAllocation.schedulerBacklogTimeout, but used only for subsequent executor requests. For more detail, see this description.",0,0,others,spark
7887,spark.eventLog.buffer.kb,"Buffer size to use when writing to output streams, in KiB unless otherwise specified.",1,1,resource,spark
7888,spark.eventLog.compress,"Whether to compress logged events, if spark.eventLog.enabled is true. Compression will use spark.io.compression.codec.",1,4,limited-side-effect,spark
7889,spark.eventLog.dir,"Base directory in which Spark events are logged, if spark.eventLog.enabled is true. Within this base directory, Spark creates a sub-directory for each application, and logs the events specific to the application in this directory. Users may want to set this to a unified location like an HDFS directory so history files can be read by the history server.",0,0,others,spark
7890,spark.eventLog.enabled,"Whether to log Spark events, useful for reconstructing the Web UI after the application has finished.",1,6,function-tradeoff,spark
7891,spark.eventLog.logBlockUpdates.enabled,"Whether to log events for every block update, if spark.eventLog.enabled is true. *Warning*: This will increase the size of the event log considerably.",1,6,function-tradeoff,spark
7892,spark.eventLog.longForm.enabled,"If true, use the long form of call sites in the event log. Otherwise use the short form.",0,0,others,spark
7893,spark.eventLog.overwrite,Whether to overwrite any existing files.,0,0,others,spark
7894,spark.executor.cores,"The number of cores to use on each executor. In standalone and Mesos coarse-grained modes, for more detail, see this description.",1,1,resource,spark
7895,spark.executor.extraClassPath,Extra classpath entries to prepend to the classpath of executors. This exists primarily for backwards-compatibility with older versions of Spark. Users typically should not need to set this option.,0,0,others,spark
7896,spark.executor.extraJavaOptions,"A string of extra JVM options to pass to executors. For instance, GC settings or other logging. Note that it is illegal to set Spark properties or maximum heap size (-Xmx) settings with this option. Spark properties should be set using a SparkConf object or the spark-defaults.conf file used with the spark-submit script. Maximum heap size settings can be set with spark.executor.memory. The following symbols, if present will be interpolated: will be replaced by application ID and will be replaced by executor ID. For example, to enable verbose gc logging to a file named for the executor ID of the app in /tmp, pass a 'value' of: -verbose:gc -Xloggc:/tmp/-.gc",0,0,others,spark
7897,spark.executor.extraLibraryPath,Set a special library path to use when launching executor JVM's.,0,0,others,spark
7898,spark.executor.heartbeatInterval,Interval between each executor's heartbeats to the driver. Heartbeats let the driver know that the executor is still alive and update it with metrics for in-progress tasks. spark.executor.heartbeatInterval should be significantly less than spark.network.timeout,0,0,others,spark
7899,spark.executor.logs.rolling.enableCompression,"Enable executor log compression. If it is enabled, the rolled executor logs will be compressed. Disabled by default.",1,4,limited-side-effect,spark
7900,spark.executor.logs.rolling.maxRetainedFiles,Sets the number of latest rolling log files that are going to be retained by the system. Older log files will be deleted. Disabled by default.,1,3,reliability-tradeoff,spark
7901,spark.executor.logs.rolling.maxSize,Set the max size of the file in bytes by which the executor logs will be rolled over. Rolling is disabled by default. See spark.executor.logs.rolling.maxRetainedFiles for automatic cleaning of old logs.,1,3,reliability-tradeoff,spark
7902,spark.executor.logs.rolling.strategy,"Set the strategy of rolling of executor logs. By default it is disabled. It can be set to ""time"" (time-based rolling) or ""size"" (size-based rolling). For ""time"", use spark.executor.logs.rolling.time.interval to set the rolling interval. For ""size"", use spark.executor.logs.rolling.maxSize to set the maximum file size for rolling.",1,3,reliability-tradeoff,spark
7903,spark.executor.logs.rolling.time.interval,"Set the time interval by which the executor logs will be rolled over. Rolling is disabled by default. Valid values are daily, hourly, minutely or any interval in seconds. See spark.executor.logs.rolling.maxRetainedFiles for automatic cleaning of old logs.",0,0,others,spark
7904,spark.executor.memory,"Amount of memory to use per executor process, in the same format as JVM memory strings with a size unit suffix (""k"", ""m"", ""g"" or ""t"") (e.g. 512m, 2g).",1,1,resource,spark
7905,spark.executor.memoryOverhead,"The amount of off-heap memory to be allocated per executor, in MiB unless otherwise specified. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the executor size (typically 6-10%). This option is currently supported on YARN and Kubernetes.",1,1,resource,spark
7906,spark.executor.pyspark.memory,"The amount of memory to be allocated to PySpark in each executor, in MiB unless otherwise specified. If set, PySpark memory for an executor will be limited to this amount. If not set, Spark will not limit Python's memory use and it is up to the application to avoid exceeding the overhead memory space shared with other non-JVM processes. When PySpark is run in YARN or Kubernetes, this memory is added to executor resource requests. NOTE: Python memory usage may not be limited on platforms that do not support resource limiting, such as Windows.",1,1,resource,spark
7907,spark.executor.userClassPathFirst,"(Experimental) Same functionality as spark.driver.userClassPathFirst, but applied to executor instances.",0,0,others,spark
7908,spark.executorEnv.[EnvironmentVariableName],Add the environment variable specified by EnvironmentVariableName to the Executor process. The user can specify multiple of these to set multiple environment variables.,0,0,others,spark
7909,spark.extraListeners,"A comma-separated list of classes that implement SparkListener; when initializing SparkContext, instances of these classes will be created and registered with Spark's listener bus. If a class has a single-argument constructor that accepts a SparkConf, that constructor will be called; otherwise, a zero-argument constructor will be called. If no valid constructor can be found, the SparkContext creation will fail with an exception.",0,0,others,spark
7910,spark.files,Comma-separated list of files to be placed in the working directory of each executor. Globs are allowed.,0,0,others,spark
7911,spark.files.fetchTimeout,Communication timeout to use when fetching files added through SparkContext.addFile() from the driver.,0,0,others,spark
7912,spark.files.maxPartitionBytes,The maximum number of bytes to pack into a single partition when reading files.,1,1,resource,spark
7913,spark.files.openCostInBytes,"The estimated cost to open a file, measured by the number of bytes could be scanned at the same time. This is used when putting multiple files into a partition. It is better to overestimate, then the partitions with small files will be faster than partitions with bigger files.",0,0,others,spark
7914,spark.files.overwrite,Whether to overwrite files added through SparkContext.addFile() when the target file exists and its contents do not match those of the source.,0,0,others,spark
7915,spark.files.useFetchCache,"If set to true (default), file fetching will use a local cache that is shared by executors that belong to the same application, which can improve task launching performance when running many executors on the same host. If set to false, these caching optimizations will be disabled and all executors will fetch their own copies of files. This optimization may be disabled in order to use Spark local directories that reside on NFS filesystems (see SPARK-6313 for more details).",1,4,limited-side-effect,spark
7916,spark.graphx.pregel.checkpointInterval,Checkpoint interval for graph and message in Pregel. It used to avoid stackOverflowError due to long lineage chains after lots of iterations. The checkpoint is disabled by default.,0,0,others,spark
7917,spark.hadoop.cloneConf,"If set to true, clones a new Hadoop Configuration object for each task. This option should be enabled to work around Configuration thread-safety issues (see SPARK-2546 for more details). This is disabled by default in order to avoid unexpected performance regressions for jobs that are not affected by these issues.",1,3,reliability-tradeoff,spark
7918,spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version,"The file output committer algorithm version, valid algorithm version number: 1 or 2. Version 2 may have better performance, but version 1 may handle failures better in certain situations, as per MAPREDUCE-4815.",0,0,others,spark
7919,spark.hadoop.validateOutputSpecs,"If set to true, validates the output specification (e.g. checking if the output directory already exists) used in saveAsHadoopFile and other variants. This can be disabled to silence exceptions due to pre-existing output directories. We recommend that users do not disable this except if trying to achieve compatibility with previous versions of Spark. Simply use Hadoop's FileSystem API to delete output directories by hand. This setting is ignored for jobs generated through Spark Streaming's StreamingContext, since data may need to be rewritten to pre-existing output directories during checkpoint recovery.",1,3,reliability-tradeoff,spark
7920,spark.io.compression.codec,"The codec used to compress internal data such as RDD partitions, event log, broadcast variables and shuffle outputs. By default, Spark provides four codecs: lz4, lzf, snappy, and zstd. You can also use fully qualified class names to specify the codec, e.g. org.apache.spark.io.LZ4CompressionCodec, org.apache.spark.io.LZFCompressionCodec, org.apache.spark.io.SnappyCompressionCodec, and org.apache.spark.io.ZStdCompressionCodec.",1,6,function-tradeoff,spark
7921,spark.io.compression.lz4.blockSize,"Block size in bytes used in LZ4 compression, in the case when LZ4 compression codec is used. Lowering this block size will also lower shuffle memory usage when LZ4 is used.",1,1,resource,spark
7922,spark.io.compression.snappy.blockSize,"Block size in bytes used in Snappy compression, in the case when Snappy compression codec is used. Lowering this block size will also lower shuffle memory usage when Snappy is used.",1,1,resource,spark
7923,spark.io.compression.zstd.bufferSize,"Buffer size in bytes used in Zstd compression, in the case when Zstd compression codec is used. Lowering this size will lower the shuffle memory usage when Zstd is used, but it might increase the compression cost because of excessive JNI call overhead.",1,1,resource,spark
7924,spark.io.compression.zstd.level,Compression level for Zstd compression codec. Increasing the compression level will result in better compression at the expense of more CPU and memory.,1,6,function-tradeoff,spark
7925,spark.jars,Comma-separated list of jars to include on the driver and executor classpaths. Globs are allowed.,0,0,others,spark
7926,spark.jars.excludes,"Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in spark.jars.packages to avoid dependency conflicts.",0,0,others,spark
7927,spark.jars.ivy,"Path to specify the Ivy user directory, used for the local Ivy cache and package files from spark.jars.packages. This will override the Ivy property ivy.default.ivy.user.dir which defaults to ~/.ivy2.",0,0,others,spark
7928,spark.jars.ivySettings,"Path to an Ivy settings file to customize resolution of jars specified using spark.jars.packages instead of the built-in defaults, such as maven central. Additional repositories given by the command-line option --repositories or spark.jars.repositories will also be included. Useful for allowing Spark to resolve artifacts from behind a firewall e.g. via an in-house artifact server like Artifactory. Details on the settings file format can be found at Settings Files",0,0,others,spark
7929,spark.jars.packages,"Comma-separated list of Maven coordinates of jars to include on the driver and executor classpaths. The coordinates should be groupId:artifactId:version. If spark.jars.ivySettings is given artifacts will be resolved according to the configuration in the file, otherwise artifacts will be searched for in the local maven repo, then maven central and finally any additional remote repositories given by the command-line option --repositories. For more details, see Advanced Dependency Management.",0,0,others,spark
7930,spark.jars.repositories,Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages or spark.jars.packages.,0,0,others,spark
7931,spark.kryo.classesToRegister,"If you use Kryo serialization, give a comma-separated list of custom class names to register with Kryo. See the tuning guide for more details.",0,0,others,spark
7932,spark.kryo.referenceTracking,"Whether to track references to the same object when serializing data with Kryo, which is necessary if your object graphs have loops and useful for efficiency if they contain multiple copies of the same object. Can be disabled to improve performance if you know this is not the case.",1,6,function-tradeoff,spark
7933,spark.kryo.registrationRequired,"Whether to require registration with Kryo. If set to 'true', Kryo will throw an exception if an unregistered class is serialized. If set to false (the default), Kryo will write unregistered class names along with each object. Writing class names can cause significant performance overhead, so enabling this option can enforce strictly that a user has not omitted classes from registration.",1,6,function-tradeoff,spark
7934,spark.kryo.registrator,"If you use Kryo serialization, give a comma-separated list of classes that register your custom classes with Kryo. This property is useful if you need to register your classes in a custom way, e.g. to specify a custom field serializer. Otherwise spark.kryo.classesToRegister is simpler. It should be set to classes that extend KryoRegistrator. See the tuning guide for more details.",0,0,others,spark
7935,spark.kryo.unsafe,Whether to use unsafe based Kryo serializer. Can be substantially faster by using Unsafe Based IO.,1,2,security-tradeoff,spark
7936,spark.kryoserializer.buffer,"Initial size of Kryo's serialization buffer, in KiB unless otherwise specified. Note that there will be one buffer per core on each worker. This buffer will grow up to spark.kryoserializer.buffer.max if needed.",1,1,resource,spark
7937,spark.kryoserializer.buffer.max,"Maximum allowable size of Kryo serialization buffer, in MiB unless otherwise specified. This must be larger than any object you attempt to serialize and must be less than 2048m. Increase this if you get a ""buffer limit exceeded"" exception inside Kryo.",1,1,resource,spark
7938,spark.local.dir,"Directory to use for ""scratch"" space in Spark, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks. NOTE: In Spark 1.0 and later this will be overridden by SPARK_LOCAL_DIRS (Standalone), MESOS_SANDBOX (Mesos) or LOCAL_DIRS (YARN) environment variables set by the cluster manager.",0,0,others,spark
7939,spark.locality.wait,"How long to wait to launch a data-local task before giving up and launching it on a less-local node. The same wait will be used to step through multiple locality levels (process-local, node-local, rack-local and then any). It is also possible to customize the waiting time for each level by setting spark.locality.wait.node, etc. You should increase this setting if your tasks are long and see poor locality, but the default usually works well.",0,0,others,spark
7940,spark.locality.wait.node,"Customize the locality wait for node locality. For example, you can set this to 0 to skip node locality and search immediately for rack locality (if your cluster has rack information).",0,0,others,spark
7941,spark.locality.wait.process,Customize the locality wait for process locality. This affects tasks that attempt to access cached data in a particular executor process.,0,0,others,spark
7942,spark.locality.wait.rack,Customize the locality wait for rack locality.,0,0,others,spark
7943,spark.log.callerContext,"Application information that will be written into Yarn RM log/HDFS audit log when running on Yarn/HDFS. Its length depends on the Hadoop configuration hadoop.caller.context.max.size. It should be concise, and typically can have up to 50 characters.",0,0,others,spark
7944,spark.logConf,Logs the effective SparkConf as INFO when a SparkContext is started.,1,6,function-tradeoff,spark
7945,spark.master,The cluster manager to connect to. See the list of allowed master URL's.,0,0,others,spark
7946,spark.maxRemoteBlockSizeFetchToMem,"The remote block will be fetched to disk when size of the block is above this threshold in bytes. This is to avoid a giant request that takes too much memory. By default, this is only enabled for blocks > 2GB, as those cannot be fetched directly into memory, no matter what resources are available. But it can be turned down to a much lower value (eg. 200m) to avoid using too much memory on smaller blocks as well. Note this configuration will affect both shuffle fetch and block manager remote block fetch. For users who enabled external shuffle service, this feature can only be used when external shuffle service is newer than Spark 2.2.",1,5,workload-specific,spark
7947,spark.memory.fraction,"Fraction of (heap space - 300MB) used for execution and storage. The lower this is, the more frequently spills and cached data eviction occur. The purpose of this config is to set aside memory for internal metadata, user data structures, and imprecise size estimation in the case of sparse, unusually large records. Leaving this at the default value is recommended. For more detail, including important information about correctly tuning JVM garbage collection when increasing this value, see this description.",1,1,resource,spark
7948,spark.memory.offHeap.enabled,"If true, Spark will attempt to use off-heap memory for certain operations. If off-heap memory use is enabled, then spark.memory.offHeap.size must be positive.",1,6,function-tradeoff,spark
7949,spark.memory.offHeap.size,"The absolute amount of memory in bytes which can be used for off-heap allocation. This setting has no impact on heap memory usage, so if your executors' total memory consumption must fit within some hard limit then be sure to shrink your JVM heap size accordingly. This must be set to a positive value when spark.memory.offHeap.enabled=true.",1,1,resource,spark
7950,spark.memory.storageFraction,"Amount of storage memory immune to eviction, expressed as a fraction of the size of the region set aside by spark.memory.fraction. The higher this is, the less working memory may be available to execution and tasks may spill to disk more often. Leaving this at the default value is recommended. For more detail, see this description.",1,1,resource,spark
7951,spark.memory.useLegacyMode,"Whether to enable the legacy memory management mode used in Spark 1.5 and before. The legacy mode rigidly partitions the heap space into fixed-size regions, potentially leading to excessive spilling if the application was not tuned. The following deprecated memory fraction configurations are not read unless this is enabled: spark.shuffle.memoryFraction spark.storage.memoryFraction spark.storage.unrollFraction",1,6,function-tradeoff,spark
7952,spark.network.timeout,"Default timeout for all network interactions. This config will be used in place of spark.core.connection.ack.wait.timeout, spark.storage.blockManagerSlaveTimeoutMs, spark.shuffle.io.connectionTimeout, spark.rpc.askTimeout or spark.rpc.lookupTimeout if they are not configured.",0,0,others,spark
7953,spark.port.maxRetries,"Maximum number of retries when binding to a port before giving up. When a port is given a specific value (non 0), each subsequent retry will increment the port used in the previous attempt by 1 before retrying. This essentially allows it to try a range of ports from the start port specified to port + maxRetries.",0,0,others,spark
7954,spark.pyspark.driver.python,Python binary executable to use for PySpark in driver. (default is spark.pyspark.python),0,0,others,spark
7955,spark.pyspark.python,Python binary executable to use for PySpark in both driver and executors.,0,0,others,spark
7956,spark.python.profile,"Enable profiling in Python worker, the profile result will show up by sc.show_profiles(), or it will be displayed before the driver exits. It also can be dumped into disk by sc.dump_profiles(path). If some of the profile results had been displayed manually, they will not be displayed automatically before driver exiting. By default the pyspark.profiler.BasicProfiler will be used, but this can be overridden by passing a profiler class in as a parameter to the SparkContext constructor.",1,6,function-tradeoff,spark
7957,spark.python.profile.dump,"The directory which is used to dump the profile result before driver exiting. The results will be dumped as separated file for each RDD. They can be loaded by pstats.Stats(). If this is specified, the profile result will not be displayed automatically.",0,0,others,spark
7958,spark.python.worker.memory,"Amount of memory to use per python worker process during aggregation, in the same format as JVM memory strings with a size unit suffix (""k"", ""m"", ""g"" or ""t"") (e.g. 512m, 2g). If the memory used during aggregation goes above this amount, it will spill the data into disks.",1,1,resource,spark
7959,spark.python.worker.reuse,"Reuse Python worker or not. If yes, it will use a fixed number of Python workers, does not need to fork() a Python process for every task. It will be very useful if there is large broadcast, then the broadcast will not be needed to transferred from JVM to Python worker for every task.",1,4,limited-side-effect,spark
7960,spark.r.backendConnectionTimeout,Connection timeout set by R process on its connection to RBackend in seconds.,0,0,others,spark
7961,spark.r.command,Executable for executing R scripts in cluster modes for both driver and workers.,0,0,others,spark
7962,spark.r.driver.command,Executable for executing R scripts in client modes for driver. Ignored in cluster modes.,0,0,others,spark
7963,spark.r.heartBeatInterval,Interval for heartbeats sent from SparkR backend to R process to prevent connection timeout.,0,0,others,spark
7964,spark.r.numRBackendThreads,Number of threads used by RBackend to handle RPC calls from SparkR package.,1,1,resource,spark
7965,spark.r.shell.command,"Executable for executing sparkR shell in client modes for driver. Ignored in cluster modes. It is the same as environment variable SPARKR_DRIVER_R, but take precedence over it. spark.r.shell.command is used for sparkR shell while spark.r.driver.command is used for running R script.",0,0,others,spark
7966,spark.rdd.compress,Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER in Java and Scala or StorageLevel.MEMORY_ONLY in Python). Can save substantial space at the cost of some extra CPU time. Compression will use spark.io.compression.codec.,1,4,limited-side-effect,spark
7967,spark.redaction.regex,"Regex to decide which Spark configuration properties and environment variables in driver and executor environments contain sensitive information. When this regex matches a property key or value, the value is redacted from the environment UI and various logs like YARN and event logs.",0,0,others,spark
7968,spark.reducer.maxBlocksInFlightPerAddress,"This configuration limits the number of remote blocks being fetched per reduce task from a given host port. When a large number of blocks are being requested from a given address in a single fetch or simultaneously, this could crash the serving executor or Node Manager. This is especially useful to reduce the load on the Node Manager when external shuffle is enabled. You can mitigate this issue by setting it to a lower value.",1,3,reliability-tradeoff,spark
7969,spark.reducer.maxReqsInFlight,"This configuration limits the number of remote requests to fetch blocks at any given point. When the number of hosts in the cluster increase, it might lead to very large number of inbound connections to one or more nodes, causing the workers to fail under load. By allowing it to limit the number of fetch requests, this scenario can be mitigated.",1,3,reliability-tradeoff,spark
7970,spark.reducer.maxSizeInFlight,"Maximum size of map outputs to fetch simultaneously from each reduce task, in MiB unless otherwise specified. Since each output requires us to create a buffer to receive it, this represents a fixed memory overhead per reduce task, so keep it small unless you have a large amount of memory.",1,1,resource,spark
7971,spark.rpc.askTimeout,Duration for an RPC ask operation to wait before timing out.,0,0,others,spark
7972,spark.rpc.lookupTimeout,Duration for an RPC remote endpoint lookup operation to wait before timing out.,0,0,others,spark
7973,spark.rpc.message.maxSize,"Maximum message size (in MB) to allow in ""control plane"" communication; generally only applies to map output size information sent between executors and the driver. Increase this if you are running jobs with many thousands of map and reduce tasks and see messages about the RPC message size.",1,1,resource,spark
7974,spark.rpc.numRetries,Number of times to retry before an RPC task gives up. An RPC task will run at most times of this number.,0,0,others,spark
7975,spark.rpc.retry.wait,Duration for an RPC ask operation to wait before retrying.,0,0,others,spark
7976,spark.scheduler.blacklist.unschedulableTaskSetTimeout,The timeout in seconds to wait to acquire a new executor and schedule a task before aborting a TaskSet which is unschedulable because of being completely blacklisted.,0,0,others,spark
7977,spark.scheduler.listenerbus.eventqueue.capacity,"Capacity for event queue in Spark listener bus, must be greater than 0. Consider increasing value (e.g. 20000) if listener events are dropped. Increasing this value may result in the driver using more memory.",1,1,resource,spark
7978,spark.scheduler.maxRegisteredResourcesWaitingTime,Maximum amount of time to wait for resources to register before scheduling begins.,0,0,others,spark
7979,spark.scheduler.minRegisteredResourcesRatio,"The minimum ratio of registered resources (registered resources / total expected resources) (resources are executors in yarn mode and Kubernetes mode, CPU cores in standalone mode and Mesos coarse-grained mode ['spark.cores.max' value is total expected resources for Mesos coarse-grained mode] ) to wait for before scheduling begins. Specified as a double between 0.0 and 1.0. Regardless of whether the minimum ratio of resources has been reached, the maximum amount of time it will wait before scheduling begins is controlled by config spark.scheduler.maxRegisteredResourcesWaitingTime.",0,0,others,spark
7980,spark.scheduler.mode,The scheduling mode between jobs submitted to the same SparkContext. Can be set to FAIR to use fair sharing instead of queueing jobs one after another. Useful for multi-user services.,1,5,workload-specific,spark
7981,spark.scheduler.revive.interval,The interval length for the scheduler to revive the worker resource offers to run tasks.,0,0,others,spark
7982,spark.serializer,"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form. The default of Java serialization works with any Serializable Java object but is quite slow, so we recommend using org.apache.spark.serializer.KryoSerializer and configuring Kryo serialization when speed is necessary. Can be any subclass of org.apache.spark.Serializer.",1,4,limited-side-effect,spark
7983,spark.serializer.objectStreamReset,"When serializing using org.apache.spark.serializer.JavaSerializer, the serializer caches objects to prevent writing redundant data, however that stops garbage collection of those objects. By calling 'reset' you flush that info from the serializer, and allow old objects to be collected. To turn off this periodic reset set it to -1. By default it will reset the serializer every 100 objects.",1,4,limited-side-effect,spark
7984,spark.shuffle.accurateBlockThreshold,Threshold in bytes above which the size of shuffle blocks in HighlyCompressedMapStatus is accurately recorded. This helps to prevent OOM by avoiding underestimating shuffle block size when fetch shuffle blocks.,1,5,workload-specific,spark
7985,spark.shuffle.compress,Whether to compress map output files. Generally a good idea. Compression will use spark.io.compression.codec.,1,4,limited-side-effect,spark
7986,spark.shuffle.file.buffer,"Size of the in-memory buffer for each shuffle file output stream, in KiB unless otherwise specified. These buffers reduce the number of disk seeks and system calls made in creating intermediate shuffle files.",1,1,resource,spark
7987,spark.shuffle.io.maxRetries,(Netty only) Fetches that fail due to IO-related exceptions are automatically retried if this is set to a non-zero value. This retry logic helps stabilize large shuffles in the face of long GC pauses or transient network connectivity issues.,0,0,others,spark
7988,spark.shuffle.io.numConnectionsPerPeer,"(Netty only) Connections between hosts are reused in order to reduce connection buildup for large clusters. For clusters with many hard disks and few hosts, this may result in insufficient concurrency to saturate all disks, and so users may consider increasing this value.",1,1,resource,spark
7989,spark.shuffle.io.preferDirectBufs,"(Netty only) Off-heap buffers are used to reduce garbage collection during shuffle and cache block transfer. For environments where off-heap memory is tightly limited, users may wish to turn this off to force all allocations from Netty to be on-heap.",1,1,resource,spark
7990,spark.shuffle.io.retryWait,"(Netty only) How long to wait between retries of fetches. The maximum delay caused by retrying is 15 seconds by default, calculated as maxRetries * retryWait.",0,0,others,spark
7991,spark.shuffle.maxChunksBeingTransferred,"The max number of chunks allowed to be transferred at the same time on shuffle service. Note that new incoming connections will be closed when the max number is hit. The client will retry according to the shuffle retry configs (see spark.shuffle.io.maxRetries and spark.shuffle.io.retryWait), if those limits are reached the task will fail with fetch failure.",1,5,workload-specific,spark
7992,spark.shuffle.memoryFraction,"(deprecated) This is read only if spark.memory.useLegacyMode is enabled. Fraction of Java heap to use for aggregation and cogroups during shuffles. At any given time, the collective size of all in-memory maps used for shuffles is bounded by this limit, beyond which the contents will begin to spill to disk. If spills are often, consider increasing this value at the expense of spark.storage.memoryFraction.",0,0,others,spark
7993,spark.shuffle.registration.maxAttempts,"When we fail to register to the external shuffle service, we will retry for maxAttempts times.",0,0,others,spark
7994,spark.shuffle.registration.timeout,Timeout in milliseconds for registration to the external shuffle service.,0,0,others,spark
7995,spark.shuffle.service.enabled,"Enables the external shuffle service. This service preserves the shuffle files written by executors so the executors can be safely removed. This must be enabled if spark.dynamicAllocation.enabled is ""true"". The external shuffle service must be set up in order to enable it. See dynamic allocation configuration and setup documentation for more information.",1,4,limited-side-effect,spark
7996,spark.shuffle.service.index.cache.size,Cache entries limited to the specified memory footprint in bytes.,1,1,resource,spark
7997,spark.shuffle.service.port,Port on which the external shuffle service will run.,0,0,others,spark
7998,spark.shuffle.sort.bypassMergeThreshold,"(Advanced) In the sort-based shuffle manager, avoid merge-sorting data if there is no map-side aggregation and there are at most this many reduce partitions.",1,5,workload-specific,spark
7999,spark.shuffle.spill.compress,Whether to compress data spilled during shuffles. Compression will use spark.io.compression.codec.,1,4,limited-side-effect,spark
8000,spark.speculation,"If set to ""true"", performs speculative execution of tasks. This means if one or more tasks are running slowly in a stage, they will be re-launched.",1,4,limited-side-effect,spark
8001,spark.speculation.interval,How often Spark will check for tasks to speculate.,0,0,others,spark
8002,spark.speculation.multiplier,How many times slower a task is than the median to be considered for speculation.,0,0,others,spark
8003,spark.speculation.quantile,Fraction of tasks which must be complete before speculation is enabled for a particular stage.,0,0,others,spark
8004,spark.sql.ui.retainedExecutions,How many finished executions the Spark UI and status APIs remember before garbage collecting.,0,0,others,spark
8005,spark.stage.maxConsecutiveAttempts,Number of consecutive stage attempts allowed before a stage is aborted.,0,0,others,spark
8006,spark.storage.memoryFraction,"(deprecated) This is read only if spark.memory.useLegacyMode is enabled. Fraction of Java heap to use for Spark's memory cache. This should not be larger than the ""old"" generation of objects in the JVM, which by default is given 0.6 of the heap, but you can increase it if you configure your own old generation size.",0,0,others,spark
8007,spark.storage.memoryMapThreshold,"Size in bytes of a block above which Spark memory maps when reading a block from disk. This prevents Spark from memory mapping very small blocks. In general, memory mapping has high overhead for blocks close to or below the page size of the operating system.",1,1,resource,spark
8008,spark.storage.replication.proactive,Enables proactive block replication for RDD blocks. Cached RDD block replicas lost due to executor failures are replenished if there are any existing available replicas. This tries to get the replication level of the block to the initial number.,1,3,reliability-tradeoff,spark
8009,spark.storage.unrollFraction,(deprecated) This is read only if spark.memory.useLegacyMode is enabled. Fraction of spark.storage.memoryFraction to use for unrolling blocks in memory. This is dynamically allocated by dropping existing blocks when there is not enough free storage space to unroll the new block in its entirety.,0,0,others,spark
8010,spark.streaming.backpressure.enabled,"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values spark.streaming.receiver.maxRate and spark.streaming.kafka.maxRatePerPartition if they are set (see below).",1,5,workload-specific,spark
8011,spark.streaming.backpressure.initialRate,This is the initial maximum receiving rate at which each receiver will receive data for the first batch when the backpressure mechanism is enabled.,1,1,resource,spark
8012,spark.streaming.blockInterval,Interval at which data received by Spark Streaming receivers is chunked into blocks of data before storing them in Spark. Minimum recommended - 50 ms. See the performance tuning section in the Spark Streaming programing guide for more details.,0,0,others,spark
8013,spark.streaming.driver.writeAheadLog.closeFileAfterWrite,Whether to close the file after writing a write-ahead log record on the driver. Set this to 'true' when you want to use S3 (or any file system that does not support flushing) for the metadata WAL on the driver.,0,0,others,spark
8014,spark.streaming.kafka.maxRatePerPartition,Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the new Kafka direct stream API. See the Kafka Integration guide for more details.,1,5,workload-specific,spark
8015,spark.streaming.kafka.maxRetries,Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the new Kafka direct stream API.,0,0,others,spark
8016,spark.streaming.kafka.minRatePerPartition,Minimum rate (number of records per second) at which data will be read from each Kafka partition when using the new Kafka direct stream API.,1,5,workload-specific,spark
8017,spark.streaming.receiver.maxRate,"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details.",0,0,others,spark
8018,spark.streaming.receiver.writeAheadLog.closeFileAfterWrite,Whether to close the file after writing a write-ahead log record on the receivers. Set this to 'true' when you want to use S3 (or any file system that does not support flushing) for the data WAL on the receivers.,0,0,others,spark
8019,spark.streaming.receiver.writeAheadLog.enable,Enable write-ahead logs for receivers. All the input data received through receivers will be saved to write-ahead logs that will allow it to be recovered after driver failures. See the deployment guide in the Spark Streaming programing guide for more details.,0,0,others,spark
8020,spark.streaming.stopGracefullyOnShutdown,"If true, Spark shuts down the StreamingContext gracefully on JVM shutdown rather than immediately.",0,0,others,spark
8021,spark.streaming.ui.retainedBatches,How many batches the Spark Streaming UI and status APIs remember before garbage collecting.,1,1,resource,spark
8022,spark.streaming.ui.retainedBatches,How many finished batches the Spark UI and status APIs remember before garbage collecting.,0,0,others,spark
8023,spark.streaming.unpersist,Force RDDs generated and persisted by Spark Streaming to be automatically unpersisted from Spark's memory. The raw input data received by Spark Streaming is also automatically cleared. Setting this to false will allow the raw data and persisted RDDs to be accessible outside the streaming application as they will not be cleared automatically. But it comes at the cost of higher memory usage in Spark.,0,0,others,spark
8024,spark.submit.deployMode,"The deploy mode of Spark driver program, either ""client"" or ""cluster"", Which means to launch driver program locally (""client"") or remotely (""cluster"") on one of the nodes inside the cluster.",0,0,others,spark
8025,spark.submit.pyFiles,"Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps. Globs are allowed.",0,0,others,spark
8026,spark.task.cpus,Number of cores to allocate for each task.,1,1,resource,spark
8027,spark.task.maxFailures,Number of failures of any particular task before giving up on the job. The total number of failures spread across different tasks will not cause the job to fail; a particular task has to fail this number of attempts. Should be greater than or equal to 1. Number of allowed retries = this value - 1.,1,3,reliability-tradeoff,spark
8028,spark.task.reaper.enabled,"Enables monitoring of killed / interrupted tasks. When set to true, any task which is killed will be monitored by the executor until that task actually finishes executing. See the other spark.task.reaper.* configurations for details on how to control the exact behavior of this monitoring. When set to false (the default), task killing will use an older code path which lacks such monitoring.",0,0,others,spark
8029,spark.task.reaper.killTimeout,"When spark.task.reaper.enabled = true, this setting specifies a timeout after which the executor JVM will kill itself if a killed task has not stopped running. The default value, -1, disables this mechanism and prevents the executor from self-destructing. The purpose of this setting is to act as a safety-net to prevent runaway noncancellable tasks from rendering an executor unusable.",0,0,others,spark
8030,spark.task.reaper.pollingInterval,"When spark.task.reaper.enabled = true, this setting controls the frequency at which executors will poll the status of killed tasks. If a killed task is still running when polled then a warning will be logged and, by default, a thread-dump of the task will be logged (this thread dump can be disabled via the spark.task.reaper.threadDump setting, which is documented below).",0,0,others,spark
8031,spark.task.reaper.threadDump,"When spark.task.reaper.enabled = true, this setting controls whether task thread dumps are logged during periodic polling of killed tasks. Set this to false to disable collection of thread dumps.",0,0,others,spark
8032,spark.ui.dagGraph.retainedRootRDDs,How many DAG graph nodes the Spark UI and status APIs remember before garbage collecting.,1,1,resource,spark
8033,spark.ui.enabled,Whether to run the web UI for the Spark application.,0,0,others,spark
8034,spark.ui.filters,"Comma separated list of filter class names to apply to the Spark Web UI. The filter should be a standard javax servlet Filter. Filter parameters can also be specified in the configuration, by setting config entries of the form spark.<class name of filter>.param.<param name>=<value> For example: spark.ui.filters=com.test.filter1 spark.com.test.filter1.param.name1=foo spark.com.test.filter1.param.name2=bar",0,0,others,spark
8035,spark.ui.killEnabled,Allows jobs and stages to be killed from the web UI.,0,0,others,spark
8036,spark.ui.liveUpdate.minFlushPeriod,Minimum time elapsed before stale UI data is flushed. This avoids UI staleness when incoming task events are not fired frequently.,0,0,others,spark
8037,spark.ui.liveUpdate.period,"How often to update live entities. -1 means ""never update"" when replaying applications, meaning only the last write will happen. For live applications, this avoids a few operations that we can live without when rapidly processing incoming task events.",0,0,others,spark
8038,spark.ui.port,"Port for your application's dashboard, which shows memory and workload data.",0,0,others,spark
8039,spark.ui.requestHeaderSize,"The maximum allowed size for a HTTP request header, in bytes unless otherwise specified. This setting applies for the Spark History Server too.",1,5,workload-specific,spark
8040,spark.ui.retainedDeadExecutors,How many dead executors the Spark UI and status APIs remember before garbage collecting.,0,0,others,spark
8041,spark.ui.retainedJobs,"How many jobs the Spark UI and status APIs remember before garbage collecting. This is a target maximum, and fewer elements may be retained in some circumstances.",1,1,resource,spark
8042,spark.ui.retainedStages,"How many stages the Spark UI and status APIs remember before garbage collecting. This is a target maximum, and fewer elements may be retained in some circumstances.",0,0,others,spark
8043,spark.ui.retainedTasks,"How many tasks the Spark UI and status APIs remember before garbage collecting. This is a target maximum, and fewer elements may be retained in some circumstances.",0,0,others,spark
8044,spark.ui.reverseProxy,"Enable running Spark Master as reverse proxy for worker and application UIs. In this mode, Spark master will reverse proxy the worker and application UIs to enable access without requiring direct access to their hosts. Use it with caution, as worker and application UI will not be accessible directly, you will only be able to access them through spark master/proxy public URL. This setting affects all the workers and application UIs running in the cluster and must be set on all the workers, drivers and masters.",1,2,security-tradeoff,spark
8045,spark.ui.reverseProxyUrl,This is the URL where your proxy is running. This URL is for proxy which is running in front of Spark Master. This is useful when running proxy for authentication e.g. OAuth proxy. Make sure this is a complete URL including scheme (http/https) and port to reach your proxy.,0,0,others,spark
8046,spark.ui.showConsoleProgress,"Show the progress bar in the console. The progress bar shows the progress of stages that run for longer than 500ms. If multiple stages run at the same time, multiple progress bars will be displayed on the same line. Note: In shell environment, the default value of spark.ui.showConsoleProgress is true.",1,6,function-tradeoff,spark
8047,spark.worker.ui.retainedDrivers,How many finished drivers the Spark UI and status APIs remember before garbage collecting.,0,0,others,spark
8048,spark.worker.ui.retainedExecutors,How many finished executors the Spark UI and status APIs remember before garbage collecting.,0,0,others,spark
8049,SPARK_LOCAL_IP,IP address of the machine to bind to.,0,0,others,spark
8050,SPARK_PUBLIC_DNS,Hostname your Spark program will advertise to other machines.,0,0,others,spark
8051,SPARKR_DRIVER_R,R binary executable to use for SparkR shell (default is R).,0,0,others,spark
8052,access_log,Configures whether and how Squid logs HTTP and ICP transactions.,1,3,reliability-tradeoff,squid
8053,acl,Defining an Access List,1,3,reliability-tradeoff,squid
8054,acl_uses_indirect_client,Controls whether the indirect client address (see follow_x_forwarded_for) is used instead of the direct client address in acl matching.,0,0,others,squid
8055,adaptation_meta,This option allows Squid administrator to add custom ICAP request headers or eCAP options to Squid ICAP requests or eCAP transactions. Use it to pass custom authentication tokens and other transaction-state related meta information to an ICAP/eCAP service.,0,0,others,squid
8056,adaptation_send_client_ip,"If enabled, Squid shares HTTP client IP information with adaptation services. For ICAP, Squid adds the X-Client-IP header to ICAP requests. For eCAP, Squid sets the libecap::metaClientIp transaction option.",0,0,others,squid
8057,adaptation_send_username,This sends authenticated HTTP client username (if available) to the adaptation service.,0,0,others,squid
8058,adaptation_service_chain,"Configures a list of complementary services that will be applied one-by-one, forming an adaptation chain or pipeline. This is useful when Squid must perform different adaptations on the same message.",0,0,others,squid
8059,adaptation_service_iteration_limit,Limits the number of iterations allowed when applying adaptation services to a message.,0,0,others,squid
8060,adaptation_service_set,"Configures an ordered set of similar, redundant services. This is useful when hot standby or backup adaptation servers are available.",0,0,others,squid
8061,adaptation_uses_indirect_client,Controls whether the indirect client IP address (instead of the direct client IP address) is passed to adaptation services.,0,0,others,squid
8062,adapted_http_access,Allowing or Denying access based on defined access lists,0,0,others,squid
8063,allow_underscore,Underscore characters is not strictly allowed in Internet hostnames but nevertheless used by many sites. Set this to off if you want Squid to be strict about the standard.,0,0,others,squid
8064,announce_file,The contents of this file will be included in the announce registration messages.,0,0,others,squid
8065,announce_host,Set the hostname where announce registration messages will be sent.,0,0,others,squid
8066,announce_period,This is how frequently to send cache announcements.,0,0,others,squid
8067,announce_port,Set the port where announce registration messages will be sent.,0,0,others,squid
8068,append_domain,Appends local domain name to hostnames without any dots in them.,0,0,others,squid
8069,as_whois_server,"WHOIS server to query for AS numbers. NOTE: AS numbers are queried only when Squid starts up, not for every request.",0,0,others,squid
8070,auth_param,This is used to define parameters for the various authentication schemes supported by Squid.,0,0,others,squid
8071,authenticate_cache_garbage_interval,The time period between garbage collection across the username cache.This is a trade-off between memory utilization (long intervals - say 2 days) and CPU (short intervals - say 1 minute).,0,0,others,squid
8072,authenticate_ip_ttl,"If you use proxy authentication and the 'max_user_ip' ACL, this directive controls how long Squid remembers the IP addresses associated with each user.",0,0,others,squid
8073,authenticate_ttl,"The time a user & their credentials stay in the logged in user cache since their last request. When the garbage interval passes, all user credentials that have passed their TTL are removed from memory.",0,0,others,squid
8074,background_ping_rate,Controls how often the ICP pings are sent to siblings that have background-ping set.,0,0,others,squid
8075,broken_posts,"A list of ACL elements which, if matched, causes Squid to send an extra CRLF pair after the body of a PUT/POST request.",0,0,others,squid
8076,buffered_logs,Whether to write/send access_log records ASAP or accumulate them and then write/send them in larger chunks. Buffering may improve performance because it decreases the number of I/Os.,1,1,resource,squid
8077,cache,Requests denied by this directive will not be served from the cache and their responses will not be stored in the cache.,0,0,others,squid
8078,cache_effective_group,Squid sets the GID to the effective user's default group ID (taken from the password file) and supplementary group list from the groups membership.,0,0,others,squid
8079,cache_effective_user,"If you start Squid as root, it will change its effective/real UID/GID to the user specified below.",0,0,others,squid
8080,cache_log,Squid administrative logging file. This is where general information about Squid behavior goes.,0,0,others,squid
8081,cache_mem,cache_mem' specifies the ideal amount of memory to be used,1,1,resource,squid
8082,cache_mgr,"Email-address of local cache manager who will receive mail if the cache dies. The default is ""webmaster"".",0,0,others,squid
8083,cache_miss_revalidate,This option determines whether Squid on cache MISS will pass the client revalidation request to the server or tries to fetch new content for caching.,0,0,others,squid
8084,cache_peer,To specify other caches in a hierarchy,1,6,function-tradeoff,squid
8085,cache_peer_access,Restricts usage of cache_peer proxies.,1,6,function-tradeoff,squid
8086,cache_replacement_policy,The cache replacement policy parameter determines which objects are evicted (replaced) when disk space is needed.,0,0,others,squid
8087,cache_store_log,"Logs the activities of the storage manager. Shows which objects are ejected from the cache, and which objects are saved and for how long.",0,0,others,squid
8088,cache_swap_high,The high-water mark for AUFS/UFS/diskd cache object eviction by the cache_replacement_policy algorithm.,1,3,reliability-tradeoff,squid
8089,cache_swap_low,The low-water mark for AUFS/UFS/diskd cache object eviction by the cache_replacement_policy algorithm.,1,3,reliability-tradeoff,squid
8090,cache_swap_state,"Location for the cache ""swap.state"" file.",0,0,others,squid
8091,cachemgr_passwd,Specify passwords for cachemgr operations.,0,0,others,squid
8092,check_hostnames,For security and stability reasons Squid can check hostnames for Internet standard RFC compliance. If you want Squid to perform these checks turn this directive on.,1,2,security-tradeoff,squid
8093,chroot,Specifies a directory where Squid should do a chroot() while initializing. This also causes Squid to fully drop root privileges after initializing.,0,0,others,squid
8094,client_db,"If you want to disable collecting per-client statistics, turn off client_db here.",0,0,others,squid
8095,client_delay_initial_bucket_level,This option determines the initial bucket size as a percentage of max_bucket_size from client_delay_parameters.,0,0,others,squid
8096,client_delay_pools,This option specifies the number of client delay pools used. It must preceed other client_delay_* options.,0,0,others,squid
8097,client_dst_passthru,With NAT or TPROXY intercepted traffic Squid may pass the request directly to the original client destination IP or seek a faster source using the HTTP Host header.,0,0,others,squid
8098,client_idle_pconn_timeout,How long to wait for the next HTTP request on a persistent client connection after the previous request completes.,0,0,others,squid
8099,client_ip_max_connections,Set an absolute limit on the number of connections a single client IP can use. Any more than this and Squid will begin to drop new connections from the client until it closes some links.,1,1,resource,squid
8100,client_lifetime,The maximum amount of time a client (browser) is allowed to remain connected to the cache process.,0,0,others,squid
8101,client_netmask,A netmask for client addresses in logfiles and cachemgr output. Change this to protect the privacy of your cache clients.,0,0,others,squid
8102,client_persistent_connections,Persistent connection support for clients.,1,3,reliability-tradeoff,squid
8103,client_request_buffer_max_size,This specifies the maximum buffer size of a client request. It prevents squid eating too much memory when somebody uploads a large file.,1,1,resource,squid
8104,clientside_mark,"Allows you to apply a Netfilter mark value to packets being transmitted on the client-side, based on an ACL.",0,0,others,squid
8105,clientside_tos,"Allows you to select a TOS/DSCP value for packets being transmitted on the client-side, based on an ACL.",0,0,others,squid
8106,collapsed_forwarding,This option controls whether Squid is allowed to merge multiple potentially cachable requests for the same URI before Squid knows whether the response is going to be cachable.,0,0,others,squid
8107,collapsed_forwarding_shared_entries_limit,This limits the size of a table used for sharing information about collapsible entries among SMP workers.,0,0,others,squid
8108,configuration_includes_quoted_values,"If set, Squid will recognize each ""quoted string"" after a configuration directive as a single parameter. The quotes are stripped before the parameter value is interpreted or used.",0,0,others,squid
8109,connect_retries,Limits the number of reopening attempts when establishing a single TCP connection. All these attempts must still complete before the applicable connection opening timeout expires.,0,0,others,squid
8110,connect_timeout,This parameter specifies how long to wait for the TCP connect to the requested server or peer to complete before Squid should attempt to find another path where to forward the request.,0,0,others,squid
8111,coredump_dir,"By default Squid leaves core files in the directory from where it was started. If you set 'coredump_dir' to a directory that exists, Squid will chdir() to that directory at startup and coredump files will be left there.",0,0,others,squid
8112,cpu_affinity_map,Sets 1:1 mapping between Squid processes and CPU cores.,1,1,resource,squid
8113,dead_peer_timeout,"This controls how long Squid waits to declare a peer cache as ""dead.""",0,0,others,squid
8114,debug_options,"Logging options are set as section,level where each source file is assigned a unique section.",0,0,others,squid
8115,delay_access,This is used to determine which delay pool a request falls into.,0,0,others,squid
8116,delay_class,This defines the class of each delay pool.,0,0,others,squid
8117,delay_initial_bucket_level,"The initial bucket percentage is used to determine how much is put in each bucket when squid starts, is reconfigured, or first notices a host accessing it (in class 2 and class 3, individual hosts and networks only have buckets associated with them once they have been ""seen"" by squid).",0,0,others,squid
8118,delay_pool_uses_indirect_client,Controls whether the indirect client address (see follow_x_forwarded_for) is used instead of the direct client address in delay pools.,0,0,others,squid
8119,delay_pools,This represents the number of delay pools to be used.,0,0,others,squid
8120,detect_broken_pconn,By enabling this directive Squid attempts to detect such broken replies and automatically assume the reply is finished after 10 seconds timeout.,1,4,limited-side-effect,squid
8121,digest_bits_per_entry,This is the number of bits of the server's Cache Digest which will be associated with the Digest entry for a given HTTP Method and URL (public key) combination.,0,0,others,squid
8122,digest_generation,This controls whether the server will generate a Cache Digest of its contents.,0,0,others,squid
8123,digest_rebuild_chunk_percentage,This is the percentage of the Cache Digest to be scanned at a time.,1,5,workload-specific,squid
8124,digest_rebuild_period,This is the wait time between Cache Digest rebuilds.,0,0,others,squid
8125,digest_rewrite_period,This is the wait time between Cache Digest writes to disk.,0,0,others,squid
8126,digest_swapout_chunk_size,This is the number of bytes of the Cache Digest to write to disk at a time.,1,1,resource,squid
8127,diskd_program,Specify the location of the diskd executable.,0,0,others,squid
8128,dns_packet_max,Maximum number of bytes packet size to advertise via EDNS.,1,1,resource,squid
8129,dns_retransmit_interval,Initial retransmit interval for DNS queries. The interval is doubled each time all configured DNS servers have been tried.,0,0,others,squid
8130,dns_timeout,DNS Query timeout. If no response is received to a DNS query within this time all DNS servers for the queried domain are assumed to be unavailable.,0,0,others,squid
8131,dns_v4_first,With the IPv6 Internet being as fast or faster than IPv4 Internet for most networks Squid prefers to contact websites over IPv6.,0,0,others,squid
8132,ecap_enable,Controls whether eCAP support is enabled.,0,0,others,squid
8133,email_err_data,"If enabled, information about the occurred error will be included in the mailto links of the ERR pages (if %W is set) so that the email body contains the data.",0,0,others,squid
8134,err_html_text,"HTML text to include in error messages. Make this a ""mailto"" URL to your admin address, or maybe just a link to your organizations Web page.",0,0,others,squid
8135,err_page_stylesheet,CSS Stylesheet to pattern the display of Squid default error pages.,0,0,others,squid
8136,error_default_language,Set the default language which squid will send error pages in if no existing translation matches the clients language preferences.,0,0,others,squid
8137,error_directory,If you wish to create your own versions of the default error files to customize them to suit your company copy the error/template files to another directory and point this tag at them.,0,0,others,squid
8138,error_log_languages,Log to cache.log what languages users are attempting to auto-negotiate for translations.,0,0,others,squid
8139,esi_parser,Selects the XML parsing library to use when interpreting responses with Edge Side Includes.,0,0,others,squid
8140,eui_lookup,Whether to lookup the EUI or MAC address of a connected client.,0,0,others,squid
8141,external_acl_type,This option defines external acl classes using a helper program to look up the status,0,0,others,squid
8142,follow_x_forwarded_for,Determine which client proxies can be trusted to provide correct information regarding real client IP address.,0,0,others,squid
8143,force_request_body_continuation,"This option controls how Squid handles data upload requests from HTTP and FTP agents that require a ""Please Continue"" control message response to actually send the request body to Squid. It is mostly useful in adaptation environments.",0,0,others,squid
8144,forward_max_tries,Limits the number of attempts to forward the request.,0,0,others,squid
8145,forward_timeout,This parameter specifies how long Squid should at most attempt in finding a forwarding path for the request before giving up.,0,0,others,squid
8146,forwarded_for,"If set to ""on"", Squid will append your client's IP address in the HTTP requests it forwards.",0,0,others,squid
8147,fqdncache_size,Maximum number of FQDN cache entries.,1,1,resource,squid
8148,ftp_client_idle_timeout,How long to wait for an FTP request on a connection to Squid ftp_port.,0,0,others,squid
8149,ftp_port,Enables Native FTP proxy by specifying the socket address where Squid listens for FTP client requests.,0,0,others,squid
8150,ftp_user,"If you want the anonymous login password to be more informative (and enable the use of picky FTP servers), set this to something reasonable for your domain, like wwwuser@somewhere.net",0,0,others,squid
8151,global_internal_static,"This directive controls is Squid should intercept all requests for /squid-internal-static/ no matter which host the URL is requesting (default on setting), or if nothing special should be done for such URLs (off setting).",0,0,others,squid
8152,half_closed_clients,"Some clients may shutdown the sending side of their TCP connections, while leaving their receiving sides open.",0,0,others,squid
8153,high_memory_warning,"If the memory usage (as determined by gnumalloc, if available and used) exceeds this amount, Squid prints a WARNING with debug level 0 to get the administrators attention.",0,0,others,squid
8154,high_page_fault_warning,"If the one-minute average page fault rate exceeds this value, Squid prints a WARNING with debug level 0 to get the administrators attention.",0,0,others,squid
8155,high_response_time_warning,"If the one-minute median response time exceeds this value, Squid prints a WARNING with debug level 0 to get the administrators attention.",0,0,others,squid
8156,hopeless_kid_revival_delay,"Normally, when a kid process dies, Squid immediately restarts the kid. A kid experiencing frequent deaths is marked as ""hopeless"" for the duration specified by this directive. Hopeless kids are not automatically restarted.",0,0,others,squid
8157,host_verify_strict,"Regardless of this option setting, when dealing with intercepted traffic, Squid always verifies that the destination IP address matches the Host header domain or IP (called 'authority form URL').",0,0,others,squid
8158,hostname_aliases,A list of other DNS names your cache has.,0,0,others,squid
8159,htcp_access,Allowing or Denying access to the HTCP port based on defined access lists,0,0,others,squid
8160,htcp_clr_access,Allowing or Denying access to purge content using HTCP based on defined access lists.,0,0,others,squid
8161,htcp_port,The port number where Squid sends and receives HTCP queries to and from neighbor caches.,0,0,others,squid
8162,http_accel_surrogate_remote,Set this to on to have squid behave as a remote surrogate.,0,0,others,squid
8163,http_access,Allowing or Denying access based on defined access lists,0,0,others,squid
8164,http_port,The socket addresses where Squid will listen for HTTP client requests.,0,0,others,squid
8165,http_reply_access,Allow replies to client requests. This is complementary to http_access.,0,0,others,squid
8166,httpd_accel_surrogate_id,"Surrogates (http://www.esi.org/architecture_spec_1.0.html) need an identification token to allow control targeting. Because a farm of surrogates may all perform the same tasks, they may share an identification token.",0,0,others,squid
8167,httpd_suppress_version_string,Suppress Squid version string info in HTTP headers and HTML error pages.,0,0,others,squid
8168,https_port,The socket address where Squid will listen for client requests made over TLS or SSL connections.,0,0,others,squid
8169,icap_client_username_encode,Whether to base64 encode the authenticated client username.,0,0,others,squid
8170,icap_client_username_header,ICAP request header name to use for adaptation_send_username.,0,0,others,squid
8171,icap_connect_timeout,This parameter specifies how long to wait for the TCP connect to the requested ICAP server to complete before giving up and either terminating the HTTP transaction or bypassing the failure.,0,0,others,squid
8172,icap_default_options_ttl,The default TTL value for ICAP OPTIONS responses that don't have an Options-TTL header.,0,0,others,squid
8173,icap_enable,"If you want to enable the ICAP module support, set this to on.",0,0,others,squid
8174,icap_io_timeout,"This parameter specifies how long to wait for an I/O activity on an established, active ICAP connection before giving up and either terminating the HTTP transaction or bypassing the failure.",0,0,others,squid
8175,icap_log,"ICAP log files record ICAP transaction summaries, one line per transaction.",0,0,others,squid
8176,icap_persistent_connections,Whether or not Squid should use persistent connections to an ICAP server.,1,3,reliability-tradeoff,squid
8177,icap_preview_enable,"The ICAP Preview feature allows the ICAP server to handle the HTTP message by looking only at the beginning of the message body or even without receiving the body at all. In some environments, previews greatly speedup ICAP processing.",0,0,others,squid
8178,icap_preview_size,The default size of preview data to be sent to the ICAP server.,1,1,resource,squid
8179,icap_retry_limit,"Limits the number of retries allowed. Communication errors due to persistent connection race conditions are unavoidable, automatically retried, and do not count against this limit.",0,0,others,squid
8180,icap_service_failure_limit,"The limit specifies the number of failures that Squid tolerates when establishing a new TCP connection with an ICAP service. If the number of failures exceeds the limit, the ICAP service is not used for new ICAP requests until it is time to refresh its OPTIONS.",1,1,resource,squid
8181,icap_service_revival_delay,"The delay specifies the number of seconds to wait after an ICAP OPTIONS request failure before requesting the options again. The failed ICAP service is considered ""down"" until fresh OPTIONS are fetched.",0,0,others,squid
8182,icon_directory,Where the icons are stored.,0,0,others,squid
8183,icp_access,Allowing or Denying access to the ICP port based on defined access lists,0,0,others,squid
8184,icp_hit_stale,"If you want to return ICP_HIT for stale cache objects, set this option to 'on'. If you have sibling relationships with caches in other administrative domains, this should be 'off'.",0,0,others,squid
8185,icp_port,The port number where Squid sends and receives ICP queries to and from neighbor caches.,0,0,others,squid
8186,icp_query_timeout,Normally Squid will automatically determine an optimal ICP query timeout value based on the round-trip-time of recent ICP queries.,0,0,others,squid
8187,ident_lookup_access,"A list of ACL elements which, if matched, cause an ident (RFC 931) lookup to be performed for this request.",0,0,others,squid
8188,ident_timeout,Maximum time to wait for IDENT lookups to complete.,0,0,others,squid
8189,ignore_unknown_nameservers,"By default Squid checks that DNS responses are received from the same IP addresses they are sent to. If they don't match, Squid ignores the response and writes a warning message to cache.log. You can allow responses from unknown nameservers by setting this option to 'off'.",0,0,others,squid
8190,ipcache_high,"The size, low-, and high-water marks for the IP cache.",1,3,reliability-tradeoff,squid
8191,ipcache_low,"The size, low-, and high-water marks for the IP cache.",1,3,reliability-tradeoff,squid
8192,ipcache_size,Maximum number of DNS IP cache entries.,1,1,resource,squid
8193,loadable_modules,Instructs Squid to load the specified dynamic module(s) or activate preloaded module(s).,0,0,others,squid
8194,log_icp_queries,"If set, ICP queries are logged to access.log. You may wish do disable this if your ICP load is VERY high to speed things up or to simplify log analysis.",1,6,function-tradeoff,squid
8195,log_mime_hdrs,The Cache can record both the request and the response MIME headers for each HTTP transaction. The headers are encoded safely and will appear as two bracketed fields at the end of the access log (for either the native or httpd-emulated log formats). To enable this logging set log_mime_hdrs to 'on'.,0,0,others,squid
8196,log_uses_indirect_client,Controls whether the indirect client address (see follow_x_forwarded_for) is used instead of the direct client address in the access log.,0,0,others,squid
8197,logfile_daemon,"Specify the path to the logfile-writing daemon. This daemon is used to write the access and store logs, if configured.",0,0,others,squid
8198,logfile_rotate,Specifies the default number of logfile rotations to make when you type 'squid -k rotate'.,1,1,resource,squid
8199,logformat,Defines an access log format.,0,0,others,squid
8200,mail_program,Email program used to send mail if the cache dies.,0,0,others,squid
8201,max_open_disk_fds,To avoid having disk as the I/O bottleneck Squid can optionally bypass the on-disk cache if more than this amount of disk file descriptors are open.,1,1,resource,squid
8202,max_stale,This option puts an upper limit on how stale content Squid will serve from the cache if cache validation fails.,0,0,others,squid
8203,maximum_icp_query_timeout,Normally the ICP query timeout is determined dynamically. But sometimes it can lead to very large values (say 5 seconds). Use this option to put an upper limit on the dynamic timeout value.,0,0,others,squid
8204,maximum_object_size,Set the default value for max-size parameter on any cache_dir.,0,0,others,squid
8205,maximum_object_size_in_memory,Objects greater than this size will not be attempted to kept in the memory cache. This should be set high enough to keep objects accessed frequently in memory to improve performance whilst low enough to keep larger objects from hoarding cache_mem.,1,1,resource,squid
8206,mcast_miss_addr,"If you enable this option, every ""cache miss"" URL will be sent out on the specified multicast address.",0,0,others,squid
8207,mcast_miss_encode_key,The URLs that are sent in the multicast miss stream are encrypted.,1,2,security-tradeoff,squid
8208,mcast_miss_port,This is the port number to be used in conjunction with 'mcast_miss_addr'.,0,0,others,squid
8209,mcast_miss_ttl,This is the time-to-live value for packets multicasted when multicasting off cache miss URLs is enabled.,0,0,others,squid
8210,memory_cache_mode,Controls which objects to keep in the memory cache (cache_mem),1,4,limited-side-effect,squid
8211,memory_cache_shared,Controls whether the memory cache is shared among SMP workers.,1,4,limited-side-effect,squid
8212,memory_pools,"If set, Squid will keep pools of allocated (but unused) memory available for future use. If memory is a premium on your system and you believe your malloc library outperforms Squid routines, disable this.",1,1,resource,squid
8213,memory_replacement_policy,The memory replacement policy parameter determines which objects are purged from memory when memory space is needed.,0,0,others,squid
8214,mime_table,Path to Squid's icon configuration file.,0,0,others,squid
8215,minimum_direct_hops,"If using the ICMP pinging stuff, do direct fetches for sites which are no more than this many hops away.",0,0,others,squid
8216,minimum_direct_rtt,"If using the ICMP pinging stuff, do direct fetches for sites which are no more than this many rtt milliseconds away.",0,0,others,squid
8217,minimum_expiry_time,The minimum caching time according to (Expires - Date) headers Squid honors if the object can't be revalidated.,0,0,others,squid
8218,minimum_icp_query_timeout,"Normally the ICP query timeout is determined dynamically. But sometimes it can lead to very small timeouts, even lower than the normal latency variance on your link due to traffic. Use this option to put an lower limit on the dynamic timeout value.",0,0,others,squid
8219,minimum_object_size,Objects smaller than this size will NOT be saved on disk.,1,4,limited-side-effect,squid
8220,miss_access,Determines whether network access is permitted when satisfying a request.,0,0,others,squid
8221,negative_dns_ttl,Time-to-Live (TTL) for negative caching of failed DNS lookups.,1,5,workload-specific,squid
8222,negative_ttl,Set the Default Time-to-Live (TTL) for failed requests.,0,0,others,squid
8223,neighbor_type_domain,Modify the cache_peer neighbor type when passing requests about specific domains to the peer.,0,0,others,squid
8224,netdb_filename,Where Squid stores it's netdb journal. When enabled this journal preserves netdb state between restarts.,0,0,others,squid
8225,netdb_high,The high water mark for the ICMP measurement database.,1,3,reliability-tradeoff,squid
8226,netdb_low,The low water mark for the ICMP measurement database.,1,3,reliability-tradeoff,squid
8227,netdb_ping_period,The minimum period for measuring a site. There will be at least this much delay between successive pings to the same network.,0,0,others,squid
8228,nonhierarchical_direct,"By default, Squid will send any non-hierarchical requests (not cacheable request type) direct to origin servers. When this is set to ""off"", Squid will prefer to send these requests to parents.",0,0,others,squid
8229,note,This option used to log custom information about the master transaction.,0,0,others,squid
8230,offline_mode,Enable this option and Squid will never try to validate cached objects.,1,3,reliability-tradeoff,squid
8231,on_unsupported_protocol,Determines Squid behavior when encountering strange requests at the beginning of an accepted TCP connection or the beginning of a bumped CONNECT tunnel.,0,0,others,squid
8232,pconn_lifetime,Desired maximum lifetime of a persistent connection.,0,0,others,squid
8233,peer_connect_timeout,This parameter specifies how long to wait for a pending TCP connection to a peer cache.,0,0,others,squid
8234,persistent_connection_after_error,With this directive the use of persistent connections after HTTP errors can be disabled.,1,3,reliability-tradeoff,squid
8235,pid_filename,A filename to write the process-id to.,0,0,others,squid
8236,pinger_enable,Control whether the pinger is active at run-time.,0,0,others,squid
8237,pinger_program,Specify the location of the executable for the pinger process.,0,0,others,squid
8238,positive_dns_ttl,Upper limit on how long Squid will cache positive DNS responses.,0,0,others,squid
8239,prefer_direct,Normally Squid tries to use parents for most requests. If you for some reason like it to first try going direct and only use a parent if going direct fails set this to on.,0,0,others,squid
8240,proxy_protocol_access,Determine which client proxies can be trusted to provide correct information regarding real client IP address using PROXY protocol.,0,0,others,squid
8241,qos_flows,"Allows you to select a TOS/DSCP value to mark outgoing connections to the client, based on where the reply was sourced.",0,0,others,squid
8242,query_icmp,"If you want to ask your peers to include ICMP data in their ICP replies, enable this option.",1,4,limited-side-effect,squid
8243,range_offset_limit,Sets an upper limit on how far (number of bytes) into the file a Range request may be to cause Squid to prefetch the whole file.,0,0,others,squid
8244,read_ahead_gap,The amount of data the cache will buffer ahead of what has been sent to the client when retrieving an object from another server.,0,0,others,squid
8245,read_timeout,"After each successful read(), the timeout will be extended by this amount. If no data is read again after this amount of time, the request is aborted and logged with ERR_READ_TIMEOUT.",0,0,others,squid
8246,refresh_all_ims,"When you enable this option, squid will always check the origin server for an update when a client sends an If-Modified-Since request.",0,0,others,squid
8247,relaxed_header_parser,"In the default ""on"" setting Squid accepts certain forms of non-compliant HTTP messages where it is unambiguous what the sending application intended even if the message is not correctly formatted. The messages is then normalized to the correct form when forwarded by Squid.",0,0,others,squid
8248,reload_into_ims,"When you enable this option, client no-cache or ''reload'' requests will be changed to If-Modified-Since requests. Doing this VIOLATES the HTTP standard. Enabling this feature could make you liable for problems which it causes.",0,0,others,squid
8249,reply_body_max_size,"This option specifies the maximum size of a reply body. It can be used to prevent users from downloading very large files, such as MP3's and movies.",1,1,resource,squid
8250,reply_header_add,"This option adds header fields to outgoing HTTP responses (i.e., response headers delivered by Squid to the client). This option has no effect on cache hit detection.",0,0,others,squid
8251,reply_header_max_size,This specifies the maximum size for HTTP headers in a reply.,1,1,resource,squid
8252,reply_header_replace,"This option allows you to change the contents of headers denied with reply_header_access above, by replacing them with some fixed string.",0,0,others,squid
8253,request_body_max_size,This specifies the maximum size for an HTTP request body.,1,1,resource,squid
8254,request_entities,Set this directive to on if you have clients which insists on sending request entities in GET or HEAD requests. But be warned that there is server software (both proxies and web servers) which can fail to properly process this kind of request which may make you vulnerable to cache pollution attacks if enabled.,0,0,others,squid
8255,request_header_add,"This option adds header fields to outgoing HTTP requests (i.e., request headers sent by Squid to the next HTTP hop such as a cache peer or an origin server).",0,0,others,squid
8256,request_header_max_size,This specifies the maximum size for HTTP headers in a request.,1,1,resource,squid
8257,request_header_replace,"This option allows you to change the contents of headers denied with request_header_access above, by replacing them with some fixed string.",0,0,others,squid
8258,request_start_timeout,How long to wait for the first request byte after initial connection establishment.,0,0,others,squid
8259,request_timeout,How long to wait for complete HTTP request headers after initial connection establishment.,0,0,others,squid
8260,retry_on_error,"If set to ON Squid will automatically retry requests when receiving an error response with status 403 (Forbidden), 500 (Internal Error), 501 or 503 (Service not available). Status 502 and 504 (Gateway errors) are always retried.",0,0,others,squid
8261,send_hit,"Responses denied by this directive will not be served from the cache (but may still be cached, see store_miss).",0,0,others,squid
8262,server_idle_pconn_timeout,Timeout for idle persistent connections to servers and other proxies.,0,0,others,squid
8263,server_pconn_for_nonretriable,This option provides fine-grained control over persistent connection reuse when forwarding HTTP requests that Squid cannot retry.,0,0,others,squid
8264,server_persistent_connections,Persistent connection support for servers.,1,3,reliability-tradeoff,squid
8265,shared_memory_locking,"Whether to ensure that all required shared memory is available by ""locking"" that shared memory into RAM when Squid starts. The alternative is faster startup time followed by slightly slower performance and, if not enough RAM is actually available during runtime, mysterious crashes.",1,1,resource,squid
8266,short_icon_urls,If this is enabled Squid will use short URLs for icons. If disabled it will revert to the old behavior of including it's own name and port in the URL.,0,0,others,squid
8267,shutdown_lifetime,"When SIGTERM or SIGHUP is received, the cache is put into ""shutdown pending"" mode until all active sockets are closed.",0,0,others,squid
8268,sleep_after_fork,"When this is set to a non-zero value, the main Squid process sleeps the specified number of microseconds after a fork() system call.",0,0,others,squid
8269,snmp_access,Allowing or denying access to the SNMP port.,0,0,others,squid
8270,snmp_incoming_address,used for the SNMP socket receiving messages from SNMP agents.,0,0,others,squid
8271,snmp_outgoing_address,used for SNMP packets returned to SNMP agents.,0,0,others,squid
8272,snmp_port,The port number where Squid listens for SNMP requests.,0,0,others,squid
8273,spoof_client_ip,Control client IP address spoofing of TPROXY traffic based on defined access lists.,0,0,others,squid
8274,ssl_bump,"This option is consulted when a CONNECT request is received on an http_port (or a new connection is intercepted at an https_port), provided that port was configured with an ssl-bump flag.",0,0,others,squid
8275,ssl_engine,The OpenSSL engine to use. You will need to set this if you would like to use hardware SSL acceleration for example.,1,3,reliability-tradeoff,squid
8276,ssl_unclean_shutdown,Some browsers (especially MSIE) bugs out on SSL shutdown messages.,0,0,others,squid
8277,sslcrtd_children,Specifies the maximum number of certificate generation processes that Squid may spawn (numberofchildren) and several related options.,1,1,resource,squid
8278,sslcrtd_program,Specify the location and options of the executable for certificate generator.,0,0,others,squid
8279,sslcrtvalidator_children,Specifies the maximum number of certificate validation processes that Squid may spawn (numberofchildren) and several related options.,1,1,resource,squid
8280,sslcrtvalidator_program,Specify the location and options of the executable for ssl_crt_validator process.,0,0,others,squid
8281,sslpassword_program,Specify a program used for entering SSL key passphrases when using encrypted SSL certificate keys.,0,0,others,squid
8282,sslproxy_cert_error,Use this ACL to bypass server certificate validation errors.,0,0,others,squid
8283,sslproxy_cert_sign_hash,Sets the hashing algorithm to use when signing generated certificates. Valid algorithm names depend on the OpenSSL library used.,1,2,security-tradeoff,squid
8284,sslproxy_foreign_intermediate_certs,"Many origin servers fail to send their full server certificate chain for verification, assuming the client already has or can easily locate any missing intermediate certificates.",0,0,others,squid
8285,sslproxy_session_cache_size,Sets the cache size to use for ssl session,1,1,resource,squid
8286,sslproxy_session_ttl,Sets the timeout value for SSL sessions,0,0,others,squid
8287,stats_collection,This options allows you to control which requests gets accounted in performance counters.,0,0,others,squid
8288,store_avg_object_size,"Average object size, used to estimate number of objects your cache can hold. The default is 13 KB.",1,1,resource,squid
8289,store_dir_select_algorithm,How Squid selects which cache_dir to use when the response object will fit into more than one.,0,0,others,squid
8290,store_id_access,"If defined, this access list specifies which requests are sent to the StoreID processes.",0,0,others,squid
8291,store_id_bypass,"When this is 'on', a request will not go through the helper if all helpers are busy. If this is 'off' and the helper queue grows too large, the action is prescribed by the on-persistent-overload option.",0,0,others,squid
8292,store_id_extras,Specifies a string to be append to request line format for the StoreId helper.,0,0,others,squid
8293,store_id_program,Specify the location of the executable StoreID helper to use.,0,0,others,squid
8294,store_miss,"Responses denied by this directive will not be cached (but may still be served from the cache, see send_hit).",0,0,others,squid
8295,strip_query_terms,"By default, Squid strips query terms from requested URLs before logging. This protects your user's privacy and reduces log size.",1,6,function-tradeoff,squid
8296,tcp_outgoing_address,Allows you to map requests to different outgoing IP addresses based on the username or source address of the user making the request.,0,0,others,squid
8297,tcp_outgoing_mark,"Allows you to apply a Netfilter mark value to outgoing packets on the server side, based on an ACL.",0,0,others,squid
8298,tcp_outgoing_tos,"Allows you to select a TOS/Diffserv value for packets outgoing on the server side, based on an ACL.",0,0,others,squid
8299,tcp_recv_bufsize,Size of receive buffer to set for TCP sockets. Probably just as easy to change your kernel's default.,1,1,resource,squid
8300,test_reachability,"When this is 'on', ICP MISS replies will be ICP_MISS_NOFETCH instead of ICP_MISS if the target host is NOT in the ICMP database, or has a zero RTT.",0,0,others,squid
8301,tproxy_uses_indirect_client,Controls whether the indirect client address (see follow_x_forwarded_for) is used instead of the direct client address when spoofing the outgoing client.,0,0,others,squid
8302,udp_incoming_address,used for UDP packets received from other caches.,0,0,others,squid
8303,udp_outgoing_address,used for UDP packets sent out to other caches.,0,0,others,squid
8304,umask,"Minimum umask which should be enforced while the proxy is running, in addition to the umask set at startup.",0,0,others,squid
8305,unique_hostname,If you want to have multiple machines with the same 'visible_hostname' you must give each machine a different 'unique_hostname' so forwarding loops can be detected.,0,0,others,squid
8306,unlinkd_program,Specify the location of the executable for file deletion process.,0,0,others,squid
8307,url_rewrite_bypass,"When this is 'on', a request will not go through the redirector if all the helpers are busy. If this is 'off' and the redirector queue grows too large, the action is prescribed by the on-persistent-overload option.",0,0,others,squid
8308,url_rewrite_children,Specifies the maximum number of redirector processes that Squid may spawn (numberofchildren) and several related options.,1,1,resource,squid
8309,url_rewrite_extras,Specifies a string to be append to request line format for the rewriter helper.,0,0,others,squid
8310,url_rewrite_host_header,"If defined, this access list specifies which requests are sent to the redirector processes.",0,0,others,squid
8311,url_rewrite_program,Specify the location of the executable URL rewriter to use.,0,0,others,squid
8312,url_rewrite_timeout,Squid times active requests to redirector.,0,0,others,squid
8313,vary_ignore_expire,Many HTTP servers supporting Vary gives such objects immediate expiry time with no cache-control header when requested by a HTTP/1.0 client. This option enables Squid to ignore such expiry times until HTTP/1.1 is fully implemented.,0,0,others,squid
8314,via,"If set (default), Squid will include a Via header in requests and replies as required by RFC2616.",0,0,others,squid
8315,visible_hostname,"If you want to present a special hostname in error messages, etc, define this. Otherwise, the return value of gethostname() will be used.",0,0,others,squid
8316,workers,"Number of main Squid processes or ""workers"" to fork and maintain.",1,1,resource,squid
8317,write_timeout,This timeout is tracked for all connections that have data available for writing and are waiting for the socket to become ready.,0,0,others,squid
