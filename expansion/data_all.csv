ID,name,description,p,s,label,software,filter,study
1,allocate_tokens_for_keyspace,Triggers automatic allocation of num_tokens tokens for this node. The allocation algorithm attempts to choose tokens in a way that optimizes replicated load over the nodes in the datacenter for the replication strategy used by the specified keyspace.,1,4,limited-side-effect,cassandra,0,0
2,authenticator,"Authentication backend, implementing IAuthenticator; used to identify users Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthenticator, PasswordAuthenticator}.",0,0,others,cassandra,0,0
3,authorizer,"Authorization backend, implementing IAuthorizer; used to limit access/provide permissions Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthorizer, CassandraAuthorizer}.",0,0,others,cassandra,0,0
4,auto_snapshot,Whether or not a snapshot is taken of the data before keyspace truncation or dropping of column families.,1,3,reliability-tradeoff,cassandra,0,0
5,back_pressure_enabled,"If enabled, the coordinator will apply the back-pressure strategy specified below to each mutation sent to replicas, with the aim of reducing pressure on overloaded replicas.",1,4,limited-side-effect,cassandra,0,0
6,batch_size_fail_threshold_in_kb,Fail any multiple-partition batch exceeding this value.,0,0,others,cassandra,0,0
7,batch_size_warn_threshold_in_kb,Log WARN on any multiple-partition batch size exceeding this value.,1,5,workload-specific,cassandra,0,0
8,batchlog_replay_throttle_in_kb,"Maximum throttle in KBs per second, total. This will be reduced proportionally to the number of nodes in the cluster.",1,1,resource,cassandra,0,1
9,broadcast_address,Address to broadcast to other Cassandra nodes Leaving this blank will set it to the same value as listen_address,0,0,others,cassandra,0,0
10,broadcast_rpc_address,RPC address to broadcast to drivers and other Cassandra nodes.,0,0,others,cassandra,0,0
11,cas_contention_timeout_in_ms,How long a coordinator should continue to retry a CAS operation that contends with other proposals for the same row,0,0,others,cassandra,0,1
12,cdc_enabled,Enable / disable CDC functionality on a per-node basis.,0,0,others,cassandra,0,0
13,cdc_free_space_check_interval_ms,"When we hit our cdc_raw limit and the CDCCompactor is either running behind or experiencing backpressure, we check at the following interval to see if any new space for cdc-tracked tables has been made available.",1,5,workload-specific,cassandra,0,0
14,cdc_raw_directory,CommitLogSegments are moved to this directory on flush if cdc_enabled: true and the segment contains mutations for a CDC-enabled table.,0,0,others,cassandra,0,0
15,cdc_total_space_in_mb,Total space to use for change-data-capture logs on disk.,0,0,others,cassandra,0,1
16,cluster_name,The name of the cluster. This is mainly used to prevent machines in one logical cluster from joining another.,0,0,others,cassandra,0,1
17,column_index_cache_size_in_kb,Per sstable indexed key cache entries (the collation index in memory mentioned above) exceeding this size will not be held on heap.,1,1,resource,cassandra,0,0
18,column_index_size_in_kb,"Granularity of the collation index of rows within a partition. Increase if your rows are large, or if you have a very large number of rows per partition. The competing goals are these: a smaller granularity means more index entries are generated and looking up rows withing the partition by collation column is faste but, Cassandra will keep the collation index in memory for hot rows (as part of the key cache), so a larger granularity means you can cache more hot rows",1,5,workload-specific,cassandra,0,0
19,commit_failure_policy,Policy for commit disk failures,1,3,reliability-tradeoff,cassandra,0,1
20,commitlog_compression,"Compression to apply to the commit log. If omitted, the commit log will be written uncompressed.",1,4,limited-side-effect,cassandra,0,1
21,commitlog_directory,"commit log. when running on magnetic HDD, this should be a separate spindle than the data directories.",0,0,others,cassandra,0,1
22,commitlog_segment_size_in_mb,"The size of the individual commitlog file segments. A commitlog segment may be archived, deleted, or recycled once all the data in it (potentially from each columnfamily in the system) has been flushed to sstables.",1,3,reliability-tradeoff,cassandra,0,0
23,commitlog_sync,"When in batch mode, Cassandra won’t ack writes until the commit log has been fsynced to disk. It will wait commitlog_sync_batch_window_in_ms milliseconds between fsyncs. This window should be kept short because the writer threads will be unable to do extra work while waiting.",1,3,reliability-tradeoff,cassandra,0,1
24,commitlog_total_space_in_mb,"Total space to use for commit logs on disk.If space gets above this value, Cassandra will flush every dirty CF in the oldest segment and remove it. So a small total commitlog space will tend to cause more flush activity on less-active columnfamilies.",1,3,reliability-tradeoff,cassandra,0,0
25,compaction_large_partition_warning_threshold_mb,Log a warning when compacting partitions larger than this value,1,5,workload-specific,cassandra,0,1
26,compaction_throughput_mb_per_sec,Throttles compaction to the given total throughput across the entire system.,1,5,workload-specific,cassandra,0,0
27,concurrent_compactors,"Number of simultaneous compactions to allow, NOT including validation ""compactions"" for anti-entropy repair. Simultaneous compactions can help preserve read performance in a mixed read/write workload, by mitigating the tendency of small sstables to accumulate during a single long running compactions. The default is usually fine and if you experience problems with compaction running too slowly or too fast, you should look at compaction_throughput_mb_per_sec first.",1,5,workload-specific,cassandra,0,0
28,concurrent_materialized_view_writes,"For materialized view writes, as there is a read involved, so this should be limited by the less of concurrent reads or concurrent writes.",1,1,resource,cassandra,0,0
29,concurrent_reads,concurrent_reads should be set to (16 * number_of_drives) in order to allow the operations to enqueue low enough in the stack that the OS and drives can reorder them.,1,1,resource,cassandra,0,0
30,concurrent_writes,"since writes are almost never IO bound, the ideal number of concurrent_writes is dependent on the number of cores in your system; (8 * number_of_cores) is a good rule of thumb.",1,1,resource,cassandra,0,0
31,counter_cache_keys_to_save,"Number of keys from the counter cache to save. Disabled by default, meaning all keys are going to be saved",1,1,resource,cassandra,0,0
32,counter_cache_save_period,Duration in seconds after which Cassandra should save the counter cache (keys only). Caches are saved to saved_caches_directory as specified in this configuration file.,1,5,workload-specific,cassandra,0,0
33,counter_cache_size_in_mb,Maximum size of the counter cache in memory.,1,1,resource,cassandra,0,1
34,counter_write_request_timeout_in_ms,How long the coordinator should wait for counter writes to complete,0,0,others,cassandra,0,0
35,credentials_update_interval_in_ms,"Refresh interval for credentials cache (if enabled). After this interval, cache entries become eligible for refresh.",1,5,workload-specific,cassandra,0,0
36,credentials_validity_in_ms,"Please note, credentials are cached in their encrypted form, so while activating this cache may reduce the number of queries made to the underlying table, it may not bring a significant reduction in the latency of individual authentication attempts.",1,2,security-tradeoff,cassandra,0,0
37,cross_node_timeout,"Enable operation timeout information exchange between nodes to accurately measure request timeouts. If disabled, replicas will assume that requests were forwarded to them instantly by the coordinator, which means that under overload conditions we will waste that much extra time processing already-timed-out requests.",1,4,limited-side-effect,cassandra,0,0
38,data_file_directories,Directories where Cassandra should store data on disk.,0,0,others,cassandra,0,0
39,disk_failure_policy,Policy for data disk failures,0,0,others,cassandra,0,0
40,dynamic_snitch_reset_interval_in_ms,"controls how often to reset all host scores, allowing a bad host to possibly recover",1,5,workload-specific,cassandra,0,0
41,dynamic_snitch_update_interval_in_ms,controls how often to perform the more expensive part of host score calculation,1,5,workload-specific,cassandra,0,0
42,enable_scripted_user_defined_functions,Enables scripted UDFs (JavaScript UDFs).,0,0,others,cassandra,0,0
43,enable_user_defined_functions,"If unset, all GC Pauses greater than gc_log_threshold_in_ms will log at INFO level. UDFs (user defined functions) are disabled by default.",1,6,function-tradeoff,cassandra,0,0
44,endpoint_snitch,Set this to a class that implements IEndpointSnitch.,0,0,others,cassandra,0,0
45,file_cache_size_in_mb,Maximum memory to use for sstable chunk cache and buffer pooling.,1,1,resource,cassandra,0,0
46,gc_log_threshold_in_ms,GC Pauses greater than 200 ms will be logged at INFO level This threshold can be adjusted to minimize logging if necessary,1,5,workload-specific,cassandra,0,0
47,gc_warn_threshold_in_ms,GC Pauses greater than gc_warn_threshold_in_ms will be logged at WARN level,1,5,workload-specific,cassandra,0,0
48,hinted_handoff_disabled_datacenters,"When hinted_handoff_enabled is true, a black list of data centers that will not perform hinted handoff",0,0,others,cassandra,0,0
49,hinted_handoff_enabled,"If a write is made and a replica node for the key is down (and hinted_handoff_enabled == true), Cassandra will write a hint.",1,6,function-tradeoff,cassandra,0,1
50,hinted_handoff_throttle_in_kb,"Maximum throttle in KBs per second, per delivery thread. This will be reduced proportionally to the number of nodes in the cluster.",1,1,resource,cassandra,0,0
51,hints_compression,"Compression to apply to the hint files. If omitted, hints files will be written uncompressed. LZ4, Snappy, and Deflate compressors are supported.",1,4,limited-side-effect,cassandra,0,0
52,hints_directory,"Directory where Cassandra should store hints. If not set, the default directory is $CASSANDRA_HOME/data/hints.",0,0,others,cassandra,0,0
53,hints_flush_period_in_ms,How often hints should be flushed from the internal buffers to disk. Will not trigger fsync.,1,3,reliability-tradeoff,cassandra,0,0
54,ideal_consistency_level,Track a metric per keyspace indicating whether replication achieved the ideal consistency level for writes without timing out. This is different from the consistency level requested by each write which may be lower in order to facilitate availability.,1,6,function-tradeoff,cassandra,0,0
55,incremental_backups,Set to true to have Cassandra create a hard link to each sstable flushed or streamed locally in a backups/ subdirectory of the keyspace data. Removing these links is the operator's responsibility.,0,0,others,cassandra,0,0
56,index_summary_capacity_in_mb,A fixed memory pool size in MB for for SSTable index summaries.,1,1,resource,cassandra,0,0
57,index_summary_resize_interval_in_minutes,How frequently index summaries should be resampled. This is done periodically to redistribute memory from the fixed-size pool to sstables proportional their recent read rates.,1,5,workload-specific,cassandra,0,1
58,initial_token,"initial_token allows you to specify tokens manually. While you can use it with vnodes (num_tokens > 1, above) in which case you should provide a comma-separated list it's primarily used when adding nodes to legacy clusters that do not have vnodes enabled.",0,0,others,cassandra,0,0
59,inter_dc_tcp_nodelay,"Enable or disable tcp_nodelay for inter-dc communication. Disabling it will result in larger (but fewer) network packets being sent, reducing overhead from the TCP protocol itself, at the cost of increasing latency if you block for cross-datacenter responses",1,4,limited-side-effect,cassandra,0,0
60,internode_authenticator,"Internode authentication backend, implementing IInternodeAuthenticator; used to allow/disallow connections from peer nodes.",0,0,others,cassandra,0,0
61,internode_compression,internode_compression controls whether traffic between nodes is compressed,1,4,limited-side-effect,cassandra,0,1
62,key_cache_keys_to_save,"Number of keys from the key cache to save. Disabled by default, meaning all keys are going to be saved",1,1,resource,cassandra,0,1
63,key_cache_save_period,"Duration in seconds after which Cassandra should save the key cache. Caches are saved to saved_caches_directory as specified in this configuration file. Saved caches greatly improve cold-start speeds, and is relatively cheap in terms of I/O for the key cache.",1,5,workload-specific,cassandra,0,0
64,key_cache_size_in_mb,"The key cache is fairly tiny for the amount of time it saves, so it's worthwhile to use it at large numbers. The row cache saves even more time, but must contain the entire row, so it is extremely space-intensive.",1,1,resource,cassandra,0,0
65,listen_address,Address or interface to bind to and tell other Cassandra nodes to connect to. You _must_ change this if you want multiple nodes to be able to communicate!,0,0,others,cassandra,0,0
66,listen_interface,"Set listen_address OR listen_interface, not both. Interfaces must correspond to a single address, IP aliasing is not supported.",0,0,others,cassandra,0,0
67,listen_interface_prefer_ipv6,If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address you can specify which should be chosen using listen_interface_prefer_ipv6.,0,0,others,cassandra,0,0
68,listen_on_broadcast_address,"When using multiple physical network interfaces, set this to true to listen on broadcast_address in addition to the listen_address, allowing nodes to communicate in both interfaces.",0,0,others,cassandra,0,0
69,max_hint_window_in_ms,"this defines the maximum amount of time a dead host will have hints generated. After it has been dead this long, new hints for it will not be created until it has been seen alive and gone down again.",0,0,others,cassandra,0,0
70,max_hints_delivery_threads,"Number of threads with which to deliver hints; Consider increasing this number when you have multi-dc deployments, since cross-dc handoff tends to be slower",1,5,workload-specific,cassandra,0,0
71,max_hints_file_size_in_mb,"Maximum size for a single hints file, in megabytes.",0,0,others,cassandra,0,0
72,max_value_size_in_mb,Maximum size of any value in SSTables. Safety measure to detect SSTable corruption early.,0,0,others,cassandra,0,0
73,memtable_allocation_type,Specify the way Cassandra allocates and manages memtable memory.,1,5,workload-specific,cassandra,0,0
74,memtable_cleanup_threshold,"Ratio of occupied non-flushing memtable size to total permitted size that will trigger a flush of the largest memtable. Larger mct will mean larger flushes and hence less compaction, but also less concurrent flush activity which can make it difficult to keep your disks fed under heavy write load.",1,5,workload-specific,cassandra,0,0
75,memtable_flush_writers,This sets the number of memtable flush writer threads per disk as well as the total number of memtables that can be flushed concurrently. These are generally a combination of compute and IO bound.,1,1,resource,cassandra,0,1
76,memtable_heap_space_in_mb,"Total permitted memory to use for memtables. Cassandra will stop accepting writes when the limit is exceeded until a flush completes, and will trigger a flush based on memtable_cleanup_threshold",1,3,reliability-tradeoff,cassandra,0,0
77,native_transport_max_concurrent_connections,The maximum number of concurrent client connections.,1,1,resource,cassandra,0,0
78,native_transport_max_concurrent_connections_per_ip,The maximum number of concurrent client connections per source ip.,1,1,resource,cassandra,0,0
79,native_transport_max_frame_size_in_mb,The maximum size of allowed frame. Frame (requests) larger than this will be rejected as invalid.,0,0,others,cassandra,0,0
80,native_transport_max_threads,The maximum threads for handling requests (note that idle threads are stopped after 30 seconds so there is not corresponding minimum setting).,1,1,resource,cassandra,0,0
81,native_transport_port,"port for the CQL native transport to listen for clients on For security reasons, you should not expose this port to the internet.",0,0,others,cassandra,0,1
82,native_transport_port_ssl,Setting native_transport_port_ssl to a different value from native_transport_port will use encryption for native_transport_port_ssl while keeping native_transport_port unencrypted.,1,2,security-tradeoff,cassandra,0,0
83,num_tokens,"This defines the number of tokens randomly assigned to this node on the ring The more tokens, relative to other nodes, the larger the proportion of data that this node will store.",0,0,others,cassandra,0,0
84,otc_backlog_expiration_interval_ms,How many milliseconds to wait between two expiration runs on the backlog (queue) of the OutboundTcpConnection.,0,0,others,cassandra,0,0
85,otc_coalescing_enough_coalesced_messages,Do not try to coalesce messages if we already got that many messages. This should be more than 2 and less than 128.,1,4,limited-side-effect,cassandra,0,0
86,otc_coalescing_window_us,How many microseconds to wait for coalescing. For fixed strategy this is the amount of time after the first message is received before it will be sent with any accompanying messages. For moving average this is the maximum amount of time that will be waited as well as the interval at which messages must arrive on average for coalescing to be enabled.,0,0,others,cassandra,0,0
87,partitioner,The partitioner is responsible for distributing groups of rows (by partition key) across nodes in the cluster. You should leave this alone for new clusters.,0,0,others,cassandra,0,0
88,permissions_update_interval_in_ms,"Refresh interval for permissions cache (if enabled). After this interval, cache entries become eligible for refresh. Upon next access, an async reload is scheduled and the old value returned until it completes.",0,0,others,cassandra,0,0
89,permissions_validity_in_ms,"Validity period for permissions cache (fetching permissions can be an expensive operation depending on the authorizer, CassandraAuthorizer is one example).",1,2,security-tradeoff,cassandra,0,0
90,prepared_statements_cache_size_mb,Maximum size of the native protocol prepared statement cache,1,1,resource,cassandra,0,0
91,range_request_timeout_in_ms,How long the coordinator should wait for seq or index scans to complete,0,0,others,cassandra,0,0
92,read_request_timeout_in_ms,How long the coordinator should wait for read operations to complete,0,0,others,cassandra,0,0
93,request_timeout_in_ms,"The default timeout for other, miscellaneous operations",0,0,others,cassandra,0,0
94,role_manager,"Part of the Authentication & Authorization backend, implementing IRoleManager; used to maintain grants and memberships between roles.",0,0,others,cassandra,0,0
95,roles_update_interval_in_ms,"Refresh interval for roles cache (if enabled). After this interval, cache entries become eligible for refresh. Upon next access, an async reload is scheduled and the old value returned until it completes.",1,2,security-tradeoff,cassandra,0,0
96,roles_validity_in_ms,"Validity period for roles cache (fetching granted roles can be an expensive operation depending on the role manager, CassandraRoleManager is one example) Granted roles are cached for authenticated sessions in AuthenticatedUser and after the period specified here, become eligible for (async) reload.",1,2,security-tradeoff,cassandra,0,0
97,row_cache_class_name,Row cache implementation class name.,0,0,others,cassandra,0,1
98,row_cache_keys_to_save,"Number of keys from the row cache to save. Specify 0 (which is the default), meaning all keys are going to be saved",1,1,resource,cassandra,0,0
99,row_cache_save_period,"Duration in seconds after which Cassandra should save the row cache. Saved caches greatly improve cold-start speeds, and is relatively cheap in terms of I/O for the key cache. Row cache saving is much more expensive and has limited use.",1,5,workload-specific,cassandra,0,0
100,row_cache_size_in_mb,Maximum size of the row cache in memory.,1,1,resource,cassandra,0,0
101,rpc_address,The address or interface to bind the native transport server to.,0,0,others,cassandra,0,1
102,rpc_interface,"Set rpc_address OR rpc_interface, not both. Interfaces must correspond to a single address, IP aliasing is not supported.",0,0,others,cassandra,0,1
103,rpc_interface_prefer_ipv6,If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address you can specify which should be chosen using rpc_interface_prefer_ipv6.,0,0,others,cassandra,0,1
104,rpc_keepalive,enable or disable keepalive on rpc/native connections,0,0,others,cassandra,0,0
105,saved_caches_directory,"saved caches If not set, the default directory is $CASSANDRA_HOME/data/saved_caches.",0,0,others,cassandra,0,0
106,slow_query_log_timeout_in_ms,How long before a node logs slow queries.,0,0,others,cassandra,0,0
107,snapshot_before_compaction,"Whether or not to take a snapshot before each compaction. Be careful using this option, since Cassandra won't clean up the snapshots for you. Mostly useful if you're paranoid when there is a data format change.",1,3,reliability-tradeoff,cassandra,0,1
108,ssl_storage_port,"SSL port, for encrypted communication.",0,0,others,cassandra,0,0
109,sstable_preemptive_open_interval_in_mb,"When compacting, the replacement sstable(s) can be opened before they are completely written, and used in place of the prior sstables for any range that has been written. This helps to smoothly transfer reads between the sstables, reducing page cache churn and keeping hot rows hot",1,1,resource,cassandra,0,1
110,start_native_transport,Whether to start the native transport server.,0,0,others,cassandra,0,1
111,storage_port,"TCP port, for commands and data For security reasons, you should not expose this port to the internet. Firewall it if needed.",0,0,others,cassandra,0,0
112,tracetype_query_ttl,TTL for different trace types used during logging of the repair process.,0,0,others,cassandra,0,1
113,transparent_data_encryption_options,Enables encrypting data at-rest (on disk).,1,2,security-tradeoff,cassandra,0,0
114,trickle_fsync,"Whether to, when doing sequential writing, fsync() at intervals in order to force the operating system to flush the dirty buffers. Enable this to avoid sudden dirty buffer flushing from impacting read latencies.",1,3,reliability-tradeoff,cassandra,0,1
115,truncate_request_timeout_in_ms,How long the coordinator should wait for truncates to complete,0,0,others,cassandra,0,0
116,write_request_timeout_in_ms,How long the coordinator should wait for writes to complete,0,0,others,cassandra,0,0
117,--analyze,Run the static analyzer,1,6,function-tradeoff,clang,0,0
118,--analyzer-output,Static analyzer report output format (html|plist|plist-multi-file|plist-html|sarif|text).,0,0,others,clang,0,0
119,-arcmt-migrate-emit-errors,Emit ARC errors even if the migrator can fix them,1,6,function-tradeoff,clang,0,1
120,-arcmt-migrate-report-output,Output path for the plist report,1,6,function-tradeoff,clang,0,0
122,-C(captal),Include comments in preprocessed output,1,6,function-tradeoff,clang,0,0
123,-c,"Only run preprocess, compile, and assemble steps",1,6,function-tradeoff,clang,0,0
124,-CC,Include comments from within macros in preprocessed output,1,6,function-tradeoff,clang,0,0
125,-cl-denorms-are-zero,OpenCL only. Allow denormals to be flushed to zero.,1,4,limited-side-effect,clang,0,1
126,-cl-fast-relaxed-math,"OpenCL only. Sets -cl-finite-math-only and -cl-unsafe-math-optimizations, and defines __FAST_RELAXED_MATH__.",1,4,limited-side-effect,clang,0,0
127,-cl-finite-math-only,OpenCL only. Allow floating-point optimizations that assume arguments and results are not NaNs or +-Inf.,1,4,limited-side-effect,clang,0,0
128,-cl-fp32-correctly-rounded-divide-sqrt,OpenCL only. Specify that single precision floating-point divide and sqrt used in the program source are correctly rounded.,0,0,others,clang,0,0
129,-cl-kernel-arg-info,OpenCL only. Generate kernel argument metadata.,0,0,others,clang,0,0
130,-cl-mad-enable,OpenCL only. Allow use of less precise MAD computations in the generated binary.,0,0,others,clang,0,0
131,-cl-no-signed-zeros,OpenCL only. Allow use of less precise no signed zeros computations in the generated binary.,0,0,others,clang,0,0
132,-cl-opt-disable,OpenCL only. This option disables all optimizations. By default optimizations are enabled.,1,4,limited-side-effect,clang,0,0
133,-cl-single-precision-constant,OpenCL only. Treat double precision floating-point constant as single precision constant.,0,0,others,clang,0,0
134,-cl-std=value,OpenCL language standard to compile for.,0,0,others,clang,0,0
135,-cl-strict-aliasing,OpenCL only. This option is added for compatibility with OpenCL 1.0.,0,0,others,clang,0,0
136,-cl-uniform-work-group-size,OpenCL only. Defines that the global work-size be a multiple of the work-group size specified to clEnqueueNDRangeKernel,0,0,others,clang,0,0
137,-cl-unsafe-math-optimizations,OpenCL only. Allow unsafe floating-point optimizations. Also implies -cl-no-signed-zeros and -cl-mad-enable.,1,4,limited-side-effect,clang,0,1
140,--cuda-device-only,Compile CUDA code for device only,0,0,others,clang,0,0
141,--cuda-gpu-arch,CUDA GPU architecture (e.g. sm_35). May be specified more than once.,0,0,others,clang,0,0
143,--cuda-include-ptx,Include PTX for the following GPU architecture (e.g. sm_35) or 'all'. May be specified more than once.,0,0,others,clang,0,1
144,--cuda-noopt-device-debug,Enable device-side debug info generation. Disables ptxas optimizations.,1,6,function-tradeoff,clang,0,1
146,--cuda-path-ignore-env,Ignore environment variables to detect CUDA installation,0,0,others,clang,0,0
148,-D,Define <macro> to <value> (or 1 if <value> omitted),0,0,others,clang,0,0
149,-dD,Print macro definitions in -E mode in addition to normal output,1,6,function-tradeoff,clang,0,0
150,-dependency-dot,Filename to write DOT-formatted header dependencies to,0,0,others,clang,0,0
151,-dependency-file,Filename (or -) to write dependency output to,0,0,others,clang,0,1
152,-dI,Print include directives in -E mode in addition to normal output,1,6,function-tradeoff,clang,0,0
153,-dM,Print macro definitions in -E mode instead of normal output,1,6,function-tradeoff,clang,0,0
154,-E,Only run the preprocessor,0,0,others,clang,0,0
155,-emit-ast,Emit Clang AST files for source inputs,1,6,function-tradeoff,clang,0,0
156,-emit-interface-stubs,Generate Inteface Stub Files.,1,6,function-tradeoff,clang,0,1
157,-emit-llvm,Use the LLVM representation for assembler and object files,1,6,function-tradeoff,clang,0,0
158,-emit-merged-ifs,"Generate Interface Stub Files, emit merged text not binary.",1,6,function-tradeoff,clang,0,0
159,-enable-trivial-auto-var-init-zero-knowing-it-will-be-removed-from-clang,"Trivial automatic variable initialization to zero is only here for benchmarks, it'll eventually be removed, and I'm OK with that because I'm only using it to benchmark",1,6,function-tradeoff,clang,0,0
161,-faddrsig,Emit an address-significance table,1,6,function-tradeoff,clang,0,0
162,-faligned-allocation,Enable C++17 aligned allocation functions,0,0,others,clang,0,0
163,-fallow-editor-placeholders,Treat editor placeholders as valid source code,0,0,others,clang,0,0
164,-fansi-escape-codes,Use ANSI escape codes for diagnostics,0,0,others,clang,0,1
165,-fapple-kext,Use Apple's kernel extensions ABI,0,0,others,clang,0,0
166,-fapple-link-rtlib,Force linking the clang builtins runtime library,0,0,others,clang,0,0
167,-fapple-pragma-pack,Enable Apple gcc-compatible #pragma pack handling,0,0,others,clang,0,0
168,-fapplication-extension,Restrict code to those available for App Extensions,0,0,others,clang,0,0
169,-fblocks,Enable the 'blocks' language feature,0,0,others,clang,0,0
170,-fborland-extensions,Accept non-standard constructs supported by the Borland compiler,0,0,others,clang,0,0
172,-fbuild-session-timestamp=value ,Time when the current build session started,0,0,others,clang,0,0
174,-fc++-static-destructors,Enable C++ static destructor registration (the default),0,0,others,clang,0,0
175,-fcall-saved-x10,Make the x10 register call-saved (AArch64 only),0,0,others,clang,0,0
176,-fcall-saved-x11,Make the x11 register call-saved (AArch64 only),0,0,others,clang,0,0
177,-fcall-saved-x12,Make the x12 register call-saved (AArch64 only),0,0,others,clang,0,0
178,-fcall-saved-x13,Make the x13 register call-saved (AArch64 only),0,0,others,clang,0,0
179,-fcall-saved-x14,Make the x14 register call-saved (AArch64 only),0,0,others,clang,0,0
180,-fcall-saved-x15,Make the x15 register call-saved (AArch64 only),0,0,others,clang,0,0
181,-fcall-saved-x18,Make the x18 register call-saved (AArch64 only),0,0,others,clang,0,0
182,-fcall-saved-x8,Make the x8 register call-saved (AArch64 only),0,0,others,clang,0,0
183,-fcall-saved-x9,Make the x9 register call-saved (AArch64 only),0,0,others,clang,0,0
184,-fcf-protection,"Instrument control-flow architecture protection. Options: return, branch, full, none.",1,2,security-tradeoff,clang,0,0
185,-fchar8_t,Enable C++ builtin type char8_t,0,0,others,clang,0,0
186,-fclang-abi-compat,Attempt to match the ABI of Clang <version>,0,0,others,clang,0,0
187,-fcolor-diagnostics,Use colors in diagnostics,0,0,others,clang,0,0
188,-fcomment-block-commands,Treat each comma separated argument in <arg> as a documentation comment block command,0,0,others,clang,0,0
189,-fcomplete-member-pointers,Require member pointer base types to be complete if they would be significant under the Microsoft ABI,0,0,others,clang,0,0
190,-fconvergent-functions,Assume functions may be convergent,0,0,others,clang,0,0
191,-fcoroutines-ts,Enable support for the C++ Coroutines TS,0,0,others,clang,0,0
192,-fcoverage-mapping,Generate coverage mapping to enable code coverage analysis,0,0,others,clang,0,0
193,-fcs-profile-generate,Generate instrumented code to collect context sensitive execution counts into default.profraw (overridden by LLVM_PROFILE_FILE env var),0,0,others,clang,0,0
194,-fcs-profile-generate=,Generate instrumented code to collect context sensitive execution counts into /default.profraw (overridden by LLVM_PROFILE_FILE env var),0,0,others,clang,0,1
195,-fcuda-approx-transcendentals,Use approximate transcendental functions,0,0,others,clang,0,1
196,-fcuda-flush-denormals-to-zero,Flush denormal floating point values to zero in CUDA device mode.,0,0,others,clang,0,0
198,-fcxx-exceptions,Enable C++ exceptions,1,3,reliability-tradeoff,clang,0,1
199,-fdata-sections,Place each data in its own section (ELF Only),0,0,others,clang,0,1
201,-fdebug-default-version,"Default DWARF version to use, if a -g option caused DWARF debug info to be produced",0,0,others,clang,0,0
202,-fdebug-info-for-profiling,Emit extra debug info to make sample profile more accurate.,1,6,function-tradeoff,clang,0,0
203,-fdebug-macro,Emit macro debug information,1,6,function-tradeoff,clang,0,0
206,-fdebug-types-section,Place debug types in their own section (ELF Only),0,0,others,clang,0,0
207,-fdeclspec,Allow __declspec as a keyword,0,0,others,clang,0,0
208,-fdelayed-template-parsing,Parse templated function definitions at the end of the translation unit,0,0,others,clang,0,0
209,-fdelete-null-pointer-checks,Treat usage of null pointers as undefined behavior.,0,0,others,clang,0,0
210,-fdiagnostics-absolute-paths,Print absolute paths in diagnostics,1,6,function-tradeoff,clang,0,0
211,-fdiagnostics-hotness-threshold,Prevent optimization remarks from being output if they do not have at least this profile count,1,6,function-tradeoff,clang,0,0
212,-fdiagnostics-parseable-fixits,Print fix-its in machine parseable form,1,6,function-tradeoff,clang,0,0
213,-fdiagnostics-print-source-range-info,Print source range spans in numeric form,1,6,function-tradeoff,clang,0,0
214,-fdiagnostics-show-hotness,Enable profile hotness information in diagnostic line,1,6,function-tradeoff,clang,0,0
215,-fdiagnostics-show-note-include-stack,Display include stacks for diagnostic notes,0,0,others,clang,0,0
216,-fdiagnostics-show-option,Print option name with mappable diagnostics,0,0,others,clang,0,0
217,-fdiagnostics-show-template-tree,Print a template comparison tree for differing templates,1,6,function-tradeoff,clang,0,0
218,-fdigraphs,"Enable alternative token representations '<:',':>','<%','%>','%:','%:%:' (default)",0,0,others,clang,0,1
219,-fdiscard-value-names,Discard value names in LLVM IR,0,0,others,clang,0,0
220,-fdollars-in-identifiers,Allow '$' in identifiers,0,0,others,clang,0,0
221,-fdouble-square-bracket-attributes,Enable '[[]]' attributes in all C and C++ language modes,0,0,others,clang,0,0
222,-fdwarf-exceptions,Use DWARF style exceptions,0,0,others,clang,0,1
223,-fembed-bitcode,Embed LLVM IR bitcode as data,0,0,others,clang,0,0
224,-fembed-bitcode-marker,Embed placeholder LLVM IR data as a marker,0,0,others,clang,0,0
225,-femit-all-decls,"Emit all declarations, even if unused",1,6,function-tradeoff,clang,0,0
226,-femulated-tls,Use emutls functions to access thread_local variables,0,0,others,clang,0,0
227,-fexceptions,Enable support for exception handling,0,0,others,clang,0,0
228,-fexperimental-isel,Enables the experimental global instruction selector,0,0,others,clang,0,1
229,-fexperimental-new-constant-interpreter,Enable the experimental new constant interpreter,0,0,others,clang,0,0
230,-fexperimental-new-pass-manager,Enables an experimental new pass manager in LLVM.,0,0,others,clang,0,0
231,-ffast-math,"Allow aggressive, lossy floating-point optimizations",1,4,limited-side-effect,clang,0,0
233,-ffine-grained-bitfield-accesses,Use separate accesses for consecutive bitfield runs with legal widths and alignments.,0,0,others,clang,0,0
234,-ffixed-point,Enable fixed point types,0,0,others,clang,0,0
235,-ffixed-r19,Reserve register r19 (Hexagon only),0,0,others,clang,0,0
236,-ffixed-r9,Reserve the r9 register (ARM only),0,0,others,clang,0,1
237,-ffixed-x1,Reserve the 1 register (AArch64/RISC-V only),0,0,others,clang,0,0
238,-ffixed-x10,Reserve the 10 register (AArch64/RISC-V only),0,0,others,clang,0,0
239,-ffixed-x11,Reserve the 11 register (AArch64/RISC-V only),0,0,others,clang,0,0
240,-ffixed-x12,Reserve the 12 register (AArch64/RISC-V only),0,0,others,clang,0,0
241,-ffixed-x13,Reserve the 13 register (AArch64/RISC-V only),0,0,others,clang,0,0
242,-ffixed-x14,Reserve the 14 register (AArch64/RISC-V only),0,0,others,clang,0,0
243,-ffixed-x15,Reserve the 15 register (AArch64/RISC-V only),0,0,others,clang,0,0
244,-ffixed-x16,Reserve the 16 register (AArch64/RISC-V only),0,0,others,clang,0,0
245,-ffixed-x17,Reserve the 17 register (AArch64/RISC-V only),0,0,others,clang,0,0
246,-ffixed-x18,Reserve the 18 register (AArch64/RISC-V only),0,0,others,clang,0,0
247,-ffixed-x19,Reserve the 19 register (AArch64/RISC-V only),0,0,others,clang,0,0
248,-ffixed-x2,Reserve the 2 register (AArch64/RISC-V only),0,0,others,clang,0,0
249,-ffixed-x20,Reserve the 20 register (AArch64/RISC-V only),0,0,others,clang,0,0
250,-ffixed-x21,Reserve the 21 register (AArch64/RISC-V only),0,0,others,clang,0,0
251,-ffixed-x22,Reserve the 22 register (AArch64/RISC-V only),0,0,others,clang,0,0
252,-ffixed-x23,Reserve the 23 register (AArch64/RISC-V only),0,0,others,clang,0,0
253,-ffixed-x24,Reserve the 24 register (AArch64/RISC-V only),0,0,others,clang,0,0
254,-ffixed-x25,Reserve the 25 register (AArch64/RISC-V only),0,0,others,clang,0,0
255,-ffixed-x26,Reserve the 26 register (AArch64/RISC-V only),0,0,others,clang,0,0
256,-ffixed-x27,Reserve the 27 register (AArch64/RISC-V only),0,0,others,clang,0,1
257,-ffixed-x28,Reserve the 28 register (AArch64/RISC-V only),0,0,others,clang,0,0
258,-ffixed-x29,Reserve the 29 register (AArch64/RISC-V only),0,0,others,clang,0,0
259,-ffixed-x3,Reserve the 3 register (AArch64/RISC-V only),0,0,others,clang,0,0
260,-ffixed-x30,Reserve the 30 register (AArch64/RISC-V only),0,0,others,clang,0,0
261,-ffixed-x31,Reserve the 31 register (AArch64/RISC-V only),0,0,others,clang,0,0
262,-ffixed-x4,Reserve the 4 register (AArch64/RISC-V only),0,0,others,clang,0,0
263,-ffixed-x5,Reserve the 5 register (AArch64/RISC-V only),0,0,others,clang,0,0
264,-ffixed-x6,Reserve the 6 register (AArch64/RISC-V only),0,0,others,clang,0,0
265,-ffixed-x7,Reserve the 7 register (AArch64/RISC-V only),0,0,others,clang,0,0
266,-ffixed-x8,Reserve the 8 register (AArch64/RISC-V only),0,0,others,clang,0,0
267,-ffixed-x9,Reserve the 9 register (AArch64/RISC-V only),0,0,others,clang,0,0
268,-fforce-dwarf-frame,Always emit a debug frame section,1,6,function-tradeoff,clang,0,1
269,-fforce-emit-vtables,Emits more virtual tables to improve devirtualization,1,4,limited-side-effect,clang,0,1
270,-fforce-enable-int128,Enable support for int128_t type,0,0,others,clang,0,0
271,-ffp-contract,Form fused FP ops (e.g. FMAs): fast (everywhere) | on (according to FP_CONTRACT pragma) | off (never fuse). Default is 'fast' for CUDA/HIP and 'on' otherwise.,1,4,limited-side-effect,clang,0,0
272,-ffp-exception-behavior,Specifies the exception behavior of floating-point operations.,0,0,others,clang,0,1
273,-ffp-model,Controls the semantics of floating-point calculations.,0,0,others,clang,0,0
274,-ffreestanding,Assert that the compilation takes place in a freestanding environment,0,0,others,clang,0,0
275,-ffunction-sections,Place each function in its own section (ELF Only),0,0,others,clang,0,1
276,-fgnu89-inline,Use the gnu89 inline semantics,0,0,others,clang,0,0
277,-fgnuc-version,Sets various macros to claim compatibility with the given GCC version (default is 4.2.1),0,0,others,clang,0,0
278,-fgnu-keywords,Allow GNU-extension keywords regardless of language standard,0,0,others,clang,0,0
279,-fgnu-runtime,Generate output compatible with the standard GNU Objective-C runtime,1,6,function-tradeoff,clang,0,0
280,-fgpu-allow-device-init,Allow device side init function in HIP,0,0,others,clang,0,0
281,-fgpu-rdc,"Generate relocatable device code, also known as separate compilation mode.",0,0,others,clang,0,1
282,-fhip-new-launch-api,Use new kernel launching API for HIP.,0,0,others,clang,0,0
284,-finline-functions,Inline suitable functions,0,0,others,clang,0,0
285,-finline-hint-functions,Inline functions which are (explicitly or implicitly) marked inline,0,0,others,clang,0,0
286,-finstrument-function-entry-bare,"Instrument function entry only, after inlining, without arguments to the instrumentation call",0,0,others,clang,0,0
287,-finstrument-functions,Generate calls to instrument function entry and exit,0,0,others,clang,0,0
288,-finstrument-functions-after-inlining,"Like -finstrument-functions, but insert the calls after inlining",0,0,others,clang,0,1
289,-fintegrated-as,Enable the integrated assembler,0,0,others,clang,0,0
290,-fintegrated-cc1,Run cc1 in-process,0,0,others,clang,0,0
291,-fkeep-static-consts,Keep static const variables even if unused,0,0,others,clang,0,0
292,-flax-vector-conversions,Enable implicit vector bit-casts,0,0,others,clang,0,0
293,-flto,Enable LTO in 'full' mode,0,0,others,clang,0,0
294,-flto-jobs,Controls the backend parallelism of -flto=thin (default of 0 means the number of threads will be derived from the number of CPUs detected),1,1,resource,clang,0,0
296,-fmath-errno,Require math functions to indicate errors by setting errno,0,0,others,clang,0,0
297,-fmax-type-align,Specify the maximum alignment to enforce on pointers lacking an explicit alignment,0,0,others,clang,0,0
298,-fmerge-all-constants,Allow merging of constants,0,0,others,clang,0,0
302,-fmodules,Enable the 'modules' language feature,0,0,others,clang,0,0
303,-fmodules-cache-path,Specify the module cache path,0,0,others,clang,0,0
304,-fmodules-decluse,Require declaration of modules used within a module,0,0,others,clang,0,0
305,-fmodules-disable-diagnostic-validation,Disable validation of the diagnostic options when loading the module,0,0,others,clang,0,0
306,-fmodules-ignore-macro,Ignore the definition of the given macro when building and loading modules,0,0,others,clang,0,1
308,-fmodules-prune-interval,Specify the interval (in seconds) between attempts to prune the module cache,0,0,others,clang,0,0
309,-fmodules-search-all,Search even non-imported modules to resolve references,0,0,others,clang,0,1
310,-fmodules-strict-decluse,Like -fmodules-decluse but requires all headers to be in modules,0,0,others,clang,0,0
311,-fmodules-ts,Enable support for the C++ Modules TS,0,0,others,clang,0,1
313,-fmodules-validate-input-files-content,Validate PCM input files based on content if mtime differs,0,0,others,clang,0,0
314,-fmodules-validate-once-per-build-session,Don't verify input files for the modules if the module has been successfully validated or loaded during this build session,1,4,limited-side-effect,clang,0,1
315,-fmodules-validate-system-headers,Validate the system headers that a module depends on when loading the module,0,0,others,clang,0,0
316,-fms-compatibility,Enable full Microsoft Visual C++ compatibility,0,0,others,clang,0,0
317,-fms-compatibility-version,Dot-separated value representing the Microsoft compiler version number to report in _MSC_VER (0 = don't define it (default)),0,0,others,clang,0,0
318,-fmsc-version,Microsoft compiler version number to report in _MSC_VER (0 = don't define it (default)),0,0,others,clang,0,1
319,-fms-extensions,Accept some non-standard constructs supported by the Microsoft compiler,0,0,others,clang,0,0
320,-fnew-alignment,Specifies the largest alignment guaranteed by '::operator new(size_t)',0,0,others,clang,0,0
321,-fno-access-control,Disable C++ access control,0,0,others,clang,0,0
322,-fno-addrsig,Don't emit an address-significance table,0,0,others,clang,0,0
323,-fno-assume-sane-operator-new,Don't assume that C++'s global operator new can't alias any pointer,0,0,others,clang,0,0
324,-fno-autolink,Disable generation of linker directives for automatic library linking,0,0,others,clang,0,0
325,-fno-builtin,Disable implicit builtin knowledge of functions,0,0,others,clang,0,0
326,-fno-builtin-value,Disable implicit builtin knowledge of a specific function,0,0,others,clang,0,0
327,-fno-c++-static-destructors,Disable C++ static destructor registration,0,0,others,clang,0,0
328,-fno-char8_t,Disable C++ builtin type char8_t,0,0,others,clang,0,0
329,-fno-common,Compile common globals like normal definitions,0,0,others,clang,0,0
330,-fno-complete-member-pointers,Do not require member pointer base types to be complete if they would be significant under the Microsoft ABI,0,0,others,clang,0,0
331,-fno-constant-cfstrings,Disable creation of CodeFoundation-type constant strings,0,0,others,clang,0,0
332,-fno-coverage-mapping,Disable code coverage analysis,0,0,others,clang,0,0
333,-fno-crash-diagnostics,Disable auto-generation of preprocessed source files and a script for reproduction during a clang crash,0,0,others,clang,0,0
334,-fno-debug-info-for-profiling,Do not emit extra debug info for sample profiler.,1,6,function-tradeoff,clang,0,0
335,-fno-debug-macro,Do not emit macro debug information,1,6,function-tradeoff,clang,0,0
336,-fno-declspec,Disallow __declspec as a keyword,0,0,others,clang,0,0
337,-fno-delayed-template-parsing,Disable delayed template parsing,0,0,others,clang,0,0
338,-fno-delete-null-pointer-checks,Do not treat usage of null pointers as undefined behavior.,0,0,others,clang,0,1
339,-fno-diagnostics-fixit-info,Do not include fixit information in diagnostics,1,6,function-tradeoff,clang,0,0
340,-fno-digraphs,"Disallow alternative token representations '<:',':>','<%','%>','%:','%:%:'",0,0,others,clang,0,0
341,-fno-discard-value-names,Do not discard value names in LLVM IR,0,0,others,clang,0,1
342,-fno-dollars-in-identifiers,Disallow '$' in identifiers,0,0,others,clang,0,0
343,-fno-double-square-bracket-attributes,Disable '[[]]' attributes in all C and C++ language modes,0,0,others,clang,0,0
344,-fno-elide-constructors,Disable C++ copy constructor elision,0,0,others,clang,0,1
345,-fno-elide-type,Do not elide types when printing diagnostics,0,0,others,clang,0,0
346,-fno-experimental-isel,Disables the experimental global instruction selector,0,0,others,clang,0,0
347,-fno-experimental-new-pass-manager,Disables an experimental new pass manager in LLVM.,0,0,others,clang,0,1
348,-fno-fine-grained-bitfield-accesses,Use large-integer access for consecutive bitfield runs.,0,0,others,clang,0,0
349,-fno-fixed-point,Disable fixed point types,0,0,others,clang,0,0
350,-fno-force-dwarf-frame,Don't always emit a debug frame section,1,6,function-tradeoff,clang,0,0
351,-fno-force-enable-int128,Disable support for int128_t type,0,0,others,clang,0,0
352,-fno-gnu-inline-asm,Disable GNU style inline asm,0,0,others,clang,0,0
353,-fno-integrated-as,Disable the integrated assembler,0,0,others,clang,0,0
354,-fno-integrated-cc1,Spawn a separate process for each cc1,0,0,others,clang,0,1
355,-fno-jump-tables,Do not use jump tables for lowering switches,0,0,others,clang,0,1
356,-fno-lto,Disable LTO mode (default),0,0,others,clang,0,0
357,-fno-merge-all-constants,Disallow merging of constants,0,0,others,clang,0,0
358,-fno-objc-infer-related-result-type,do not infer Objective-C related result type based on method family,1,6,function-tradeoff,clang,0,0
360,-fno-plt,Do not use the PLT to make function calls,0,0,others,clang,0,0
361,-fno-preserve-as-comments,Do not preserve comments in inline assembly,0,0,others,clang,0,0
362,-fno-profile-generate,Disable generation of profile instrumentation.,0,0,others,clang,0,0
363,-fno-profile-instr-generate,Disable generation of profile instrumentation.,0,0,others,clang,0,0
364,-fno-profile-instr-use,Disable using instrumentation data for profile-guided optimization,0,0,others,clang,0,0
365,-fno-register-global-dtors-with-atexit,Don't use atexit or __cxa_atexit to register global destructors,0,0,others,clang,0,0
366,-fno-reroll-loops,Turn off loop reroller,0,0,others,clang,0,0
368,-fno-rtti,Disable generation of rtti information,0,0,others,clang,0,0
369,-fno-rtti-data,Control emission of RTTI data,0,0,others,clang,0,0
370,-fno-sanitize-address-poison-custom-array-cookie,Disable poisoning array cookies when using custom operator new[] in AddressSanitizer,0,0,others,clang,0,1
371,-fno-sanitize-address-use-after-scope,Disable use-after-scope detection in AddressSanitizer,0,0,others,clang,0,0
372,-fno-sanitize-address-use-odr-indicator,Disable ODR indicator globals,0,0,others,clang,0,0
374,-fno-sanitize-cfi-canonical-jump-tables,Do not make the jump table addresses canonical in the symbol table,0,0,others,clang,0,0
375,-fno-sanitize-cfi-cross-dso,Disable control flow integrity (CFI) checks for cross-DSO calls.,1,2,security-tradeoff,clang,0,1
376,-fno-sanitize-coverage,Disable specified features of coverage instrumentation for Sanitizers,0,0,others,clang,0,0
377,-fno-sanitize-memory-track-origins,Disable origins tracking in MemorySanitizer,0,0,others,clang,0,0
378,-fno-sanitize-memory-use-after-dtor,Disable use-after-destroy detection in MemorySanitizer,0,0,others,clang,0,0
379,-fno-sanitize-recover,Disable recovery for specified sanitizers,0,0,others,clang,0,0
380,-fno-sanitize-stats,Disable sanitizer statistics gathering.,1,6,function-tradeoff,clang,0,0
381,-fno-sanitize-thread-atomics,Disable atomic operations instrumentation in ThreadSanitizer,0,0,others,clang,0,0
382,-fno-sanitize-thread-func-entry-exit,Disable function entry/exit instrumentation in ThreadSanitizer,0,0,others,clang,0,0
383,-fno-sanitize-thread-memory-access,Disable memory access instrumentation in ThreadSanitizer,0,0,others,clang,0,0
384,-fno-sanitize-trap,Disable trapping for specified sanitizers,0,0,others,clang,0,0
385,-fno-short-wchar,Force wchar_t to be an unsigned int,0,0,others,clang,0,0
386,-fno-show-column,Do not include column number on diagnostics,0,0,others,clang,0,0
388,-fno-signed-char,Char is unsigned,0,0,others,clang,0,0
389,-fno-signed-zeros,Allow optimizations that ignore the sign of floating point zeros,1,4,limited-side-effect,clang,0,0
390,-fno-spell-checking,Disable spell-checking,1,6,function-tradeoff,clang,0,0
391,-fno-stack-protector,Disable the use of stack protectors,1,2,security-tradeoff,clang,0,0
392,-fno-stack-size-section,Don't emit section containing metadata on function stack sizes,1,6,function-tradeoff,clang,0,0
393,-fno-standalone-debug,Limit debug information produced to reduce size of debug binary,1,6,function-tradeoff,clang,0,0
394,-fno-strict-float-cast-overflow,Relax language rules and try to match the behavior of the target's native float-to-int conversion instructions,0,0,others,clang,0,0
395,-fno-temp-file,Directly create compilation output files. This may lead to incorrect incremental builds if the compiler crashes,0,0,others,clang,0,0
396,-fno-threadsafe-statics,Do not emit code to make initialization of local statics thread safe,1,6,function-tradeoff,clang,0,0
397,-fno-trigraphs,Do not process trigraph sequences,0,0,others,clang,0,0
398,-fno-unroll-loops,Turn off loop unroller,1,4,limited-side-effect,clang,0,0
399,-fno-use-cxa-atexit,Don't use __cxa_atexit for calling destructors,0,0,others,clang,0,0
400,-fno-use-init-array,Don't use .init_array instead of .ctors,0,0,others,clang,0,0
401,-fobjc-arc,Synthesize retain and release calls for Objective-C pointers,0,0,others,clang,0,0
402,-fobjc-arc-exceptions,Use EH-safe code when synthesizing retains and releases in -fobjc-arc,0,0,others,clang,0,0
403,-fobjc-exceptions,Enable Objective-C exceptions,0,0,others,clang,0,0
404,-fobjc-runtime,Specify the target Objective-C runtime kind and version,0,0,others,clang,0,0
405,-fobjc-weak,Enable ARC-style weak references in Objective-C,0,0,others,clang,0,0
406,-fopenmp,Parse OpenMP pragmas and generate parallel code.,0,0,others,clang,0,0
407,-fopenmp-simd,Emit OpenMP code only for SIMD-based constructs.,1,6,function-tradeoff,clang,0,1
408,-fopenmp-targets,Specify comma-separated list of triples OpenMP offloading targets to be supported,0,0,others,clang,0,0
409,-foptimization-record-file,"Specify the output name of the file containing the optimization remarks. Implies -fsave-optimization-record. On Darwin platforms, this cannot be used with multiple -arch <arch> options.",0,0,others,clang,0,0
410,-foptimization-record-passes,"Only include passes which match a specified regular expression in the generated optimization record (by default, include all passes)",0,0,others,clang,0,0
412,-fpack-struct,Specify the default maximum struct packing alignment,0,0,others,clang,0,0
413,-fpascal-strings,Recognize and construct Pascal-style string literals,0,0,others,clang,0,1
415,-fpatchable-function-entry,Generate M NOPs before function entry and N-M NOPs after function entry,0,0,others,clang,0,0
416,-fpcc-struct-return,Override the default ABI to return all structs on the stack,0,0,others,clang,0,0
417,-fpch-validate-input-files-content,Validate PCH input files based on content if mtime differs,0,0,others,clang,0,1
418,-fplt,Use the PLT to make function calls,0,0,others,clang,0,0
419,-fplugin,Load the named plugin (dynamic shared object),0,0,others,clang,0,0
421,-fprofile-exclude-files,Instrument only functions from files where names don't match all the regexes separated by a semi-colon,0,0,others,clang,0,0
422,-fprofile-filter-files,Instrument only functions from files where names match any regex separated by a semi-colon,0,0,others,clang,0,0
423,-fprofile-generate,Generate instrumented code to collect execution counts into default.profraw (overridden by LLVM_PROFILE_FILE env var),0,0,others,clang,0,0
427,-fprofile-instr-use,Use instrumentation data for profile-guided optimization,0,0,others,clang,0,1
429,-fprofile-sample-accurate,Specifies that the sample profile is accurate,0,0,others,clang,0,0
430,-fprofile-sample-use,Enable sample-based profile guided optimizations,1,4,limited-side-effect,clang,0,0
431,-fprofile-use,"Use instrumentation data for profile-guided optimization. If pathname is a directory, it reads from <pathname>/default.profdata. Otherwise, it reads from file <pathname>.",0,0,others,clang,0,0
432,-freciprocal-math,Allow division operations to be reassociated,0,0,others,clang,0,0
433,-fregister-global-dtors-with-atexit,Use atexit or __cxa_atexit to register global destructors,0,0,others,clang,0,0
434,-freg-struct-return,Override the default ABI to return small structs in registers,0,0,others,clang,0,0
435,-frelaxed-template-template-args,Enable C++17 relaxed template template argument matching,0,0,others,clang,0,1
436,-freroll-loops,Turn on loop reroller,0,0,others,clang,0,0
438,-fsanitize,Turn on runtime checks for various forms of undefined or suspicious behavior. See user manual for available checks,1,2,security-tradeoff,clang,0,0
439,-fsanitize-address-field-padding,Level of field padding for AddressSanitizer,0,0,others,clang,0,1
440,-fsanitize-address-globals-dead-stripping,Enable linker dead stripping of globals in AddressSanitizer,0,0,others,clang,0,0
441,-fsanitize-address-poison-custom-array-cookie,Enable poisoning array cookies when using custom operator new[] in AddressSanitizer,0,0,others,clang,0,0
442,-fsanitize-address-use-after-scope,Enable use-after-scope detection in AddressSanitizer,0,0,others,clang,0,0
443,-fsanitize-address-use-odr-indicator,Enable ODR indicator globals to avoid false ODR violation reports in partially sanitized programs at the cost of an increase in binary size,0,0,others,clang,0,0
445,-fsanitize-cfi-canonical-jump-tables,Make the jump table addresses canonical in the symbol table,0,0,others,clang,0,1
446,-fsanitize-cfi-cross-dso,Enable control flow integrity (CFI) checks for cross-DSO calls.,1,2,security-tradeoff,clang,0,0
447,-fsanitize-cfi-icall-generalize-pointers,Generalize pointers in CFI indirect call type signature checks,0,0,others,clang,0,0
448,-fsanitize-coverage,Specify the type of coverage instrumentation for Sanitizers,0,0,others,clang,0,0
449,-fsanitize-hwaddress-abi,"Select the HWAddressSanitizer ABI to target (interceptor or platform, default interceptor). This option is currently unused.",0,0,others,clang,0,0
450,-fsanitize-memory-track-origins,Enable origins tracking in MemorySanitizer,0,0,others,clang,0,0
451,-fsanitize-memory-use-after-dtor,Enable use-after-destroy detection in MemorySanitizer,0,0,others,clang,0,1
452,-fsanitize-recover,Enable recovery for specified sanitizers,0,0,others,clang,0,0
453,-fsanitize-stats,Enable sanitizer statistics gathering.,1,6,function-tradeoff,clang,0,1
455,-fsanitize-thread-atomics,Enable atomic operations instrumentation in ThreadSanitizer (default),0,0,others,clang,0,0
456,-fsanitize-thread-func-entry-exit,Enable function entry/exit instrumentation in ThreadSanitizer (default),0,0,others,clang,0,0
457,-fsanitize-thread-memory-access,Enable memory access instrumentation in ThreadSanitizer (default),0,0,others,clang,0,0
458,-fsanitize-trap,Enable trapping for specified sanitizers,0,0,others,clang,0,0
459,-fsanitize-undefined-strip-path-components,"Strip (or keep only, if negative) a given number of path components when emitting check metadata.",0,0,others,clang,0,0
460,-fsave-optimization-record,Generate an optimization record file in a specific format,0,0,others,clang,0,0
461,-fseh-exceptions,Use SEH style exceptions,0,0,others,clang,0,0
462,-fshort-enums,Allocate to an enum type only as many bytes as it needs for the declared range of possible values,0,0,others,clang,0,0
463,-fshort-wchar,Force wchar_t to be a short unsigned int,0,0,others,clang,0,0
464,-fshow-overloads,Which overload candidates to show when overload resolution fails: best|all; defaults to all,0,0,others,clang,0,0
465,-fsized-deallocation,Enable C++14 sized global deallocation functions,0,0,others,clang,0,0
466,-fsjlj-exceptions,Use SjLj style exceptions,0,0,others,clang,0,1
467,-fslp-vectorize,Enable the superword-level parallelism vectorization passes,0,0,others,clang,0,0
468,-fsplit-dwarf-inlining,Provide minimal debug info in the object/executable to facilitate online symbolication/stack traces in the absence of .dwo/.dwp files when using Split DWARF,0,0,others,clang,0,1
469,-fsplit-lto-unit,Enables splitting of the LTO unit.,0,0,others,clang,0,0
470,-fstack-protector,"Enable stack protectors for some functions vulnerable to stack smashing. This uses a loose heuristic which considers functions vulnerable if they contain a char (or 8bit integer) array or constant sized calls to alloca, which are of greater size than ssp-buffer-size (default: 8 bytes). All variable sized calls to alloca are considered vulnerable",1,2,security-tradeoff,clang,0,0
471,-fstack-protector-all,Enable stack protectors for all functions,1,2,security-tradeoff,clang,0,1
472,-fstack-protector-strong,"Enable stack protectors for some functions vulnerable to stack smashing. Compared to -fstack-protector, this uses a stronger heuristic that includes functions containing arrays of any size (and any type), as well as any calls to alloca or the taking of an address from a local variable",1,2,security-tradeoff,clang,0,1
473,-fstack-size-section,Emit section containing metadata on function stack sizes,1,6,function-tradeoff,clang,0,0
474,-fstandalone-debug,Emit full debug info for all types used by the program,0,0,others,clang,0,0
475,-fstrict-enums,Enable optimizations based on the strict definition of an enum's value range,1,4,limited-side-effect,clang,0,0
476,-fstrict-float-cast-overflow,Assume that overflowing float-to-int casts are undefined (default),0,0,others,clang,0,0
477,-fstrict-return,Always treat control flow paths that fall off the end of a non-void function as unreachable,0,0,others,clang,0,0
478,-fstrict-vtable-pointers,Enable optimizations based on the strict rules for overwriting polymorphic C++ objects,1,4,limited-side-effect,clang,0,0
480,-fthinlto-index,Perform ThinLTO importing using provided function summary index,0,0,others,clang,0,0
482,-ftime-trace-granularity,Minimum time granularity (in microseconds) traced by time profiler,0,0,others,clang,0,1
483,-ftrap-function,Issue call to specified function rather than a trap instruction,0,0,others,clang,0,0
484,-ftrapv,Trap on integer overflow,0,0,others,clang,0,0
485,-ftrapv-handler=function,name> Specify the function to be called on overflow,0,0,others,clang,0,0
486,-ftrigraphs,Process trigraph sequences,0,0,others,clang,0,0
487,-ftrivial-auto-var-init,Initialize trivial automatic stack variables: uninitialized (default) | pattern,0,0,others,clang,0,0
488,-funique-section-names,Use unique names for text and data sections (ELF Only),0,0,others,clang,0,0
489,-funroll-loops,Turn on loop unroller,0,0,others,clang,0,0
490,-fuse-init-array,Use .init_array instead of .ctors,0,0,others,clang,0,0
491,-fvalidate-ast-input-files-content,Compute and store the hash of input files used to build an AST. Files with mismatching mtime's are considered valid if both contents is identical,0,0,others,clang,0,0
492,-fveclib,Use the given vector functions library,0,0,others,clang,0,1
493,-fvectorize,Enable the loop vectorization passes,0,0,others,clang,0,0
494,-fvirtual-function-elimination,Enables dead virtual function elimination optimization. Requires -flto=full,0,0,others,clang,0,0
495,-fvisibility,Set the default symbol visibility for all global declarations,0,0,others,clang,0,0
496,-fvisibility-global-new-delete-hidden,Give global C++ operator new and delete declarations hidden visibility,0,0,others,clang,0,0
497,-fvisibility-inlines-hidden,Give inline C++ member functions hidden visibility by default,0,0,others,clang,0,0
498,-fvisibility-ms-compat,Give global types 'default' visibility and global functions and variables 'hidden' visibility by default,0,0,others,clang,0,0
499,-fwasm-exceptions,Use WebAssembly style exceptions,0,0,others,clang,0,0
500,-fwhole-program-vtables,Enables whole-program vtable optimization. Requires -flto,1,4,limited-side-effect,clang,0,0
501,-fwrapv,Treat signed integer overflow as two's complement,0,0,others,clang,0,0
502,-fwritable-strings,Store string literals as writable data,0,0,others,clang,0,0
503,-fxray-always-emit-customevents,Determine whether to always emit __xray_customevent(...) calls even if the function it appears in is not always instrumented.,0,0,others,clang,0,0
504,-fxray-always-emit-typedevents,Determine whether to always emit __xray_typedevent(...) calls even if the function it appears in is not always instrumented.,0,0,others,clang,0,0
505,-fxray-always-instrument=,DEPRECATED: Filename defining the whitelist for imbuing the 'always instrument' XRay attribute.,0,0,others,clang,0,0
506,-fxray-attr-list=,Filename defining the list of functions/types for imbuing XRay attributes.,0,0,others,clang,0,0
507,-fxray-instruction-threshold=,Sets the minimum function size to instrument with XRay,1,5,workload-specific,clang,0,0
508,-fxray-instrument,Generate XRay instrumentation sleds on function entry and exit,0,0,others,clang,0,0
509,-fxray-link-deps,Tells clang to add the link dependencies for XRay.,0,0,others,clang,0,0
510,-fxray-modes=,List of modes to link in by default into XRay instrumented binaries.,0,0,others,clang,0,0
511,-fxray-never-instrument=,DEPRECATED: Filename defining the whitelist for imbuing the 'never instrument' XRay attribute.,0,0,others,clang,0,0
512,-fzvector,Enable System z vector language extension,0,0,others,clang,0,1
513,-g,Generate source-level debug information,1,6,function-tradeoff,clang,0,0
514,-G(captal),Put objects of at most <size> bytes into small data section (MIPS / Hexagon),0,0,others,clang,0,0
516,-gcodeview,Generate CodeView debug information,1,6,function-tradeoff,clang,0,0
517,-gcodeview-ghash,Emit type record hashes in a .debug$H section,1,6,function-tradeoff,clang,0,1
518,-gdwarf,Generate source-level debug information with the default dwarf version,0,0,others,clang,0,1
519,-gdwarf-2,Generate source-level debug information with dwarf version 2,0,0,others,clang,0,0
520,-gdwarf-3,Generate source-level debug information with dwarf version 3,0,0,others,clang,0,1
521,-gdwarf-4,Generate source-level debug information with dwarf version 4,0,0,others,clang,0,0
522,-gdwarf-5,Generate source-level debug information with dwarf version 5,0,0,others,clang,0,0
523,-gembed-source,Embed source text in DWARF debug sections,0,0,others,clang,0,0
524,-gline-directives-only,Emit debug line info directives only,1,6,function-tradeoff,clang,0,1
525,-gline-tables-only,Emit debug line number tables only,1,6,function-tradeoff,clang,0,0
526,-gmodules,Generate debug info with external references to clang modules or precompiled headers,0,0,others,clang,0,1
527,-gno-embed-source,Restore the default behavior of not embedding source text in DWARF debug sections,0,0,others,clang,0,0
528,-gno-inline-line-tables,Don't emit inline line tables,1,6,function-tradeoff,clang,0,0
529,--gpu-max-threads-per-block,Default max threads per block for kernel launch bounds for HIP,1,1,resource,clang,0,0
530,-gsplit-dwarf,Set DWARF fission mode to either 'split' or 'single',0,0,others,clang,0,0
531,-gz,DWARF debug sections compression type,1,6,function-tradeoff,clang,0,0
532,-H,Show header includes and nesting depth,0,0,others,clang,0,0
533,-help,Display available options,0,0,others,clang,0,0
534,--help-hidden,Display help for hidden options,0,0,others,clang,0,0
535,--hip-device-lib,HIP device library,0,0,others,clang,0,0
537,--hip-link,Link clang-offload-bundler bundles for HIP,0,0,others,clang,0,0
546,-iprefix,Set the -iwithprefix/-iwithprefixbefore prefix,0,0,others,clang,0,0
556,-M,"Like -MD, but also implies -E and writes to stdout by default",0,0,others,clang,0,0
557,-mabicalls,Enable SVR4-style position-independent code (Mips only),0,0,others,clang,0,0
558,-malign-double,Align doubles to two words in structs (x86 only),0,0,others,clang,0,0
559,-mbackchain,Link stack frames through backchain on System Z,0,0,others,clang,0,0
560,-mbranch-protection,Enforce targets of indirect branches and function returns,0,0,others,clang,0,0
561,-mcmodel=medany,"Equivalent to -mcmodel=medium, compatible with RISC-V gcc.",0,0,others,clang,0,0
562,-mcmodel=medlow,"Equivalent to -mcmodel=small, compatible with RISC-V gcc.",0,0,others,clang,0,1
563,-mcmse,Allow use of CMSE (Armv8-M Security Extensions),0,0,others,clang,0,0
564,-mcode-object-v3,Enable code object v3 (AMDGPU only),0,0,others,clang,0,0
565,-mcrc,Allow use of CRC instructions (ARM/Mips only),0,0,others,clang,0,0
566,-mcumode,CU wavefront execution mode is used (AMDGPU only),0,0,others,clang,0,0
567,-MD,Write a depfile containing user and system headers,0,0,others,clang,0,1
568,-membedded-data,Place constants in the .rodata section instead of the .sdata section even if they meet the -G <size> threshold (MIPS),1,5,workload-specific,clang,0,0
569,-mexecute-only,Disallow generation of data access to code sections (ARM only),0,0,others,clang,0,0
570,-mextern-sdata,Assume that externally defined data is in the small data if it meets the -G <size> threshold (MIPS),1,5,workload-specific,clang,0,1
571,-mfentry,Insert calls to fentry at function entry (x86/SystemZ only),0,0,others,clang,0,0
572,-mfix-cortex-a53-835769,Workaround Cortex-A53 erratum 835769 (AArch64 only),0,0,others,clang,0,0
573,-mfp32,Use 32-bit floating point registers (MIPS only),0,0,others,clang,0,1
574,-mfp64,Use 64-bit floating point registers (MIPS only),0,0,others,clang,0,0
575,-MG,Add missing headers to depfile,0,0,others,clang,0,0
576,-mgeneral-regs-only,Generate code which only uses the general purpose registers (AArch64 only),0,0,others,clang,0,0
577,-mglobal-merge,Enable merging of globals,0,0,others,clang,0,0
578,-mgpopt,Use GP relative accesses for symbols known to be in a small data section (MIPS),0,0,others,clang,0,0
579,-mhvx,Enable Hexagon Vector eXtensions,0,0,others,clang,0,0
580,-mhvx-length,Set Hexagon Vector Length,0,0,others,clang,0,0
581,-miamcu,Use Intel MCU ABI,0,0,others,clang,0,0
582,--migrate,Run the migrator,0,0,others,clang,0,0
583,-mincremental-linker-compatible,(integrated-as) Emit an object file which can be used with an incremental linker,0,0,others,clang,0,0
584,-mindirect-jump,Change indirect jump instructions to inhibit speculation,0,0,others,clang,0,1
585,-mios-version-min,Set iOS deployment target,0,0,others,clang,0,1
586,-MJ,Write a compilation database entry per input,0,0,others,clang,0,0
587,-mllvm,Additional arguments to forward to LLVM's option processing,0,0,others,clang,0,1
588,-mlocal-sdata,Extend the -G behaviour to object local data (MIPS),0,0,others,clang,0,0
589,-mlong-calls,"Generate branches with extended addressability, usually via indirect jumps.",0,0,others,clang,0,0
590,-mlong-double-128,Force long double to be 128 bits,0,0,others,clang,0,0
591,-mlong-double-64,Force long double to be 64 bits,0,0,others,clang,0,1
592,-mlong-double-80,"Force long double to be 80 bits, padded to 128 bits for storage",0,0,others,clang,0,0
593,-MM,"Like -MMD, but also implies -E and writes to stdout by default",0,0,others,clang,0,1
594,-mmacosx-version-min,Set Mac OS X deployment target,0,0,others,clang,0,0
595,-mmadd4,"Enable the generation of 4-operand madd.s, madd.d and related instructions.",0,0,others,clang,0,0
596,-MMD,Write a depfile containing user headers,0,0,others,clang,0,1
597,-mmemops,Enable generation of memop instructions,0,0,others,clang,0,0
598,-mmsa,Enable MSA ASE (MIPS only),0,0,others,clang,0,0
599,-mms-bitfields,Set the default structure layout to be compatible with the Microsoft compiler standard,0,0,others,clang,0,0
600,-mmt,Enable MT ASE (MIPS only),0,0,others,clang,0,0
601,-mno-abicalls,Disable SVR4-style position-independent code (Mips only),0,0,others,clang,0,0
602,-mno-code-object-v3,Disable code object v3 (AMDGPU only),0,0,others,clang,0,1
603,-mnocrc,Disallow use of CRC instructions (ARM only),0,0,others,clang,0,0
604,-mno-crc,Disallow use of CRC instructions (Mips only),0,0,others,clang,0,0
605,-mno-cumode,WGP wavefront execution mode is used (AMDGPU only),0,0,others,clang,0,1
606,-mno-embedded-data,Do not place constants in the .rodata section instead of the .sdata if they meet the -G <size> threshold (MIPS),1,5,workload-specific,clang,0,0
607,-mno-execute-only,Allow generation of data access to code sections (ARM only),0,0,others,clang,0,0
608,-mno-extern-sdata,Do not assume that externally defined data is in the small data if it meets the -G <size> threshold (MIPS),1,5,workload-specific,clang,0,1
609,-mno-fix-cortex-a53-835769,Don't workaround Cortex-A53 erratum 835769 (AArch64 only),0,0,others,clang,0,0
610,-mno-global-merge,Disable merging of globals,0,0,others,clang,0,1
611,-mno-gpopt,Do not use GP relative accesses for symbols known to be in a small data section (MIPS),0,0,others,clang,0,0
612,-mno-hvx,Disable Hexagon Vector eXtensions,0,0,others,clang,0,0
613,-mno-implicit-float,Don't generate implicit floating point instructions,0,0,others,clang,0,0
614,-mno-incremental-linker-compatible,(integrated-as) Emit an object file which cannot be used with an incremental linker,0,0,others,clang,0,0
615,-mno-local-sdata,Do not extend the -G behaviour to object local data (MIPS),0,0,others,clang,0,0
616,-mno-long-calls,Restore the default behaviour of not generating long calls,0,0,others,clang,0,1
617,-mno-madd4,"Disable the generation of 4-operand madd.s, madd.d and related instructions.",0,0,others,clang,0,1
618,-mno-memops,Disable generation of memop instructions,0,0,others,clang,0,0
619,-mno-movt,Disallow use of movt/movw pairs (ARM only),0,0,others,clang,0,1
620,-mno-msa,Disable MSA ASE (MIPS only),0,0,others,clang,0,0
621,-mno-ms-bitfields,Do not set the default structure layout to be compatible with the Microsoft compiler standard,0,0,others,clang,0,1
622,-mno-mt,Disable MT ASE (MIPS only),0,0,others,clang,0,0
623,-mno-neg-immediates,Disallow converting instructions with negative immediates to their negation or inversion.,1,6,function-tradeoff,clang,0,0
624,-mno-nvj,Disable generation of new-value jumps,0,0,others,clang,0,0
625,-mno-nvs,Disable generation of new-value stores,0,0,others,clang,0,0
626,-mno-outline,Disable function outlining (AArch64 only),0,0,others,clang,0,0
627,-mno-packets,Disable generation of instruction packets,0,0,others,clang,0,0
628,-mnop-mcount,Generate mcount/__fentry__ calls as nops. To activate they need to be patched in.,0,0,others,clang,0,0
629,-mno-relax,Disable linker relaxation,0,0,others,clang,0,1
631,-mno-save-restore,Disable using library calls for save and restore,0,0,others,clang,0,0
632,-mno-sram-ecc,Disable SRAM ECC (AMDGPU only),0,0,others,clang,0,0
633,-mno-stack-arg-probe,Disable stack probes which are enabled by default,0,0,others,clang,0,0
634,-mno-tls-direct-seg-refs,Disable direct TLS access through segment registers,0,0,others,clang,0,0
635,-mno-unaligned-access,Force all memory accesses to be aligned (AArch32/AArch64 only),0,0,others,clang,0,0
636,-mno-wavefrontsize64,Wavefront size 32 is used,0,0,others,clang,0,0
637,-mno-xnack,Disable XNACK (AMDGPU only),0,0,others,clang,0,0
638,-mnvj,Enable generation of new-value jumps,0,0,others,clang,0,0
639,-mnvs,Enable generation of new-value stores,0,0,others,clang,0,0
642,-momit-leaf-frame-pointer,Omit frame pointer setup for leaf functions,0,0,others,clang,0,0
643,-moutline,Enable function outlining (AArch64 only),0,0,others,clang,0,0
645,-mpacked-stack,Use packed stack layout (SystemZ only).,0,0,others,clang,0,0
646,-mpackets,Enable generation of instruction packets,0,0,others,clang,0,0
647,-mpie-copy-relocations,Use copy relocations support for PIE builds,0,0,others,clang,0,0
648,-mprefer-vector-width,Specifies preferred vector width for auto-vectorization. Defaults to 'none' which allows target specific decisions.,0,0,others,clang,0,0
650,-mqdsp6-compat,Enable hexagon-qdsp6 backward compatibility,0,0,others,clang,0,0
651,-mrecord-mcount,Generate a __mcount_loc section entry for each __fentry__ call.,0,0,others,clang,0,0
652,-mrelax,Enable linker relaxation,0,0,others,clang,0,0
653,-mrelax-all,(integrated-as) Relax all machine instructions,0,0,others,clang,0,0
655,-mrtd,Make StdCall calling convention the default,0,0,others,clang,0,0
656,-msave-restore,Enable using library calls for save and restore,0,0,others,clang,0,0
658,-msoft-float,Use software floating point,0,0,others,clang,0,1
659,-msram-ecc,Enable SRAM ECC (AMDGPU only),0,0,others,clang,0,0
660,-mstack-alignment,Set the stack alignment,0,0,others,clang,0,1
661,-mstack-arg-probe,Enable stack probes,0,0,others,clang,0,0
662,-mstack-probe-size,Set the stack probe size,1,5,workload-specific,clang,0,1
663,-mstackrealign,Force realign the stack at entry to every function,0,0,others,clang,0,0
665,-mtls-direct-seg-refs,Enable direct TLS access through segment registers (default),0,0,others,clang,0,0
666,-mtls-size,"Specify bit size of immediate TLS offsets (AArch64 ELF only): 12 (for 4KB) | 24 (for 16MB, default) | 32 (for 4GB) | 48 (for 256TB, needs -mcmodel=large)",0,0,others,clang,0,0
667,-mtp,Thread pointer access method (AArch32/AArch64 only),0,0,others,clang,0,0
668,-munaligned-access,Allow memory accesses to be unaligned (AArch32/AArch64 only),0,0,others,clang,0,0
669,-MV,Use NMake/Jom format for the depfile,0,0,others,clang,0,0
670,-mwavefrontsize64,Wavefront size 64 is used,0,0,others,clang,0,0
671,-mxnack,Enable XNACK (AMDGPU only),0,0,others,clang,0,1
672,-nobuiltininc,Disable builtin #include directories,0,0,others,clang,0,0
673,--no-cuda-gpu-arch,Remove GPU architecture (e.g. sm_35) from the list of GPUs to compile for. 'all' resets the list to its default value.,0,0,others,clang,0,0
674,--no-cuda-include-ptx,Do not include PTX for the following GPU architecture (e.g. sm_35) or 'all'. May be specified more than once.,0,0,others,clang,0,0
676,-nogpulib,Do not link device library for CUDA/HIP device compilation,0,0,others,clang,0,0
677,-nostdinc++,Disable standard #include directories for the C++ standard library,0,0,others,clang,0,0
678,--no-system-header-prefix,Treat all #include paths starting with <prefix> as not including a system header.,0,0,others,clang,0,0
680,-ObjC,Treat source input files as Objective-C inputs,0,0,others,clang,0,0
681,-ObjC++,Treat source input files as Objective-C++ inputs,0,0,others,clang,0,0
682,-objcmt-atomic-property,Make migration to 'atomic' properties,0,0,others,clang,0,0
683,-objcmt-migrate-all,Enable migration to modern ObjC,0,0,others,clang,0,0
684,-objcmt-migrate-annotation,Enable migration to property and method annotations,0,0,others,clang,0,0
685,-objcmt-migrate-designated-init,Enable migration to infer NS_DESIGNATED_INITIALIZER for initializer methods,0,0,others,clang,0,0
686,-objcmt-migrate-instancetype,Enable migration to infer instancetype for method result type,0,0,others,clang,0,0
687,-objcmt-migrate-literals,Enable migration to modern ObjC literals,0,0,others,clang,0,0
688,-objcmt-migrate-ns-macros,Enable migration to NS_ENUM/NS_OPTIONS macros,0,0,others,clang,0,0
689,-objcmt-migrate-property,Enable migration to modern ObjC property,0,0,others,clang,0,0
690,-objcmt-migrate-property-dot-syntax,Enable migration of setter/getter messages to property-dot syntax,0,0,others,clang,0,0
691,-objcmt-migrate-protocol-conformance,Enable migration to add protocol conformance on classes,0,0,others,clang,0,0
692,-objcmt-migrate-readonly-property,Enable migration to modern ObjC readonly property,0,0,others,clang,0,0
693,-objcmt-migrate-readwrite-property,Enable migration to modern ObjC readwrite property,0,0,others,clang,0,0
694,-objcmt-migrate-subscripting,Enable migration to modern ObjC subscripting,0,0,others,clang,0,0
695,-objcmt-ns-nonatomic-iosonly,Enable migration to use NS_NONATOMIC_IOSONLY macro for setting property's 'atomic' attribute,0,0,others,clang,0,0
696,-objcmt-returns-innerpointer-property,Enable migration to annotate property with NS_RETURNS_INNER_POINTER,0,0,others,clang,0,0
698,-P,Disable linemarker output in -E mode,1,6,function-tradeoff,clang,0,0
699,-pg,Enable mcount instrumentation,0,0,others,clang,0,0
700,-pipe,"Use pipes between commands, when possible",0,0,others,clang,0,0
701,--precompile,Only precompile the input,0,0,others,clang,0,1
702,-print-effective-triple,Print the effective target triple,1,6,function-tradeoff,clang,0,0
703,-print-file-name,Print the full library path of <file>,1,6,function-tradeoff,clang,0,0
704,-print-ivar-layout,Enable Objective-C Ivar layout bitmap print trace,1,6,function-tradeoff,clang,0,1
705,-print-libgcc-file-name,"Print the library path for the currently used compiler runtime library (""libgcc.a"" or ""libclang_rt.builtins.*.a"")",1,6,function-tradeoff,clang,0,0
706,-print-prog-name,Print the full program path of <name>,1,6,function-tradeoff,clang,0,0
707,-print-resource-dir,Print the resource directory pathname,1,6,function-tradeoff,clang,0,0
708,-print-search-dirs,Print the paths used for finding libraries and programs,1,6,function-tradeoff,clang,0,0
709,-print-supported-cpus,"Print supported cpu models for the given target (if target is not specified, it will print the supported cpus for the default target)",1,6,function-tradeoff,clang,0,0
710,-print-target-triple,Print the normalized target triple,1,6,function-tradeoff,clang,0,0
711,-pthread,Support POSIX threads in generated code,0,0,others,clang,0,0
713,-Qn,Do not emit metadata containing compiler name and version,0,0,others,clang,0,0
714,-Qunused-arguments,Don't emit warning for unused driver arguments,0,0,others,clang,0,0
715,-Qy,Emit metadata containing compiler name and version,0,0,others,clang,0,0
716,-relocatable-pch,Whether to build a relocatable precompiled header,0,0,others,clang,0,1
717,-rewrite-legacy-objc,Rewrite Legacy Objective-C source to C++,0,0,others,clang,0,0
718,-rewrite-objc,Rewrite Objective-C source to C++,0,0,others,clang,0,1
719,-Rpass,Report transformations performed by optimization passes whose name matches the given POSIX regular expression,0,0,others,clang,0,0
720,-Rpass-analysis,Report transformation analysis from optimization passes whose name matches the given POSIX regular expression,1,6,function-tradeoff,clang,0,0
721,-Rpass-missed,Report missed transformations by optimization passes whose name matches the given POSIX regular expression,1,6,function-tradeoff,clang,0,0
722,-Rremark,Enable the specified remark,0,0,others,clang,0,0
723,-rtlib,Compiler runtime library to use,0,0,others,clang,0,0
724,-S,Only run preprocess and compilation steps,0,0,others,clang,0,0
725,-save-stats,Save llvm statistics.,1,6,function-tradeoff,clang,0,0
726,-save-temps,Save intermediate compilation results,1,6,function-tradeoff,clang,0,0
728,-shared-libsan,Dynamically link the sanitizer runtime,0,0,others,clang,0,0
729,-static-libsan,Statically link the sanitizer runtime,0,0,others,clang,0,1
731,-std,Language standard to compile for,0,0,others,clang,0,1
732,-stdlib,C++ standard library to use,0,0,others,clang,0,0
734,--system-header-prefix,Treat all #include paths starting with <prefix> as including a system header.,0,0,others,clang,0,0
735,-T,Specify <script> as linker script,0,0,others,clang,0,0
736,--target,Generate code for the given target,0,0,others,clang,0,1
739,-time,Time individual commands,0,0,others,clang,0,1
740,-traditional-cpp,Enable some traditional CPP emulation,0,0,others,clang,0,1
741,-trigraphs,Process trigraph sequences,0,0,others,clang,0,0
743,-U,Undefine macro <macro>,0,0,others,clang,0,0
744,-undef,undef all system defines,0,0,others,clang,0,0
745,-unwindlib,Unwind library to use,0,0,others,clang,0,0
746,-v,Show commands to run and use verbose output,1,6,function-tradeoff,clang,0,1
747,--verify-debug-info,Verify the binary representation of debug output,1,6,function-tradeoff,clang,0,0
748,-verify-pch,Load and verify that a pre-compiled header file is not stale,0,0,others,clang,0,0
749,--version,Print version information,1,6,function-tradeoff,clang,0,1
750,-w,Suppress all warnings,1,6,function-tradeoff,clang,0,0
751,"-Wa,arg",Pass the comma separated arguments in <arg> to the assembler,0,0,others,clang,0,0
753,"-Wl,arg",Pass the comma separated arguments in <arg> to the linker,0,0,others,clang,0,0
755,"-Wp,arg",Pass the comma separated arguments in <arg> to the preprocessor,0,0,others,clang,0,1
756,-Wwarning,Enable the specified warning,1,6,function-tradeoff,clang,0,0
757,-x,Treat subsequent input files as having type <language>,0,0,others,clang,0,0
758,-Xanalyzer,Pass <arg> to the static analyzer,0,0,others,clang,0,0
759,-Xassembler,Pass <arg> to the assembler,0,0,others,clang,0,0
760,-Xclang,Pass <arg> to the clang compiler,0,0,others,clang,0,0
761,-Xcuda-fatbinary,Pass <arg> to fatbinary invocation,0,0,others,clang,0,1
762,-Xcuda-ptxas,Pass <arg> to the ptxas assembler,0,0,others,clang,0,1
763,-Xlinker,Pass <arg> to the linker,0,0,others,clang,0,0
764,-Xopenmp-target,Pass <arg> to the target offloading toolchain identified by <triple>.,0,0,others,clang,0,0
765,-Xpreprocessor,Pass <arg> to the preprocessor,0,0,others,clang,0,0
766,-z,Pass -z <arg> to the linker,0,0,others,clang,0,0
767,adl.feature.ownerandgroup.enableupn,"When true : User and Group in FileStatus/AclStatus response is represented as user friendly name as per Azure AD profile. When false (default) : User and Group in FileStatus/AclStatus response is represented by the unique identifier from Azure AD profile (Object ID as GUID). For optimal performance, false is recommended.",1,6,function-tradeoff,core,0,1
768,dfs.ha.fencing.methods,List of fencing methods to use for service fencing. May contain builtin methods (eg shell and sshfence) or user-defined method.,0,0,others,core,0,0
769,dfs.ha.fencing.ssh.connect-timeout,"SSH connection timeout, in milliseconds, to use with the builtin sshfence fencer.",0,0,others,core,0,0
770,dfs.ha.fencing.ssh.private-key-files,The SSH private key files to use with the builtin sshfence fencer.,0,0,others,core,0,0
771,file.blocksize,Block size,1,5,workload-specific,core,0,0
772,file.bytes-per-checksum,The number of bytes per checksum. Must not be larger than file.stream-buffer-size,0,0,others,core,0,1
773,file.client-write-packet-size,Packet size for clients to write,0,0,others,core,0,0
774,file.replication,Replication factor,1,3,reliability-tradeoff,core,0,1
775,file.stream-buffer-size,"The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",1,1,resource,core,0,0
777,fs.AbstractFileSystem.ftp.impl,The FileSystem for Ftp: uris.,0,0,others,core,0,0
778,fs.AbstractFileSystem.har.impl,The AbstractFileSystem for har: uris.,0,0,others,core,0,1
779,fs.AbstractFileSystem.hdfs.impl,The FileSystem for hdfs: uris.,0,0,others,core,0,1
780,fs.AbstractFileSystem.s3a.impl,The implementation class of the S3A AbstractFileSystem.,0,0,others,core,0,0
781,fs.AbstractFileSystem.swebhdfs.impl,The FileSystem for swebhdfs: uris.,0,0,others,core,0,0
783,fs.AbstractFileSystem.webhdfs.impl,The FileSystem for webhdfs: uris.,0,0,others,core,0,0
787,fs.adl.oauth2.credential,The OAuth2 access key.,0,0,others,core,0,1
788,fs.adl.oauth2.devicecode.clientapp.id,The app id of the AAD native app in whose context the auth request should be made. Used by DeviceCode token provider.,0,0,others,core,0,0
790,fs.adl.oauth2.refresh.token,The OAuth2 refresh token.,0,0,others,core,0,0
792,fs.automatic.close,"By default, FileSystem instances are automatically closed at program exit using a JVM shutdown hook. Setting this property to false disables this behavior. This is an advanced option that should only be used by server applications requiring a more carefully orchestrated shutdown sequence.",1,3,reliability-tradeoff,core,0,0
793,fs.azure.authorization,"Config flag to enable authorization support in WASB. Setting it to ""true"" enables authorization support to WASB. Currently WASB authorization requires a remote service to provide authorization that needs to be specified via fs.azure.authorization.remote.service.url configuration",1,2,security-tradeoff,core,0,0
794,fs.azure.authorization.caching.enable,Config flag to enable caching of authorization results and saskeys in WASB. This flag is relevant only when fs.azure.authorization is enabled.,1,2,security-tradeoff,core,0,1
797,fs.azure.saskey.usecontainersaskeyforallaccess,Use container saskey for access to all blobs within the container. Blob-specific saskeys are not used when this setting is enabled. This setting provides better performance compared to blob-specific saskeys.,1,4,limited-side-effect,core,0,0
798,fs.azure.secure.mode,"Config flag to identify the mode in which fs.azure.NativeAzureFileSystem needs to run under. Setting it ""true"" would make fs.azure.NativeAzureFileSystem use SAS keys to communicate with Azure storage.",1,2,security-tradeoff,core,0,0
799,fs.azure.user.agent.prefix,"WASB passes User-Agent header to the Azure back-end. The default value contains WASB version, Java Runtime version, Azure Client library version, and the value of the configuration option fs.azure.user.agent.prefix.",0,0,others,core,0,0
800,fs.client.htrace.sampler.classes,The class names of the HTrace Samplers to use for Hadoop filesystem clients.,0,0,others,core,0,0
801,fs.client.resolve.remote.symlinks,"Whether to resolve symlinks when accessing a remote Hadoop filesystem. Setting this to false causes an exception to be thrown upon encountering a symlink. This setting does not apply to local filesystems, which automatically resolve local symlinks.",1,6,function-tradeoff,core,0,1
803,fs.default.name,Deprecated. Use (fs.defaultFS) property instead,0,0,others,core,0,0
805,fs.df.interval,Disk usage statistics refresh interval in msec.,0,0,others,core,0,0
807,fs.ftp.data.connection.mode,"Set the FTPClient's data connection mode based on configuration. Valid values are ACTIVE_LOCAL_DATA_CONNECTION_MODE, PASSIVE_LOCAL_DATA_CONNECTION_MODE and PASSIVE_REMOTE_DATA_CONNECTION_MODE.",0,0,others,core,0,0
810,fs.ftp.transfer.mode,"Set FTP's transfer mode based on configuration. Valid values are STREAM_TRANSFER_MODE, BLOCK_TRANSFER_MODE and COMPRESSED_TRANSFER_MODE.",0,0,others,core,0,1
811,fs.har.impl.disable.cache,Don't cache 'har' filesystem instances.,1,4,limited-side-effect,core,0,0
812,fs.permissions.umask-mode,"The umask used when creating files and directories. Can be in octal or in symbolic. Examples are: ""022"" (octal for u=rwx,g=r-x,o=r-x in symbolic), or ""u=rwx,g=rwx,o="" (symbolic for 007 in octal).",0,0,others,core,0,0
813,fs.protected.directories,A comma-separated list of directories which cannot be deleted even by the superuser unless they are empty. This setting can be used to guard important system directories against accidental deletion due to administrator error.,0,0,others,core,0,0
816,fs.s3.block.size,Block size to use when writing files to S3.,1,5,workload-specific,core,0,0
818,fs.s3.maxRetries,"The maximum number of retries for reading or writing files to S3, before we signal failure to the application.",0,0,others,core,0,0
819,fs.s3.sleepTimeSeconds,The number of seconds to sleep between each S3 retry.,0,0,others,core,0,1
820,fs.s3a.access.key,AWS access key ID used by S3A file system. Omit for IAM role-based or provider-based authentication.,0,0,others,core,0,1
821,fs.s3a.acl.default,"Set a canned ACL for newly created and copied objects. Value may be Private, PublicRead, PublicReadWrite, AuthenticatedRead, LogDeliveryWrite, BucketOwnerRead, or BucketOwnerFullControl.",0,0,others,core,0,1
822,fs.s3a.attempts.maximum,How many times we should retry commands on transient errors.,0,0,others,core,0,0
823,fs.s3a.aws.credentials.provider,"Comma-separated class names of credential provider classes which implement com.amazonaws.auth.AWSCredentialsProvider. These are loaded and queried in sequence for a valid set of credentials. Each listed class must implement one of the following means of construction, which are attempted in order: 1. a public constructor accepting java.net.URI and org.apache.hadoop.conf.Configuration, 2. a public static method named getInstance that accepts no arguments and returns an instance of com.amazonaws.auth.AWSCredentialsProvider, or 3. a public default constructor. Specifying org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider allows anonymous access to a publicly accessible S3 bucket without any credentials. Please note that allowing anonymous access to an S3 bucket compromises security and therefore is unsuitable for most use cases. It can be useful for accessing public data sets without requiring AWS credentials. If unspecified, then the default list of credential provider classes, queried in sequence, is: 1. org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider: supports static configuration of AWS access key ID and secret access key. See also fs.s3a.access.key and fs.s3a.secret.key. 2. com.amazonaws.auth.EnvironmentVariableCredentialsProvider: supports configuration of AWS access key ID and secret access key in environment variables named AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY, as documented in the AWS SDK. 3. org.apache.hadoop.fs.s3a.SharedInstanceProfileCredentialsProvider: a shared instance of com.amazonaws.auth.InstanceProfileCredentialsProvider from the AWS SDK, which supports use of instance profile credentials if running in an EC2 VM. Using this shared instance potentially reduces load on the EC2 instance metadata service for multi-threaded applications.",0,0,others,core,0,0
824,fs.s3a.block.size,"Block size to use when reading files using s3a: file system. A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.",1,5,workload-specific,core,0,0
825,fs.s3a.buffer.dir,Comma separated list of directories that will be used to buffer file uploads to.,0,0,others,core,0,1
826,fs.s3a.connection.establish.timeout,Socket connection setup timeout in milliseconds.,0,0,others,core,0,0
827,fs.s3a.connection.maximum,Controls the maximum number of simultaneous connections to S3.,1,1,resource,core,0,0
828,fs.s3a.connection.ssl.enabled,Enables or disables SSL connections to S3.,1,2,security-tradeoff,core,0,0
829,fs.s3a.connection.timeout,Socket connection timeout in milliseconds.,0,0,others,core,0,0
831,fs.s3a.fast.upload,Use the incremental block-based fast upload mechanism with the buffering mechanism set in fs.s3a.fast.upload.buffer.,1,4,limited-side-effect,core,0,1
832,fs.s3a.fast.upload.active.blocks,"Maximum Number of blocks a single output stream can have active (uploading, or queued to the central FileSystem instance's pool of queued operations. This stops a single stream overloading the shared thread pool.",1,1,resource,core,0,1
833,fs.s3a.fast.upload.buffer,"The buffering mechanism to use when using S3A fast upload (fs.s3a.fast.upload=true). Values: disk, array, bytebuffer. This configuration option has no effect if fs.s3a.fast.upload is false. ""disk"" will use the directories listed in fs.s3a.buffer.dir as the location(s) to save data prior to being uploaded. ""array"" uses arrays in the JVM heap ""bytebuffer"" uses off-heap memory within the JVM. Both ""array"" and ""bytebuffer"" will consume memory in a single stream up to the number of blocks set by: fs.s3a.multipart.size * fs.s3a.fast.upload.active.blocks. If using either of these mechanisms, keep this value low The total number of threads performing work across all threads is set by fs.s3a.threads.max, with fs.s3a.max.total.tasks values setting the number of queued work items.",0,0,others,core,0,0
834,fs.s3a.impl,The implementation class of the S3A Filesystem,0,0,others,core,0,0
835,fs.s3a.max.total.tasks,The number of operations which can be queued for execution,1,1,resource,core,0,1
836,fs.s3a.metadatastore.authoritative,"When true, allow MetadataStore implementations to act as source of truth for getting file status and directory listings. Even if this is set to true, MetadataStore implementations may choose not to return authoritative results. If the configured MetadataStore does not support being authoritative, this setting will have no effect.",1,6,function-tradeoff,core,0,1
838,fs.s3a.multiobjectdelete.enable,"When enabled, multiple single-object delete requests are replaced by a single 'delete multiple objects'-request, reducing the number of requests. Beware: legacy S3-compatible object stores might not support this request.",0,0,others,core,0,0
839,fs.s3a.multipart.purge,"True if you want to purge existing multipart uploads that may not have been completed/aborted correctly. The corresponding purge age is defined in fs.s3a.multipart.purge.age. If set, when the filesystem is instantiated then all outstanding uploads older than the purge age will be terminated -across the entire bucket. This will impact multipart uploads by other applications and users. so should be used sparingly, with an age value chosen to stop failed uploads, without breaking ongoing operations.",1,3,reliability-tradeoff,core,0,0
841,fs.s3a.multipart.size,"How big (in bytes) to split upload or copy operations up into. A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.",1,5,workload-specific,core,0,0
842,fs.s3a.multipart.threshold,"How big (in bytes) to split upload or copy operations up into. This also controls the partition size in renamed files, as rename() involves copying the source file(s). A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.",1,5,workload-specific,core,0,0
844,fs.s3a.path.style.access,Enable S3 path style access ie disabling the default virtual hosting behaviour. Useful for S3A-compliant storage providers as it removes the need to set up DNS for virtual hosting.,1,6,function-tradeoff,core,0,0
845,fs.s3a.proxy.domain,Domain for authenticating with proxy server.,0,0,others,core,0,0
847,fs.s3a.proxy.password,Password for authenticating with proxy server.,0,0,others,core,0,0
849,fs.s3a.proxy.username,Username for authenticating with proxy server.,0,0,others,core,0,0
850,fs.s3a.proxy.workstation,Workstation for authenticating with proxy server.,0,0,others,core,0,0
851,fs.s3a.readahead.range,"Bytes to read ahead during a seek() before closing and re-opening the S3 HTTP connection. This option will be overridden if any call to setReadahead() is made to an open stream. A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.",0,0,others,core,0,1
853,fs.s3a.s3guard.ddb.background.sleep,Length (in milliseconds) of pause between each batch of deletes when pruning metadata. Prevents prune operations (which can typically be low priority background operations) from overly interfering with other I/O operations.,0,0,others,core,0,0
854,fs.s3a.s3guard.ddb.max.retries,"Max retries on batched DynamoDB operations before giving up and throwing an IOException. Each retry is delayed with an exponential backoff timer which starts at 100 milliseconds and approximately doubles each time. The minimum wait before throwing an exception is sum(100, 200, 400, 800, .. 100*2^N-1 ) == 100 * ((2^N)-1) So N = 9 yields at least 51.1 seconds (51,100) milliseconds of blocking before throwing an IOException.",0,0,others,core,0,1
855,fs.s3a.s3guard.ddb.region,"AWS DynamoDB region to connect to. An up-to-date list is provided in the AWS Documentation: regions and endpoints. Without this property, the S3Guard will operate table in the associated S3 bucket region.",0,0,others,core,0,0
857,fs.s3a.s3guard.ddb.table.capacity.read,"Provisioned throughput requirements for read operations in terms of capacity units for the DynamoDB table. This config value will only be used when creating a new DynamoDB table, though later you can manually provision by increasing or decreasing read capacity as needed for existing tables. See DynamoDB documents for more information.",0,0,others,core,0,0
858,fs.s3a.s3guard.ddb.table.capacity.write,Provisioned throughput requirements for write operations in terms of capacity units for the DynamoDB table. Refer to related config fs.s3a.s3guard.ddb.table.capacity.read before usage.,0,0,others,core,0,0
859,fs.s3a.s3guard.ddb.table.create,"If true, the S3A client will create the table if it does not already exist.",0,0,others,core,0,0
860,fs.s3a.secret.key,AWS secret key used by S3A file system. Omit for IAM role-based or provider-based authentication.,0,0,others,core,0,0
862,fs.s3a.server-side-encryption.key,"Specific encryption key to use if fs.s3a.server-side-encryption-algorithm has been set to 'SSE-KMS' or 'SSE-C'. In the case of SSE-C, the value of this property should be the Base64 encoded key. If you are using SSE-KMS and leave this property empty, you'll be using your default's S3 KMS key, otherwise you should set this property to the specific KMS key id.",0,0,others,core,0,0
863,fs.s3a.server-side-encryption-algorithm,"Specify a server-side encryption algorithm for s3a: file system. Unset by default. It supports the following values: 'AES256' (for SSE-S3), 'SSE-KMS' and 'SSE-C'.",1,2,security-tradeoff,core,0,0
864,fs.s3a.session.token,"Session token, when using org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider as one of the providers.",0,0,others,core,0,0
865,fs.s3a.signing-algorithm,Override the default signing algorithm so legacy implementations can still be used,0,0,others,core,0,1
866,fs.s3a.socket.recv.buffer,Socket receive buffer hint to amazon connector. Represented in bytes.,1,1,resource,core,0,0
867,fs.s3a.socket.send.buffer,Socket send buffer hint to amazon connector. Represented in bytes.,1,1,resource,core,0,0
868,fs.s3a.threads.keepalivetime,Number of seconds a thread can be idle before being terminated.,0,0,others,core,0,0
869,fs.s3a.threads.max,The total number of threads available in the filesystem for data uploads *or any other queued filesystem operation*.,1,1,resource,core,0,0
870,fs.s3a.user.agent.prefix,"Sets a custom value that will be prepended to the User-Agent header sent in HTTP requests to the S3 back-end by S3AFileSystem. The User-Agent header always includes the Hadoop version number followed by a string generated by the AWS SDK. An example is ""User-Agent: Hadoop 2.8.0, aws-sdk-java/1.10.6"". If this optional property is set, then its value is prepended to create a customized User-Agent. For example, if this configuration property was set to ""MyApp"", then an example of the resulting User-Agent would be ""User-Agent: MyApp, Hadoop 2.8.0, aws-sdk-java/1.10.6"".",0,0,others,core,0,0
873,fs.s3n.block.size,Block size to use when reading files using the native S3 filesystem (s3n: URIs).,1,5,workload-specific,core,0,0
874,fs.s3n.multipart.copy.block.size,The block size for multipart copy in native S3 filesystem. Default size is 5GB.,1,5,workload-specific,core,0,1
875,fs.s3n.multipart.uploads.block.size,The block size for multipart uploads to native S3 filesystem. Default size is 64MB.,1,5,workload-specific,core,0,1
876,fs.s3n.multipart.uploads.enabled,"Setting this property to true enables multiple uploads to native S3 filesystem. When uploading a file, it is split into blocks if the size is larger than fs.s3n.multipart.uploads.block.size.",1,4,limited-side-effect,core,0,0
877,fs.s3n.server-side-encryption-algorithm,"Specify a server-side encryption algorithm for S3. Unset by default, and the only other currently allowable value is AES256.",0,0,others,core,0,0
878,fs.swift.impl,The implementation class of the OpenStack Swift Filesystem,0,0,others,core,0,0
879,fs.trash.checkpoint.interval,"Number of minutes between trash checkpoints. Should be smaller or equal to fs.trash.interval. If zero, the value is set to the value of fs.trash.interval. Every time the checkpointer runs it creates a new checkpoint out of current and removes checkpoints created more than fs.trash.interval minutes ago.",0,0,others,core,0,0
880,fs.trash.interval,"Number of minutes after which the checkpoint gets deleted. If zero, the trash feature is disabled. This option may be configured both on the server and the client. If trash is disabled server side then the client side configuration is checked. If trash is enabled on the server side then the value configured on the server is used and the client configuration value is ignored.",0,0,others,core,0,1
881,fs.viewfs.rename.strategy,"Allowed rename strategy to rename between multiple mountpoints. Allowed values are SAME_MOUNTPOINT,SAME_TARGET_URI_ACROSS_MOUNTPOINT and SAME_FILESYSTEM_ACROSS_MOUNTPOINT.",0,0,others,core,0,0
882,fs.wasb.impl,The implementation class of the Native Azure Filesystem,0,0,others,core,0,0
883,fs.wasbs.impl,The implementation class of the Secure Native Azure Filesystem,0,0,others,core,0,0
884,ftp.blocksize,Block size,1,5,workload-specific,core,0,1
885,ftp.bytes-per-checksum,The number of bytes per checksum. Must not be larger than ftp.stream-buffer-size,0,0,others,core,0,0
886,ftp.client-write-packet-size,Packet size for clients to write,0,0,others,core,0,0
887,ftp.replication,Replication factor,1,3,reliability-tradeoff,core,0,0
888,ftp.stream-buffer-size,"The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",1,1,resource,core,0,0
889,ha.failover-controller.cli-check.rpc-timeout.ms,"Timeout that the CLI (manual) FC waits for monitorHealth, getServiceState",0,0,others,core,0,0
890,ha.failover-controller.graceful-fence.connection.retries,FC connection retries for graceful fencing,0,0,others,core,0,0
891,ha.failover-controller.graceful-fence.rpc-timeout.ms,Timeout that the FC waits for the old active to go to standby,0,0,others,core,0,1
892,ha.failover-controller.new-active.rpc-timeout.ms,Timeout that the FC waits for the new active to become active,0,0,others,core,0,0
893,ha.health-monitor.check-interval.ms,How often to check the service.,1,5,workload-specific,core,0,0
894,ha.health-monitor.connect-retry-interval.ms,How often to retry connecting to the service.,0,0,others,core,0,1
895,ha.health-monitor.rpc-timeout.ms,Timeout for the actual monitorHealth() calls.,0,0,others,core,0,0
896,ha.health-monitor.sleep-after-disconnect.ms,How long to sleep after an unexpected RPC error.,0,0,others,core,0,0
898,ha.zookeeper.auth,"A comma-separated list of ZooKeeper authentications to add when connecting to ZooKeeper. These are specified in the same format as used by the ""addauth"" command in the ZK CLI. It is important that the authentications specified here are sufficient to access znodes with the ACL specified in ha.zookeeper.acl. If the auths contain secrets, you may instead specify a path to a file, prefixed with the '@' symbol, and the value of this configuration will be loaded from within.",0,0,others,core,0,1
901,ha.zookeeper.session-timeout.ms,"The session timeout to use when the ZKFC connects to ZooKeeper. Setting this value to a lower value implies that server crashes will be detected more quickly, but risks triggering failover too aggressively in the case of a transient error or network blip.",0,0,others,core,0,0
902,hadoop.caller.context.enabled,"When the feature is enabled, additional fields are written into name-node audit log records for auditing coarse granularity operations.",1,6,function-tradeoff,core,0,0
903,hadoop.caller.context.max.size,"The maximum bytes a caller context string can have. If the passed caller context is longer than this maximum bytes, client will truncate it before sending to server. Note that the server may have a different maximum size, and will truncate the caller context to the maximum size it allows.",0,0,others,core,0,0
904,hadoop.caller.context.signature.max.size,"The caller's signature (optional) is for offline validation. If the signature exceeds the maximum allowed bytes in server, the caller context will be abandoned, in which case the caller context will not be recorded in audit logs.",1,5,workload-specific,core,0,1
906,hadoop.htrace.span.receiver.classes,The class names of the Span Receivers to use for Hadoop.,0,0,others,core,0,0
907,haDoop.http.authentication.cookie.domain,"The domain to use for the HTTP cookie that stores the authentication token. In order to authentiation to work correctly across all Hadoop nodes web-consoles the domain must be correctly set. IMPORTANT: when using IP addresses, browsers ignore cookies with domain settings. For this setting to work properly all nodes in the cluster must be configured to generate URLs with hostname.domain names on it.",0,0,others,core,0,1
910,hadoop.http.authentication.signature.secret.file,The signature secret for signing the authentication tokens. The same secret should be used for JT/NN/DN/TT configurations.,0,0,others,core,0,0
911,hadoop.http.authentication.simple.anonymous.allowed,Indicates if anonymous requests are allowed when using 'simple' authentication.,0,0,others,core,0,0
912,hadoop.http.authentication.token.validity,Indicates how long (in seconds) an authentication token is valid before it has to be renewed.,0,0,others,core,0,0
913,hadoop.http.authentication.type,Defines authentication used for Oozie HTTP endpoint.,1,2,security-tradeoff,core,0,1
914,hadoop.http.cross-origin.allowed-headers,Comma separated list of headers that are allowed for web services needing cross-origin (CORS) support.,0,0,others,core,0,0
915,hadoop.http.cross-origin.allowed-methods,Comma separated list of methods that are allowed for web services needing cross-origin (CORS) support.,0,0,others,core,0,0
916,hadoop.http.cross-origin.allowed-origins,Comma separated list of origins that are allowed for web services needing cross-origin (CORS) support. Wildcards (*) and patterns allowed,0,0,others,core,0,1
917,hadoop.http.cross-origin.enabled,Enable/disable the cross-origin (CORS) filter.,1,2,security-tradeoff,core,0,0
918,hadoop.http.cross-origin.max-age,The number of seconds a pre-flighted request can be cached for web services needing cross-origin (CORS) support.,0,0,others,core,0,0
920,hadoop.http.logs.enabled,"Enable the ""/logs"" endpoint on all Hadoop daemons, which serves local logs, but may be considered a security risk due to it listing the contents of a directory.",1,6,function-tradeoff,core,0,1
922,hadoop.jetty.logs.serve.aliases,Enable/Disable aliases serving from jetty,1,4,limited-side-effect,core,0,0
924,hadoop.kerberos.min.seconds.before.relogin,"The minimum time between relogin attempts for Kerberos, in seconds.",0,0,others,core,0,0
925,hadoop.registry.jaas.context,Key to define the JAAS context. Used in secure mode,0,0,others,core,0,0
926,hadoop.registry.kerberos.realm,"The kerberos realm: used to set the realm of system principals which do not declare their realm, and any other accounts that need the value. If empty, the default realm of the running process is used. If neither are known and the realm is needed, then the registry service/client will fail.",0,0,others,core,0,1
927,hadoop.registry.rm.enabled,"Is the registry enabled in the YARN Resource Manager? If true, the YARN RM will, as needed. create the user and system paths, and purge service records when containers, application attempts and applications complete. If false, the paths must be created by other means, and no automatic cleanup of service records will take place.",1,6,function-tradeoff,core,0,0
928,hadoop.registry.secure,"Key to set if the registry is secure. Turning it on changes the permissions policy from ""open access"" to restrictions on kerberos with the option of a user adding one or more auth key pairs down their own tree.",1,2,security-tradeoff,core,0,0
929,hadoop.registry.system.acls,"A comma separated list of Zookeeper ACL identifiers with system access to the registry in a secure cluster. These are given full access to all entries. If there is an ""@"" at the end of a SASL entry it instructs the registry client to append the default kerberos domain.",0,0,others,core,0,0
930,hadoop.registry.zk.connection.timeout.ms,Zookeeper connection timeout in milliseconds,0,0,others,core,0,1
932,hadoop.registry.zk.retry.ceiling.ms,"Zookeeper retry limit in milliseconds, during exponential backoff. This places a limit even if the retry times and interval limit, combined with the backoff policy, result in a long retry period",0,0,others,core,0,0
933,hadoop.registry.zk.retry.times,Zookeeper connection retry count before failing,0,0,others,core,0,1
935,hadoop.rpc.protection,"A comma-separated list of protection values for secured sasl connections. Possible values are authentication, integrity and privacy. authentication means authentication only and no integrity or privacy; integrity implies authentication and integrity are enabled; and privacy implies all of authentication, integrity and privacy are enabled. hadoop.security.saslproperties.resolver.class can be used to override the hadoop.rpc.protection for a connection at the server side.",1,2,security-tradeoff,core,0,0
936,hadoop.rpc.socket.factory.class.ClientProtocol,"SocketFactory to use to connect to a DFS. If null or empty, use hadoop.rpc.socket.class.default. This socket factory is also used by DFSClient to create sockets to DataNodes.",0,0,others,core,0,0
937,hadoop.rpc.socket.factory.class.default,"Default SocketFactory to use. This parameter is expected to be formatted as ""package.FactoryClassName"".",0,0,others,core,0,0
938,hadoop.security.auth_to_local,Maps kerberos principals to local user names,0,0,others,core,0,1
939,hadoop.security.authentication,"Possible values are simple (no authentication), and kerberos",1,2,security-tradeoff,core,0,0
940,hadoop.security.authorization,Is service-level authorization enabled?,1,2,security-tradeoff,core,0,1
941,hadoop.security.credential.clear-text-fallback,true or false to indicate whether or not to fall back to storing credential password as clear text. The default value is true. This property only works when the password can't not be found from credential providers.,1,2,security-tradeoff,core,0,0
944,hadoop.security.crypto.buffer.size,The buffer size used by CryptoInputStream and CryptoOutputStream.,1,1,resource,core,0,0
945,hadoop.security.crypto.cipher.suite,Cipher suite for crypto codec.,1,2,security-tradeoff,core,0,0
946,hadoop.security.crypto.codec.classes.aes.ctr.nopadding,"Comma-separated list of crypto codec implementations for AES/CTR/NoPadding. The first implementation will be used if available, others are fallbacks.",0,0,others,core,0,1
947,hadoop.security.crypto.codec.classes.EXAMPLECIPHERSUITE,"The prefix for a given crypto codec, contains a comma-separated list of implementation classes for a given crypto codec (eg EXAMPLECIPHERSUITE). The first implementation will be used if available, others are fallbacks.",0,0,others,core,0,1
949,hadoop.security.crypto.jceks.key.serialfilter,"Enhanced KeyStore Mechanisms in JDK 8u171 introduced jceks.key.serialFilter. If jceks.key.serialFilter is configured, the JCEKS KeyStore uses it during the deserialization of the encrypted Key object stored inside a SecretKeyEntry. If jceks.key.serialFilter is not configured it will cause an error when recovering keystore file in KeyProviderFactory when recovering key from keystore file using JDK 8u171 or newer. The filter pattern uses the same format as jdk.serialFilter. The value of this property will be used as the following: 1. The value of jceks.key.serialFilter system property takes precedence over the value of this property. 2. In the absence of jceks.key.serialFilter system property the value of this property will be set as the value of jceks.key.serialFilter. 3. If the value of this property and jceks.key.serialFilter system property has not been set, org.apache.hadoop.crypto.key.KeyProvider sets a default value for jceks.key.serialFilter.",0,0,others,core,0,0
951,hadoop.security.dns.log-slow-lookups.enabled,Time name lookups (via SecurityUtil) and log them if they exceed the configured threshold.,1,5,workload-specific,core,0,0
952,hadoop.security.dns.log-slow-lookups.threshold.ms,"If slow lookup logging is enabled, this threshold is used to decide if a lookup is considered slow enough to be logged.",1,5,workload-specific,core,0,0
954,hadoop.security.group.mapping,"Class for user to group mapping (get groups for a given user) for ACL. The default implementation, org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, will determine if the Java Native Interface (JNI) is available. If JNI is available the implementation will use the API within hadoop to resolve a list of groups for a user. If JNI is not available then the shell implementation, ShellBasedUnixGroupsMapping, is used. This implementation shells out to the Linux/Unix environment with the bash -c groups command to resolve a list of groups for a user.",0,0,others,core,0,0
959,hadoop.security.group.mapping.ldap.connection.timeout.ms,"This property is the connection timeout (in milliseconds) for LDAP operations. If the LDAP provider doesn't establish a connection within the specified period, it will abort the connect attempt. Non-positive value means no LDAP connection timeout is specified in which case it waits for the connection to establish until the underlying network times out.",0,0,others,core,0,0
960,hadoop.security.group.mapping.ldap.directory.search.timeout,The attribute applied to the LDAP SearchControl properties to set a maximum time limit when searching and awaiting a result. Set to 0 if infinite wait period is desired. Default is 10 seconds. Units in milliseconds.,0,0,others,core,0,0
963,hadoop.security.group.mapping.ldap.posix.attr.uid.name,The attribute of posixAccount to use when groups for membership. Mostly useful for schemas wherein groups have memberUids that use an attribute other than uidNumber.,0,0,others,core,0,0
964,hadoop.security.group.mapping.ldap.read.timeout.ms,"This property is the read timeout (in milliseconds) for LDAP operations. If the LDAP provider doesn't get a LDAP response within the specified period, it will abort the read attempt. Non-positive value means no read timeout is specified in which case it waits for the response infinitely.",0,0,others,core,0,1
966,hadoop.security.group.mapping.ldap.search.attr.member,The attribute of the group object that identifies the users that are members of the group. The default will usually be appropriate for any LDAP installation.,0,0,others,core,0,0
969,hadoop.security.group.mapping.ldap.search.filter.user,"An additional filter to use when searching for LDAP users. The default will usually be appropriate for Active Directory installations. If connecting to an LDAP server with a non-AD schema, this should be replaced with (&(objectClass=inetOrgPerson)(uid={0}). {0} is a special string used to denote where the username fits into the filter. If the LDAP server supports posixGroups, Hadoop can enable the feature by setting the value of this property to ""posixAccount"" and the value of the hadoop.security.group.mapping.ldap.search.filter.group property to ""posixGroup"".",0,0,others,core,0,0
970,hadoop.security.group.mapping.ldap.search.group.hierarchy.levels,The number of levels to go up the group hierarchy when determining which groups a user is part of. 0 Will represent checking just the group that the user belongs to. Each additional level will raise the time it takes to execute a query by at most hadoop.security.group.mapping.ldap.directory.search.timeout. The default will usually be appropriate for all LDAP systems.,0,0,others,core,0,0
971,hadoop.security.group.mapping.ldap.ssl,Whether or not to use SSL when connecting to the LDAP server.,1,2,security-tradeoff,core,0,0
972,hadoop.security.group.mapping.ldap.ssl.keystore,File path to the SSL keystore that contains the SSL certificate required by the LDAP server.,0,0,others,core,0,0
973,hadoop.security.group.mapping.ldap.ssl.keystore.password,The password of the LDAP SSL keystore. this property name is used as an alias to get the password from credential providers. If the password can not be found and hadoop.security.credential.clear-text-fallback is true LDAPGroupsMapping uses the value of this property for password.,0,0,others,core,0,1
974,hadoop.security.group.mapping.ldap.ssl.keystore.password.file,"The path to a file containing the password of the LDAP SSL keystore. If the password is not configured in credential providers and the property hadoop.security.group.mapping.ldap.ssl.keystore.password is not set, LDAPGroupsMapping reads password from the file. IMPORTANT: This file should be readable only by the Unix user running the daemons and should be a local file.",0,0,others,core,0,0
975,hadoop.security.group.mapping.ldap.ssl.truststore,File path to the SSL truststore that contains the root certificate used to sign the LDAP server's certificate. Specify this if the LDAP server's certificate is not signed by a well known certificate authority.,0,0,others,core,0,0
976,hadoop.security.group.mapping.ldap.ssl.truststore.password.file,The path to a file containing the password of the LDAP SSL truststore. IMPORTANT: This file should be readable only by the Unix user running the daemons.,0,0,others,core,0,0
979,hadoop.security.group.mapping.providers,Comma separated of names of other providers to provide user to group mapping. Used by CompositeGroupsMapping.,0,0,others,core,0,0
980,hadoop.security.group.mapping.providers.combined,"true or false to indicate whether groups from the providers are combined or not. The default value is true. If true, then all the providers will be tried to get groups and all the groups are combined to return as the final results. Otherwise, providers are tried one by one in the configured list order, and if any groups are retrieved from any provider, then the groups will be returned without trying the left ones.",1,2,security-tradeoff,core,0,1
981,hadoop.security.groups.cache.background.reload,"Whether to reload expired user->group mappings using a background thread pool. If set to true, a pool of hadoop.security.groups.cache.background.reload.threads is created to update the cache in the background.",1,4,limited-side-effect,core,0,0
982,hadoop.security.groups.cache.background.reload.threads,Only relevant if hadoop.security.groups.cache.background.reload is true. Controls the number of concurrent background user->group cache entry refreshes. Pending refresh requests beyond this value are queued and processed when a thread is free.,1,1,resource,core,0,0
983,hadoop.security.groups.cache.secs,"This is the config controlling the validity of the entries in the cache containing the user->group mapping. When this duration has expired, then the implementation of the group mapping provider is invoked to get the groups of the user and then cached back.",1,5,workload-specific,core,0,0
984,hadoop.security.groups.cache.warn.after.ms,"If looking up a single user to group takes longer than this amount of milliseconds, we will log a warning message.",1,5,workload-specific,core,0,0
985,hadoop.security.groups.negative-cache.secs,"Expiration time for entries in the the negative user-to-group mapping caching, in seconds. This is useful when invalid users are retrying frequently. It is suggested to set a small value for this expiration, since a transient error in group lookup could temporarily lock out a legitimate user. Set this to zero or negative value to disable negative user-to-group caching.",1,5,workload-specific,core,0,1
986,hadoop.security.groups.shell.command.timeout,"Used by the ShellBasedUnixGroupsMapping class, this property controls how long to wait for the underlying shell command that is run to fetch groups. Expressed in seconds (e.g. 10s, 1m, etc.), if the running command takes longer than the value configured, the command is aborted and the groups resolver would return a result of no groups found. A value of 0s (default) would mean an infinite wait (i.e. wait until the command exits on its own).",0,0,others,core,0,0
987,hadoop.security.impersonation.provider.class,"A class which implements ImpersonationProvider interface, used to authorize whether one user can impersonate a specific user. If not specified, the DefaultImpersonationProvider will be used. If a class is specified, then that class will be used to determine the impersonation capability.",0,0,others,core,0,1
988,hadoop.security.instrumentation.requires.admin,"Indicates if administrator ACLs are required to access instrumentation servlets (JMX, METRICS, CONF, STACKS).",1,2,security-tradeoff,core,0,0
989,hadoop.security.java.secure.random.algorithm,The java secure random algorithm.,0,0,others,core,0,0
990,hadoop.security.key.provider.path,"The KeyProvider to use when managing zone keys, and interacting with encryption keys when reading and writing to an encryption zone. For hdfs clients, the provider path will be same as namenode's provider path.",0,0,others,core,0,0
991,hadoop.security.kms.client.authentication.retry-count,Number of time to retry connecting to KMS on authentication failure,0,0,others,core,0,0
992,hadoop.security.kms.client.encrypted.key.cache.expiry,"Cache expiry time for a Key, after which the cache Queue for this key will be dropped. Default = 12hrs",1,5,workload-specific,core,0,0
993,hadoop.security.kms.client.encrypted.key.cache.low-watermark,"If size of the EncryptedKeyVersion cache Queue falls below the low watermark, this cache queue will be scheduled for a refill",1,5,workload-specific,core,0,0
994,hadoop.security.kms.client.encrypted.key.cache.num.refill.threads,Number of threads to use for refilling depleted EncryptedKeyVersion cache Queues,1,1,resource,core,0,1
995,hadoop.security.kms.client.encrypted.key.cache.size,Size of the EncryptedKeyVersion cache Queue for each key,1,1,resource,core,0,0
996,hadoop.security.kms.client.failover.sleep.base.millis,"Expert only. The time to wait, in milliseconds, between failover attempts increases exponentially as a function of the number of attempts made so far, with a random factor of +/- 50%. This option specifies the base value used in the failover calculation. The first failover will retry immediately. The 2nd failover attempt will delay at least hadoop.security.client.failover.sleep.base.millis milliseconds. And so on.",0,0,others,core,0,0
997,hadoop.security.kms.client.failover.sleep.max.millis,"Expert only. The time to wait, in milliseconds, between failover attempts increases exponentially as a function of the number of attempts made so far, with a random factor of +/- 50%. This option specifies the maximum value to wait between failovers. Specifically, the time between two failover attempts will not exceed +/- 50% of hadoop.security.client.failover.sleep.max.millis milliseconds.",0,0,others,core,0,0
998,hadoop.security.kms.client.timeout,"Sets value for KMS client connection timeout, and the read timeout to KMS servers.",0,0,others,core,0,0
999,hadoop.security.random.device.file.path,OS security random device file path.,0,0,others,core,0,0
1000,hadoop.security.saslproperties.resolver.class,"SaslPropertiesResolver used to resolve the QOP used for a connection. If not specified, the full set of values specified in hadoop.rpc.protection is used while determining the QOP used for the connection. If a class is specified, then the QOP values returned by the class will be used while determining the QOP used for the connection.",0,0,others,core,0,0
1001,hadoop.security.secure.random.impl,Implementation of secure random.,0,0,others,core,0,0
1002,hadoop.security.sensitive-config-keys,"A comma-separated or multi-line list of regular expressions to match configuration keys that should be redacted where appropriate, for example, when logging modified properties during a reconfiguration, private credentials should not be logged.",0,0,others,core,0,1
1004,hadoop.security.uid.cache.secs,This is the config controlling the validity of the entries in the cache containing the userId to userName and groupId to groupName used by NativeIO getFstat().,1,5,workload-specific,core,0,0
1005,hadoop.service.shutdown.timeout,"Timeout to wait for each shutdown operation to complete. If a hook takes longer than this time to complete, it will be interrupted, so the service will shutdown. This allows the service shutdown to recover from a blocked operation. Some shutdown hooks may need more time than this, for example when a large amount of data needs to be uploaded to an object store. In this situation: increase the timeout. The minimum duration of the timeout is 1 second, ""1s"".",0,0,others,core,0,1
1006,hadoop.shell.missing.defaultFs.warning,Enable hdfs shell commands to display warnings if (fs.defaultFS) property is not set.,1,6,function-tradeoff,core,0,0
1007,hadoop.shell.safely.delete.limit.num.files,"Used by -safely option of hadoop fs shell -rm command to avoid accidental deletion of large directories. When enabled, the -rm command requires confirmation if the number of files to be deleted is greater than this limit. The default limit is 100 files. The warning is disabled if the limit is 0 or the -safely is not specified in -rm command.",0,0,others,core,0,0
1009,hadoop.ssl.client.conf,"Resource file from which ssl client keystore information will be extracted This file is looked up in the classpath, typically it should be in Hadoop conf/ directory.",0,0,others,core,0,0
1010,hadoop.ssl.enabled,Deprecated. Use dfs.http.policy and yarn.http.policy instead.,0,0,others,core,0,0
1011,hadoop.ssl.enabled.protocols,The supported SSL protocols.,0,0,others,core,0,1
1012,hadoop.ssl.hostname.verifier,The hostname verifier to provide for HttpsURLConnections.,1,2,security-tradeoff,core,0,0
1013,hadoop.ssl.keystores.factory.class,The keystores factory to use for retrieving certificates.,0,0,others,core,0,0
1014,hadoop.ssl.require.client.cert,Whether client certificates are required,1,2,security-tradeoff,core,0,0
1015,hadoop.ssl.server.conf,"Resource file from which ssl server keystore information will be extracted. This file is looked up in the classpath, typically it should be in Hadoop conf/ directory.",0,0,others,core,0,0
1017,hadoop.token.files,List of token cache files that have delegation tokens for hadoop service,0,0,others,core,0,0
1018,hadoop.user.group.static.mapping.overrides,"Static mapping of user to groups. This will override the groups if available in the system for the specified user. In other words, groups look-up will not happen for these users, instead groups mapped in this configuration will be used. Mapping should be in this format. user1=group1,group2;user2=;user3=group2; Default, ""dr.who=;"" will consider ""dr.who"" as user without groups.",0,0,others,core,0,0
1019,hadoop.util.hash.type,The default implementation of Hash. Currently this can take one of the two values: 'murmur' to select MurmurHash and 'jenkins' to select JenkinsHash.,0,0,others,core,0,1
1020,hadoop.workaround.non.threadsafe.getpwuid,"Some operating systems or authentication modules are known to have broken implementations of getpwuid_r and getpwgid_r, such that these calls are not thread-safe. Symptoms of this problem include JVM crashes with a stack trace inside these functions. If your system exhibits this issue, enable this configuration parameter to include a lock around the calls as a workaround. An incomplete list of some systems known to have this issue is available at https://wiki.apache.org/hadoop/KnownBrokenPwuidImplementations",0,0,others,core,0,0
1021,hadoop.zk.acl,ACL's to be used for ZooKeeper znodes.,0,0,others,core,0,0
1023,hadoop.zk.auth,"Specify the auths to be used for the ACL's specified in hadoop.zk.acl. This takes a comma-separated list of authentication mechanisms, each of the form 'scheme:auth' (the same syntax used for the 'addAuth' command in the ZK CLI).",1,2,security-tradeoff,core,0,1
1024,hadoop.zk.num-retries,Number of tries to connect to ZooKeeper.,0,0,others,core,0,0
1025,hadoop.zk.retry-interval-ms,Retry interval in milliseconds when connecting to ZooKeeper.,0,0,others,core,0,0
1026,io.bytes.per.checksum,The number of bytes per checksum. Must not be larger than io.file.buffer.size.,1,1,resource,core,0,0
1027,io.compression.codec.bzip2.library,"The native-code library to be used for compression and decompression by the bzip2 codec. This library could be specified either by by name or the full pathname. In the former case, the library is located by the dynamic linker, usually searching the directories specified in the environment variable LD_LIBRARY_PATH. The value of ""system-native"" indicates that the default system library should be used. To indicate that the algorithm should operate entirely in Java, specify ""java-builtin"".",1,6,function-tradeoff,core,0,0
1028,io.compression.codecs,"A comma-separated list of the compression codec classes that can be used for compression/decompression. In addition to any classes specified with this property (which take precedence), codec classes on the classpath are discovered using a Java ServiceLoader.",0,0,others,core,0,0
1029,io.file.buffer.size,"The size of buffer for use in sequence files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",1,1,resource,core,0,0
1031,io.map.index.skip,Number of index entries to skip between each entry. Zero by default. Setting this to values larger than zero can facilitate opening large MapFiles using less memory.,1,5,workload-specific,core,0,0
1032,io.mapfile.bloom.error.rate,"The rate of false positives in BloomFilter-s used in BloomMapFile. As this value decreases, the size of BloomFilter-s increases exponentially. This value is the probability of encountering false positives (default is 0.5%).",0,0,others,core,0,1
1033,io.mapfile.bloom.size,"The size of BloomFilter-s used in BloomMapFile. Each time this many keys is appended the next BloomFilter will be created (inside a DynamicBloomFilter). Larger values minimize the number of filters, which slightly increases the performance, but may waste too much space if the total number of keys is usually much smaller than this number.",1,1,resource,core,0,0
1034,io.native.lib.available,Controls whether to use native libraries for bz2 and zlib compression codecs or not. The property does not control any other native libraries.,1,6,function-tradeoff,core,0,1
1035,io.seqfile.compress.blocksize,The minimum block size for compression in block compressed SequenceFiles.,1,5,workload-specific,core,0,1
1036,io.seqfile.local.dir,The local directory where sequence file stores intermediate data files during merge. May be a comma-separated list of directories on different devices in order to spread disk i/o. Directories that do not exist are ignored.,0,0,others,core,0,0
1037,io.serializations,A list of serialization classes that can be used for obtaining serializers and deserializers.,0,0,others,core,0,0
1039,ipc.client.connect.max.retries,Indicates the number of retries a client will make to establish a server connection.,0,0,others,core,0,0
1040,ipc.client.connect.max.retries.on.timeouts,Indicates the number of retries a client will make on socket timeout to establish a server connection.,0,0,others,core,0,0
1041,ipc.client.connect.retry.interval,Indicates the number of milliseconds a client will wait for before retrying to establish a server connection.,0,0,others,core,0,0
1042,ipc.client.connect.timeout,Indicates the number of milliseconds a client will wait for the socket to establish a server connection.,0,0,others,core,0,0
1044,ipc.client.fallback-to-simple-auth-allowed,"When a client is configured to attempt a secure connection, but attempts to connect to an insecure server, that server may instruct the client to switch to SASL SIMPLE (unsecure) authentication. This setting controls whether or not the client will accept this instruction from the server. When false (the default), the client will not allow the fallback to SIMPLE authentication, and will abort the connection.",1,2,security-tradeoff,core,0,0
1045,ipc.client.idlethreshold,Defines the threshold number of connections after which connections will be inspected for idleness.,1,5,workload-specific,core,0,1
1046,ipc.client.kill.max,Defines the maximum number of clients to disconnect in one go.,0,0,others,core,0,0
1047,ipc.client.low-latency,Use low-latency QoS markers for IPC connections.,1,4,limited-side-effect,core,0,0
1048,ipc.client.ping,"Send a ping to the server when timeout on reading the response, if set to true. If no failure is detected, the client retries until at least a byte is read or the time given by ipc.client.rpc-timeout.ms is passed.",1,5,workload-specific,core,0,1
1049,ipc.client.rpc-timeout.ms,"Timeout on waiting response from server, in milliseconds. If ipc.client.ping is set to true and this rpc-timeout is greater than the value of ipc.ping.interval, the effective value of the rpc-timeout is rounded up to multiple of ipc.ping.interval.",0,0,others,core,0,1
1050,ipc.client.tcpnodelay,Use TCP_NODELAY flag to bypass Nagle's algorithm transmission delays.,1,4,limited-side-effect,core,0,0
1052,ipc.maximum.response.length,This indicates the maximum IPC message length (bytes) that can be accepted by the client. Messages larger than this value are rejected immediately to avoid possible OOMs. This setting should rarely need to be changed. Set to 0 to disable.,0,0,others,core,0,0
1054,ipc.server.listen.queue.size,Indicates the length of the listen queue for servers accepting client connections.,0,0,others,core,0,0
1055,ipc.server.log.slow.rpc,This setting is useful to troubleshoot performance issues for various services. If this value is set to true then we log requests that fall into 99th percentile as well as increment RpcSlowCalls counter.,1,6,function-tradeoff,core,0,0
1056,ipc.server.max.connections,"The maximum number of concurrent connections a server is allowed to accept. If this limit is exceeded, incoming connections will first fill the listen queue and then may go to an OS-specific listen overflow queue. The client may fail or timeout, but the server can avoid running out of file descriptors using this feature. 0 means no limit.",1,1,resource,core,0,1
1057,net.topology.impl,The default implementation of NetworkTopology which is classic three layer one.,0,0,others,core,0,0
1060,net.topology.script.number.args,The max number of args that the script configured with net.topology.script.file.name should be run with. Each arg is an IP address.,0,0,others,core,0,0
1062,nfs.exports.allowed.hosts,"By default, the export can be mounted by any client. The value string contains machine name and access privilege, separated by whitespace characters. The machine name format can be a single host, a Java regular expression, or an IPv4 address. The access privilege uses rw or ro to specify read/write or read-only access of the machines to exports. If the access privilege is not provided, the default is read-only. Entries are separated by "";"". For example: ""192.168.0.0/22 rw ; host.*\.example\.com ; host1.test.org ro;"". Only the NFS gateway needs to restart after this property is updated.",0,0,others,core,0,0
1063,rpc.metrics.percentiles.intervals,A comma-separated list of the granularity in seconds for the metrics which describe the 50/75/90/95/99th percentile latency for rpc queue/processing time. The metrics are outputted if rpc.metrics.quantile.enable is set to true.,0,0,others,core,0,0
1064,rpc.metrics.quantile.enable,"Setting this property to true and rpc.metrics.percentiles.intervals to a comma-separated list of the granularity in seconds, the 50/75/90/95/99th percentile latency for rpc queue/processing time in milliseconds are added to rpc metrics.",0,0,others,core,0,0
1065,s3.blocksize,Block size,1,5,workload-specific,core,0,0
1066,s3.bytes-per-checksum,The number of bytes per checksum. Must not be larger than s3.stream-buffer-size,0,0,others,core,0,0
1067,s3.client-write-packet-size,Packet size for clients to write,0,0,others,core,0,0
1068,s3.replication,Replication factor,1,3,reliability-tradeoff,core,0,0
1069,s3.stream-buffer-size,"The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",1,1,resource,core,0,0
1070,s3native.blocksize,Block size,1,5,workload-specific,core,0,0
1071,s3native.bytes-per-checksum,The number of bytes per checksum. Must not be larger than s3native.stream-buffer-size,0,0,others,core,0,0
1072,s3native.client-write-packet-size,Packet size for clients to write,0,0,others,core,0,0
1073,s3native.replication,Replication factor,1,3,reliability-tradeoff,core,0,0
1074,s3native.stream-buffer-size,"The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",1,1,resource,core,0,0
1075,seq.io.sort.factor,The number of streams to merge at once while sorting files using SequenceFile.Sorter. This determines the number of open file handles.,0,0,others,core,0,1
1076,seq.io.sort.mb,"The total amount of buffer memory to use while sorting files, while using SequenceFile.Sorter, in megabytes. By default, gives each merge stream 1MB, which should minimize seeks.",1,1,resource,core,0,0
1077,tfile.fs.input.buffer.size,Buffer size used for FSDataInputStream in bytes.,1,1,resource,core,0,1
1078,tfile.fs.output.buffer.size,Buffer size used for FSDataOutputStream in bytes.,1,1,resource,core,0,0
1079,tfile.io.chunk.size,Value chunk size in bytes. Default to 1MB. Values of the length less than the chunk size is guaranteed to have known value length in read time (See also TFile.Reader.Scanner.Entry.isValueLengthKnown()).,0,0,others,core,0,0
1080,akka.ask.callstack,"If true, call stack for asynchronous asks are captured. That way, when an ask fails (for example times out), you get a proper exception, describing to the original method call and call site. Note that in case of having millions of concurrent RPC calls, this may add to the memory footprint.",1,6,function-tradeoff,flink,0,0
1081,akka.ask.timeout,Timeout used for all futures and blocking Akka calls. If Flink fails due to timeouts then you should try to increase this value. Timeouts can be caused by slow machines or a congested network. The timeout value requires a time-unit specifier (ms/s/min/h/d).,0,0,others,flink,0,1
1082,akka.client-socket-worker-pool.pool-size-factor,The pool size factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the pool-size-min and pool-size-max values.,1,1,resource,flink,0,0
1083,akka.client-socket-worker-pool.pool-size-max,Max number of threads to cap factor-based number to.,1,1,resource,flink,0,0
1084,akka.client-socket-worker-pool.pool-size-min,Min number of threads to cap factor-based number to.,1,1,resource,flink,0,0
1085,akka.fork-join-executor.parallelism-factor,The parallelism factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the parallelism-min and parallelism-max values.,1,1,resource,flink,0,0
1086,akka.fork-join-executor.parallelism-max,Max number of threads to cap factor-based parallelism number to.,1,1,resource,flink,0,0
1087,akka.fork-join-executor.parallelism-min,Min number of threads to cap factor-based parallelism number to.,1,1,resource,flink,0,0
1088,akka.framesize,"Maximum size of messages which are sent between the JobManager and the TaskManagers. If Flink fails because messages exceed this limit, then you should increase it. The message size requires a size-unit specifier.",1,5,workload-specific,flink,0,1
1089,akka.jvm-exit-on-fatal-error,Exit JVM on fatal Akka errors.,0,0,others,flink,0,0
1090,akka.log.lifecycle.events,Turns on the Akka's remote logging of events. Set this value to 'true' in case of debugging.,1,6,function-tradeoff,flink,0,0
1091,akka.lookup.timeout,Timeout used for the lookup of the JobManager. The timeout value has to contain a time-unit specifier (ms/s/min/h/d).,0,0,others,flink,0,0
1092,akka.retry-gate-closed-for,Milliseconds a gate should be closed for after a remote connection was disconnected.,0,0,others,flink,0,0
1093,akka.server-socket-worker-pool.pool-size-factor,The pool size factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the pool-size-min and pool-size-max values.,1,1,resource,flink,0,0
1094,akka.server-socket-worker-pool.pool-size-max,Max number of threads to cap factor-based number to.,1,1,resource,flink,0,0
1095,akka.server-socket-worker-pool.pool-size-min,Min number of threads to cap factor-based number to.,1,1,resource,flink,0,1
1096,akka.ssl.enabled,Turns on SSL for Akka's remote communication. This is applicable only when the global ssl flag security.ssl.enabled is set to true.,0,0,others,flink,0,0
1097,akka.startup-timeout,Timeout after which the startup of a remote component is considered being failed.,0,0,others,flink,0,1
1098,akka.tcp.timeout,"Timeout for all outbound connections. If you should experience problems with connecting to a TaskManager due to a slow network, you should increase this value.",0,0,others,flink,0,0
1099,akka.throughput,Number of messages that are processed in a batch before returning the thread to the pool. Low values denote a fair scheduling whereas high values can increase the performance at the cost of unfairness.,1,1,resource,flink,0,0
1100,akka.transport.heartbeat.interval,"Heartbeat interval for Akka's transport failure detector. Since Flink uses TCP, the detector is not necessary. Therefore, the detector is disabled by setting the interval to a very high value. In case you should need the transport failure detector, set the interval to some reasonable value. The interval value requires a time-unit specifier (ms/s/min/h/d).",0,0,others,flink,0,0
1101,akka.transport.heartbeat.pause,"Acceptable heartbeat pause for Akka's transport failure detector. Since Flink uses TCP, the detector is not necessary. Therefore, the detector is disabled by setting the pause to a very high value. In case you should need the transport failure detector, set the pause to some reasonable value. The pause value requires a time-unit specifier (ms/s/min/h/d).",0,0,others,flink,0,0
1102,akka.transport.threshold,"Threshold for the transport failure detector. Since Flink uses TCP, the detector is not necessary and, thus, the threshold is set to a high value.",1,5,workload-specific,flink,0,0
1103,blob.client.connect.timeout,The connection timeout in milliseconds for the blob client.,0,0,others,flink,0,0
1104,blob.client.socket.timeout,The socket timeout in milliseconds for the blob client.,0,0,others,flink,0,0
1105,blob.fetch.backlog,The config parameter defining the backlog of BLOB fetches on the JobManager.,1,5,workload-specific,flink,0,0
1106,blob.fetch.num-concurrent,The config parameter defining the maximum number of concurrent BLOB fetches that the JobManager serves.,1,5,workload-specific,flink,0,0
1107,blob.fetch.retries,The config parameter defining number of retires for failed BLOB fetches.,0,0,others,flink,0,0
1108,blob.offload.minsize,The minimum size for messages to be offloaded to the BlobServer.,1,5,workload-specific,flink,0,0
1110,blob.service.cleanup.interval,Cleanup interval of the blob caches at the task managers (in seconds).,0,0,others,flink,0,0
1111,blob.service.ssl.enabled,Flag to override ssl support for the blob service transport.,1,2,security-tradeoff,flink,0,1
1113,classloader.check-leaked-classloader,"Fails attempts at loading classes if the user classloader of a job is used after it has terminated. This is usually caused by the classloader being leaked by lingering threads or misbehaving libraries, which may also result in the classloader being used by other jobs. This check should only be disabled if such a leak prevents further jobs from running.",0,0,others,flink,0,0
1114,classloader.fail-on-metaspace-oom-error,Fail Flink JVM processes if 'OutOfMemoryError: Metaspace' is thrown while trying to load a user code class.,0,0,others,flink,0,0
1115,classloader.parent-first-patterns.additional,"A (semicolon-separated) list of patterns that specifies which classes should always be resolved through the parent ClassLoader first. A pattern is a simple prefix that is checked against the fully qualified class name. These patterns are appended to ""classloader.parent-first-patterns.default"".",0,0,others,flink,0,0
1116,classloader.parent-first-patterns.default,"A (semicolon-separated) list of patterns that specifies which classes should always be resolved through the parent ClassLoader first. A pattern is a simple prefix that is checked against the fully qualified class name. This setting should generally not be modified. To add another pattern we recommend to use ""classloader.parent-first-patterns.additional"" instead.",0,0,others,flink,0,1
1117,classloader.resolve-order,"Defines the class resolution strategy when loading classes from user code, meaning whether to first check the user code jar (""child-first"") or the application classpath (""parent-first""). The default settings indicate to load classes first from the user code jar, which means that user code jars can include and load different dependencies than Flink uses (transitively).",0,0,others,flink,0,1
1118,client.retry-period,"The interval (in ms) between consecutive retries of failed attempts to execute commands through the CLI or Flink's clients, wherever retry is supported (default 2sec).",0,0,others,flink,0,0
1119,client.timeout,Timeout on the client side.,0,0,others,flink,0,0
1120,cluster.evenly-spread-out-slots,Enable the slot spread out allocation strategy. This strategy tries to spread out the slots evenly across all available TaskExecutors.,1,6,function-tradeoff,flink,0,0
1121,cluster.io-pool.size,The size of the IO executor pool used by the cluster to execute blocking IO operations (Master as well as TaskManager processes). By default it will use 4 * the number of CPU cores (hardware contexts) that the cluster process has access to. Increasing the pool size allows to run more IO operations concurrently.,1,1,resource,flink,0,0
1122,cluster.processes.halt-on-fatal-error,"Whether processes should halt on fatal errors instead of performing a graceful shutdown. In some environments (e.g. Java 8 with the G1 garbage collector), a regular graceful shutdown can lead to a JVM deadlock. See FLINK-16510 for details.",1,6,function-tradeoff,flink,0,0
1123,cluster.registration.error-delay,The pause made after an registration attempt caused an exception (other than timeout) in milliseconds.,0,0,others,flink,0,0
1124,cluster.registration.initial-timeout,Initial registration timeout between cluster components in milliseconds.,0,0,others,flink,0,0
1125,cluster.registration.max-timeout,Maximum registration timeout between cluster components in milliseconds.,0,0,others,flink,0,0
1126,cluster.registration.refused-registration-delay,The pause made after the registration attempt was refused in milliseconds.,0,0,others,flink,0,0
1127,cluster.services.shutdown-timeout,The shutdown timeout for cluster services like executors in milliseconds.,0,0,others,flink,0,0
1128,compiler.delimited-informat.max-line-samples,The maximum number of line samples taken by the compiler for delimited inputs. The samples are used to estimate the number of records. This value can be overridden for a specific input with the input format's parameters.,1,5,workload-specific,flink,0,0
1129,compiler.delimited-informat.max-sample-len,"The maximal length of a line sample that the compiler takes for delimited inputs. If the length of a single sample exceeds this value (possible because of misconfiguration of the parser), the sampling aborts. This value can be overridden for a specific input with the input format's parameters.",1,5,workload-specific,flink,0,1
1130,compiler.delimited-informat.min-line-samples,The minimum number of line samples taken by the compiler for delimited inputs. The samples are used to estimate the number of records. This value can be overridden for a specific input with the input format's parameters,1,5,workload-specific,flink,0,1
1133,env.java.opts,Java options to start the JVM of all Flink processes with.,0,0,others,flink,0,0
1134,env.java.opts.client,Java options to start the JVM of the Flink Client with.,0,0,others,flink,0,0
1135,env.java.opts.historyserver,Java options to start the JVM of the HistoryServer with.,0,0,others,flink,0,0
1136,env.java.opts.jobmanager,Java options to start the JVM of the JobManager with.,0,0,others,flink,0,0
1137,env.java.opts.taskmanager,Java options to start the JVM of the TaskManager with.,0,0,others,flink,0,0
1138,env.log.dir,Defines the directory where the Flink logs are saved. It has to be an absolute path. (Defaults to the log directory under Flink's home),0,0,others,flink,0,0
1139,env.log.max,The maximum number of old log files to keep.,1,5,workload-specific,flink,0,0
1140,env.ssh.opts,"Additional command line options passed to SSH clients when starting or stopping JobManager, TaskManager, and Zookeeper services (start-cluster.sh, stop-cluster.sh, start-zookeeper-quorum.sh, stop-zookeeper-quorum.sh).",0,0,others,flink,0,1
1142,execution.attached,Specifies if the pipeline is submitted in attached or detached mode.,1,6,function-tradeoff,flink,0,0
1143,execution.buffer-timeout,"The maximum time frequency (milliseconds) for the flushing of the output buffers. By default the output buffers flush frequently to provide low latency and to aid smooth developer experience. Setting the parameter can result in three logical modes: A positive value triggers flushing periodically by that interval, 0 triggers flushing after every record thus minimizing latency, -1 ms triggers flushing only when the output buffer is full thus maximizing throughput",0,0,others,flink,0,1
1144,execution.checkpointing.externalized-checkpoint-retention,"Externalized checkpoints write their meta data out to persistent storage and are not automatically cleaned up when the owning job fails or is suspended (terminating with job status JobStatus#FAILED or JobStatus#SUSPENDED. In this case, you have to manually clean up the checkpoint state, both the meta data and actual program state. The mode defines how an externalized checkpoint should be cleaned up on job cancellation. If you choose to retain externalized checkpoints on cancellation you have to handle checkpoint clean up manually when you cancel the job as well (terminating with job status JobStatus#CANCELED). The target directory for externalized checkpoints is configured via state.checkpoints.dir.",0,0,others,flink,0,0
1145,execution.checkpointing.interval,Gets the interval in which checkpoints are periodically scheduled. This setting defines the base interval. Checkpoint triggering may be delayed by the settings execution.checkpointing.max-concurrent-checkpoints and execution.checkpointing.min-pause,0,0,others,flink,0,0
1146,execution.checkpointing.max-concurrent-checkpoints,"The maximum number of checkpoint attempts that may be in progress at the same time. If this value is n, then no checkpoints will be triggered while n checkpoint attempts are currently in flight. For the next checkpoint to be triggered, one checkpoint attempt would need to finish or expire.",1,5,workload-specific,flink,0,0
1147,execution.checkpointing.min-pause,"The minimal pause between checkpointing attempts. This setting defines how soon thecheckpoint coordinator may trigger another checkpoint after it becomes possible to triggeranother checkpoint with respect to the maximum number of concurrent checkpoints(see execution.checkpointing.max-concurrent-checkpoints). If the maximum number of concurrent checkpoints is set to one, this setting makes effectively sure that a minimum amount of time passes where no checkpoint is in progress at all.",0,0,others,flink,0,0
1148,execution.checkpointing.mode,The checkpointing mode (exactly-once vs. at-least-once).,1,6,function-tradeoff,flink,0,1
1149,execution.checkpointing.prefer-checkpoint-for-recovery,"If enabled, a job recovery should fallback to checkpoint when there is a more recent savepoint.",1,6,function-tradeoff,flink,0,0
1150,execution.checkpointing.snapshot-compression,Tells if we should use compression for the state snapshot data or not,1,4,limited-side-effect,flink,0,0
1151,execution.checkpointing.timeout,The maximum time that a checkpoint may take before being discarded.,0,0,others,flink,0,0
1152,execution.checkpointing.tolerable-failed-checkpoints,"The tolerable checkpoint failure number. If set to 0, that meanswe do not tolerance any checkpoint failure.",0,0,others,flink,0,0
1153,execution.checkpointing.unaligned,"Enables unaligned checkpoints, which greatly reduce checkpointing times under backpressure. Unaligned checkpoints contain data stored in buffers as part of the checkpoint state, which allows checkpoint barriers to overtake these buffers. Thus, the checkpoint duration becomes independent of the current throughput as checkpoint barriers are effectively not embedded into the stream of data anymore. Unaligned checkpoints can only be enabled if execution.checkpointing.mode is EXACTLY_ONCE and if execution.checkpointing.max-concurrent-checkpoints is 1",1,6,function-tradeoff,flink,0,0
1154,execution.job-listeners,Custom JobListeners to be registered with the execution environment. The registered listeners cannot have constructors with arguments.,0,0,others,flink,0,1
1155,execution.runtime-mode,"Runtime execution mode of DataStream programs. Among other things, this controls task scheduling, network shuffle behavior, and time semantics.",0,0,others,flink,0,1
1156,execution.savepoint.ignore-unclaimed-state,Allow to skip savepoint state that cannot be restored. Allow this if you removed an operator from your pipeline after the savepoint was triggered.,1,6,function-tradeoff,flink,0,1
1158,execution.shutdown-on-attached-exit,"If the job is submitted in attached mode, perform a best-effort cluster shutdown when the CLI is terminated abruptly, e.g., in response to a user interrupt, such as typing Ctrl + C.",1,6,function-tradeoff,flink,0,0
1159,execution.target,"The deployment target for the execution. This can take one of the following values: remote, local, yarn-per-job, yarn-session, kubernetes-session",0,0,others,flink,0,0
1160,external-resource.resource_name.kubernetes.config-key,"If configured, Flink will add ""resources.limits.<config-key>"" and ""resources.requests.<config-key>"" to the main container of TaskExecutor and set the value to the value of external-resource.<resource_name>.amount.",0,0,others,flink,0,0
1161,external-resource.resource_name.yarn.config-key,"If configured, Flink will add this key to the resource profile of container request to Yarn. The value will be set to the value of external-resource.<resource_name>.amount.",0,0,others,flink,0,0
1164,fs.output.always-create-directory,"File writers running with a parallelism larger than one create a directory for the output file path and put the different result files (one per parallel writer task) into that directory. If this option is set to ""true"", writers with a parallelism of 1 will also create a directory and place a single result file into it. If the option is set to ""false"", the writer will directly create the file directly at the output path, without creating a containing directory.",0,0,others,flink,0,0
1166,heartbeat.interval,Time interval for requesting heartbeat from sender side.,0,0,others,flink,0,0
1167,heartbeat.timeout,Timeout for requesting and receiving heartbeat for both sender and receiver sides.,0,0,others,flink,0,0
1168,high-availability,"Defines high-availability mode used for the cluster execution. To enable high-availability, set this mode to ""ZOOKEEPER"" or specify FQN of factory class.",0,0,others,flink,0,0
1175,high-availability.zookeeper.client.acl,Defines the ACL (open|creator) to be configured on ZK node. The configuration value can be set to 'creator' if the ZooKeeper server configuration has the 'authProvider' property mapped to use SASLAuthenticationProvider and the cluster is configured to run in secure mode (Kerberos).,0,0,others,flink,0,0
1176,high-availability.zookeeper.client.connection-timeout,Defines the connection timeout for ZooKeeper in ms.,0,0,others,flink,0,0
1177,high-availability.zookeeper.client.max-retry-attempts,Defines the number of connection retries before the client gives up.,0,0,others,flink,0,1
1178,high-availability.zookeeper.client.retry-wait,Defines the pause between consecutive retries in ms.,0,0,others,flink,0,1
1179,high-availability.zookeeper.client.session-timeout,Defines the session timeout for the ZooKeeper session in ms.,0,0,others,flink,0,0
1180,high-availability.zookeeper.path.checkpoint-counter,ZooKeeper root path (ZNode) for checkpoint counters.,0,0,others,flink,0,1
1181,high-availability.zookeeper.path.checkpoints,ZooKeeper root path (ZNode) for completed checkpoints.,0,0,others,flink,0,0
1182,high-availability.zookeeper.path.latch,Defines the znode of the leader latch which is used to elect the leader.,0,0,others,flink,0,0
1186,high-availability.zookeeper.quorum,"The ZooKeeper quorum to use, when running Flink in a high-availability mode with ZooKeeper.",0,0,others,flink,0,0
1187,historyserver.archive.clean-expired-jobs,Whether HistoryServer should cleanup jobs that are no longer present `historyserver.archive.fs.dir`.,1,6,function-tradeoff,flink,0,0
1189,historyserver.archive.fs.refresh-interval,Interval in milliseconds for refreshing the archived job directories.,0,0,others,flink,0,0
1190,historyserver.archive.retained-jobs,"The maximum number of jobs to retain in each archive directory defined by `historyserver.archive.fs.dir`. If set to `-1`(default), there is no limit to the number of archives. If set to `0` or less than `-1` HistoryServer will throw an IllegalConfigurationException.",0,0,others,flink,0,0
1193,historyserver.web.refresh-interval,The refresh interval for the HistoryServer web-frontend in milliseconds.,0,0,others,flink,0,0
1194,historyserver.web.ssl.enabled,Enable HTTPs access to the HistoryServer web frontend. This is applicable only when the global SSL flag security.ssl.enabled is set to true.,1,6,function-tradeoff,flink,0,0
1198,jobmanager.archive.fs.dir,Dictionary for JobManager to store the archives of completed jobs.,0,0,others,flink,0,0
1199,jobmanager.execution.attempts-history-size,The maximum number of prior execution attempts kept in history.,1,5,workload-specific,flink,0,0
1200,jobmanager.execution.failover-strategy,This option specifies how the job computation recovers from task failures.,0,0,others,flink,0,0
1201,jobmanager.memory.enable-jvm-direct-memory-limit,Whether to enable the JVM direct memory limit of the JobManager process (-XX:MaxDirectMemorySize). The limit will be set to the value of 'jobmanager.memory.off-heap.size' option.,1,6,function-tradeoff,flink,0,0
1202,jobmanager.memory.flink.size,"Total Flink Memory size for the JobManager. This includes all the memory that a JobManager consumes, except for JVM Metaspace and JVM Overhead. It consists of JVM Heap Memory and Off-heap Memory. See also 'jobmanager.memory.process.size' for total process memory size configuration.",1,1,resource,flink,0,0
1203,jobmanager.memory.heap.size,JVM Heap Memory size for JobManager. The minimum recommended JVM Heap size is 128.000mb (134217728 bytes).,1,1,resource,flink,0,0
1204,jobmanager.memory.jvm-metaspace.size,JVM Metaspace Size for the JobManager.,1,1,resource,flink,0,0
1205,jobmanager.memory.jvm-overhead.fraction,"Fraction of Total Process Memory to be reserved for JVM Overhead. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value.",1,1,resource,flink,0,0
1206,jobmanager.memory.jvm-overhead.max,"Max JVM Overhead size for the JobManager. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value.",1,1,resource,flink,0,0
1207,jobmanager.memory.jvm-overhead.min,"Min JVM Overhead size for the JobManager. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value.",1,1,resource,flink,0,0
1208,jobmanager.memory.off-heap.size,Off-heap Memory size for JobManager. This option covers all off-heap memory usage including direct and native memory allocation. The JVM direct memory limit of the JobManager process (-XX:MaxDirectMemorySize) will be set to this value if the limit is enabled by 'jobmanager.memory.enable-jvm-direct-memory-limit'.,1,1,resource,flink,0,1
1209,jobmanager.memory.process.size,"Total Process Memory size for the JobManager. This includes all the memory that a JobManager JVM process consumes, consisting of Total Flink Memory, JVM Metaspace, and JVM Overhead. In containerized setups, this should be set to the container memory. See also 'jobmanager.memory.flink.size' for Total Flink Memory size configuration.",1,1,resource,flink,0,0
1210,jobmanager.retrieve-taskmanager-hostname,"Flag indicating whether JobManager would retrieve canonical host name of TaskManager during registration. If the option is set to ""false"", TaskManager registration with JobManager could be faster, since no reverse DNS lookup is performed. However, local input split assignment (such as for HDFS files) may be impacted.",1,6,function-tradeoff,flink,0,0
1211,jobmanager.rpc.address,"The config parameter defining the network address to connect to for communication with the job manager. This value is only interpreted in setups where a single JobManager with static name or address exists (simple standalone setups, or container setups with dynamic service name resolution). It is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers.",0,0,others,flink,0,0
1212,jobmanager.rpc.port,"The config parameter defining the network port to connect to for communication with the job manager. Like jobmanager.rpc.address, this value is only interpreted in setups where a single JobManager with static name/address and port exists (simple standalone setups, or container setups with dynamic service name resolution). This config option is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers.",0,0,others,flink,0,0
1213,jobstore.cache-size,The job store cache size in bytes which is used to keep completed jobs in memory.,1,1,resource,flink,0,0
1214,jobstore.expiration-time,The time in seconds after which a completed job expires and is purged from the job store.,0,0,others,flink,0,0
1215,jobstore.max-capacity,The max number of completed jobs that can be kept in the job store.,1,5,workload-specific,flink,0,0
1218,kubernetes.container.image,Image to use for Flink containers. The specified image must be based upon the same Apache Flink and Scala versions as used by the application. Visit https://hub.docker.com/_/flink tab=tags for the official docker images provided by the Flink project. The Flink project also publishes docker images here: https://hub.docker.com/r/apache/flink,0,0,others,flink,0,1
1219,kubernetes.container.image.pull-policy,The Kubernetes container image pull policy (IfNotPresent or Always or Never). The default policy is IfNotPresent to avoid putting pressure to image repository.,1,4,limited-side-effect,flink,0,0
1220,kubernetes.container.image.pull-secrets,A semicolon-separated list of the Kubernetes secrets used to access private image registries.,0,0,others,flink,0,0
1221,kubernetes.container-start-command-template,Template for the kubernetes jobmanager and taskmanager container start invocation.,0,0,others,flink,0,0
1223,kubernetes.entry.path,The entrypoint script of kubernetes in the image. It will be used as command for jobmanager and taskmanager container.,0,0,others,flink,0,0
1224,kubernetes.env.secretKeyRef,"The user-specified secrets to set env variables in Flink container. The value should be in the form of env:FOO_ENV,secret:foo_secret,key:foo_key;env:BAR_ENV,secret:bar_secret,key:bar_key.",0,0,others,flink,0,0
1226,kubernetes.flink.log.dir,The directory that logs of jobmanager and taskmanager be saved in the pod.,0,0,others,flink,0,0
1228,kubernetes.jobmanager.annotations,"The user-specified annotations that are set to the JobManager pod. The value could be in the form of a1:v1,a2:v2",0,0,others,flink,0,1
1229,kubernetes.jobmanager.cpu,The number of cpu used by job manager,1,1,resource,flink,0,1
1230,kubernetes.jobmanager.labels,"The labels to be set for JobManager pod. Specified as key:value pairs separated by commas. For example, version:alphav1,deploy:test.",0,0,others,flink,0,0
1231,kubernetes.jobmanager.node-selector,"The node selector to be set for JobManager pod. Specified as key:value pairs separated by commas. For example, environment:production,disk:ssd.",0,0,others,flink,0,0
1234,kubernetes.jobmanager.tolerations,"The user-specified tolerations to be set to the JobManager pod. The value should be in the form of key:key1,operator:Equal,value:value1,effect:NoSchedule;key:key2,operator:Exists,effect:NoExecute,tolerationSeconds:6000",0,0,others,flink,0,0
1235,kubernetes.namespace,The namespace that will be used for running the jobmanager and taskmanager pods.,0,0,others,flink,0,0
1236,kubernetes.rest-service.annotations,"The user-specified annotations that are set to the rest Service. The value should be in the form of a1:v1,a2:v2",0,0,others,flink,0,1
1238,kubernetes.secrets,"The user-specified secrets that will be mounted into Flink container. The value should be in the form of foo:/opt/secrets-foo,bar:/opt/secrets-bar.",0,0,others,flink,0,0
1239,kubernetes.service-account,Service account that is used by jobmanager and taskmanager within kubernetes cluster. Notice that this can be overwritten by config options 'kubernetes.jobmanager.service-account' and 'kubernetes.taskmanager.service-account' for jobmanager and taskmanager respectively.,0,0,others,flink,0,0
1240,kubernetes.taskmanager.annotations,"The user-specified annotations that are set to the TaskManager pod. The value could be in the form of a1:v1,a2:v2",0,0,others,flink,0,0
1241,kubernetes.taskmanager.cpu,"The number of cpu used by task manager. By default, the cpu is set to the number of slots per TaskManager",1,1,resource,flink,0,0
1242,kubernetes.taskmanager.labels,"The labels to be set for TaskManager pods. Specified as key:value pairs separated by commas. For example, version:alphav1,deploy:test.",0,0,others,flink,0,1
1243,kubernetes.taskmanager.node-selector,"The node selector to be set for TaskManager pods. Specified as key:value pairs separated by commas. For example, environment:production,disk:ssd.",0,0,others,flink,0,0
1245,kubernetes.taskmanager.tolerations,"The user-specified tolerations to be set to the TaskManager pod. The value should be in the form of key:key1,operator:Equal,value:value1,effect:NoSchedule;key:key2,operator:Exists,effect:NoExecute,tolerationSeconds:6000",0,0,others,flink,0,0
1246,kubernetes.transactional-operation.max-retries,"Defines the number of Kubernetes transactional operation retries before the client gives up. For example, FlinkKubeClient#checkAndUpdateConfigMap.",0,0,others,flink,0,0
1247,mesos.constraints.hard.hostattribute,"Constraints for task placement on Mesos based on agent attributes. Takes a comma-separated list of key:value pairs corresponding to the attributes exposed by the target mesos agents. Example: az:eu-west-1a,series:t2",0,0,others,flink,0,0
1248,mesos.failover-timeout,"The failover timeout in seconds for the Mesos scheduler, after which running tasks are automatically shut down.",0,0,others,flink,0,0
1251,mesos.resourcemanager.artifactserver.ssl.enabled,Enables SSL for the Flink artifact server. Note that security.ssl.enabled also needs to be set to true encryption to enable encryption.,1,2,security-tradeoff,flink,0,1
1252,mesos.resourcemanager.declined-offer-refuse-duration,Amount of time to ask the Mesos master to not resend a declined resource offer again. This ensures a declined resource offer isn't resent immediately after being declined,0,0,others,flink,0,0
1254,mesos.resourcemanager.framework.principal,Mesos framework principal,0,0,others,flink,0,0
1255,mesos.resourcemanager.framework.role,Mesos framework role definition,0,0,others,flink,0,0
1256,mesos.resourcemanager.framework.secret,Mesos framework secret,0,0,others,flink,0,0
1257,mesos.resourcemanager.framework.user,Mesos framework user,0,0,others,flink,0,0
1259,mesos.resourcemanager.tasks.bootstrap-cmd,A command which is executed before the TaskManager is started.,0,0,others,flink,0,0
1260,mesos.resourcemanager.tasks.container.docker.force-pull-image,Instruct the docker containerizer to forcefully pull the image rather than reuse a cached version.,1,6,function-tradeoff,flink,0,0
1263,mesos.resourcemanager.tasks.container.type,Type of the containerization used: 'mesos' or 'docker'.,0,0,others,flink,0,0
1265,mesos.resourcemanager.tasks.cpus,CPUs to assign to the Mesos workers.,1,1,resource,flink,0,0
1266,mesos.resourcemanager.tasks.disk,Disk space to assign to the Mesos workers in MB.,1,1,resource,flink,0,0
1267,mesos.resourcemanager.tasks.gpus,GPUs to assign to the Mesos workers.,1,1,resource,flink,0,0
1269,mesos.resourcemanager.tasks.network.bandwidth,Network bandwidth to assign to the Mesos workers in MB per sec.,1,1,resource,flink,0,0
1272,mesos.resourcemanager.unused-offer-expiration,Amount of time to wait for unused expired offers before declining them. This ensures your scheduler will not hoard unuseful offers.,0,0,others,flink,0,0
1273,metrics.fetcher.update-interval,Update interval for the metric fetcher used by the web UI in milliseconds. Decrease this value for faster updating metrics. Increase this value if the metric fetcher causes too much load. Setting this value to 0 disables the metric fetching completely.,0,0,others,flink,0,0
1275,metrics.internal.query-service.thread-priority,"The thread priority used for Flink's internal metric query service. The thread is created by Akka's thread pool executor. The range of the priority is from 1 (MIN_PRIORITY) to 10 (MAX_PRIORITY). Warning, increasing this value may bring the main Flink components down.",0,0,others,flink,0,0
1276,metrics.latency.granularity,"Defines the granularity of latency metrics. Accepted values are: single - Track latency without differentiating between sources and subtasks. operator - Track latency while differentiating between sources, but not subtasks. subtask - Track latency while differentiating between sources and subtasks.",0,0,others,flink,0,0
1277,metrics.latency.history-size,Defines the number of measured latencies to maintain at each operator.,0,0,others,flink,0,0
1278,metrics.latency.interval,Defines the interval at which latency tracking marks are emitted from the sources. Disables latency tracking if set to 0 or a negative value. Enabling this feature can significantly impact the performance of the cluster.,1,5,workload-specific,flink,0,0
1283,metrics.scope.delimiter,Delimiter used to assemble the metric identifier.,0,0,others,flink,0,0
1284,metrics.scope.jm,Defines the scope format string that is applied to all metrics scoped to a JobManager.,0,0,others,flink,0,0
1285,metrics.scope.jm.job,Defines the scope format string that is applied to all metrics scoped to a job on a JobManager.,0,0,others,flink,0,1
1286,metrics.scope.operator,Defines the scope format string that is applied to all metrics scoped to an operator.,0,0,others,flink,0,0
1287,metrics.scope.task,Defines the scope format string that is applied to all metrics scoped to a task.,0,0,others,flink,0,1
1288,metrics.scope.tm,Defines the scope format string that is applied to all metrics scoped to a TaskManager.,0,0,others,flink,0,1
1289,metrics.scope.tm.job,Defines the scope format string that is applied to all metrics scoped to a job on a TaskManager.,0,0,others,flink,0,0
1290,metrics.system-resource,"Flag indicating whether Flink should report system resource metrics such as machine's CPU, memory or network usage.",1,6,function-tradeoff,flink,0,0
1291,metrics.system-resource-probing-interval,Interval between probing of system resource metrics specified in milliseconds. Has an effect only when 'metrics.system-resource' is enabled.,0,0,others,flink,0,1
1293,pipeline.auto-type-registration,Controls whether Flink is automatically registering all types in the user programs with Kryo.,0,0,others,flink,0,1
1294,pipeline.auto-watermark-interval,"The interval of the automatic watermark emission. Watermarks are used throughout the streaming system to keep track of the progress of time. They are used, for example, for time based windowing.",0,0,others,flink,0,0
1295,pipeline.cached-files,"Files to be registered at the distributed cache under the given name. The files will be accessible from any user-defined function in the (distributed) runtime under a local path. Files may be local files (which will be distributed via BlobServer), or files in a distributed file system. The runtime will copy the files temporarily to a local cache, if needed.",0,0,others,flink,0,0
1296,pipeline.classpaths,A semicolon-separated list of the classpaths to package with the job jars to be sent to the cluster. These have to be valid URLs.,0,0,others,flink,0,0
1298,pipeline.default-kryo-serializers,Semicolon separated list of pairs of class names and Kryo serializers class names to be used as Kryo default serializers,0,0,others,flink,0,0
1299,pipeline.force-avro,Forces Flink to use the Apache Avro serializer for POJOs. Important: Make sure to include the flink-avro module.,0,0,others,flink,0,1
1300,pipeline.force-kryo,"If enabled, forces TypeExtractor to use Kryo serializer for POJOS even though we could analyze as POJO. In some cases this might be preferable. For example, when using interfaces with subclasses that cannot be analyzed as POJO.",0,0,others,flink,0,0
1301,pipeline.generic-types,"If the use of generic types is disabled, Flink will throw an UnsupportedOperationException whenever it encounters a data type that would go through Kryo for serialization. Disabling generic types can be helpful to eagerly find and eliminate the use of types that would go through Kryo serialization during runtime. Rather than checking types individually, using this option will throw exceptions eagerly in the places where generic types are used. We recommend to use this option only during development and pre-production phases, not during actual production use. The application program and/or the input data may be such that new, previously unseen, types occur at some point. In that case, setting this option would cause the program to fail.",0,0,others,flink,0,0
1302,pipeline.global-job-parameters,"Register a custom, serializable user configuration object. The configuration can be accessed in operators",0,0,others,flink,0,0
1303,pipeline.jars,A semicolon-separated list of the jars to package with the job jars to be sent to the cluster. These have to be valid paths.,0,0,others,flink,0,0
1304,pipeline.max-parallelism,The program-wide maximum parallelism used for operators which haven't specified a maximum parallelism. The maximum parallelism specifies the upper limit for dynamic scaling and the number of key groups used for partitioned state.,1,1,resource,flink,0,1
1306,pipeline.object-reuse,When enabled objects that Flink internally uses for deserialization and passing data to user-code functions will be reused. Keep in mind that this can lead to bugs when the user-code function of an operation is not aware of this behaviour.,0,0,others,flink,0,0
1307,pipeline.operator-chaining,Operator chaining allows non-shuffle operations to be co-located in the same thread fully avoiding serialization and de-serialization.,0,0,others,flink,0,1
1308,pipeline.registered-kryo-types,"Semicolon separated list of types to be registered with the serialization stack. If the type is eventually serialized as a POJO, then the type is registered with the POJO serializer. If the type ends up being serialized with Kryo, then it will be registered at Kryo to make sure that only tags are written.",0,0,others,flink,0,0
1309,pipeline.registered-pojo-types,"Semicolon separated list of types to be registered with the serialization stack. If the type is eventually serialized as a POJO, then the type is registered with the POJO serializer. If the type ends up being serialized with Kryo, then it will be registered at Kryo to make sure that only tags are written.",0,0,others,flink,0,1
1310,pipeline.time-characteristic,"The time characteristic for all created streams, e.g., processingtime, event time, or ingestion time. If you set the characteristic to IngestionTime or EventTime this will set a default watermark update interval of 200 ms. If this is not applicable for your application you should change it using pipeline.auto-watermark-interval.",0,0,others,flink,0,0
1311,queryable-state.client.network-threads,Number of network (Netty's event loop) Threads for queryable state client.,1,1,resource,flink,0,0
1313,queryable-state.proxy.network-threads,Number of network (Netty's event loop) Threads for queryable state proxy.,1,1,resource,flink,0,0
1315,queryable-state.proxy.query-threads,Number of query Threads for queryable state proxy. Uses the number of slots if set to 0.,1,1,resource,flink,0,0
1316,queryable-state.server.network-threads,Number of network (Netty's event loop) Threads for queryable state server.,1,1,resource,flink,0,0
1318,queryable-state.server.query-threads,Number of query Threads for queryable state server. Uses the number of slots if set to 0.,1,1,resource,flink,0,0
1319,resourcemanager.job.timeout,Timeout for jobs which don't have a job manager as leader assigned.,0,0,others,flink,0,0
1321,resourcemanager.standalone.start-up-time,"Time in milliseconds of the start-up period of a standalone cluster. During this time, resource manager of the standalone cluster expects new task executors to be registered, and will not fail slot requests that can not be satisfied by any current registered slots. After this time, it will fail pending and new coming requests immediately that can not be satisfied by registered slots. If not set, 'slotmanager.request-timeout' will be used by default.",0,0,others,flink,0,1
1322,resourcemanager.taskmanager-timeout,The timeout for an idle task manager to be released.,0,0,others,flink,0,0
1327,rest.client.max-content-length,The maximum content length in bytes that the client will handle.,1,5,workload-specific,flink,0,0
1328,rest.connection-timeout,The maximum time in ms for the client to establish a TCP connection.,0,0,others,flink,0,0
1329,rest.idleness-timeout,The maximum time in ms for a connection to stay idle before failing.,0,0,others,flink,0,0
1331,rest.retry.delay,The time in ms that the client waits between retries (See also `rest.retry.max-attempts`).,0,0,others,flink,0,1
1332,rest.retry.max-attempts,The number of retries the client will attempt if a retryable operations fails.,0,0,others,flink,0,0
1333,rest.server.max-content-length,The maximum content length in bytes that the server will handle.,1,5,workload-specific,flink,0,0
1334,rest.server.numThreads,The number of threads for the asynchronous processing of requests.,1,1,resource,flink,0,1
1335,rest.server.thread-priority,Thread priority of the REST server's executor for processing asynchronous requests. Lowering the thread priority will give Flink's main components more CPU time whereas increasing will allocate more time for the REST server's processing.,0,0,others,flink,0,0
1336,restart-strategy,"Defines the restart strategy to use in case of job failures. If checkpointing is disabled, the default value is none. If checkpointing is enabled, the default value is fixed-delay with Integer.MAX_VALUE restart attempts and '1 s' delay.",0,0,others,flink,0,0
1337,restart-strategy.failure-rate.delay,"Delay between two consecutive restart attempts if restart-strategy has been set to failure-rate. It can be specified using notation: ""1 min"", ""20 s""",0,0,others,flink,0,0
1338,restart-strategy.failure-rate.failure-rate-interval,"Time interval for measuring failure rate if restart-strategy has been set to failure-rate. It can be specified using notation: ""1 min"", ""20 s""",0,0,others,flink,0,0
1339,restart-strategy.failure-rate.max-failures-per-interval,Maximum number of restarts in given time interval before failing a job if restart-strategy has been set to failure-rate.,0,0,others,flink,0,0
1340,restart-strategy.fixed-delay.attempts,The number of times that Flink retries the execution before the job is declared as failed if restart-strategy has been set to fixed-delay.,0,0,others,flink,0,1
1342,security.kerberos.login.contexts,"A comma-separated list of login contexts to provide the Kerberos credentials to (for example, `Client,KafkaClient` to use the credentials for ZooKeeper authentication and for Kafka authentication)",0,0,others,flink,0,0
1345,security.kerberos.login.use-ticket-cache,Indicates whether to read from your Kerberos ticket cache.,1,6,function-tradeoff,flink,0,0
1346,security.ssl.algorithms,The comma separated list of standard SSL algorithms to be supported. Read more here,0,0,others,flink,0,0
1348,security.ssl.internal.close-notify-flush-timeout,The timeout (in ms) for flushing the `close_notify` that was triggered by closing a channel. If the `close_notify` was not flushed in the given timeout the channel will be closed forcibly. (-1 = use system default),0,0,others,flink,0,0
1349,security.ssl.internal.enabled,"Turns on SSL for internal network communication. Optionally, specific components may override this through their own settings (rpc, data transport, REST, etc).",1,2,security-tradeoff,flink,0,1
1350,security.ssl.internal.handshake-timeout,The timeout (in ms) during SSL handshake. (-1 = use system default),0,0,others,flink,0,0
1351,security.ssl.internal.key-password,"The secret to decrypt the key in the keystore for Flink's internal endpoints (rpc, data transport, blob server).",0,0,others,flink,0,0
1352,security.ssl.internal.keystore,"The Java keystore file with SSL Key and Certificate, to be used Flink's internal endpoints (rpc, data transport, blob server).",0,0,others,flink,0,0
1353,security.ssl.internal.keystore-password,"The secret to decrypt the keystore file for Flink's for Flink's internal endpoints (rpc, data transport, blob server).",0,0,others,flink,0,1
1354,security.ssl.internal.session-cache-size,"The size of the cache used for storing SSL session objects. According to https://github.com/netty/netty/issues/832, you should always set this to an appropriate number to not run into a bug with stalling IO threads during garbage collection. (-1 = use system default).",1,1,resource,flink,0,1
1355,security.ssl.internal.session-timeout,The timeout (in ms) for the cached SSL session objects. (-1 = use system default),0,0,others,flink,0,0
1356,security.ssl.internal.truststore,"The truststore file containing the public CA certificates to verify the peer for Flink's internal endpoints (rpc, data transport, blob server).",0,0,others,flink,0,0
1357,security.ssl.internal.truststore-password,"The password to decrypt the truststore for Flink's internal endpoints (rpc, data transport, blob server).",0,0,others,flink,0,0
1358,security.ssl.protocol,The SSL protocol version to be supported for the ssl transport. Note that it doesn't support comma separated list.,0,0,others,flink,0,1
1359,security.ssl.provider,The SSL engine provider to use for the ssl transport.,0,0,others,flink,0,0
1360,security.ssl.rest.authentication-enabled,Turns on mutual SSL authentication for external communication via the REST endpoints.,1,2,security-tradeoff,flink,0,0
1362,security.ssl.rest.enabled,Turns on SSL for external communication via the REST endpoints.,1,2,security-tradeoff,flink,0,1
1363,security.ssl.rest.key-password,The secret to decrypt the key in the keystore for Flink's external REST endpoints.,0,0,others,flink,0,0
1364,security.ssl.rest.keystore,"The Java keystore file with SSL Key and Certificate, to be used Flink's external REST endpoints.",0,0,others,flink,0,0
1365,security.ssl.rest.keystore-password,The secret to decrypt the keystore file for Flink's for Flink's external REST endpoints.,0,0,others,flink,0,0
1366,security.ssl.rest.truststore,The truststore file containing the public CA certificates to verify the peer for Flink's external REST endpoints.,0,0,others,flink,0,0
1367,security.ssl.rest.truststore-password,The password to decrypt the truststore for Flink's external REST endpoints.,0,0,others,flink,0,1
1368,security.ssl.verify-hostname,Flag to enable peer's hostname verification during ssl handshake.,1,2,security-tradeoff,flink,0,0
1369,slot.idle.timeout,The timeout in milliseconds for a idle slot in Slot Pool.,0,0,others,flink,0,0
1370,slot.request.timeout,The timeout in milliseconds for requesting a slot from Slot Pool.,0,0,others,flink,0,0
1371,slotmanager.number-of-slots.max,"Defines the maximum number of slots that the Flink cluster allocates. This configuration option is meant for limiting the resource consumption for batch workloads. It is not recommended to configure this option for streaming workloads, which may fail if there are not enough slots. Note that this configuration option does not take effect for standalone clusters, where how many slots are allocated is not controlled by Flink.",1,1,resource,flink,0,1
1372,slotmanager.redundant-taskmanager-num,"The number of redundant task managers. Redundant task managers are extra task managers started by Flink, in order to speed up job recovery in case of failures due to task manager lost. Note that this feature is available only to the active deployments (native K8s, Yarn and Mesos).",1,3,reliability-tradeoff,flink,0,1
1373,state.backend,The state backend to be used to store and checkpoint state.,0,0,others,flink,0,0
1374,state.backend.async,"Option whether the state backend should use an asynchronous snapshot method where possible and configurable. Some state backends may not support asynchronous snapshots, or only support asynchronous snapshots, and ignore this option.",1,4,limited-side-effect,flink,0,0
1375,state.backend.fs.memory-threshold,The minimum size of state data files. All state chunks smaller than that are stored inline in the root checkpoint metadata file. The max memory threshold for this configuration is 1MB.,1,1,resource,flink,0,1
1376,state.backend.fs.write-buffer-size,The default size of the write buffer for the checkpoint streams that write to file systems. The actual write buffer size is determined to be the maximum of the value of this option and option 'state.backend.fs.memory-threshold'.,1,1,resource,flink,0,1
1377,state.backend.incremental,"Option whether the state backend should create incremental checkpoints, if possible. For an incremental checkpoint, only a diff from the previous checkpoint is stored, rather than the complete checkpoint state. Once enabled, the state size shown in web UI or fetched from rest API only represents the delta checkpoint size instead of full checkpoint size. Some state backends may not support incremental checkpoints and ignore this option.",1,6,function-tradeoff,flink,0,0
1378,state.backend.local-recovery,"This option configures local recovery for this state backend. By default, local recovery is deactivated. Local recovery currently only covers keyed state backends. Currently, MemoryStateBackend does not support local recovery and ignore this option.",1,6,function-tradeoff,flink,0,0
1379,state.backend.rocksdb.block.blocksize,The approximate size (in bytes) of user data packed per block. RocksDB has default blocksize as '4KB'.,1,1,resource,flink,0,0
1380,state.backend.rocksdb.block.cache-size,The amount of the cache for data blocks in RocksDB. RocksDB has default block-cache size as '8MB'.,1,1,resource,flink,0,0
1381,state.backend.rocksdb.checkpoint.transfer.thread.num,The number of threads (per stateful operator) used to transfer (download and upload) files in RocksDBStateBackend.,1,1,resource,flink,0,1
1382,state.backend.rocksdb.compaction.level.max-size-level-base,The upper-bound of the total size of level base files in bytes. RocksDB has default configuration as '256MB'.,1,1,resource,flink,0,0
1383,state.backend.rocksdb.compaction.level.target-file-size-base,"The target file size for compaction, which determines a level-1 file size. RocksDB has default configuration as '64MB'.",1,1,resource,flink,0,0
1384,state.backend.rocksdb.compaction.level.use-dynamic-size,"If true, RocksDB will pick target size of each level dynamically. From an empty DB, RocksDB would make last level the base level, which means merging L0 data into the last level, until it exceeds max_bytes_for_level_base. And then repeat this process for second last level and so on. RocksDB has default configuration as 'false'. For more information, please refer to RocksDB's doc.",1,6,function-tradeoff,flink,0,0
1385,state.backend.rocksdb.compaction.style,"The specified compaction style for DB. Candidate compaction style is LEVEL, FIFO or UNIVERSAL, and RocksDB choose 'LEVEL' as default style.",1,6,function-tradeoff,flink,0,1
1386,state.backend.rocksdb.files.open,"The maximum number of open files (per TaskManager) that can be used by the DB, '-1' means no limit. RocksDB has default configuration as '-1'.",1,1,resource,flink,0,0
1388,state.backend.rocksdb.memory.fixed-per-slot,"The fixed total amount of memory, shared among all RocksDB instances per slot. This option overrides the 'state.backend.rocksdb.memory.managed' option when configured. If neither this option, nor the 'state.backend.rocksdb.memory.managed' optionare set, then each RocksDB column family state has its own memory caches (as controlled by the column family options).",1,1,resource,flink,0,0
1389,state.backend.rocksdb.memory.high-prio-pool-ratio,"The fraction of cache memory that is reserved for high-priority data like index, filter, and compression dictionary blocks. This option only has an effect when 'state.backend.rocksdb.memory.managed' or 'state.backend.rocksdb.memory.fixed-per-slot' are configured.",1,1,resource,flink,0,0
1390,state.backend.rocksdb.memory.managed,"If set, the RocksDB state backend will automatically configure itself to use the managed memory budget of the task slot, and divide the memory over write buffers, indexes, block caches, etc. That way, the three major uses of memory of RocksDB will be capped.",1,6,function-tradeoff,flink,0,0
1391,state.backend.rocksdb.memory.write-buffer-ratio,"The maximum amount of memory that write buffers may take, as a fraction of the total shared memory. This option only has an effect when 'state.backend.rocksdb.memory.managed' or 'state.backend.rocksdb.memory.fixed-per-slot' are configured.",1,1,resource,flink,0,0
1392,state.backend.rocksdb.metrics.actual-delayed-write-rate,Monitor the current actual delayed write rate. 0 means no delay.,1,6,function-tradeoff,flink,0,1
1393,state.backend.rocksdb.metrics.background-errors,Monitor the number of background errors in RocksDB.,0,0,others,flink,0,0
1394,state.backend.rocksdb.metrics.block-cache-capacity,Monitor block cache capacity.,1,6,function-tradeoff,flink,0,1
1395,state.backend.rocksdb.metrics.block-cache-pinned-usage,Monitor the memory size for the entries being pinned in block cache.,1,6,function-tradeoff,flink,0,0
1396,state.backend.rocksdb.metrics.block-cache-usage,Monitor the memory size for the entries residing in block cache.,1,6,function-tradeoff,flink,0,0
1397,state.backend.rocksdb.metrics.column-family-as-variable,Whether to expose the column family as a variable.,0,0,others,flink,0,0
1398,state.backend.rocksdb.metrics.compaction-pending,"Track pending compactions in RocksDB. Returns 1 if a compaction is pending, 0 otherwise.",0,0,others,flink,0,0
1399,state.backend.rocksdb.metrics.cur-size-active-mem-table,Monitor the approximate size of the active memtable in bytes.,1,6,function-tradeoff,flink,0,0
1400,state.backend.rocksdb.metrics.cur-size-all-mem-tables,Monitor the approximate size of the active and unflushed immutable memtables in bytes.,1,6,function-tradeoff,flink,0,0
1401,state.backend.rocksdb.metrics.estimate-live-data-size,Estimate of the amount of live data in bytes.,1,6,function-tradeoff,flink,0,0
1402,state.backend.rocksdb.metrics.estimate-num-keys,Estimate the number of keys in RocksDB.,0,0,others,flink,0,0
1403,state.backend.rocksdb.metrics.estimate-pending-compaction-bytes,Estimated total number of bytes compaction needs to rewrite to get all levels down to under target size. Not valid for other compactions than level-based.,0,0,others,flink,0,1
1404,state.backend.rocksdb.metrics.estimate-table-readers-mem,"Estimate the memory used for reading SST tables, excluding memory used in block cache (e.g.,filter and index blocks) in bytes.",0,0,others,flink,0,1
1405,state.backend.rocksdb.metrics.is-write-stopped,"Track whether write has been stopped in RocksDB. Returns 1 if write has been stopped, 0 otherwise.",0,0,others,flink,0,0
1406,state.backend.rocksdb.metrics.mem-table-flush-pending,Monitor the number of pending memtable flushes in RocksDB.,1,6,function-tradeoff,flink,0,0
1407,state.backend.rocksdb.metrics.num-deletes-active-mem-table,Monitor the total number of delete entries in the active memtable.,1,6,function-tradeoff,flink,0,1
1408,state.backend.rocksdb.metrics.num-deletes-imm-mem-tables,Monitor the total number of delete entries in the unflushed immutable memtables.,1,6,function-tradeoff,flink,0,1
1409,state.backend.rocksdb.metrics.num-entries-active-mem-table,Monitor the total number of entries in the active memtable.,1,6,function-tradeoff,flink,0,0
1410,state.backend.rocksdb.metrics.num-entries-imm-mem-tables,Monitor the total number of entries in the unflushed immutable memtables.,1,6,function-tradeoff,flink,0,0
1411,state.backend.rocksdb.metrics.num-immutable-mem-table,Monitor the number of immutable memtables in RocksDB.,1,6,function-tradeoff,flink,0,0
1412,state.backend.rocksdb.metrics.num-live-versions,"Monitor number of live versions. Version is an internal data structure. See RocksDB file version_set.h for details. More live versions often mean more SST files are held from being deleted, by iterators or unfinished compactions.",1,6,function-tradeoff,flink,0,1
1413,state.backend.rocksdb.metrics.num-running-compactions,Monitor the number of currently running compactions.,1,6,function-tradeoff,flink,0,0
1414,state.backend.rocksdb.metrics.num-running-flushes,Monitor the number of currently running flushes.,1,6,function-tradeoff,flink,0,0
1415,state.backend.rocksdb.metrics.num-snapshots,Monitor the number of unreleased snapshots of the database.,1,6,function-tradeoff,flink,0,0
1416,state.backend.rocksdb.metrics.size-all-mem-tables,"Monitor the approximate size of the active, unflushed immutable, and pinned immutable memtables in bytes.",1,6,function-tradeoff,flink,0,0
1417,state.backend.rocksdb.metrics.total-sst-files-size,Monitor the total size (bytes) of all SST files.WARNING: may slow down online queries if there are too many files.,1,6,function-tradeoff,flink,0,0
1418,state.backend.rocksdb.options-factory,"The options factory class for RocksDB to create DBOptions and ColumnFamilyOptions. The default options factory is org.apache.flink.contrib.streaming.state.DefaultConfigurableOptionsFactory, and it would read the configured options which provided in 'RocksDBConfigurableOptions'.",0,0,others,flink,0,0
1419,state.backend.rocksdb.predefined-options,"The predefined settings for RocksDB DBOptions and ColumnFamilyOptions by Flink community. Current supported candidate predefined-options are DEFAULT, SPINNING_DISK_OPTIMIZED, SPINNING_DISK_OPTIMIZED_HIGH_MEM or FLASH_SSD_OPTIMIZED. Note that user customized options and options from the RocksDBOptionsFactory are applied on top of these predefined ones.",0,0,others,flink,0,0
1420,state.backend.rocksdb.thread.num,The maximum number of concurrent background flush and compaction jobs (per TaskManager). RocksDB has default configuration as '1'.,1,1,resource,flink,0,1
1421,state.backend.rocksdb.timer-service.factory,This determines the factory for timer service state implementation. Options are either HEAP (heap-based) or ROCKSDB for an implementation based on RocksDB.,0,0,others,flink,0,0
1422,state.backend.rocksdb.write-batch-size,"The max size of the consumed memory for RocksDB batch write, will flush just based on item count if this config set to 0.",1,1,resource,flink,0,0
1423,state.backend.rocksdb.writebuffer.count,The maximum number of write buffers that are built up in memory. RocksDB has default configuration as '2'.,1,1,resource,flink,0,1
1424,state.backend.rocksdb.writebuffer.number-to-merge,The minimum number of write buffers that will be merged together before writing to storage. RocksDB has default configuration as '1'.,1,1,resource,flink,0,0
1425,state.backend.rocksdb.writebuffer.size,The amount of data built up in memory (backed by an unsorted log on disk) before converting to a sorted on-disk files. RocksDB has default writebuffer size as '64MB'.,1,1,resource,flink,0,0
1426,state.checkpoints.dir,The default directory used for storing the data files and meta data of checkpoints in a Flink supported filesystem. The storage path must be accessible from all participating processes/nodes(i.e. all TaskManagers and JobManagers).,0,0,others,flink,0,1
1427,state.checkpoints.num-retained,The maximum number of completed checkpoints to retain.,0,0,others,flink,0,0
1429,task.cancellation.interval,Time interval between two successive task cancellation attempts in milliseconds.,0,0,others,flink,0,0
1430,task.cancellation.timeout,Timeout in milliseconds after which a task cancellation times out and leads to a fatal TaskManager error. A value of 0 deactivates the watch dog.,0,0,others,flink,0,1
1431,task.cancellation.timers.timeout,Time we wait for the timers in milliseconds to finish all pending timer threads when the stream task is cancelled.,0,0,others,flink,0,1
1433,taskmanager.data.ssl.enabled,Enable SSL support for the taskmanager data transport. This is applicable only when the global flag for internal SSL (security.ssl.internal.enabled) is set to true,1,6,function-tradeoff,flink,0,0
1434,taskmanager.debug.memory.log,"Flag indicating whether to start a thread, which repeatedly logs the memory usage of the JVM.",1,6,function-tradeoff,flink,0,0
1435,taskmanager.debug.memory.log-interval,The interval (in ms) for the log thread to log the current memory usage.,0,0,others,flink,0,0
1437,taskmanager.jvm-exit-on-oom,Whether to kill the TaskManager when the task thread throws an OutOfMemoryError.,1,6,function-tradeoff,flink,0,0
1438,taskmanager.memory.flink.size,"Total Flink Memory size for the TaskExecutors. This includes all the memory that a TaskExecutor consumes, except for JVM Metaspace and JVM Overhead. It consists of Framework Heap Memory, Task Heap Memory, Task Off-Heap Memory, Managed Memory, and Network Memory. See also 'taskmanager.memory.process.size' for total process memory size configuration.",1,1,resource,flink,0,0
1439,taskmanager.memory.framework.heap.size,"Framework Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for TaskExecutor framework, which will not be allocated to task slots.",1,1,resource,flink,0,0
1440,taskmanager.memory.framework.off-heap.size,"Framework Off-Heap Memory size for TaskExecutors. This is the size of off-heap memory (JVM direct memory and native memory) reserved for TaskExecutor framework, which will not be allocated to task slots. The configured value will be fully counted when Flink calculates the JVM max direct memory size parameter.",1,1,resource,flink,0,0
1441,taskmanager.memory.jvm-metaspace.size,JVM Metaspace Size for the TaskExecutors.,1,1,resource,flink,0,0
1442,taskmanager.memory.jvm-overhead.fraction,"Fraction of Total Process Memory to be reserved for JVM Overhead. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min/max size to the same value.",1,1,resource,flink,0,1
1443,taskmanager.memory.jvm-overhead.max,"Max JVM Overhead size for the TaskExecutors. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min/max size to the same value.",1,1,resource,flink,0,0
1444,taskmanager.memory.jvm-overhead.min,"Min JVM Overhead size for the TaskExecutors. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min/max size to the same value.",1,1,resource,flink,0,1
1445,taskmanager.memory.managed.consumer-weights,"Managed memory weights for different kinds of consumers. A slot's managed memory is shared by all kinds of consumers it contains, proportionally to the kinds' weights and regardless of the number of consumers from each kind. Currently supported kinds of consumers are DATAPROC (for RocksDB state backend in streaming and built-in algorithms in batch) and PYTHON (for Python processes).",1,1,resource,flink,0,0
1446,taskmanager.memory.managed.fraction,"Fraction of Total Flink Memory to be used as Managed Memory, if Managed Memory size is not explicitly specified.",1,1,resource,flink,0,0
1447,taskmanager.memory.managed.size,"Managed Memory size for TaskExecutors. This is the size of off-heap memory managed by the memory manager, reserved for sorting, hash tables, caching of intermediate results and RocksDB state backend. Memory consumers can either allocate memory from the memory manager in the form of MemorySegments, or reserve bytes from the memory manager and keep their memory usage within that boundary. If unspecified, it will be derived to make up the configured fraction of the Total Flink Memory.",1,1,resource,flink,0,0
1448,taskmanager.memory.network.fraction,"Fraction of Total Flink Memory to be used as Network Memory. Network Memory is off-heap memory reserved for ShuffleEnvironment (e.g., network buffers). Network Memory size is derived to make up the configured fraction of the Total Flink Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of Network Memory can be explicitly specified by setting the min/max size to the same value.",1,1,resource,flink,0,0
1449,taskmanager.memory.network.max,"Max Network Memory size for TaskExecutors. Network Memory is off-heap memory reserved for ShuffleEnvironment (e.g., network buffers). Network Memory size is derived to make up the configured fraction of the Total Flink Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of Network Memory can be explicitly specified by setting the min/max to the same value.",1,1,resource,flink,0,1
1450,taskmanager.memory.network.min,"Min Network Memory size for TaskExecutors. Network Memory is off-heap memory reserved for ShuffleEnvironment (e.g., network buffers). Network Memory size is derived to make up the configured fraction of the Total Flink Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of Network Memory can be explicitly specified by setting the min/max to the same value.",1,1,resource,flink,0,0
1451,taskmanager.memory.process.size,"Total Process Memory size for the TaskExecutors. This includes all the memory that a TaskExecutor consumes, consisting of Total Flink Memory, JVM Metaspace, and JVM Overhead. On containerized setups, this should be set to the container memory. See also 'taskmanager.memory.flink.size' for total Flink memory size configuration.",1,1,resource,flink,0,0
1452,taskmanager.memory.segment-size,Size of memory buffers used by the network stack and the memory manager.,1,1,resource,flink,0,0
1453,taskmanager.memory.task.heap.size,"Task Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for tasks. If not specified, it will be derived as Total Flink Memory minus Framework Heap Memory, Task Off-Heap Memory, Managed Memory and Network Memory.",1,1,resource,flink,0,1
1454,taskmanager.memory.task.off-heap.size,Task Off-Heap Memory size for TaskExecutors. This is the size of off heap memory (JVM direct memory and native memory) reserved for tasks. The configured value will be fully counted when Flink calculates the JVM max direct memory size parameter.,1,1,resource,flink,0,0
1456,taskmanager.network.blocking-shuffle.compression.enabled,"Boolean flag indicating whether the shuffle data will be compressed for blocking shuffle mode. Note that data is compressed per buffer and compression can incur extra CPU overhead, so it is more effective for IO bounded scenario when data compression ratio is high. Currently, shuffle data compression is an experimental feature and the config option can be changed in the future.",1,4,limited-side-effect,flink,0,1
1457,taskmanager.network.blocking-shuffle.type,"The blocking shuffle type, either ""mmap"" or ""file"". The ""auto"" means selecting the property type automatically based on system memory architecture (64 bit for mmap and 32 bit for file). Note that the memory usage of mmap is not accounted by configured memory limits, but some resource frameworks like yarn would track this memory usage and kill the container once memory exceeding some threshold. Also note that this option is experimental and might be changed future.",0,0,others,flink,0,1
1458,taskmanager.network.detailed-metrics,Boolean flag to enable/disable more detailed metrics about inbound/outbound network queue lengths.,1,6,function-tradeoff,flink,0,1
1459,taskmanager.network.memory.buffers-per-channel,Number of exclusive network buffers to use for each outgoing/incoming channel (subpartition/inputchannel) in the credit-based flow control model. It should be configured at least 2 for good performance. 1 buffer is for receiving in-flight data in the subpartition and 1 buffer is for parallel serialization.,1,1,resource,flink,0,1
1460,taskmanager.network.memory.floating-buffers-per-gate,"Number of extra network buffers to use for each outgoing/incoming gate (result partition/input gate). In credit-based flow control mode, this indicates how many floating credits are shared among all the input channels. The floating buffers are distributed based on backlog (real-time output buffers in the subpartition) feedback, and can help relieve back-pressure caused by unbalanced data distribution among the subpartitions. This value should be increased in case of higher round trip times between nodes and/or larger number of machines in the cluster.",1,1,resource,flink,0,0
1461,taskmanager.network.memory.max-buffers-per-channel,"Number of max buffers that can be used for each channel. If a channel exceeds the number of max buffers, it will make the task become unavailable, cause the back pressure and block the data processing. This might speed up checkpoint alignment by preventing excessive growth of the buffered in-flight data in case of data skew and high number of configured floating buffers. This limit is not strictly guaranteed, and can be ignored by things like flatMap operators, records spanning multiple buffers or single timer producing large amount of data.",1,1,resource,flink,0,0
1462,taskmanager.network.netty.client.connectTimeoutSec,The Netty client connection timeout.,0,0,others,flink,0,1
1463,taskmanager.network.netty.client.numThreads,The number of Netty client threads.,1,1,resource,flink,0,0
1464,taskmanager.network.netty.num-arenas,The number of Netty arenas.,1,1,resource,flink,0,0
1465,taskmanager.network.netty.sendReceiveBufferSize,The Netty send and receive buffer size. This defaults to the system buffer size (cat /proc/sys/net/ipv4/tcp_[rw]mem) and is 4 MiB in modern Linux.,1,1,resource,flink,0,0
1466,taskmanager.network.netty.server.backlog,The netty server connection backlog.,1,3,reliability-tradeoff,flink,0,0
1467,taskmanager.network.netty.server.numThreads,The number of Netty server threads.,1,1,resource,flink,0,0
1468,taskmanager.network.netty.transport,"The Netty transport type, either ""nio"" or ""epoll"". The ""auto"" means selecting the property mode automatically based on the platform. Note that the ""epoll"" mode can get better performance, less GC and have more advanced features which are only available on modern Linux.",1,4,limited-side-effect,flink,0,1
1469,taskmanager.network.request-backoff.initial,Minimum backoff in milliseconds for partition requests of input channels.,0,0,others,flink,0,0
1470,taskmanager.network.request-backoff.max,Maximum backoff in milliseconds for partition requests of input channels.,0,0,others,flink,0,1
1471,taskmanager.network.retries,The number of retry attempts for network communication. Currently it's only used for establishing input/output channel connections,0,0,others,flink,0,0
1472,taskmanager.network.sort-shuffle.min-buffers,"Minimum number of network buffers required per sort-merge blocking result partition. For large scale batch jobs, it is suggested to increase this config value to improve compression ratio and reduce small network packets. Note: to increase this config value, you may also need to increase the size of total network memory to avoid ""insufficient number of network buffers"" error.",1,1,resource,flink,0,0
1473,taskmanager.network.sort-shuffle.min-parallelism,"Parallelism threshold to switch between sort-merge blocking shuffle and the default hash-based blocking shuffle, which means for small parallelism, hash-based blocking shuffle will be used and for large parallelism, sort-merge blocking shuffle will be used. Note: sort-merge blocking shuffle uses unmanaged direct memory for shuffle data writing and reading so just increase the size of direct memory if direct memory OOM error occurs.",1,1,resource,flink,0,0
1474,taskmanager.numberOfTaskSlots,"The number of parallel operator or user function instances that a single TaskManager can run. If this value is larger than 1, a single TaskManager takes multiple instances of a function or operator. That way, the TaskManager can utilize multiple CPU cores, but at the same time, the available memory is divided between the different operator or function instances. This value is typically proportional to the number of physical CPU cores that the TaskManager's machine has (e.g., equal to the number of cores, or half the number of cores).",1,1,resource,flink,0,1
1476,taskmanager.resource-id,"The TaskManager's ResourceID. If not configured, the ResourceID will be generated with the ""RpcAddress:RpcPort"" and a 6-character random string. Notice that this option is not valid in Yarn / Mesos and Native Kubernetes mode.",0,0,others,flink,0,0
1478,taskmanager.runtime.hashjoin-bloom-filters,"Flag to activate/deactivate bloom filters in the hybrid hash join implementation. In cases where the hash join needs to spill to disk (datasets larger than the reserved fraction of memory), these bloom filters can greatly reduce the number of spilled records, at the cost some CPU cycles.",1,6,function-tradeoff,flink,0,0
1479,taskmanager.runtime.large-record-handler,Whether to use the LargeRecordHandler when spilling. If a record will not fit into the sorting buffer. The record will be spilled on disk and the sorting will continue with only the key. The record itself will be read afterwards when merging.,1,6,function-tradeoff,flink,0,0
1480,taskmanager.runtime.max-fan,"The maximal fan-in for external merge joins and fan-out for spilling hash tables. Limits the number of file handles per operator, but may cause intermediate merging/partitioning, if set too small.",1,5,workload-specific,flink,0,0
1481,taskmanager.runtime.sort-spilling-threshold,A sort operation starts spilling when this fraction of its memory budget is full.,1,5,workload-specific,flink,0,0
1483,web.access-control-allow-origin,Access-Control-Allow-Origin header for all responses from the web-frontend.,0,0,others,flink,0,0
1484,web.backpressure.cleanup-interval,"Time, in milliseconds, after which cached stats are cleaned up if not accessed.",0,0,others,flink,0,1
1485,web.backpressure.delay-between-samples,Delay between samples to determine back pressure in milliseconds.,0,0,others,flink,0,1
1486,web.backpressure.num-samples,Number of samples to take to determine back pressure.,1,5,workload-specific,flink,0,0
1488,web.checkpoints.history,Number of checkpoints to remember for recent history.,1,5,workload-specific,flink,0,1
1489,web.history,Number of archived jobs for the JobManager.,1,5,workload-specific,flink,0,1
1490,web.log.path,Path to the log file (may be in /log for standalone but under log directory when using YARN).,0,0,others,flink,0,0
1491,web.refresh-interval,Refresh interval for the web-frontend in milliseconds.,0,0,others,flink,0,0
1492,web.submit.enable,Flag indicating whether jobs can be uploaded and run from the web-frontend.,1,6,function-tradeoff,flink,0,0
1493,web.timeout,Timeout for asynchronous operations by the web monitor in milliseconds.,0,0,others,flink,0,0
1498,yarn.application.node-label,Specify YARN node label for the YARN application.,0,0,others,flink,0,0
1499,yarn.application.priority,"A non-negative integer indicating the priority for submitting a Flink YARN application. It will only take effect if YARN priority scheduling setting is enabled. Larger integer corresponds with higher priority. If priority is negative or set to '-1'(default), Flink will unset yarn priority setting and use cluster default priority. Please refer to YARN's official documentation for specific settings required to enable priority scheduling for the targeted YARN version.",0,0,others,flink,0,1
1500,yarn.application.queue,The YARN queue on which to put the current pipeline.,0,0,others,flink,0,0
1501,yarn.application.type,A custom type for your YARN application..,0,0,others,flink,0,0
1502,yarn.application-attempt-failures-validity-interval,Time window in milliseconds which defines the number of application attempt failures when restarting the AM. Failures which fall outside of this window are not being considered. Set this value to -1 in order to count globally. See here for more information.,0,0,others,flink,0,0
1503,yarn.application-attempts,"Number of ApplicationMaster restarts. By default, the value will be set to 1. If high availability is enabled, then the default value will be 2. The restart number is also limited by YARN (configured via yarn.resourcemanager.am.max-attempts). Note that that the entire Flink cluster will restart and the YARN Client will lose the connection.",0,0,others,flink,0,0
1505,yarn.appmaster.vcores,The number of virtual cores (vcores) used by YARN application master.,1,1,resource,flink,0,0
1506,yarn.containers.vcores,"The number of virtual cores (vcores) per YARN container. By default, the number of vcores is set to the number of slots per TaskManager, if set, or to 1, otherwise. In order for this parameter to be used your cluster must have CPU scheduling enabled. You can do this by setting the org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.",1,1,resource,flink,0,0
1507,yarn.file-replication,"Number of file replication of each local resource file. If it is not configured, Flink will use the default replication value in hadoop configuration.",1,3,reliability-tradeoff,flink,0,1
1509,yarn.heartbeat.container-request-interval,"Time between heartbeats with the ResourceManager in milliseconds if Flink requests containers: The lower this value is, the faster Flink will get notified about container allocations since requests and allocations are transmitted via heartbeats.The lower this value is, the more excessive containers might get allocated which will eventually be released but put pressure on Yarn. If you observe too many container allocations on the ResourceManager, then it is recommended to increase this value. See this link for more information.",0,0,others,flink,0,1
1510,yarn.heartbeat.interval,Time between heartbeats with the ResourceManager in seconds.,0,0,others,flink,0,0
1512,yarn.properties-file.location,"When a Flink job is submitted to YARN, the JobManager's host and the number of available processing slots is written into a properties file, so that the Flink client is able to pick those details up. This configuration parameter allows changing the default location of that file (for example for environments sharing a Flink installation between users).",0,0,others,flink,0,1
1513,yarn.provided.lib.dirs,"A semicolon-separated list of provided lib directories. They should be pre-uploaded and world-readable. Flink will use them to exclude the local Flink jars(e.g. flink-dist, lib/, plugins/)uploading to accelerate the job submission process. Also YARN will cache them on the nodes so that they doesn't need to be downloaded every time for each application. An example could be hdfs://$namenode_address/path/of/flink/lib",0,0,others,flink,0,0
1514,yarn.security.kerberos.additionalFileSystems,"A comma-separated list of additional Kerberos-secured Hadoop filesystems Flink is going to access. For example, yarn.security.kerberos.additionalFileSystems=hdfs://namenode2:9002,hdfs://namenode3:9003. The client submitting to YARN needs to have access to these file systems to retrieve the security tokens.",0,0,others,flink,0,0
1516,yarn.security.kerberos.ship-local-keytab,When this is true Flink will ship the keytab file configured via security.kerberos.login.keytab as a localized YARN resource.,1,2,security-tradeoff,flink,0,0
1517,yarn.ship-archives,"A semicolon-separated list of archives to be shipped to the YARN cluster. These archives will be un-packed when localizing and they can be any of the following types: "".tar.gz"", "".tar"", "".tgz"", "".dst"", "".jar"", "".zip"".",0,0,others,flink,0,0
1518,yarn.ship-files,A semicolon-separated list of files and/or directories to be shipped to the YARN cluster.,0,0,others,flink,0,0
1520,yarn.tags,A comma-separated list of tags to apply to the Flink YARN application.,0,0,others,flink,0,1
1521,-A predicate=answer,"Make an assertion with the predicate predicate and answer answer. This form is preferred to the older form -A predicate(answer), which is still supported, because it does not use shell special characters.",0,0,others,gcc,0,0
1522,-A -predicate=answer,Cancel an assertion with the predicate predicate and answer answer.,0,0,others,gcc,0,0
1523,aarch64-autovec-preference,"Force an ISA selection strategy for auto-vectorization. Accepts values from 0 to 4, inclusive.",1,4,limited-side-effect,gcc,0,0
1524,aarch64-double-recp-precision,The number of Newton iterations for calculating the reciprocal for double type. The precision of division is propotional to this param when division approximation is enabled. The default value is 2.,1,4,limited-side-effect,gcc,0,0
1525,aarch64-float-recp-precision,The number of Newton iterations for calculating the reciprocal for float type. The precision of division is proportional to this param when division approximation is enabled. The default value is 1.,1,4,limited-side-effect,gcc,0,0
1526,aarch64-loop-vect-issue-rate-niters,"The tuning for some AArch64 CPUs tries to take both latencies and issue rates into account when deciding whether a loop should be vectorized using SVE, vectorized using Advanced SIMD, or not vectorized at all. If this parameter is set to n, GCC will not use this heuristic for loops that are known to execute in fewer than n Advanced SIMD iterations.",1,4,limited-side-effect,gcc,0,1
1527,aarch64-sve-compare-costs,"When vectorizing for SVE, consider using npackedvectors for smaller elements and use the cost model to pick the cheapest approach. Also use the cost model to choose between SVE and Advanced SIMD vectorization.",1,4,limited-side-effect,gcc,0,0
1528,align-loop-iterations,A loop expected to iterate at least the selected number of iterations is aligned.,1,4,limited-side-effect,gcc,0,1
1529,align-threshold,Select fraction of the maximal frequency of executions of a basic block in a function to align the basic block.,1,4,limited-side-effect,gcc,0,0
1530,analyzer-bb-explosion-factor,"The maximum number of after supernodeexploded nodes within the analyzer per supernode, before terminating analysis.",1,4,limited-side-effect,gcc,0,0
1531,analyzer-max-constraints,The maximum number of constraints per state.,1,5,workload-specific,gcc,0,1
1532,analyzer-max-enodes-for-full-dump,The maximum depth of exploded nodes that should appear in a dot dump before switching to a less verbose format.,1,4,limited-side-effect,gcc,0,0
1533,analyzer-max-enodes-per-program-point,"The maximum number of exploded nodes per program point within the analyzer, before terminating analysis of that point.",1,4,limited-side-effect,gcc,0,0
1534,analyzer-max-infeasible-edges,The maximum number of infeasible edges to reject before declaring a diagnostic as infeasible.,1,4,limited-side-effect,gcc,0,0
1535,analyzer-max-recursion-depth,"The maximum number of times a callsite can appear in a call stack within the analyzer, before terminating analysis of a call that would recurse deeper.",1,4,limited-side-effect,gcc,0,1
1536,analyzer-max-svalue-depth,"The maximum depth of a symbolic value, before approximating the value as unknown.",1,5,workload-specific,gcc,0,0
1537,analyzer-min-snodes-for-call-summary,The minimum number of supernodes within a function for the analyzer to consider summarizing its effects at call sites.,1,5,workload-specific,gcc,0,0
1538,-ansi,"In C mode, this is equivalent to -std=c90. In C++ mode, it is equivalent to -std=c++98.",0,0,others,gcc,0,0
1539,asan-globals,Enable buffer overflow detection for global objects. This kind of protection is enabled by default if you are using -fsanitize=address option. To disable global objects protection use --param asan-globals=0.,1,4,limited-side-effect,gcc,0,0
1540,asan-instrument-allocas,Enable asan allocas/VLAs protection.,1,2,security-tradeoff,gcc,0,1
1541,asan-instrumentation-with-call-threshold,"If number of memory accesses in function being instrumented is greater or equal to this number, use callbacks instead of inline checks. E.g. to disable inline code use --param asan-instrumentation-with-call-threshold=0.",1,4,limited-side-effect,gcc,0,0
1542,asan-instrument-reads,Enable buffer overflow detection for memory reads. This kind of protection is enabled by default when using -fsanitize=address. To disable memory reads protection use --param asan-instrument-reads=0.,1,4,limited-side-effect,gcc,0,0
1543,asan-instrument-writes,Enable buffer overflow detection for memory writes. This kind of protection is enabled by default when using -fsanitize=address. To disable memory writes protection use --param asan-instrument-writes=0 option.,1,4,limited-side-effect,gcc,0,0
1544,asan-memintrin,Enable detection for built-in functions. This kind of protection is enabled by default when using -fsanitize=address. To disable built-in functions protection use --param asan-memintrin=0.,1,4,limited-side-effect,gcc,0,0
1545,asan-stack,Enable buffer overflow detection for stack objects. This kind of protection is enabled by default when using -fsanitize=address. To disable stack protection use --param asan-stack=0 option.,1,4,limited-side-effect,gcc,0,0
1546,asan-use-after-return,Enable detection of use-after-return. This kind of protection is enabled by default when using the -fsanitize=address option. To disable it use --param asan-use-after-return=0.,1,4,limited-side-effect,gcc,0,0
1547,-aux-info filename,"Output to the given filename prototyped declarations for all functions declared and/or defined in a translation unit, including those in header files. This option is silently ignored in any language other than C.",1,6,function-tradeoff,gcc,0,0
1548,avg-loop-niter,Average number of iterations of a loop.,1,4,limited-side-effect,gcc,0,0
1549,avoid-fma-max-bits,Maximum number of bits for which we avoid creating FMAs.,1,5,workload-specific,gcc,0,1
1551,builtin-expect-probability,Control the probability of the expression having the specified value. This parameter takes a percentage (i.e. 0 ... 100) as input.,1,4,limited-side-effect,gcc,0,0
1552,builtin-string-cmp-inline-length,The maximum length of a constant string for a builtin string cmp call eligible for inlining.,1,4,limited-side-effect,gcc,0,0
1555,case-values-threshold,"The smallest number of different values for which it is best to use a jump-table instead of a tree of conditional branches. If the value is 0, use the default for the machine.",1,4,limited-side-effect,gcc,0,0
1557,cgraph,"Dumps information about call-graph optimization, unused function removal, and inlining decisions.",1,6,function-tradeoff,gcc,0,0
1558,class,Dump class hierarchy information. Virtual table information is emitted unless limis specified. This option is applicable to C++ only.,1,6,function-tradeoff,gcc,0,0
1559,comdat-sharing-probability,Probability (in percent) that C++ inline function with comdat visibility are shared across multiple compilation units.,1,4,limited-side-effect,gcc,0,0
1560,common,Display the options that are common to all languages.,1,6,function-tradeoff,gcc,0,0
1561,--coverage,This option is used to compile and link code instrumented for coverage analysis. The option is a synonym for -fprofile-arcs -ftest-coverage (when compiling) and -lgcov (when linking). See the documentation for those options for more details.,0,0,others,gcc,0,0
1562,cxx-max-namespaces-for-diagnostic-help,The maximum number of namespaces to consult for suggestions when C++ name lookup fails for an identifier.,1,4,limited-side-effect,gcc,0,0
1564,-D name=definition,"The contents of definition are tokenized and processed as if they appeared during translation phase three in a definedirective. In particular, the definition is truncated by embedded newline characters.",0,0,others,gcc,0,0
1565,-dA,Annotate the assembler output with miscellaneous debugging information.,1,6,function-tradeoff,gcc,0,0
1566,-dD,"Dump all macro definitions, at the end of preprocessing, in addition to normal output.",1,6,function-tradeoff,gcc,0,0
1567,-dH,Produce a core dump whenever an error occurs.,0,0,others,gcc,0,1
1568,-dI,Output includedirectives in addition to the result of preprocessing.,1,6,function-tradeoff,gcc,0,0
1569,diff-delete=,SGR substring for deleted lines within generated patches.,0,0,others,gcc,0,0
1570,diff-filename=,SGR substring for filename headers within generated patches.,0,0,others,gcc,0,0
1571,diff-hunk=,SGR substring for the starts of hunks within generated patches.,0,0,others,gcc,0,0
1572,diff-insert=,SGR substring for inserted lines within generated patches.,0,0,others,gcc,0,0
1573,-dletters,"Says to make debugging dumps during compilation as specified by letters. The flags documented here are those relevant to the preprocessor. Other letters are interpreted by the compiler proper, or reserved for future versions of GCC, and so are silently ignored. If you specify letters whose behavior conflicts, the result is undefined. See Developer Options, for more information.",1,6,function-tradeoff,gcc,0,0
1575,-dN,"Like -dD, but emit only the macro names, not their expansions.",1,6,function-tradeoff,gcc,0,0
1576,-dp,Annotate the assembler output with a comment indicating which pattern and alternative is used. The length and cost of each instruction are also printed.,1,6,function-tradeoff,gcc,0,0
1577,-dP(captal),Dump the RTL in the assembler output as a comment before each instruction. Also turns on -dp annotation.,1,6,function-tradeoff,gcc,0,0
1578,dse-max-alias-queries-per-store,Maximum number of queries into the alias oracle per store. Larger values result in larger compilation times and may result in more removed dead stores.,1,4,limited-side-effect,gcc,0,0
1579,dse-max-object-size,Maximum size (in bytes) of objects tracked bytewise by dead store elimination. Larger values may result in larger compilation times.,1,5,workload-specific,gcc,0,0
1580,-dU,"Like -dD except that only macros that are expanded, or whose definedness is tested in preprocessor directives, are output; the output is delayed until the use or test of the macro; and undefdirectives are also output for macros tested but undefined at the time.",0,0,others,gcc,0,1
1584,-dumpfullversion,"Print the full compiler version and don't do anything else. The output is always three numbers separated by dots, major, minor and patchlevel version.",1,6,function-tradeoff,gcc,0,0
1585,-dumpmachine,"Print the compiler target machine (for example, i686-pc-linux-gnu and don't do anything else.",1,6,function-tradeoff,gcc,0,0
1586,-dumpspecs,Print the compiler's built-in specs nd don't do anything else. (This is used when GCC itself is being built.) See Spec Files.,1,6,function-tradeoff,gcc,0,0
1587,-dumpversion,"Print the compiler version (for example, 3.0, 6.3.0 or 7) and don't do anything else. This is the compiler version used in filesystem paths and specs. Depending on how the compiler has been configured it can be just a single number (major version), two numbers separated by a dot (major and minor version) or three numbers separated by dots (major, minor and patchlevel version).",1,6,function-tradeoff,gcc,0,1
1588,-dx,Just generate RTL for a function instead of compiling it. Usually used with -fdump-rtl-expand.,1,6,function-tradeoff,gcc,0,1
1589,-E,"Stop after the preprocessing stage; do not run the compiler proper. The output is in the form of preprocessed source code, which is sent to the standard output.",0,0,others,gcc,0,0
1590,early-inlining-insns,Specify growth that the early inliner can make. In effect it increases the amount of inlining for code having a large abstraction penalty.,1,4,limited-side-effect,gcc,0,0
1591,eh,Enable showing the EH region number holding each statement.,1,6,function-tradeoff,gcc,0,0
1593,error=,SGR substring for error: markers.,0,0,others,gcc,0,0
1594,evrp-mode,Specifies the mode Early VRP should operate in.,1,6,function-tradeoff,gcc,0,1
1596,-fabi-version=n,Use version n of the C++ ABI. The default is version 0.,0,0,others,gcc,0,0
1597,-fada-spec-parent=unit,"In conjunction with -fdump-ada-spec[-slim] above, generate Ada specs as child units of parent unit.",0,0,others,gcc,0,0
1598,-faggressive-loop-optimizations,This option tells the loop optimizer to use language constraints to derive bounds for the number of iterations of a loop. This assumes that loop code does not invoke undefined behavior by for example causing signed integer overflows or out-of-bound array accesses. The bounds for the number of iterations of a loop are used to guide loop unrolling and peeling and loop exit test optimizations. This option is enabled by default.,1,4,limited-side-effect,gcc,0,0
1599,-faligned-new,"Enable support for C++17 new of types that require more alignment than void* ::operator new(std::size_t) provides. A numeric argument such as -faligned-new=32 can be used to specify how much alignment (in bytes) is provided by that function, but few users will need to override the default of alignof(std::max_align_t).",0,0,others,gcc,0,0
1600,-falign-functions=n:m:n2:m2,"Align the start of functions to the next power-of-two greater than or equal to n, skipping up to m-1 bytes. This ensures that at least the first m bytes of the function can be fetched by the CPU without crossing an n-byte alignment boundary.",1,4,limited-side-effect,gcc,0,0
1601,-falign-jumps=n:m:n2:m2,"Align branch targets to a power-of-two boundary, for branch targets where the targets can only be reached by jumping. In this case, no dummy operations need be executed.",1,4,limited-side-effect,gcc,0,1
1602,-falign-labels=n:m:n2:m2,Align all branch targets to a power-of-two boundary.,1,4,limited-side-effect,gcc,0,0
1603,-falign-loops=n:m:n2:m2,"Align loops to a power-of-two boundary. If the loops are executed many times, this makes up for any execution of the dummy padding instructions.",1,4,limited-side-effect,gcc,0,0
1604,-fallow-parameterless-variadic-functions,Accept variadic functions without named parameters.,0,0,others,gcc,0,0
1605,-fallow-store-data-races,"Allow the compiler to perform optimizations that may introduce new data races on stores, without proving that the variable cannot be concurrently accessed by other threads. Does not affect optimization of local data. It is safe to use this option if it is known that global data will not be accessed by multiple threads.",1,4,limited-side-effect,gcc,0,1
1606,-fasan-shadow-offset=number,This option forces GCC to use custom shadow offset in AddressSanitizer checks. It is useful for experimenting with different shadow memory layouts in Kernel AddressSanitizer.,0,0,others,gcc,0,0
1607,-fassociative-math,"Allow re-association of operands in series of floating-point operations. This violates the ISO C and C++ language standard by possibly changing computation result. NOTE: re-ordering may change the sign of zero as well as ignore NaNs and inhibit or create underflow or overflow (and thus cannot be used on code that relies on rounding behavior like (x + 2**52) - 2**52. May also reorder floating-point comparisons and thus may not be used when ordered comparisons are required. This option requires that both -fno-signed-zeros and -fno-trapping-math be in effect. Moreover, it doesn't make much sense with -frounding-math. For Fortran the option is automatically enabled when both -fno-signed-zeros and -fno-trapping-math are in effect.",1,4,limited-side-effect,gcc,0,0
1608,-fasynchronous-unwind-tables,"Generate unwind table in DWARF format, if supported by target machine. The table is exact at each instruction boundary, so it can be used for stack unwinding from asynchronous events (such as debugger or garbage collector).",0,0,others,gcc,0,1
1609,-fauto-inc-dec,Combine increments or decrements of addresses with memory accesses. This pass is always skipped on architectures that do not have instructions to support this. Enabled by default at -O and higher on architectures that support this.,1,4,limited-side-effect,gcc,0,0
1610,-fauto-profile=path,"Enable sampling-based feedback-directed optimizations, and the following optimizations, many of which are generally profitable only with profile feedback available:",1,4,limited-side-effect,gcc,0,0
1611,-fbranch-probabilities,"After running a program compiled with -fprofile-arcs (see Instrumentation Options), you can compile it a second time using -fbranch-probabilities, to improve optimizations based on the number of times each branch was taken. When a program compiled with -fprofile-arcs exits, it saves arc execution counts to a file called sourcename.gcda for each source file. The information in this data file is very dependent on the structure of the generated code, so you must use the same source code and the same optimization options for both compilations.",1,4,limited-side-effect,gcc,0,0
1612,-fcaller-saves,"Enable allocation of values to registers that are clobbered by function calls, by emitting extra instructions to save and restore the registers around such calls. Such allocation is done only when it seems to result in better code.",1,4,limited-side-effect,gcc,0,0
1614,-fcall-saved-reg,Treat the register named reg as an allocable register saved by functions. It may be allocated even for temporaries or variables that live across a call. Functions compiled this way save and restore the register reg if they use it.,0,0,others,gcc,0,1
1615,-fcall-used-reg,Treat the register named reg as an allocable register that is clobbered by function calls. It may be allocated for temporaries or variables that do not live across a call. Functions compiled this way do not save and restore the register reg.,0,0,others,gcc,0,0
1616,-fcf-protection=[full|branch|return|none|check],"Enable code instrumentation of control-flow transfers to increase program security by checking that target addresses of control-flow transfer instructions (such as indirect function call, function return, indirect jump) are valid. This prevents diverting the flow of control to an unexpected target. This is intended to protect against such threats as Return-oriented Programming (ROP), and similarly call/jmp-oriented programming (COP/JOP).",0,0,others,gcc,0,0
1617,-fchecking=n,Enable internal consistency checking. The default depends on the compiler configuration. -fchecking=2 enables further internal consistency checking that might affect code generation.,1,3,reliability-tradeoff,gcc,0,0
1618,-fcheck-new,"Check that the pointer returned by operator new is non-null before attempting to modify the storage allocated. This check is normally unnecessary because the C++ standard specifies that operator new only returns 0 if it is declared throw(), in which case the compiler always checks the return value even without this option. In all other cases, when operator new has a non-empty exception specification, memory exhaustion is signalled by throwing std::bad_alloc. See also new (nothrow)",0,0,others,gcc,0,1
1619,-fcode-hoisting,"Perform code hoisting. Code hoisting tries to move the evaluation of expressions executed on all paths to the function exit as early as possible. This is especially useful as a code size optimization, but it often helps for code speed as well. This flag is enabled by default at -O2 and higher.",1,4,limited-side-effect,gcc,0,0
1620,-fcombine-stack-adjustments,Tracks stack adjustments (pushes and pops) and stack memory references and then tries to find ways to combine them.,1,4,limited-side-effect,gcc,0,0
1621,-fcommon,"In C code, this option controls the placement of global variables defined without an initializer, known as tentative definitions in the C standard. Tentative definitions are distinct from declarations of a variable with the extern keyword, which do not allocate storage.",0,0,others,gcc,0,0
1622,-fcompare-debug[=opts],"If no error occurs during compilation, run the compiler a second time, adding opts and -fcompare-debug-second to the arguments passed to the second compilation. Dump the final internal representation in both compilations, and print an error if they differ.",1,6,function-tradeoff,gcc,0,0
1623,-fcompare-debug-second,"This option is implicitly passed to the compiler for the second compilation requested by -fcompare-debug, along with options to silence warnings, and omitting other options that would cause the compiler to produce output to files or to standard output as a side effect. Dump files and preserved temporary files are renamed so as to contain the .gk additional extension during the second compilation, to avoid overwriting those generated by the first.",1,6,function-tradeoff,gcc,0,0
1624,-fcompare-elim,"After register allocation and post-register allocation instruction splitting, identify arithmetic instructions that compute processor flags similar to a comparison operation based on that arithmetic. If possible, eliminate the explicit comparison operation.",1,4,limited-side-effect,gcc,0,0
1625,-fconcepts-ts,"Below -std=c++20, -fconcepts enables support for the C++ Extensions for Concepts Technical Specification, ISO 19217 (2015).",0,0,others,gcc,0,1
1627,-fconserve-stack,"Attempt to minimize stack usage. The compiler attempts to use less stack space, even if that makes the program slower. This option implies setting the large-stack-frame parameter to 100 and the large-stack-frame-growth parameter to 400.",1,4,limited-side-effect,gcc,0,1
1629,-fconstexpr-cache-depth=n,"Set the maximum level of nested evaluation depth for C++11 constexpr functions that will be cached to n. This is a heuristic that trades off compilation speed (when the cache avoids repeated calculations) against memory consumption (when the cache grows very large from highly recursive evaluations). The default is 8. Very few users are likely to want to adjust it, but if your code does heavy constexpr calculations you might want to experiment to find which value works best for you.",1,5,workload-specific,gcc,0,0
1630,-fconstexpr-depth=n,Set the maximum nested evaluation depth for C++11 constexpr functions to n. A limit is needed to detect endless recursion during constant expression evaluation. The minimum specified by the standard is 512.,0,0,others,gcc,0,0
1631,-fconstexpr-loop-limit=n,Set the maximum number of iterations for a loop in C++14 constexpr functions to n. A limit is needed to detect infinite loops during constant expression evaluation. The default is 262144 (1<<18).,1,5,workload-specific,gcc,0,0
1632,-fconstexpr-ops-limit=n,"Set the maximum number of operations during a single constexpr evaluation. Even when number of iterations of a single loop is limited with the above limit, if there are several nested loops and each of them has many iterations but still smaller than the above limit, or if in a body of some loop or even outside of a loop too many expressions need to be evaluated, the resulting constexpr evaluation might take too long. The default is 33554432 (1<<25).",1,5,workload-specific,gcc,0,0
1633,-fcoroutines,Enable support for the C++ coroutines extension (experimental).,0,0,others,gcc,0,0
1634,-fcprop-registers,"After register allocation and post-register allocation instruction splitting, perform a copy-propagation pass to try to reduce scheduling dependencies and occasionally eliminate the copy.",1,4,limited-side-effect,gcc,0,0
1635,-fcrossjumping,Perform cross-jumping transformation. This transformation unifies equivalent code and saves code size. The resulting code may or may not perform better than without cross-jumping.,1,4,limited-side-effect,gcc,0,1
1636,-fcse-follow-jumps,"In common subexpression elimination (CSE), scan through jump instructions when the target of the jump is not reached by any other path. For example, when CSE encounters an if statement with an else clause, CSE follows the jump when the condition tested is false.",1,4,limited-side-effect,gcc,0,0
1637,-fcse-skip-blocks,"This is similar to -fcse-follow-jumps, but causes CSE to follow jumps that conditionally skip over blocks. When CSE encounters a simple if statement with no else clause, -fcse-skip-blocks causes CSE to follow the jump around the body of the if.",1,4,limited-side-effect,gcc,0,0
1638,-fcx-fortran-rules,"Complex multiplication and division follow Fortran rules. Range reduction is done as part of complex division, but there is no checking whether the result of a complex multiplication or division is NaN + I*NaN, with an attempt to rescue the situation in that case.",1,4,limited-side-effect,gcc,0,0
1639,-fcx-limited-range,"When enabled, this option states that a range reduction step is not needed when performing complex division. Also, there is no checking whether the result of a complex multiplication or division is NaN + I*NaN, with an attempt to rescue the situation in that case. The default is -fno-cx-limited-range, but is enabled by -ffast-math.",1,4,limited-side-effect,gcc,0,0
1640,-fdata-sections,Place each function or data item into its own section in the output file if the target supports arbitrary sections. The name of the function or the name of the data item determines the section's name in the output file.,1,4,limited-side-effect,gcc,0,1
1642,-fdbg-cnt-list,Print the name and the counter upper bound for all debug counters.,1,6,function-tradeoff,gcc,0,0
1643,-fdce,Perform dead code elimination (DCE) on RTL. Enabled by default at -O and higher.,1,4,limited-side-effect,gcc,0,0
1646,-fdebug-types-section,"When using DWARF Version 4 or higher, type DIEs can be put into their own .debug_types section instead of making them part of the .debug_info section. It is more efficient to put them in a separate comdat section since the linker can then remove duplicates. But not all DWARF consumers support .debug_types sections yet and on some objects .debug_types produces larger instead of smaller debugging information.",0,0,others,gcc,0,0
1647,-fdeclone-ctor-dtor,"The C++ ABI requires multiple entry points for constructors and destructors: one for a base subobject, one for a complete object, and one for a virtual destructor that calls operator delete afterwards. For a hierarchy with virtual bases, the base and complete variants are clones, which means two copies of the function. With this option, the base and complete variants are changed to be thunks that call a common implementation.",1,4,limited-side-effect,gcc,0,0
1648,-fdelayed-branch,"If supported for the target machine, attempt to reorder instructions to exploit instruction slots available after delayed branch instructions.",1,4,limited-side-effect,gcc,0,0
1649,-fdelete-dead-exceptions,"Consider that instructions that may throw exceptions but don't otherwise contribute to the execution of the program can be optimized away. This option is enabled by default for the Ada compiler, as permitted by the Ada language specification. Optimization passes that cause dead exceptions to be removed are enabled independently at different optimization levels.",1,4,limited-side-effect,gcc,0,0
1650,-fdelete-null-pointer-checks,"Assume that programs cannot safely dereference null pointers, and that no code or data element resides at address zero. This option enables simple constant folding optimizations at all optimization levels. In addition, other optimization passes in GCC use this flag to control global dataflow analyses that eliminate useless checks for null pointers; these assume that a memory access to address zero always results in a trap, so that if a pointer is checked after it has already been dereferenced, it cannot be null.",1,4,limited-side-effect,gcc,0,1
1651,-fdevirtualize,"Attempt to convert calls to virtual functions to direct calls. This is done both within a procedure and interprocedurally as part of indirect inlining (-findirect-inlining) and interprocedural constant propagation (-fipa-cp). Enabled at levels -O2, -O3, -Os.",1,4,limited-side-effect,gcc,0,0
1652,-fdevirtualize-at-ltrans,Stream extra information needed for aggressive devirtualization when running the link-time optimizer in local transformation mode. This option enables more devirtualization but significantly increases the size of streamed data. For this reason it is disabled by default.,1,6,function-tradeoff,gcc,0,0
1653,-fdevirtualize-speculatively,"Attempt to convert calls to virtual functions to speculative direct calls. Based on the analysis of the type inheritance graph, determine for a given call the set of likely targets. If the set is small, preferably of size 1, change the call into a conditional deciding between direct and indirect calls. The speculative calls enable more optimizations, such as inlining. When they seem useless after further optimization, they are converted back into original form.",1,4,limited-side-effect,gcc,0,0
1654,-fdiagnostics-column-origin=ORIGIN,"Select the origin for column numbers, i.e. the column number assigned to the first column. The default value of 1 corresponds to traditional GCC behavior and to the GNU style guide. Some utilities may perform better with an origin of 0; any non-negative value may be specified.",0,0,others,gcc,0,0
1655,-fdiagnostics-column-unit=UNIT,"Select the units for the column number. This affects traditional diagnostics (in the absence of -fno-show-column), as well as JSON format diagnostics if requested.",0,0,others,gcc,0,0
1656,-fdiagnostics-format=FORMAT,Select a different format for printing diagnostics. FORMAT is textor json The default is text,0,0,others,gcc,0,0
1657,-fdiagnostics-generate-patch,"Print fix-it hints to stderr in unified diff format, after any diagnostics are printed. For example:",1,6,function-tradeoff,gcc,0,0
1658,-fdiagnostics-minimum-margin-width=width,This option controls the minimum width of the left margin printed by -fdiagnostics-show-line-numbers. It defaults to 6.,0,0,others,gcc,0,0
1659,-fdiagnostics-parseable-fixits,"Emit fix-it hints in a machine-parseable format, suitable for consumption by IDEs. For each fix-it, a line will be printed after the relevant diagnostic, starting with the string ix-it: For example:",0,0,others,gcc,0,0
1660,-fdiagnostics-path-format=KIND,Specify how to print paths of control-flow events for diagnostics that have such a path associated with them.,0,0,others,gcc,0,0
1661,-fdiagnostics-plain-output,"This option requests that diagnostic output look as plain as possible, which may be useful when running dejagnu or other utilities that need to parse diagnostics output and prefer that it remain more stable over time. -fdiagnostics-plain-output is currently equivalent to the following options:",1,6,function-tradeoff,gcc,0,0
1662,-fdiagnostics-show-location=every-line,Only meaningful in line-wrapping mode. Instructs the diagnostic messages reporter to emit the same source location information (as prefix) for physical lines that result from the process of breaking a message which is too long to fit on a single line.,0,0,others,gcc,0,0
1663,-fdiagnostics-show-location=once,"Only meaningful in line-wrapping mode. Instructs the diagnostic messages reporter to emit source location information once; that is, in case the message is too long to fit on a single physical line and has to be wrapped, the source location won't be emitted (as prefix) again, over and over, in subsequent continuation lines. This is the default behavior.",0,0,others,gcc,0,0
1664,-fdiagnostics-show-path-depths,This option provides additional information when printing control-flow paths associated with a diagnostic.,0,0,others,gcc,0,0
1665,-fdiagnostics-show-template-tree,"In the C++ frontend, when printing diagnostics showing mismatching template types, such as:",1,6,function-tradeoff,gcc,0,0
1666,-fdiagnostics-urls[=WHEN],"Use escape sequences to embed URLs in diagnostics. For example, when -fdiagnostics-show-option emits text showing the command-line option controlling a diagnostic, embed a URL for documentation of that option.",0,0,others,gcc,0,0
1667,-fdirectives-only,"When preprocessing, handle directives, but do not expand macros.",0,0,others,gcc,0,0
1668,-fdisable-ipa-pass,"Disable IPA pass pass. pass is the pass name. If the same pass is statically invoked in the compiler multiple times, the pass name should be appended with a sequential number starting from 1.",0,0,others,gcc,0,1
1669,-fdisable-kind-pass=range-list,This is a set of options that are used to explicitly disable/enable optimization passes. These options are intended for use for debugging GCC. Compiler users should use regular options for enabling/disabling passes instead.,0,0,others,gcc,0,1
1670,-fdisable-rtl-pass=range-list,"Disable RTL pass pass. pass is the pass name. If the same pass is statically invoked in the compiler multiple times, the pass name should be appended with a sequential number starting from 1. range-list is a comma-separated list of function ranges or assembler names. Each range is a number pair separated by a colon. The range is inclusive in both ends. If the range is trivial, the number pair can be simplified as a single number. If the function call graph node uid falls within one of the specified ranges, the pass is disabled for that function. The uid is shown in the function header of a dump file, and the pass names can be dumped by using option -fdump-passes.",0,0,others,gcc,0,1
1671,-fdisable-tree-pass=range-list,Disable tree pass pass. See -fdisable-rtl for the description of option arguments.,0,0,others,gcc,0,0
1672,-fdollars-in-identifiers,Accept in identifiers.,0,0,others,gcc,0,0
1673,-fdse,Perform dead store elimination (DSE) on RTL. Enabled by default at -O and higher.,1,4,limited-side-effect,gcc,0,1
1674,-fdump-ada-spec[-slim],"For C and C++ source and include files, generate corresponding Ada specs. See Generating Ada Bindings for C and C++ headers in GNAT User Guide, which provides detailed documentation on this feature.",0,0,others,gcc,0,0
1675,-fdump-debug,Dump debugging information generated during the debug generation phase.,1,6,function-tradeoff,gcc,0,0
1676,-fdump-earlydebug,Dump debugging information generated during the early debug generation phase.,1,6,function-tradeoff,gcc,0,0
1677,-fdump-final-insns[=file],"Dump the final internal representation (RTL) to file. If the optional argument is omitted (or if file is .), the name of the dump file is determined by appending .gkd to the dump base name, see -dumpbase.",1,6,function-tradeoff,gcc,0,0
1680,-fdump-lang,Dump language-specific information. The file name is made by appending .lang to the source file name.,1,6,function-tradeoff,gcc,0,0
1681,-fdump-lang-switch-options=filename,Control the dumping of language-specific information. The options and filename portions behave as described in the -fdump-tree option. The following switch values are accepted:,0,0,others,gcc,0,1
1683,-fdump-passes,Print on stderr the list of optimization passes that are turned on and off by the current command-line options.,1,6,function-tradeoff,gcc,0,0
1684,-fdump-rtl-alignments,Dump after branch alignments have been computed.,1,6,function-tradeoff,gcc,0,0
1685,-fdump-rtl-all,Produce all the dumps listed above.,0,0,others,gcc,0,0
1686,-fdump-rtl-asmcons,Dump after fixing rtl statements that have unsatisfied in/out constraints.,1,6,function-tradeoff,gcc,0,0
1687,-fdump-rtl-auto_inc_dec,Dump after auto-inc-dec discovery. This pass is only run on architectures that have auto inc or auto dec instructions.,1,6,function-tradeoff,gcc,0,0
1688,-fdump-rtl-barriers,Dump after cleaning up the barrier instructions.,1,6,function-tradeoff,gcc,0,1
1689,-fdump-rtl-bbpart,Dump after partitioning hot and cold basic blocks.,1,6,function-tradeoff,gcc,0,0
1690,-fdump-rtl-bbro,Dump after block reordering.,1,6,function-tradeoff,gcc,0,0
1691,-fdump-rtl-bypass,Dump after jump bypassing and control flow optimizations.,1,6,function-tradeoff,gcc,0,0
1692,-fdump-rtl-ce3,"-fdump-rtl-ce1, -fdump-rtl-ce2, and -fdump-rtl-ce3 enable dumping after the three if conversion passes.",1,6,function-tradeoff,gcc,0,0
1693,-fdump-rtl-combine,Dump after the RTL instruction combination pass.,1,6,function-tradeoff,gcc,0,0
1694,-fdump-rtl-compgotos,Dump after duplicating the computed gotos.,1,6,function-tradeoff,gcc,0,0
1695,-fdump-rtl-cprop_hardreg,Dump after hard register copy propagation.,1,6,function-tradeoff,gcc,0,1
1696,-fdump-rtl-csa,Dump after combining stack adjustments.,1,6,function-tradeoff,gcc,0,1
1697,-fdump-rtl-dbr,Dump after delayed branch scheduling.,1,6,function-tradeoff,gcc,0,0
1698,-fdump-rtl-dce,Dump after the standalone dead code elimination passes.,1,6,function-tradeoff,gcc,0,1
1699,-fdump-rtl-dce2,-fdump-rtl-DCD1 and -fdump-rtl-DCD2 enable dumping after the two dead store elimination passes.',1,6,function-tradeoff,gcc,0,0
1700,-fdump-rtl-dfinish,These dumps are defined but always produce empty files.,1,6,function-tradeoff,gcc,0,0
1701,-fdump-rtl-eh,Dump after finalization of EH handling code.,1,6,function-tradeoff,gcc,0,1
1702,-fdump-rtl-eh_ranges,Dump after conversion of EH handling range regions.,1,6,function-tradeoff,gcc,0,0
1703,-fdump-rtl-expand,Dump after RTL generation.,1,6,function-tradeoff,gcc,0,0
1704,-fdump-rtl-init-regs,Dump after the initialization of the registers.,1,6,function-tradeoff,gcc,0,0
1705,-fdump-rtl-initvals,Dump after the computation of the initial value sets.,1,6,function-tradeoff,gcc,0,0
1706,-fdump-rtl-into_cfglayout,Dump after converting to cfglayout mode.,1,6,function-tradeoff,gcc,0,0
1707,-fdump-rtl-ira,Dump after iterated register allocation.,1,6,function-tradeoff,gcc,0,0
1708,-fdump-rtl-jump,Dump after the second jump optimization.,1,6,function-tradeoff,gcc,0,0
1709,-fdump-rtl-mach,"Dump after performing the machine dependent reorganization pass, if that pass exists.",1,6,function-tradeoff,gcc,0,0
1710,-fdump-rtl-mode_sw,Dump after removing redundant mode switches.,1,6,function-tradeoff,gcc,0,0
1711,-fdump-rtl-outof_cfglayout,Dump after converting from cfglayout mode.,1,6,function-tradeoff,gcc,0,1
1712,-fdump-rtl-pass=filename,Says to make debugging dumps during compilation at times specified by letters. This is used for debugging the RTL-based passes of the compiler.,1,6,function-tradeoff,gcc,0,0
1713,-fdump-rtl-peephole2,Dump after the peephole pass.,1,6,function-tradeoff,gcc,0,0
1714,-fdump-rtl-postreload,Dump after post-reload optimizations.,1,6,function-tradeoff,gcc,0,0
1715,-fdump-rtl-pro_and_epilogue,Dump after generating the function prologues and epilogues.,1,6,function-tradeoff,gcc,0,0
1716,-fdump-rtl-ree,Dump after sign/zero extension elimination.,1,6,function-tradeoff,gcc,0,1
1717,-fdump-rtl-rnreg,Dump after register renumbering.,1,6,function-tradeoff,gcc,0,1
1718,-fdump-rtl-seqabstr,Dump after common sequence discovery.,1,6,function-tradeoff,gcc,0,0
1719,-fdump-rtl-shorten,Dump after shortening branches.,1,6,function-tradeoff,gcc,0,0
1720,-fdump-rtl-sibling,Dump after sibling call optimizations.,1,6,function-tradeoff,gcc,0,1
1721,-fdump-rtl-sms,Dump after modulo scheduling. This pass is only run on some architectures.,1,6,function-tradeoff,gcc,0,0
1722,-fdump-rtl-split5,These options enable dumping after five rounds of instruction splitting.,1,6,function-tradeoff,gcc,0,0
1723,-fdump-rtl-stack,Dump after conversion from GCC's flat register file registers to the x87's stack-like registers. This pass is only run on x86 variants.,1,6,function-tradeoff,gcc,0,1
1724,-fdump-rtl-unshare,Dump after all rtl has been unshared.,1,6,function-tradeoff,gcc,0,0
1725,-fdump-rtl-vartrack,Dump after variable tracking.,1,6,function-tradeoff,gcc,0,0
1726,-fdump-rtl-vregs,Dump after converting virtual registers to hard registers.,1,6,function-tradeoff,gcc,0,1
1727,-fdump-rtl-web,Dump after live range splitting.,1,6,function-tradeoff,gcc,0,1
1728,-fdump-statistics-option,"Enable and control dumping of pass statistics in a separate file. The file name is generated by appending a suffix ending in statisticsto the source file name, and the file is created in the same directory as the output file. If the optionform is used, statscauses counters to be summed over the whole compilation unit while detailsdumps every event as the passes generate them. The default with no option is to sum counters for each function compiled.",0,0,others,gcc,0,1
1730,-fdump-unnumbered,"When doing debugging dumps, suppress instruction numbers and address output. This makes it more feasible to use diff on debugging dumps for compiler invocations with different options, in particular with and without -g.",0,0,others,gcc,0,0
1731,-fdump-unnumbered-links,"When doing debugging dumps (see -d option above), suppress instruction numbers for the links to the previous and next instructions in a sequence.",0,0,others,gcc,0,0
1732,-fearly-inlining,Inline functions marked by always_inline and functions whose body seems smaller than the function call overhead early before doing -fprofile-generate instrumentation and real inlining pass. Doing so makes profiling significantly cheaper and usually inlining faster on programs having large chains of nested wrapper functions.,1,4,limited-side-effect,gcc,0,0
1733,-femit-class-debug-always,"Instead of emitting debugging information for a C++ class in only one object file, emit it in all object files using the class. This option should be used only with debuggers that are unable to handle the way GCC normally emits debugging information for classes because using this option increases the size of debugging information by as much as a factor of two.",0,0,others,gcc,0,0
1734,-femit-struct-debug-baseonly,Emit debug information for struct-like types only when the base name of the compilation source file matches the base name of file in which the struct is defined.,0,0,others,gcc,0,0
1735,-femit-struct-debug-detailed[=spec-list],Specify the struct-like types for which the compiler generates debug information. The intent is to reduce duplicate struct debug information between different object files within the same program.,0,0,others,gcc,0,0
1736,-femit-struct-debug-reduced,"Emit debug information for struct-like types only when the base name of the compilation source file matches the base name of file in which the type is defined, unless the struct is a template or defined in a system header.",0,0,others,gcc,0,0
1737,-fenable-ipa-pass,"Enable IPA pass pass. pass is the pass name. If the same pass is statically invoked in the compiler multiple times, the pass name should be appended with a sequential number starting from 1.",0,0,others,gcc,0,0
1738,-fenable-rtl-pass=range-list,Enable RTL pass pass. See -fdisable-rtl for option argument description and examples.,0,0,others,gcc,0,0
1739,-fenable-tree-pass=range-list,Enable tree pass pass. See -fdisable-rtl for the description of option arguments.,1,6,function-tradeoff,gcc,0,0
1740,-fexceptions,"Enable exception handling. Generates extra code needed to propagate exceptions. For some targets, this implies GCC generates frame unwind information for all functions, which can produce significant data size overhead, although it does not affect execution. If you do not specify this option, GCC enables it by default for languages like C++ that normally require exception handling, and disables it for languages like C that do not normally require it. However, you may need to enable this option when compiling C code that needs to interoperate properly with exception handlers written in C++. You may also wish to disable this option if you are compiling older C++ programs that don't use exception handling.",0,0,others,gcc,0,0
1741,-fexcess-precision=style,"This option allows further control over excess precision on machines where floating-point operations occur in a format with more precision or range than the IEEE standard and interchange floating-point types. By default, -fexcess-precision=fast is in effect; this means that operations may be carried out in a wider precision than the types specified in the source if that would result in faster code, and it is unpredictable when rounding to the types specified in the source code takes place. When compiling C, if -fexcess-precision=standard is specified then excess precision follows the rules specified in ISO C99; in particular, both casts and assignments cause values to be rounded to their semantic types (whereas -ffloat-store only affects assignments). This option is enabled by default for C if a strict conformance option such as -std=c99 is used. -ffast-math enables -fexcess-precision=fast by default regardless of whether a strict conformance option is used.",1,4,limited-side-effect,gcc,0,1
1742,-fexec-charset=charset,"Set the execution character set, used for string and character constants. The default is UTF-8. charset can be any encoding supported by the system's iconv library routine.",0,0,others,gcc,0,1
1743,-fexpensive-optimizations,Perform a number of minor optimizations that are relatively expensive.,1,4,limited-side-effect,gcc,0,1
1744,-fextended-identifiers,Accept universal character names and extended characters in identifiers. This option is enabled by default for C99 (and later C standard versions) and C++.,0,0,others,gcc,0,0
1745,-fext-numeric-literals (C++ and Objective-C++ only),"Accept imaginary, fixed-point, or machine-defined literal number suffixes as GNU extensions. When this option is turned off these suffixes are treated as C++11 user-defined literal numeric suffixes. This is on by default for all pre-C++11 dialects and all GNU dialects: -std=c++98, -std=gnu++98, -std=gnu++11, -std=gnu++14. This option is off by default for ISO C++11 onwards (-std=c++11, ...).",0,0,others,gcc,0,0
1746,-ffast-math,"Sets the options -fno-math-errno, -funsafe-math-optimizations, -ffinite-math-only, -fno-rounding-math, -fno-signaling-nans, -fcx-limited-range and -fexcess-precision=fast.",1,4,limited-side-effect,gcc,0,0
1747,-ffat-lto-objects,Fat LTO objects are object files that contain both the intermediate language and the object code. This makes them usable for both LTO linking and normal linking. This option is effective only when compiling with -flto and is ignored at link time.,1,4,limited-side-effect,gcc,0,0
1749,-ffinite-loops,"Assume that a loop with an exit will eventually take the exit and not loop indefinitely. This allows the compiler to remove loops that otherwise have no side-effects, not considering eventual endless looping as such.",1,4,limited-side-effect,gcc,0,0
1750,-ffinite-math-only,Allow optimizations for floating-point arithmetic that assume that arguments and results are not NaNs or +-Infs.,1,4,limited-side-effect,gcc,0,0
1751,-ffixed-reg,"Treat the register named reg as a fixed register; generated code should never refer to it (except perhaps as a stack pointer, frame pointer or in some other fixed role).",0,0,others,gcc,0,1
1752,-ffloat-store,"Do not store floating-point variables in registers, and inhibit other options that might change whether a floating-point value is taken from a register or memory.",1,4,limited-side-effect,gcc,0,0
1753,-fforward-propagate,"Perform a forward propagation pass on RTL. The pass tries to combine two instructions and checks if the result can be simplified. If loop unrolling is active, two passes are performed and the second is scheduled after loop unrolling.",1,4,limited-side-effect,gcc,0,0
1754,-ffreestanding,"Assert that compilation targets a freestanding environment. This implies -fno-builtin. A freestanding environment is one in which the standard library may not exist, and program startup may not necessarily be at main. The most obvious example is an OS kernel. This is equivalent to -fno-hosted.",0,0,others,gcc,0,0
1755,-fgcse,Perform a global common subexpression elimination pass. This pass also performs global constant and copy propagation.,1,4,limited-side-effect,gcc,0,0
1756,-fgcse-after-reload,"When -fgcse-after-reload is enabled, a redundant load elimination pass is performed after reload. The purpose of this pass is to clean up redundant spilling.",1,4,limited-side-effect,gcc,0,0
1757,-fgcse-las,"When -fgcse-las is enabled, the global common subexpression elimination pass eliminates redundant loads that come after stores to the same memory location (both partial and full redundancies).",1,4,limited-side-effect,gcc,0,0
1758,-fgcse-lm,"When -fgcse-lm is enabled, global common subexpression elimination attempts to move loads that are only killed by stores into themselves. This allows a loop containing a load/store sequence to be changed to a load outside the loop, and a copy/store within the loop.",1,4,limited-side-effect,gcc,0,0
1759,-fgcse-sm,"When -fgcse-sm is enabled, a store motion pass is run after global common subexpression elimination. This pass attempts to move stores out of loops. When used in conjunction with -fgcse-lm, loops containing a load/store sequence can be changed to a load before the loop and a store after the loop.",1,4,limited-side-effect,gcc,0,1
1760,-fgimple,Enable parsing of function definitions marked with __GIMPLE. This is an experimental feature that allows unit testing of GIMPLE passes.,0,0,others,gcc,0,0
1761,-fgnu89-inline,The option -fgnu89-inline tells GCC to use the traditional GNU semantics for inline functions when in C99 mode. See An Inline Function is As Fast As a Macro. Using this option is roughly equivalent to adding the gnu_inline function attribute to all inline functions (see Function Attributes).,0,0,others,gcc,0,0
1762,-fgnu-runtime,Generate object code compatible with the standard GNU Objective-C runtime. This is the default for most types of systems.,0,0,others,gcc,0,1
1763,-fgnu-tm,"When the option -fgnu-tm is specified, the compiler generates code for the Linux variant of Intel current Transactional Memory ABI specification document (Revision 1.1, May 6 2009). This is an experimental feature whose interface may change in future versions of GCC, as the official specification changes. Please note that not all architectures are supported for this feature.",0,0,others,gcc,0,0
1764,-fgraphite-identity,"Enable the identity transformation for graphite. For every SCoP we generate the polyhedral representation and transform it back to gimple. Using -fgraphite-identity we can check the costs or benefits of the GIMPLE -> GRAPHITE -> GIMPLE transformation. Some minimal optimizations are also performed by the code generator isl, like index splitting and dead code elimination in loops.",1,4,limited-side-effect,gcc,0,0
1765,-fhoist-adjacent-loads,Speculatively hoist loads from both branches of an if-then-else if the loads are from adjacent locations in the same structure and the target architecture has a conditional move instruction. This flag is enabled by default at -O2 and higher.,1,4,limited-side-effect,gcc,0,0
1766,-fhosted,"Assert that compilation targets a hosted environment. This implies -fbuiltin. A hosted environment is one in which the entire standard library is available, and in which main has a return type of int. Examples are nearly everything except a kernel. This is equivalent to -fno-freestanding.",0,0,others,gcc,0,0
1767,-fif-conversion,"Attempt to transform conditional jumps into branch-less equivalents. This includes use of conditional moves, min, max, set flags and abs instructions, and some tricks doable by standard arithmetics. The use of conditional execution on chips where it is available is controlled by -fif-conversion2.",1,4,limited-side-effect,gcc,0,0
1768,-fif-conversion2,Use conditional execution (where available) to transform conditional jumps into branch-less equivalents.,1,4,limited-side-effect,gcc,0,1
1769,-findirect-inlining,Inline also indirect calls that are discovered to be known at compile time thanks to previous inlining. This option has any effect only when inlining itself is turned on by the -finline-functions or -finline-small-functions options.,1,4,limited-side-effect,gcc,0,0
1770,-finhibit-size-directive,"Don't output a .size assembler directive, or anything else that would cause trouble if the function is split in the middle, and the two halves are placed at locations far apart in memory. This option is used when compiling crtstuff.c; you should not need to use it for anything else.",0,0,others,gcc,0,1
1771,-finline-functions,"Consider all functions for inlining, even if they are not declared inline. The compiler heuristically decides which functions are worth integrating in this way.",1,4,limited-side-effect,gcc,0,0
1772,-finline-functions-called-once,"Consider all static functions called once for inlining into their caller even if they are not marked inline. If a call to a given function is integrated, then the function is not output as assembler code in its own right.",1,4,limited-side-effect,gcc,0,1
1773,-finline-limit=n,"By default, GCC limits the size of functions that can be inlined. This flag allows coarse control of this limit. n is the size of functions that can be inlined in number of pseudo instructions.",1,6,function-tradeoff,gcc,0,0
1774,-finline-small-functions,"Integrate functions into their callers when their body is smaller than expected function call code (so overall size of program gets smaller). The compiler heuristically decides which functions are simple enough to be worth integrating in this way. This inlining applies to all functions, even those not declared inline.",1,4,limited-side-effect,gcc,0,0
1776,-finstrument-functions,"Generate instrumentation calls for entry and exit to functions. Just after function entry and just before function exit, the following profiling functions are called with the address of the current function and its call site. (On some platforms, __builtin_return_address does not work beyond the current function, so the call site information may not be available to the profiling functions otherwise.)",0,0,others,gcc,0,1
1777,"-finstrument-functions-exclude-file-list=file,file,","Set the list of functions that are excluded from instrumentation (see the description of -finstrument-functions). If the file that contains a function definition matches with one of file, then that function is not instrumented. The match is done on substrings: if the file parameter is a substring of the file name, it is considered to be a match.",0,0,others,gcc,0,0
1778,"-finstrument-functions-exclude-function-list=sym,sym,","This is similar to -finstrument-functions-exclude-file-list, but this option sets the list of function names to be excluded from instrumentation. The function name to be matched is its user-visible name, such as vector<int> blah(const vector<int> &), not the internal mangled name (e.g., _Z4blahRSt6vectorIiSaIiEE). The match is done on substrings: if the sym parameter is a substring of the function name, it is considered to be a match. For C99 and C++ extended identifiers, the function name must be given in UTF-8, not using universal character names.",0,0,others,gcc,0,0
1779,-fipa-bit-cp,"When enabled, perform interprocedural bitwise constant propagation. This flag is enabled by default at -O2 and by -fprofile-use and -fauto-profile. It requires that -fipa-cp is enabled.",1,4,limited-side-effect,gcc,0,0
1780,-fipa-cp,"Perform interprocedural constant propagation. This optimization analyzes the program to determine when values passed to functions are constants and then optimizes accordingly. This optimization can substantially increase performance if the application has constants passed to functions. This flag is enabled by default at -O2, -Os and -O3. It is also enabled by -fprofile-use and -fauto-profile.",1,4,limited-side-effect,gcc,0,1
1781,-fipa-cp-clone,"Perform function cloning to make interprocedural constant propagation stronger. When enabled, interprocedural constant propagation performs function cloning when externally visible function can be called with constant arguments. Because this optimization can create multiple copies of functions, it may significantly increase code size (see --param ipa-cp-unit-growth=value). This flag is enabled by default at -O3. It is also enabled by -fprofile-use and -fauto-profile.",1,4,limited-side-effect,gcc,0,1
1782,-fipa-icf,Perform Identical Code Folding for functions and read-only variables. The optimization reduces code size and may disturb unwind stacks by replacing a function by equivalent one with a different name. The optimization works more effectively with link-time optimization enabled.,1,4,limited-side-effect,gcc,0,1
1783,-fipa-modref,Perform interprocedural mod/ref analysis. This optimization analyzes the side effects of functions (memory locations that are modified or referenced) and enables better optimization across the function call boundary. This flag is enabled by default at -O and higher.,1,4,limited-side-effect,gcc,0,0
1784,-fipa-profile,"Perform interprocedural profile propagation. The functions called only from cold functions are marked as cold. Also functions executed once (such as cold, noreturn, static constructors or destructors) are identified. Cold functions and loop less parts of functions executed once are then optimized for size. Enabled by default at -O and higher.",1,4,limited-side-effect,gcc,0,1
1785,-fipa-pta,Perform interprocedural pointer analysis and interprocedural modification and reference analysis. This option can cause excessive memory and compile-time usage on large compilation units. It is not enabled by default at any optimization level.,1,4,limited-side-effect,gcc,0,0
1786,-fipa-pure-const,Discover which functions are pure or constant. Enabled by default at -O and higher.,1,4,limited-side-effect,gcc,0,0
1787,-fipa-ra,Use caller save registers for allocation if those registers are not used by any called function. In that case it is not necessary to save and restore them around calls. This is only possible if called functions are part of same compilation unit as current function and they are compiled before it.,1,4,limited-side-effect,gcc,0,0
1788,-fipa-reference,Discover which static variables do not escape the compilation unit. Enabled by default at -O and higher.,1,4,limited-side-effect,gcc,0,0
1789,-fipa-reference-addressable,"Discover read-only, write-only and non-addressable static variables. Enabled by default at -O and higher.",1,4,limited-side-effect,gcc,0,0
1790,-fipa-sra,"Perform interprocedural scalar replacement of aggregates, removal of unused parameters and replacement of parameters passed by reference by parameters passed by value.",1,4,limited-side-effect,gcc,0,0
1791,-fipa-stack-alignment,Reduce stack alignment on call sites if possible. Enabled by default.,1,4,limited-side-effect,gcc,0,0
1792,-fipa-vrp,"When enabled, perform interprocedural propagation of value ranges. This flag is enabled by default at -O2. It requires that -fipa-cp is enabled.",1,4,limited-side-effect,gcc,0,0
1793,-fira-algorithm=algorithm,"Use the specified coloring algorithm for the integrated register allocator. The algorithm argument can be priority which specifies Chow priority coloring, or CB which specifies Chaitin-Briggs coloring. Chaitin-Briggs coloring is not implemented for all architectures, but for those targets that do support it, it is the default because it generates better code.",1,4,limited-side-effect,gcc,0,0
1794,-fira-hoist-pressure,"Use IRA to evaluate register pressure in the code hoisting pass for decisions to hoist expressions. This option usually results in smaller code, but it can slow the compiler down.",1,4,limited-side-effect,gcc,0,0
1795,-fira-loop-pressure,"Use IRA to evaluate register pressure in loops for decisions to move loop invariants. This option usually results in generation of faster and smaller code on machines with large register files (>= 32 registers), but it can slow the compiler down.",1,4,limited-side-effect,gcc,0,0
1796,-fira-region=region,Use specified regions for the integrated register allocator. The region argument should be one of the following:,1,4,limited-side-effect,gcc,0,0
1798,-fisolate-erroneous-paths-attribute,"Detect paths that trigger erroneous or undefined behavior due to a null value being used in a way forbidden by a returns_nonnull or nonnull attribute. Isolate those paths from the main control flow and turn the statement with erroneous or undefined behavior into a trap. This is not currently enabled, but may be enabled by -O2 in the future.",1,4,limited-side-effect,gcc,0,0
1799,-fisolate-erroneous-paths-dereference,Detect paths that trigger erroneous or undefined behavior due to dereferencing a null pointer. Isolate those paths from the main control flow and turn the statement with erroneous or undefined behavior into a trap. This flag is enabled by default at -O2 and higher and depends on -fdelete-null-pointer-checks also being enabled.,1,4,limited-side-effect,gcc,0,0
1800,-fivar-visibility=[public|protected|private|package],Set the default instance variable visibility to the specified option so that instance variables declared outside the scope of any access modifier directives default to the specified visibility.,0,0,others,gcc,0,0
1801,-fivopts,"Perform induction variable optimizations (strength reduction, induction variable merging and induction variable elimination) on trees.",1,4,limited-side-effect,gcc,0,0
1802,fixit-delete,SGR substring for fix-it hints suggesting text to be deleted.,0,0,others,gcc,0,0
1803,fixit-insert,SGR substring for fix-it hints suggesting text to be inserted or replaced.,0,0,others,gcc,0,0
1804,-fkeep-inline-functions,"In C, emit static functions that are declared inline into the object file, even if the function has been inlined into all of its callers. This switch does not affect functions using the extern inline extension in GNU C90. In C++, emit any and all inline functions into the object file.",1,4,limited-side-effect,gcc,0,0
1805,-fkeep-static-consts,"Emit variables declared static const when optimization isn't turned on, even if the variables aren't referenced.",1,6,function-tradeoff,gcc,0,0
1806,-fkeep-static-functions,"Emit static functions into the object file, even if the function is never used.",1,4,limited-side-effect,gcc,0,0
1809,-flarge-source-files,"Adjust GCC to expect large source files, at the expense of slower compilation and higher memory usage.",1,6,function-tradeoff,gcc,0,0
1810,-flax-vector-conversions,Allow implicit conversions between vectors with differing numbers of elements and/or incompatible element types. This option should not be used for new code.,0,0,others,gcc,0,1
1812,-flimit-function-alignment,"If this option is enabled, the compiler tries to avoid unnecessarily overaligning functions. It attempts to instruct the assembler to align by the amount specified by -falign-functions, but not to skip more bytes than the size of the function.",1,4,limited-side-effect,gcc,0,0
1813,-flinker-output=type,"This option controls code generation of the link-time optimizer. By default the linker output is automatically determined by the linker plugin. For debugging the compiler and if incremental linking with a non-LTO object file is desired, it may be useful to control the type manually.",0,0,others,gcc,0,0
1814,-flive-patching=level,Control GCC's optimizations to produce output suitable for live-patching.,1,4,limited-side-effect,gcc,0,0
1815,-flive-range-shrinkage,Attempt to decrease register pressure through register live range shrinkage. This is helpful for fast processors with small or moderate size register sets.,1,4,limited-side-effect,gcc,0,1
1816,-floop-block,"Perform loop nest optimizations. Same as -floop-nest-optimize. To use this code transformation, GCC has to be configured with --with-isl to enable the Graphite loop transformation infrastructure.",1,4,limited-side-effect,gcc,0,0
1817,-floop-interchange,"Perform loop interchange outside of graphite. This flag can improve cache performance on loop nest and allow further loop optimizations, like vectorization, to take place. For example, the loop",1,4,limited-side-effect,gcc,0,0
1818,-floop-nest-optimize,Enable the isl based loop nest optimizer. This is a generic loop nest optimizer based on the Pluto optimization algorithms. It calculates a loop structure optimized for data-locality and parallelism. This option is experimental.,1,4,limited-side-effect,gcc,0,1
1819,-floop-parallelize-all,Use the Graphite data dependence analysis to identify loops that can be parallelized. Parallelize all the loops that can be analyzed to not contain loop carried dependences without checking that it is profitable to parallelize the loops.,1,4,limited-side-effect,gcc,0,0
1820,-floop-unroll-and-jam,Apply unroll and jam transformations on feasible loops. In a loop nest this unrolls the outer loop by some factor and fuses the resulting multiple inner loops. This flag is enabled by default at -O3. It is also enabled by -fprofile-use and -fauto-profile.,1,4,limited-side-effect,gcc,0,0
1821,-flra-remat,"Enable CFG-sensitive rematerialization in LRA. Instead of loading values of spilled pseudos, LRA tries to rematerialize (recalculate) values if it is profitable.",1,4,limited-side-effect,gcc,0,0
1822,-flto[=n],"This option runs the standard link-time optimizer. When invoked with source code, it generates GIMPLE (one of GCC's internal representations) and writes it to special ELF sections in the object file. When the object files are linked together, all the function bodies are read from these ELF sections and instantiated as if they had been part of the same translation unit.",1,4,limited-side-effect,gcc,0,0
1823,-flto-compression-level=n,"This option specifies the level of compression used for intermediate language written to LTO object files, and is only meaningful in conjunction with LTO mode (-flto). Valid values are 0 (no compression) to 9 (maximum compression). Values outside this range are clamped to either 0 or 9. If the option is not given, a default balanced compression setting is used.",1,6,function-tradeoff,gcc,0,1
1824,-flto-partition=alg,"Specify the partitioning algorithm used by the link-time optimizer. The value is either to1to specify a partitioning mirroring the original source files or balancedto specify partitioning into equally sized chunks (whenever possible) or maxto create new partition for every symbol where possible. Specifying noneas an algorithm disables partitioning and streaming completely. The default value is balanced While to1can be used as an workaround for various code ordering issues, the maxpartitioning is intended for internal testing only. The value onespecifies that exactly one partition should be used while the value nonebypasses partitioning and executes the link-time optimization step directly from the WPA phase.",1,4,limited-side-effect,gcc,0,0
1825,-flto-report,Prints a report with internal details on the workings of the link-time optimizer. The contents of this report vary from version to version. It is meant to be useful to GCC developers when processing object files in LTO mode (via -flto).,1,6,function-tradeoff,gcc,0,0
1826,-flto-report-wpa,"Like -flto-report, but only print for the WPA phase of link-time optimization.",1,6,function-tradeoff,gcc,0,0
1828,-fmax-errors=n,"Limits the maximum number of error messages to n, at which point GCC bails out rather than attempting to continue processing the source code. If n is 0 (the default), there is no limit on the number of error messages produced. If -Wfatal-errors is also specified, then -Wfatal-errors takes precedence over this option.",1,6,function-tradeoff,gcc,0,0
1829,-fmax-include-depth=depth,Set the maximum depth of the nested #include. The default is 200.,0,0,others,gcc,0,0
1830,-fmem-report,Makes the compiler print some statistics about permanent memory allocation when it finishes.,1,6,function-tradeoff,gcc,0,1
1831,-fmem-report-wpa,Makes the compiler print some statistics about permanent memory allocation for the WPA phase only.,1,6,function-tradeoff,gcc,0,0
1832,-fmerge-all-constants,Attempt to merge identical constants and identical variables.,1,4,limited-side-effect,gcc,0,0
1833,-fmerge-constants,Attempt to merge identical constants (string constants and floating-point constants) across compilation units.,1,4,limited-side-effect,gcc,0,0
1834,-fmessage-length=n,"Try to format error messages so that they fit on lines of about n characters. If n is zero, then no line-wrapping is done; each error message appears on a single line. This is the default for all front ends.",0,0,others,gcc,0,0
1836,-fmodule-implicit-inline,"Member functions defined in their class definitions are not implicitly inline for modular code. This is different to traditional C++ behavior, for good reasons. However, it may result in a difficulty during code porting. This option makes such function definitions implicitly inline. It does however generate an ABI incompatibility, so you must use it everywhere or nowhere. (Such definitions outside of a named module remain implicitly inline, regardless.)",0,0,others,gcc,0,0
1838,-fmodule-only,"Only emit the Compiled Module Interface, inhibiting any object file.",0,0,others,gcc,0,0
1839,-fmodulo-sched,Perform swing modulo scheduling immediately before the first scheduling pass. This pass looks at innermost loops and reorders their instructions by overlapping different iterations.,1,4,limited-side-effect,gcc,0,1
1840,-fmodulo-sched-allow-regmoves,"Perform more aggressive SMS-based modulo scheduling with register moves allowed. By setting this flag certain anti-dependences edges are deleted, which triggers the generation of reg-moves based on the life-range analysis. This option is effective only with -fmodulo-sched enabled.",1,4,limited-side-effect,gcc,0,0
1841,-fmove-loop-invariants,"Enables the loop invariant motion pass in the RTL loop optimizer. Enabled at level -O1 and higher, except for -Og.",1,4,limited-side-effect,gcc,0,0
1842,-fms-extensions,"Disable Wpedantic warnings about constructs used in MFC, such as implicit int and getting a pointer to member function via non-standard syntax.",0,0,others,gcc,0,0
1843,-fnew-inheriting-ctors,Enable the P0136 adjustment to the semantics of C++11 constructor inheritance. This is part of C++17 but also considered to be a Defect Report against C++11 and C++14. This flag is enabled by default unless -fabi-version=10 or lower is specified.,0,0,others,gcc,0,1
1844,-fnew-ttp-matching,"Enable the P0522 resolution to Core issue 150, template template parameters and default arguments: this allows a template with default template arguments as an argument for a template template parameter with fewer template parameters. This flag is enabled by default for -std=c++17.",0,0,others,gcc,0,0
1845,-fnext-runtime,"Generate output compatible with the NeXT runtime. This is the default for NeXT-based systems, including Darwin and Mac OS X. The macro __NEXT_RUNTIME__ is predefined if (and only if) this option is used.",0,0,others,gcc,0,0
1846,-fno-access-control,Turn off all access checking. This switch is mainly useful for working around bugs in the access control code.,1,2,security-tradeoff,gcc,0,0
1847,-fno-allocation-dce,Do not remove unused C++ allocations in dead code elimination.,1,4,limited-side-effect,gcc,0,0
1848,-fno-asm,"Do not recognize asm, inline or typeof as a keyword, so that code can use these words as identifiers. You can use the keywords __asm__, __inline__ and __typeof__ instead. -ansi implies -fno-asm.",0,0,others,gcc,0,0
1850,-fno-branch-count-reg,"Disable the optimization pass that scans for opportunities to use iecrement and branchinstructions on a count register instead of instruction sequences that decrement a register, compare it against zero, and then branch based upon the result. This option is only meaningful on architectures that support such instructions, which include x86, PowerPC, IA-64 and S/390. Note that the -fno-branch-count-reg option doesn't remove the decrement and branch instructions from the generated instruction stream introduced by other optimization passes.",1,4,limited-side-effect,gcc,0,0
1851,-fno-builtin-function,"Don't recognize built-in functions that do not begin with __builtin_as prefix. See Other built-in functions provided by GCC, for details of the functions affected, including those which are not built-in functions when -ansi or -std options for strict ISO C conformance are used because they do not have an ISO standard meaning.",0,0,others,gcc,0,0
1852,-fno-canonical-system-headers,"When preprocessing, do not shorten system header paths with canonicalization.",0,0,others,gcc,0,1
1853,-fno-char8_t,"Enable support for char8_t as adopted for C++20. This includes the addition of a new char8_t fundamental type, changes to the types of UTF-8 string and character literals, new signatures for user-defined literals, associated standard library updates, and new __cpp_char8_t and __cpp_lib_char8_t feature test macros.",0,0,others,gcc,0,0
1854,-fno-defer-pop,"For machines that must pop arguments after a function call, always pop the arguments as soon as each function returns. At levels -O1 and higher, -fdefer-pop is the default; this allows the compiler to let arguments accumulate on the stack for several function calls and pop them all at once.",1,4,limited-side-effect,gcc,0,0
1855,-fno-diagnostics-color,"Use color in diagnostics. WHEN is never always or auto The default depends on how the compiler has been configured, it can be any of the above WHEN options or also neverif GCC_COLORS environment variable isn't present in the environment, and autootherwise. automakes GCC use color only when the standard error is a terminal, and when not executing in an emacs shell. The forms -fdiagnostics-color and -fno-diagnostics-color are aliases for -fdiagnostics-color=always and -fdiagnostics-color=never, respectively.",0,0,others,gcc,0,1
1856,-fno-diagnostics-show-caret,"By default, each diagnostic emitted includes the original source line and a caret ^indicating the column. This option suppresses this information. The source line is truncated to n characters, if the -fmessage-length=n option is given. When the output is done to the terminal, the width is limited to the width given by the COLUMNS environment variable or, if not set, to the terminal width.",0,0,others,gcc,0,0
1857,-fno-diagnostics-show-cwe,"Diagnostic messages can optionally have an associated CWE identifier. GCC itself only provides such metadata for some of the -fanalyzer diagnostics. GCC plugins may also provide diagnostics with such metadata. By default, if this information is present, it will be printed with the diagnostic. This option suppresses the printing of this metadata.",0,0,others,gcc,0,0
1858,-fno-diagnostics-show-labels,"By default, when printing source code (via -fdiagnostics-show-caret), diagnostics can label ranges of source code with pertinent information, such as the types of expressions:",0,0,others,gcc,0,0
1859,-fno-diagnostics-show-line-numbers,"By default, when printing source code (via -fdiagnostics-show-caret), a left margin is printed, showing line numbers. This option suppresses this left margin.",0,0,others,gcc,0,1
1860,-fno-diagnostics-show-option,"By default, each diagnostic emitted includes text indicating the command-line option that directly controls the diagnostic (if such an option is known to the diagnostic machinery). Specifying the -fno-diagnostics-show-option flag suppresses that behavior.",0,0,others,gcc,0,0
1861,-fno-dwarf2-cfi-asm,Emit DWARF unwind info as compiler generated .eh_frame section instead of using GAS .cfi_* directives.,0,0,others,gcc,0,0
1862,-fno-elide-constructors,"The C++ standard allows an implementation to omit creating a temporary that is only used to initialize another object of the same type. Specifying this option disables that optimization, and forces G++ to call the copy constructor in all cases. This option also causes G++ to call trivial member functions which otherwise would be expanded inline.",0,0,others,gcc,0,1
1863,-fno-elide-type,"By default when the C++ frontend prints diagnostics showing mismatching template types, common parts of the types are printed as to simplify the error message. For example:",0,0,others,gcc,0,0
1864,-fno-eliminate-unused-debug-symbols,"By default, no debug information is produced for symbols that are not actually used. Use this option if you want debug information for all symbols.",0,0,others,gcc,0,0
1865,-fno-eliminate-unused-debug-types,"Normally, when producing DWARF output, GCC avoids producing debug symbol output for types that are nowhere used in the source file being compiled. Sometimes it is useful to have GCC emit debugging information for all types declared in a compilation unit, regardless of whether or not they are actually used in that compilation unit, for example if, in the debugger, you want to cast a value to a type that is not actually used in your program (but is declared). More often, however, this results in a significant amount of wasted space.",0,0,others,gcc,0,0
1866,-fno-enforce-eh-specs,"Don't generate code to check for violation of exception specifications at run time. This option violates the C++ standard, but may be useful for reducing code size in production builds, much like defining NDEBUG. This does not give user code permission to throw exceptions in violation of the exception specifications; the compiler still optimizes based on the specifications, so throwing an unexpected exception results in undefined behavior at run time.",0,0,others,gcc,0,1
1867,-fno-extern-tls-init,"The C++11 and OpenMP standards allow thread_local and threadprivate variables to have dynamic (runtime) initialization. To support this, any use of such a variable goes through a wrapper function that performs any necessary initialization. When the use and definition of the variable are in the same translation unit, this overhead can be optimized away, but when the use is in a different translation unit there is significant overhead even if the variable doesn't actually need dynamic initialization. If the programmer can be sure that no use of the variable in a non-defining TU needs to trigger dynamic initialization (either because the variable is statically initialized, or a use of the variable in the defining TU will be executed before any uses in another TU), they can avoid this overhead with the -fno-extern-tls-init option.",0,0,others,gcc,0,0
1868,-fno-fp-int-builtin-inexact,"Do not allow the built-in functions ceil, floor, round and trunc, and their float and long double variants, to generate code that raises the nexactfloating-point exception for noninteger arguments. ISO C99 and C11 allow these functions to raise the nexactexception, but ISO/IEC TS 18661-1:2014, the C bindings to IEEE 754-2008, as integrated into ISO C2X, does not allow these functions to do so.",1,4,limited-side-effect,gcc,0,0
1869,-fno-function-cse,Do not put function addresses in registers; make each instruction that calls a constant function contain the function address explicitly.,1,4,limited-side-effect,gcc,0,0
1870,-fno-gnu-keywords,"Do not recognize typeof as a keyword, so that code can use this word as an identifier. You can use the keyword __typeof__ instead. This option is implied by the strict ISO C++ dialects: -ansi, -std=c++98, -std=c++11, etc.",0,0,others,gcc,0,0
1871,-fno-gnu-unique,"On systems with recent GNU assembler and C library, the C++ compiler uses the STB_GNU_UNIQUE binding to make sure that definitions of template static data members and static local variables in inline functions are unique even in the presence of RTLD_LOCAL; this is necessary to avoid problems with a library used by two different RTLD_LOCAL plugins depending on a definition in one of them and therefore disagreeing with the other one about the binding of the symbol. But this causes dlclose to be ignored for affected DSOs; if your program relies on reinitialization of a DSO via dlclose and dlopen, you can use -fno-gnu-unique.",0,0,others,gcc,0,0
1872,-fno-guess-branch-probability,Do not guess branch probabilities using heuristics.,1,4,limited-side-effect,gcc,0,0
1873,-fno-ident,Ignore the #ident directive.,0,0,others,gcc,0,0
1874,-fno-implement-inlines,"To save space, do not emit out-of-line copies of inline functions controlled by #pragma implementation. This causes linker errors if these functions are not inlined everywhere they are called.",0,0,others,gcc,0,0
1875,-fno-implicit-inline-templates,"Don't emit code for implicit instantiations of inline templates, either. The default is to handle inlines differently so that compiles with and without optimization need the same set of explicit instantiations.",0,0,others,gcc,0,0
1876,-fno-implicit-templates,"Never emit code for non-inline templates that are instantiated implicitly (i.e. by use); only emit code for explicit instantiations. If you use this option, you must take care to structure your code to include all the necessary explicit instantiations to avoid getting undefined symbols at link time. See Template Instantiation, for more information.",0,0,others,gcc,0,0
1877,-fno-inline,Do not expand any functions inline apart from those marked with the always_inline attribute. This is the default when not optimizing.,1,4,limited-side-effect,gcc,0,0
1878,-fno-ira-share-save-slots,"Disable sharing of stack slots used for saving call-used hard registers living through a call. Each hard register gets a separate stack slot, and as a result function stack frames are larger.",1,4,limited-side-effect,gcc,0,1
1879,-fno-ira-share-spill-slots,"Disable sharing of stack slots allocated for pseudo-registers. Each pseudo-register that does not get a hard register gets a separate stack slot, and as a result function stack frames are larger.",1,4,limited-side-effect,gcc,0,0
1881,-fno-keep-inline-dllexport,"This is a more fine-grained version of -fkeep-inline-functions, which applies only to functions that are declared using the dllexport attribute or declspec. See Declaring Attributes of Functions.",1,4,limited-side-effect,gcc,0,1
1882,-fno-lifetime-dse,"In C++ the value of an object is only affected by changes within its lifetime: when the constructor begins, the object has an indeterminate value, and any changes during the lifetime of the object are dead when the object is destroyed. Normally dead store elimination will take advantage of this; if your code relies on the value of the object storage persisting beyond the lifetime of the object, you can use this flag to disable this optimization. To preserve stores before the constructor starts (e.g. because your operator new clears the object storage) but still treat the object as dead after the destructor, you can use -flifetime-dse=1. The default behavior can be explicitly selected with -flifetime-dse=2. -flifetime-dse=0 is equivalent to -fno-lifetime-dse.",1,4,limited-side-effect,gcc,0,0
1884,-fno-math-errno,"Do not set errno after calling math functions that are executed with a single instruction, e.g., sqrt. A program that relies on IEEE exceptions for math error handling may want to use this flag for speed while maintaining IEEE arithmetic compatibility.",1,4,limited-side-effect,gcc,0,0
1885,-fno-merge-debug-strings,Direct the linker to not merge together strings in the debugging information that are identical in different object files. Merging is not supported by all assemblers or linkers. Merging decreases the size of the debug information in the output file at the cost of increasing link processing time. Merging is enabled by default.,0,0,others,gcc,0,0
1886,-fno-module-lazy,Disable lazy module importing and module mapper creation.,0,0,others,gcc,0,0
1887,-fno-modules-ts,"Enable support for C++20 modules (See C++ Modules). The -fno-modules-ts is usually not needed, as that is the default. Even though this is a C++20 feature, it is not currently implicitly enabled by selecting that standard version.",0,0,others,gcc,0,0
1888,-fnon-call-exceptions,"Generate code that allows trapping instructions to throw exceptions. Note that this requires platform-specific runtime support that does not exist everywhere. Moreover, it only allows trapping instructions to throw exceptions, i.e. memory references or floating-point instructions. It does not allow exceptions to be thrown from arbitrary signal handlers such as SIGALRM.",0,0,others,gcc,0,0
1889,-fno-nil-receivers,Assume that all Objective-C message dispatches ([receiver message:arg]) in this translation unit ensure that the receiver is not nil. This allows for more efficient entry points in the runtime to be used. This option is only available in conjunction with the NeXT runtime and ABI version 0 or 1.,0,0,others,gcc,0,0
1890,-fno-nonansi-builtins,"Disable built-in declarations of functions that are not mandated by ANSI/ISO C. These include ffs, alloca, _exit, index, bzero, conjf, and other related functions.",0,0,others,gcc,0,0
1893,-fno-peephole2,"Disable any machine-specific peephole optimizations. The difference between -fno-peephole and -fno-peephole2 is in how they are implemented in the compiler; some targets use one, some use the other, a few use both.",1,4,limited-side-effect,gcc,0,0
1894,-fno-plt,"Do not use the PLT for external function calls in position-independent code. Instead, load the callee address at call sites from the GOT and branch to it. This leads to more efficient code by eliminating PLT stubs and exposing GOT loads to optimizations. On architectures such as 32-bit x86 where PLT stubs expect the GOT pointer in a specific register, this gives more register allocation freedom to the compiler. Lazy binding requires use of the PLT; with -fno-plt all external symbols are resolved at load time.",0,0,others,gcc,0,0
1895,-fno-pretty-templates,"When an error message refers to a specialization of a function template, the compiler normally prints the signature of the template followed by the template arguments and any typedefs or typenames in the signature (e.g. void f(T) [with T = int] rather than void f(int)) so that it's clear which template is involved. When an error message refers to a specialization of a class template, the compiler omits any template arguments that match the default template arguments for that template. If either of these behaviors make it harder to understand the error message rather than easier, you can use -fno-pretty-templates to disable them.",0,0,others,gcc,0,0
1896,-fno-printf-return-value,"Do not substitute constants for known return value of formatted output functions such as sprintf, snprintf, vsprintf, and vsnprintf (but not printf of fprintf). This transformation allows GCC to optimize or even eliminate branches based on the known return value of these functions called with arguments that are either constant, or whose values are known to be in a range that makes determining the exact return value possible. For example, when -fprintf-return-value is in effect, both the branch and the body of the if statement (but not the call to snprint) can be optimized away when i is a 32-bit or smaller integer because the return value is guaranteed to be at most 8.",1,4,limited-side-effect,gcc,0,0
1897,-fno-rtti,"Disable generation of information about every class with virtual functions for use by the C++ run-time type identification features (dynamic_cast and typeid). If you don't use those parts of the language, you can save some space by using this flag. Note that exception handling uses the same information, but G++ generates it as needed. The dynamic_cast operator can still be used for casts that do not require run-time type information, i.e. casts to void * or to unambiguous base classes.",0,0,others,gcc,0,0
1898,-fno-sanitize=all,"This option disables all previously enabled sanitizers. -fsanitize=all is not allowed, as some sanitizers cannot be used together.",1,2,security-tradeoff,gcc,0,0
1899,-fno-sched-interblock,"Disable instruction scheduling across basic blocks, which is normally enabled when scheduling before register allocation, i.e. with -fschedule-insns or at -O2 or higher.",1,4,limited-side-effect,gcc,0,1
1900,-fno-sched-spec,"Disable speculative motion of non-load instructions, which is normally enabled when scheduling before register allocation, i.e. with -fschedule-insns or at -O2 or higher.",1,4,limited-side-effect,gcc,0,0
1901,-fno-show-column,"Do not print column numbers in diagnostics. This may be necessary if diagnostics are being scanned by a program that does not understand the column numbers, such as dejagnu.",0,0,others,gcc,0,0
1902,-fno-signed-zeros,"Allow optimizations for floating-point arithmetic that ignore the signedness of zero. IEEE arithmetic specifies the behavior of distinct +0.0 and -0.0 values, which then prohibits simplification of expressions such as x+0.0 or 0.0*x (even with -ffinite-math-only). This option implies that the sign of a zero result isn't significant.",1,4,limited-side-effect,gcc,0,0
1904,-fno-threadsafe-statics,Do not emit the extra code to use the routines specified in the C++ ABI for thread-safe initialization of local statics. You can use this option to reduce code size slightly in code that doesn't need to be thread-safe.,1,4,limited-side-effect,gcc,0,0
1905,-fnothrow-opt,"Treat a throw() exception specification as if it were a noexcept specification to reduce or eliminate the text size overhead relative to a function with no exception specification. If the function has local variables of types with non-trivial destructors, the exception specification actually makes the function smaller because the EH cleanups for those variables can be optimized away. The semantic effect is that an exception thrown out of a function with such an exception specification results in a call to terminate rather than unexpected.",0,0,others,gcc,0,0
1906,-fno-toplevel-reorder,"Do not reorder top-level functions, variables, and asm statements. Output them in the same order that they appear in the input file. When this option is used, unreferenced static variables are not removed. This option is intended to support existing code that relies on a particular ordering. For new code, it is better to use attributes when possible.",1,4,limited-side-effect,gcc,0,0
1907,-fno-trapping-math,"Compile code assuming that floating-point operations cannot generate user-visible traps. These traps include division by zero, overflow, underflow, inexact result and invalid operation. This option requires that -fno-signaling-nans be in effect. Setting this option may allow faster code if one relies on non-stopIEEE arithmetic, for example.",1,4,limited-side-effect,gcc,0,0
1908,-fno-unsigned-bitfields,"These options control whether a bit-field is signed or unsigned, when the declaration does not use either signed or unsigned. By default, such a bit-field is signed, because this is consistent: the basic integer types such as int are signed types.",0,0,others,gcc,0,0
1909,-fno-use-cxa-get-exception-ptr,"Don't use the __cxa_get_exception_ptr runtime routine. This causes std::uncaught_exception to be incorrect, but is necessary if the runtime routine is not available.",0,0,others,gcc,0,0
1910,-fno-weak,"Do not use weak symbol support, even if it is provided by the linker. By default, G++ uses weak symbols if they are available. This option exists only for testing, and should not be used by end-users; it results in inferior code and has no benefits. This option may be removed in a future release of G++.",0,0,others,gcc,0,0
1911,-fno-zero-initialized-in-bss,"If the target supports a BSS section, GCC by default puts variables that are initialized to zero into BSS. This can save space in the resulting code.",1,4,limited-side-effect,gcc,0,0
1912,-fobjc-abi-version=n,"Use version n of the Objective-C ABI for the selected runtime. This option is currently supported only for the NeXT runtime. In that case, Version 0 is the traditional (32-bit) ABI without support for properties and other Objective-C 2.0 additions. Version 1 is the traditional (32-bit) ABI with support for properties and other Objective-C 2.0 additions. Version 2 is the modern (64-bit) ABI. If nothing is specified, the default is Version 0 on 32-bit target machines, and Version 2 on 64-bit target machines.",0,0,others,gcc,0,0
1913,-fobjc-call-cxx-cdtors,"For each Objective-C class, check if any of its instance variables is a C++ object with a non-trivial default constructor. If so, synthesize a special - (id) .cxx_construct instance method which runs non-trivial default constructors on any such instance variables, in order, and then return self. Similarly, check if any instance variable is a C++ object with a non-trivial destructor, and if so, synthesize a special - (void) .cxx_destruct method which runs all such default destructors, in reverse order.",0,0,others,gcc,0,0
1914,-fobjc-direct-dispatch,Allow fast jumps to the message dispatcher. On Darwin this is accomplished via the comm page.,0,0,others,gcc,0,0
1915,-fobjc-exceptions,"Enable syntactic support for structured exception handling in Objective-C, similar to what is offered by C++. This option is required to use the Objective-C keywords @try, @throw, @catch, @finally and @synchronized. This option is available with both the GNU runtime and the NeXT runtime (but not available in conjunction with the NeXT runtime on Mac OS X 10.2 and earlier).",0,0,others,gcc,0,0
1916,-fobjc-gc,Enable garbage collection (GC) in Objective-C and Objective-C++ programs. This option is only available with the NeXT runtime; the GNU runtime has a different garbage collection implementation that does not require special compiler flags.,1,6,function-tradeoff,gcc,0,1
1917,-fobjc-nilcheck,"For the NeXT runtime with version 2 of the ABI, check for a nil receiver in method invocations before doing the actual method call. This is the default and can be disabled using -fno-objc-nilcheck. Class methods and super calls are never checked for nil in this way no matter what this flag is set to. Currently this flag does nothing when the GNU runtime, or an older version of the NeXT runtime ABI, is used.",0,0,others,gcc,0,0
1918,-fobjc-std=objc1,"Conform to the language syntax of Objective-C 1.0, the language recognized by GCC 4.0. This only affects the Objective-C additions to the C/C++ language; it does not affect conformance to C/C++ standards, which is controlled by the separate C/C++ dialect option flags. When this option is used with the Objective-C or Objective-C++ compiler, any Objective-C syntax that is not recognized by GCC 4.0 is rejected. This is useful if you need to make sure that your Objective-C code can be compiled with older versions of GCC.",0,0,others,gcc,0,0
1919,-fomit-frame-pointer,"Omit the frame pointer in functions that don't need one. This avoids the instructions to save, set up and restore the frame pointer; on many targets it also makes an extra register available.",1,4,limited-side-effect,gcc,0,0
1920,-fopenacc,"Enable handling of OpenACC directives #pragma acc in C/C++ and !$acc in Fortran. When -fopenacc is specified, the compiler generates accelerated code according to the OpenACC Application Programming Interface v2.6 https://www.openacc.org. This option implies -pthread, and thus is only supported on targets that have support for -pthread.",0,0,others,gcc,0,0
1921,-fopenacc-dim=geom,"Specify default compute dimensions for parallel offload regions that do not explicitly specify. The geom value is a triple of separated sizes, in order ang orkerand, ector A size can be omitted, to use a target-specific default value.",0,0,others,gcc,0,1
1922,-fopenacc-kernels=mode,"Specify mode of OpenACC kernelsconstructs handling. With -fopenacc-kernels=decompose, OpenACC kernelsconstructs are decomposed into parts, a sequence of compute constructs, each then handled individually. This is work in progress. With -fopenacc-kernels=parloops, OpenACC kernelsconstructs are handled by the parloopspass, en bloc. This is the current default.",0,0,others,gcc,0,0
1923,-fopenmp,"Enable handling of OpenMP directives #pragma omp in C/C++ and !$omp in Fortran. When -fopenmp is specified, the compiler generates parallel code according to the OpenMP Application Program Interface v4.5 https://www.openmp.org. This option implies -pthread, and thus is only supported on targets that have support for -pthread. -fopenmp implies -fopenmp-simd.",0,0,others,gcc,0,0
1924,-fopenmp-simd,Enable handling of OpenMP SIMD directives with #pragma omp in C/C++ and !$omp in Fortran. Other OpenMP directives are ignored.,0,0,others,gcc,0,0
1925,-foptimize-sibling-calls,Optimize sibling and tail recursive calls.,1,4,limited-side-effect,gcc,0,0
1926,-foptimize-strlen,"Optimize various standard C string functions (e.g. strlen, strchr or strcpy) and their _FORTIFY_SOURCE counterparts into faster alternatives.",1,4,limited-side-effect,gcc,0,0
1927,-fopt-info-options=filename,"Controls optimization dumps from various optimization passes. If the optionsform is used, options is a list of separated option keywords to select the dump details and optimizations.",0,0,others,gcc,0,0
1929,-fpartial-inlining,Inline parts of functions. This option has any effect only when inlining itself is turned on by the -finline-functions or -finline-small-functions options.,1,4,limited-side-effect,gcc,0,0
1930,"-fpatchable-function-entry=N[,M]","Generate N NOPs right at the beginning of each function, with the function entry point before the Mth NOP. If M is omitted, it defaults to 0 so the function entry points to the address just at the first NOP. The NOP instructions reserve extra space which can be used to patch in any desired instrumentation at run time, provided that the code segment is writable. The amount of space is controllable indirectly via the number of NOPs; the NOP instruction used corresponds to the instruction emitted by the internal GCC back-end interface gen_nop. This behavior is target-specific and may also depend on the architecture variant and/or other compilation options.",0,0,others,gcc,0,0
1931,-fpcc-struct-return,"Return hortstruct and union values in memory like longer ones, rather than in registers. This convention is less efficient, but it has the advantage of allowing intercallability between GCC-compiled files and files compiled with other compilers, particularly the Portable C Compiler (pcc).",0,0,others,gcc,0,0
1932,-fpch-deps,"When using precompiled headers (see Precompiled Headers), this flag causes the dependency-output flags to also list the files from the precompiled header dependencies. If not specified, only the precompiled header are listed and not the files that were used to create it, because those files are not consulted when a precompiled header is used.",0,0,others,gcc,0,0
1933,-fpch-preprocess,"This option allows use of a precompiled header (see Precompiled Headers) together with -E. It inserts a special #pragma, #pragma GCC pch_preprocess ""filename"" in the output to mark the place where the precompiled header was found, and its filename. When -fpreprocessed is in use, GCC recognizes this #pragma and loads the PCH.",0,0,others,gcc,0,1
1934,-fpeel-loops,Peels loops for which there is enough information that they do not roll much (from profile feedback or static analysis). It also turns on complete loop peeling (i.e. complete removal of loops with small constant number of iterations).,1,4,limited-side-effect,gcc,0,0
1935,-fpermissive,"Downgrade some diagnostics about nonconformant code from errors to warnings. Thus, using -fpermissive allows some nonconforming code to compile.",0,0,others,gcc,0,0
1936,-fpermitted-flt-eval-methods=style,"ISO/IEC TS 18661-3 defines new permissible values for FLT_EVAL_METHOD that indicate that operations and constants with a semantic type that is an interchange or extended format should be evaluated to the precision and range of that type. These new values are a superset of those permitted under C99/C11, which does not specify the meaning of other positive values of FLT_EVAL_METHOD. As such, code conforming to C11 may not have been written expecting the possibility of the new values.",0,0,others,gcc,0,0
1937,-fpic,"Generate position-independent code (PIC) suitable for use in a shared library, if supported for the target machine. Such code accesses all constant addresses through a global offset table (GOT). The dynamic loader resolves the GOT entries when the program starts (the dynamic loader is not part of GCC; it is part of the operating system). If the GOT size for the linked executable exceeds a machine-specific maximum size, you get an error message from the linker indicating that -fpic does not work; in that case, recompile with -fPIC instead. (These maximums are 8k on the SPARC, 28k on AArch64 and 32k on the m68k and RS/6000. The x86 has no such limit.)",0,0,others,gcc,0,0
1938,-fPIC(capital),"If supported for the target machine, emit position-independent code, suitable for dynamic linking and avoiding any limit on the size of the global offset table. This option makes a difference on AArch64, m68k, PowerPC and SPARC.",0,0,others,gcc,0,0
1939,-fPIE,"These options are similar to -fpic and -fPIC, but the generated position-independent code can be only linked into executables. Usually these options are used to compile code that will be linked using the -pie GCC option.",0,0,others,gcc,0,0
1940,-fplan9-extensions,Accept some non-standard constructs used in Plan 9 code.,0,0,others,gcc,0,0
1941,-fplugin=name.so,"Load the plugin code in file name.so, assumed to be a shared object to be dlopen by the compiler. The base name of the shared object file is used to identify the plugin for the purposes of argument parsing (See -fplugin-arg-name-key=value below). Each plugin should define the callback functions specified in the Plugins API.",0,0,others,gcc,0,0
1943,-fpost-ipa-mem-report,Makes the compiler print some statistics about permanent memory allocation before or after interprocedural optimization.,1,6,function-tradeoff,gcc,0,0
1944,-fpredictive-commoning,"Perform predictive commoning optimization, i.e., reusing computations (especially memory loads and stores) performed in previous iterations of loops.",1,4,limited-side-effect,gcc,0,0
1945,-fprefetch-loop-arrays,"If supported by the target machine, generate instructions to prefetch memory to improve the performance of loops that access large arrays.",1,4,limited-side-effect,gcc,0,1
1948,-fprofile-arcs,"Add code so that program flow arcs are instrumented. During execution the program records how many times each branch and call is executed and how many times it is taken or returns. On targets that support constructors with priority support, profiling properly handles constructors, destructors and C++ constructors (and destructors) of classes which are used as a type of a global variable.",0,0,others,gcc,0,0
1949,-fprofile-correction,"Profiles collected using an instrumented binary for multi-threaded programs may be inconsistent due to missed counter updates. When this option is specified, GCC uses heuristics to correct or smooth out such inconsistencies. By default, GCC emits an error message when an inconsistent profile is detected.",1,4,limited-side-effect,gcc,0,0
1951,-fprofile-exclude-files=regex,Instrument only functions from files whose name does not match any of the regular expressions (separated by semi-colons).,0,0,others,gcc,0,0
1952,-fprofile-filter-files=regex,Instrument only functions from files whose name matches any of the regular expressions (separated by semi-colons).,0,0,others,gcc,0,0
1953,-fprofile-generate=path,Enable options usually used for instrumenting application to produce profile useful for later recompilation with profile feedback based optimization. You must use -fprofile-generate both when compiling and when linking your program.,0,0,others,gcc,0,0
1954,-fprofile-info-section=name,"Register the profile information in the specified section instead of using a constructor/destructor. The section name is name if it is specified, otherwise the section name defaults to .gcov_info. A pointer to the profile information generated by -fprofile-arcs or -ftest-coverage is placed in the specified section for each translation unit. This option disables the profile information registration through a constructor and it disables the profile information processing through a destructor. This option is not intended to be used in hosted environments such as GNU/Linux. It targets systems with limited resources which do not support constructors and destructors. The linker could collect the input sections in a continuous memory block and define start and end symbols. The runtime support could dump the profiling information registered in this linker set during program termination to a serial line for example. A GNU linker script example which defines a linker output section follows:",0,0,others,gcc,0,0
1956,-fprofile-partial-training,"With -fprofile-use all portions of programs not executed during train run are optimized agressively for size rather than speed. In some cases it is not practical to train all possible hot paths in the program. (For example, program may contain functions specific for a given hardware and trianing may not cover all hardware configurations program is run on.) With -fprofile-partial-training profile feedback will be ignored for all functions not executed during the train run leading them to be optimized as if they were compiled without profile feedback. This leads to better performance when train run is not representative but also leads to significantly bigger code.",1,4,limited-side-effect,gcc,0,0
1957,-fprofile-prefix-path=path,This option can be used in combination with profile-generate=profile_dir and profile-use=profile_dir to inform GCC where is the base directory of built source tree. By default profile_dir will contain files with mangled absolute paths of all object files in the built project. This is not desirable when directory used to build the instrumented binary differs from the directory used to build the binary optimized with profile feedback because the profile data will not be found during the optimized build. In such setups -fprofile-prefix-path=path with path pointing to the base directory of the build can be used to strip the irrelevant part of the path and keep all file names relative to the main build directory.,0,0,others,gcc,0,0
1958,-fprofile-reorder-functions,Function reordering based on profile instrumentation collects first time of execution of a function and orders these functions in ascending order.,1,4,limited-side-effect,gcc,0,0
1959,-fprofile-report,Makes the compiler print some statistics about consistency of the (estimated) profile and effect of individual passes.,1,6,function-tradeoff,gcc,0,0
1960,-fprofile-reproducible=[multithreaded|parallel-runs|serial],"Control level of reproducibility of profile gathered by -fprofile-generate. This makes it possible to rebuild program with same outcome which is useful, for example, for distribution packages.",0,0,others,gcc,0,0
1961,-fprofile-update=method,"Alter the update method for an application instrumented for profile feedback based optimization. The method argument should be one of single atomicor prefer-atomic The first one is useful for single-threaded applications, while the second one prevents profile corruption by emitting thread-safe code.",0,0,others,gcc,0,0
1962,-fprofile-use=path,"Enable profile feedback-directed optimizations, and the following optimizations, many of which are generally profitable only with profile feedback available:",1,4,limited-side-effect,gcc,0,1
1963,-fprofile-values,"If combined with -fprofile-arcs, it adds code so that some data about values of expressions in the program is gathered.",1,4,limited-side-effect,gcc,0,1
1964,-frandom-seed=string,This option provides a seed that GCC uses in place of random numbers in generating certain symbol names that have to be different in every compiled file. It is also used to place unique stamps in coverage data files and the object files that produce them. You can use the -frandom-seed option to produce reproducibly identical object files.,0,0,others,gcc,0,0
1965,-freciprocal-math,"Allow the reciprocal of a value to be used instead of dividing by the value if this enables optimizations. For example x / y can be replaced with x * (1/y), which is useful if (1/y) is subject to common subexpression elimination. Note that this loses precision and increases the number of flops operating on the value.",1,4,limited-side-effect,gcc,0,0
1967,-free,"Attempt to remove redundant extension instructions. This is especially helpful for the x86-64 architecture, which implicitly zero-extends in 64-bit registers after writing to their lower 32-bit half.",1,4,limited-side-effect,gcc,0,0
1968,-freg-struct-return,Return struct and union values in registers when possible. This is more efficient for small structures than -fpcc-struct-return.,0,0,others,gcc,0,0
1969,-frename-registers,"Attempt to avoid false dependencies in scheduled code by making use of registers left over after register allocation. This optimization most benefits processors with lots of registers. Depending on the debug information format adopted by the target, however, it can make debugging impossible, since variables no longer stay in a some register",1,4,limited-side-effect,gcc,0,0
1970,-freorder-blocks,Reorder basic blocks in the compiled function in order to reduce number of taken branches and improve code locality.,1,4,limited-side-effect,gcc,0,0
1971,-freorder-blocks-algorithm=algorithm,"Use the specified algorithm for basic block reordering. The algorithm argument can be simple which does not increase code size (except sometimes due to secondary effects like alignment), or stc the software trace cachealgorithm, which tries to put all often executed code together, minimizing the number of branches executed by making extra copies of code.",1,4,limited-side-effect,gcc,0,1
1972,-freorder-blocks-and-partition,"In addition to reordering basic blocks in the compiled function, in order to reduce number of taken branches, partitions hot and cold basic blocks into separate sections of the assembly and .o files, to improve paging and cache locality performance.",1,4,limited-side-effect,gcc,0,1
1973,-freorder-functions,Reorder functions in the object file in order to improve code locality. This is implemented by using special subsections .text.hot for most frequently executed functions and .text.unlikely for unlikely executed functions. Reordering is done by the linker so object file format must support named sections and linker must place them in a reasonable way.,1,4,limited-side-effect,gcc,0,0
1974,-freplace-objc-classes,"Emit a special marker instructing ld(1) not to statically link in the resulting object file, and allow dyld(1) to load it in at run time instead. This is used in conjunction with the Fix-and-Continue debugging mode, where the object file in question may be recompiled and dynamically reloaded in the course of program execution, without the need to restart the program itself. Currently, Fix-and-Continue functionality is only available in conjunction with the NeXT runtime on Mac OS X 10.3 and later.",0,0,others,gcc,0,0
1975,-freport-bug,Collect and dump debug information into a temporary file if an internal compiler error (ICE) occurs.,1,6,function-tradeoff,gcc,0,1
1976,-frerun-cse-after-loop,Re-run common subexpression elimination after loop optimizations are performed.,1,4,limited-side-effect,gcc,0,0
1977,-freschedule-modulo-scheduled-loops,"Modulo scheduling is performed before traditional scheduling. If a loop is modulo scheduled, later scheduling passes may change its schedule. Use this option to control that behavior.",1,4,limited-side-effect,gcc,0,0
1978,-frounding-math,"Disable transformations and optimizations that assume default floating-point rounding behavior. This is round-to-zero for all floating point to integer conversions, and round-to-nearest for all other arithmetic truncations. This option should be specified for programs that change the FP rounding mode dynamically, or that may be executed with a non-default rounding mode. This option disables constant folding of floating-point expressions at compile time (which may be affected by rounding mode) and arithmetic transformations that are unsafe in the presence of sign-dependent rounding modes.",1,4,limited-side-effect,gcc,0,0
1979,-fsanitize=address,"Enable AddressSanitizer, a fast memory error detector. Memory access instructions are instrumented to detect out-of-bounds and use-after-free bugs. The option enables -fsanitize-address-use-after-scope. See https://github.com/google/sanitizers/wiki/AddressSanitizer for more details. The run-time behavior can be influenced using the ASAN_OPTIONS environment variable. When set to help=1, the available options are shown at startup of the instrumented program. See https://github.com/google/sanitizers/wiki/AddressSanitizerFlags#run-time-flags for a list of supported options. The option cannot be combined with -fsanitize=thread or -fsanitize=hwaddress. Note that the only target -fsanitize=hwaddress is currently supported on is AArch64.",0,0,others,gcc,0,0
1980,-fsanitize=alignment,"This option enables checking of alignment of pointers when they are dereferenced, or when a reference is bound to insufficiently aligned target, or when a method or constructor is invoked on insufficiently aligned object.",0,0,others,gcc,0,0
1981,-fsanitize=bool,"This option enables instrumentation of loads from bool. If a value other than 0/1 is loaded, a run-time error is issued.",0,0,others,gcc,0,0
1982,-fsanitize=bounds,"This option enables instrumentation of array bounds. Various out of bounds accesses are detected. Flexible array members, flexible array member-like arrays, and initializers of variables with static storage are not instrumented.",0,0,others,gcc,0,0
1983,-fsanitize=bounds-strict,"This option enables strict instrumentation of array bounds. Most out of bounds accesses are detected, including flexible array members and flexible array member-like arrays. Initializers of variables with static storage are not instrumented.",0,0,others,gcc,0,0
1984,-fsanitize=builtin,"This option enables instrumentation of arguments to selected builtin functions. If an invalid value is passed to such arguments, a run-time error is issued. E.g.passing 0 as the argument to __builtin_ctz or __builtin_clz invokes undefined behavior and is diagnosed by this option.",0,0,others,gcc,0,0
1985,-fsanitize=enum,"This option enables instrumentation of loads from an enum type. If a value outside the range of values for the enum type is loaded, a run-time error is issued.",0,0,others,gcc,0,0
1986,-fsanitize=float-cast-overflow,"This option enables floating-point type to integer conversion checking. We check that the result of the conversion does not overflow. Unlike other similar options, -fsanitize=float-cast-overflow is not enabled by -fsanitize=undefined. This option does not work well with FE_INVALID exceptions enabled.",0,0,others,gcc,0,1
1987,-fsanitize=float-divide-by-zero,"Detect floating-point division by zero. Unlike other similar options, -fsanitize=float-divide-by-zero is not enabled by -fsanitize=undefined, since floating-point division by zero can be a legitimate way of obtaining infinities and NaNs.",0,0,others,gcc,0,0
1988,-fsanitize=hwaddress,"Enable Hardware-assisted AddressSanitizer, which uses a hardware ability to ignore the top byte of a pointer to allow the detection of memory errors with a low memory overhead. Memory access instructions are instrumented to detect out-of-bounds and use-after-free bugs. The option enables -fsanitize-address-use-after-scope. See https://clang.llvm.org/docs/HardwareAssistedAddressSanitizerDesign.html for more details. The run-time behavior can be influenced using the HWASAN_OPTIONS environment variable. When set to help=1, the available options are shown at startup of the instrumented program. The option cannot be combined with -fsanitize=thread or -fsanitize=address, and is currently only available on AArch64.",0,0,others,gcc,0,0
1989,-fsanitize=integer-divide-by-zero,Detect integer division by zero as well as INT_MIN / -1 division.,0,0,others,gcc,0,0
1990,-fsanitize=kernel-address,Enable AddressSanitizer for Linux kernel. See https://github.com/google/kasan for more details.,1,2,security-tradeoff,gcc,0,0
1992,-fsanitize=leak,"Enable LeakSanitizer, a memory leak detector. This option only matters for linking of executables and the executable is linked against a library that overrides malloc and other allocator functions. See https://github.com/google/sanitizers/wiki/AddressSanitizerLeakSanitizer for more details. The run-time behavior can be influenced using the LSAN_OPTIONS environment variable. The option cannot be combined with -fsanitize=thread.",0,0,others,gcc,0,0
1993,-fsanitize=nonnull-attribute,"This option enables instrumentation of calls, checking whether null values are not passed to arguments marked as requiring a non-null value by the nonnull function attribute.",0,0,others,gcc,0,1
1994,-fsanitize=null,"This option enables pointer checking. Particularly, the application built with this option turned on will issue an error message when it tries to dereference a NULL pointer, or if a reference (possibly an rvalue reference) is bound to a NULL pointer, or if a method is invoked on an object pointed by a NULL pointer.",0,0,others,gcc,0,1
1995,-fsanitize=object-size,This option enables instrumentation of memory references using the __builtin_object_size function. Various out of bounds pointer accesses are detected.,0,0,others,gcc,0,0
1996,-fsanitize=pointer-compare,"Instrument comparison operation (<, <=, >, >=) with pointer operands. The option must be combined with either -fsanitize=kernel-address or -fsanitize=address The option cannot be combined with -fsanitize=thread. Note: By default the check is disabled at run time. To enable it, add detect_invalid_pointer_pairs=2 to the environment variable ASAN_OPTIONS. Using detect_invalid_pointer_pairs=1 detects invalid operation only when both pointers are non-null.",0,0,others,gcc,0,0
1997,-fsanitize=pointer-overflow,"This option enables instrumentation of pointer arithmetics. If the pointer arithmetics overflows, a run-time error is issued.",0,0,others,gcc,0,0
1998,-fsanitize=pointer-subtract,"Instrument subtraction with pointer operands. The option must be combined with either -fsanitize=kernel-address or -fsanitize=address The option cannot be combined with -fsanitize=thread. Note: By default the check is disabled at run time. To enable it, add detect_invalid_pointer_pairs=2 to the environment variable ASAN_OPTIONS. Using detect_invalid_pointer_pairs=1 detects invalid operation only when both pointers are non-null.",0,0,others,gcc,0,0
1999,-fsanitize=return,This option enables return statement checking. Programs built with this option turned on will issue an error message when the end of a non-void function is reached without actually returning a value. This option works in C++ only.,1,6,function-tradeoff,gcc,0,0
2000,-fsanitize=returns-nonnull-attribute,"This option enables instrumentation of return statements in functions marked with returns_nonnull function attribute, to detect returning of null values from such functions.",0,0,others,gcc,0,1
2001,-fsanitize=shift,"This option enables checking that the result of a shift operation is not undefined. Note that what exactly is considered undefined differs slightly between C and C++, as well as between ISO C90 and C99, etc. This option has two suboptions, -fsanitize=shift-base and -fsanitize=shift-exponent.",0,0,others,gcc,0,1
2002,-fsanitize=shift-base,"If the second argument of a shift operation is within range, check that the result of a shift operation is not undefined. Note that what exactly is considered undefined differs slightly between C and C++, as well as between ISO C90 and C99, etc.",1,2,security-tradeoff,gcc,0,0
2003,-fsanitize=shift-exponent,This option enables checking that the second argument of a shift operation is not negative and is smaller than the precision of the promoted first argument.,0,0,others,gcc,0,0
2004,-fsanitize=signed-integer-overflow,"This option enables signed integer overflow checking. We check that the result of +, *, and both unary and binary - does not overflow in the signed arithmetics. Note, integer promotion rules must be taken into account. That is, the following is not an overflow:",0,0,others,gcc,0,1
2005,-fsanitize=thread,"Enable ThreadSanitizer, a fast data race detector. Memory access instructions are instrumented to detect data race bugs. See https://github.com/google/sanitizers/wiki#threadsanitizer for more details. The run-time behavior can be influenced using the TSAN_OPTIONS environment variable; see https://github.com/google/sanitizers/wiki/ThreadSanitizerFlags for a list of supported options. The option cannot be combined with -fsanitize=address, -fsanitize=leak.",0,0,others,gcc,0,0
2006,-fsanitize=undefined,"Enable UndefinedBehaviorSanitizer, a fast undefined behavior detector. Various computations are instrumented to detect undefined behavior at runtime. Current suboptions are:",0,0,others,gcc,0,0
2007,-fsanitize=unreachable,"With this option, the compiler turns the __builtin_unreachable call into a diagnostics message call instead. When reaching the __builtin_unreachable call, the behavior is undefined.",0,0,others,gcc,0,0
2008,-fsanitize=vla-bound,This option instructs the compiler to check that the size of a variable length array is positive.,0,0,others,gcc,0,0
2009,-fsanitize=vptr,"This option enables instrumentation of C++ member function calls, member accesses and some conversions between pointers to base and derived classes, to verify the referenced object has the correct dynamic type.",1,2,security-tradeoff,gcc,0,0
2010,-fsanitize-address-use-after-scope,Enable sanitization of local variables to detect use-after-scope bugs. The option sets -fstack-reuse to none,0,0,others,gcc,0,0
2011,-fsanitize-coverage=trace-cmp,"Enable dataflow guided fuzzing code instrumentation. Inserts a call to __sanitizer_cov_trace_cmp1, __sanitizer_cov_trace_cmp2, __sanitizer_cov_trace_cmp4 or __sanitizer_cov_trace_cmp8 for integral comparison with both operands variable or __sanitizer_cov_trace_const_cmp1, __sanitizer_cov_trace_const_cmp2, __sanitizer_cov_trace_const_cmp4 or __sanitizer_cov_trace_const_cmp8 for integral comparison with one operand constant, __sanitizer_cov_trace_cmpf or __sanitizer_cov_trace_cmpd for float or double comparisons and __sanitizer_cov_trace_switch for switch statements.",0,0,others,gcc,0,0
2012,-fsanitize-coverage=trace-pc,Enable coverage-guided fuzzing code instrumentation. Inserts a call to __sanitizer_cov_trace_pc into every basic block.,1,2,security-tradeoff,gcc,0,0
2013,"-fsanitize-sections=s1,s2,...",Sanitize global variables in selected user-defined sections. si may contain wildcards.,0,0,others,gcc,0,0
2014,-fsanitize-undefined-trap-on-error,"The -fsanitize-undefined-trap-on-error option instructs the compiler to report undefined behavior using __builtin_trap rather than a libubsan library routine. The advantage of this is that the libubsan library is not needed and is not linked in, so this is usable even in freestanding environments.",0,0,others,gcc,0,0
2015,-fsave-optimization-record,"Write a SRCFILE.opt-record.json.gz file detailing what optimizations were performed, for those optimizations that support -fopt-info.",0,0,others,gcc,0,0
2016,-fsched2-use-superblocks,"When scheduling after register allocation, use superblock scheduling. This allows motion across basic block boundaries, resulting in faster schedules. This option is experimental, as not all machine descriptions used by GCC model the CPU closely enough to avoid unreliable results from the algorithm.",1,4,limited-side-effect,gcc,0,0
2017,-fsched-critical-path-heuristic,"Enable the critical-path heuristic in the scheduler. This heuristic favors instructions on the critical path. This is enabled by default when scheduling is enabled, i.e. with -fschedule-insns or -fschedule-insns2 or at -O2 or higher.",1,4,limited-side-effect,gcc,0,1
2018,-fsched-dep-count-heuristic,"Enable the dependent-count heuristic in the scheduler. This heuristic favors the instruction that has more instructions depending on it. This is enabled by default when scheduling is enabled, i.e. with -fschedule-insns or -fschedule-insns2 or at -O2 or higher.",1,4,limited-side-effect,gcc,0,0
2019,-fsched-group-heuristic,"Enable the group heuristic in the scheduler. This heuristic favors the instruction that belongs to a schedule group. This is enabled by default when scheduling is enabled, i.e. with -fschedule-insns or -fschedule-insns2 or at -O2 or higher.",1,4,limited-side-effect,gcc,0,0
2020,-fsched-last-insn-heuristic,"Enable the last-instruction heuristic in the scheduler. This heuristic favors the instruction that is less dependent on the last instruction scheduled. This is enabled by default when scheduling is enabled, i.e. with -fschedule-insns or -fschedule-insns2 or at -O2 or higher.",1,4,limited-side-effect,gcc,0,0
2021,-fsched-pressure,"Enable register pressure sensitive insn scheduling before register allocation. This only makes sense when scheduling before register allocation is enabled, i.e. with -fschedule-insns or at -O2 or higher. Usage of this option can improve the generated code and decrease its size by preventing register pressure increase above the number of available hard registers and subsequent spills in register allocation.",1,4,limited-side-effect,gcc,0,0
2022,-fsched-rank-heuristic,"Enable the rank heuristic in the scheduler. This heuristic favors the instruction belonging to a basic block with greater size or frequency. This is enabled by default when scheduling is enabled, i.e. with -fschedule-insns or -fschedule-insns2 or at -O2 or higher.",1,4,limited-side-effect,gcc,0,0
2023,-fsched-spec-insn-heuristic,"Enable the speculative instruction heuristic in the scheduler. This heuristic favors speculative instructions with greater dependency weakness. This is enabled by default when scheduling is enabled, i.e. with -fschedule-insns or -fschedule-insns2 or at -O2 or higher.",1,4,limited-side-effect,gcc,0,0
2024,-fsched-spec-load,"Allow speculative motion of some load instructions. This only makes sense when scheduling before register allocation, i.e. with -fschedule-insns or at -O2 or higher.",1,4,limited-side-effect,gcc,0,1
2025,-fsched-spec-load-dangerous,"Allow speculative motion of more load instructions. This only makes sense when scheduling before register allocation, i.e. with -fschedule-insns or at -O2 or higher.",1,4,limited-side-effect,gcc,0,0
2026,-fsched-stalled-insns=n,"Define how many insns (if any) can be moved prematurely from the queue of stalled insns into the ready list during the second scheduling pass. -fno-sched-stalled-insns means that no insns are moved prematurely, -fsched-stalled-insns=0 means there is no limit on how many queued insns can be moved prematurely. -fsched-stalled-insns without a value is equivalent to -fsched-stalled-insns=1.",1,4,limited-side-effect,gcc,0,0
2027,-fsched-stalled-insns-dep=n,"Define how many insn groups (cycles) are examined for a dependency on a stalled insn that is a candidate for premature removal from the queue of stalled insns. This has an effect only during the second scheduling pass, and only if -fsched-stalled-insns is used. -fno-sched-stalled-insns-dep is equivalent to -fsched-stalled-insns-dep=0. -fsched-stalled-insns-dep without a value is equivalent to -fsched-stalled-insns-dep=1.",1,4,limited-side-effect,gcc,0,0
2028,-fschedule-fusion,Performs a target dependent pass over the instruction stream to schedule instructions of same type together because target machine can execute them more efficiently if they are adjacent to each other in the instruction flow.,1,4,limited-side-effect,gcc,0,0
2029,-fschedule-insns,"If supported for the target machine, attempt to reorder instructions to eliminate execution stalls due to required data being unavailable. This helps machines that have slow floating point or memory load instructions by allowing other instructions to be issued until the result of the load or floating-point instruction is required.",1,4,limited-side-effect,gcc,0,1
2030,-fschedule-insns2,"Similar to -fschedule-insns, but requests an additional pass of instruction scheduling after register allocation has been done. This is especially useful on machines with a relatively small number of registers and where memory load instructions take more than one cycle.",1,4,limited-side-effect,gcc,0,0
2031,-fsched-verbose=n,"On targets that use instruction scheduling, this option controls the amount of debugging output the scheduler prints to the dump files.",1,6,function-tradeoff,gcc,0,0
2032,-fsection-anchors,Try to reduce the number of symbolic address calculations by using shared synchorsymbols to address nearby objects. This transformation can help to reduce the number of GOT entries and GOT accesses on some targets.,1,4,limited-side-effect,gcc,0,0
2033,-fselective-scheduling,Schedule instructions using selective scheduling algorithm. Selective scheduling runs instead of the first scheduler pass.,1,4,limited-side-effect,gcc,0,0
2034,-fselective-scheduling2,Schedule instructions using selective scheduling algorithm. Selective scheduling runs instead of the second scheduler pass.,1,4,limited-side-effect,gcc,0,0
2035,-fsel-sched-pipelining,Enable software pipelining of innermost loops during selective scheduling. This option has no effect unless one of -fselective-scheduling or -fselective-scheduling2 is turned on.,1,4,limited-side-effect,gcc,0,0
2036,-fsel-sched-pipelining-outer-loops,"When pipelining loops during selective scheduling, also pipeline outer loops. This option has no effect unless -fsel-sched-pipelining is turned on.",1,4,limited-side-effect,gcc,0,0
2037,-fsemantic-interposition,"Some object formats, like ELF, allow interposing of symbols by the dynamic linker. This means that for symbols exported from the DSO, the compiler cannot perform interprocedural propagation, inlining and other optimizations in anticipation that the function or variable in question may change. While this feature is useful, for example, to rewrite memory allocation functions by a debugging implementation, it is expensive in the terms of code quality. With -fno-semantic-interposition the compiler assumes that if interposition happens for functions the overwriting function will have precisely the same semantics (and side effects). Similarly if interposition happens for variables, the constructor of the variable will be the same. The flag has no effect for functions explicitly declared inline (where it is never allowed for interposition to change semantics) and for symbols explicitly declared weak.",1,4,limited-side-effect,gcc,0,0
2038,-fshort-enums,"Allocate to an enum type only as many bytes as it needs for the declared range of possible values. Specifically, the enum type is equivalent to the smallest integer type that has enough room.",0,0,others,gcc,0,1
2039,-fshort-wchar,Override the underlying type for wchar_t to be short unsigned int instead of the default for the target. This option is useful for building programs to run under WINE.,0,0,others,gcc,0,0
2040,-fshrink-wrap,"Emit function prologues only before parts of the function that need it, rather than at the top of the function. This flag is enabled by default at -O and higher.",1,6,function-tradeoff,gcc,0,0
2041,-fshrink-wrap-separate,"Shrink-wrap separate parts of the prologue and epilogue separately, so that those parts are only executed when needed. This option is on by default, but has no effect unless -fshrink-wrap is also turned on and the target supports this.",1,4,limited-side-effect,gcc,0,0
2042,-fsignaling-nans,Compile code assuming that IEEE signaling NaNs may generate user-visible traps during floating-point operations. Setting this option disables optimizations that may change the number of exceptions visible with signaling NaNs. This option implies -ftrapping-math.,1,4,limited-side-effect,gcc,0,0
2043,-fsigned-char,"Let the type char be signed, like signed char.",0,0,others,gcc,0,1
2044,-fsimd-cost-model=model,Alter the cost model used for vectorization of loops marked with the OpenMP simd directive. The model argument should be one of unlimited dynamic cheap All values of model have the same meaning as described in -fvect-cost-model and by default a cost model defined with -fvect-cost-model is used.,1,4,limited-side-effect,gcc,0,1
2045,-fsingle-precision-constant,Treat floating-point constants as single precision instead of implicitly converting them to double-precision constants.,1,4,limited-side-effect,gcc,0,0
2046,-fsized-deallocation,Enable the built-in global declarations,0,0,others,gcc,0,0
2047,fsm-maximum-phi-arguments,Maximum number of arguments a PHI may have before the FSM threader will not try to thread through its block.,1,4,limited-side-effect,gcc,0,0
2048,fsm-scale-path-blocks,Scale factor to apply to the number of blocks in a threading path when comparing to the number of (scaled) statements.,1,4,limited-side-effect,gcc,0,0
2049,fsm-scale-path-stmts,Scale factor to apply to the number of statements in a threading path when comparing to the number of (scaled) blocks.,1,4,limited-side-effect,gcc,0,0
2050,-fsplit-ivs-in-unroller,"Enables expression of values of induction variables in later iterations of the unrolled loop using the value in the first iteration. This breaks long dependency chains, thus improving efficiency of the scheduling passes.",1,4,limited-side-effect,gcc,0,0
2051,-fsplit-loops,Split a loop into two if it contains a condition that's always true for one side of the iteration space and false for the other.,1,4,limited-side-effect,gcc,0,1
2052,-fsplit-paths,Split paths leading to loop backedges. This can improve dead code elimination and common subexpression elimination. This is enabled by default at -O3 and above.,1,4,limited-side-effect,gcc,0,0
2053,-fsplit-stack,"Generate code to automatically split the stack before it overflows. The resulting program has a discontiguous stack which can only overflow if the program is unable to allocate any more memory. This is most useful when running threaded programs, as it is no longer necessary to calculate a good stack size to use for each thread. This is currently only implemented for the x86 targets running GNU/Linux.",1,3,reliability-tradeoff,gcc,0,0
2054,-fsplit-wide-types,"When using a type that occupies multiple registers, such as long long on a 32-bit system, split the registers apart and allocate them independently. This normally generates better code for those types, but may make debugging more difficult.",1,4,limited-side-effect,gcc,0,1
2055,-fsplit-wide-types-early,"Fully split wide types early, instead of very late. This option has no effect unless -fsplit-wide-types is turned on.",1,4,limited-side-effect,gcc,0,0
2056,-fssa-backprop,"Propagate information about uses of a value up the definition chain in order to simplify the definitions. For example, this pass strips sign operations if the sign of a value never matters. The flag is enabled by default at -O and higher.",1,4,limited-side-effect,gcc,0,0
2057,-fssa-phiopt,"Perform pattern matching on SSA PHI nodes to optimize conditional code. This pass is enabled by default at -O1 and higher, except for -Og.",1,4,limited-side-effect,gcc,0,0
2058,-fsso-struct=endianness,Set the default scalar storage order of structures and unions to the specified endianness. The accepted values are big-endian little-endianand nativefor the native endianness of the target (the default). This option is not supported for C++.,0,0,others,gcc,0,0
2059,-fstack-check,"Generate code to verify that you do not go beyond the boundary of the stack. You should specify this flag if you are running in an environment with multiple threads, but you only rarely need to specify it in a single-threaded environment since stack overflow is automatically detected on nearly all systems if there is only one stack.",0,0,others,gcc,0,0
2060,-fstack-clash-protection,"Generate code to prevent stack clash style attacks. When this option is enabled, the compiler will only allocate one page of stack space at a time and each page is accessed immediately after allocation. Thus, it prevents allocations from jumping over any stack guard page provided by the operating system.",0,0,others,gcc,0,0
2061,-fstack-protector,"Emit extra code to check for buffer overflows, such as stack smashing attacks. This is done by adding a guard variable to functions with vulnerable objects. This includes functions that call alloca, and functions with buffers larger than or equal to 8 bytes. The guards are initialized when a function is entered and then checked when the function exits. If a guard check fails, an error message is printed and the program exits. Only variables that are actually allocated on the stack are considered, optimized away variables or variables allocated in registers don't count.",1,2,security-tradeoff,gcc,0,0
2062,-fstack-protector-all,Like -fstack-protector except that all functions are protected.,0,0,others,gcc,0,0
2063,-fstack-protector-explicit,Like -fstack-protector but only protects those functions which have the stack_protect attribute.,0,0,others,gcc,0,1
2064,-fstack-protector-strong,"Like -fstack-protector but includes additional functions to be protected those that have local array definitions, or have references to local frame addresses. Only variables that are actually allocated on the stack are considered, optimized away variables or variables allocated in registers don't count.",1,2,security-tradeoff,gcc,0,0
2065,-fstack-reuse=reuse-level,"This option controls stack space reuse for user declared local/auto variables and compiler generated temporaries. reuse_level can be all named_vars or none allenables stack reuse for all local variables and temporaries, named_varsenables the reuse only for user defined local variables with names, and nonedisables stack reuse completely. The default value is all The option is needed when the program extends the lifetime of a scoped local variable or a compiler generated temporary beyond the end point defined by the language. When a lifetime of a variable ends, and if the variable lives in memory, the optimizing compiler has the freedom to reuse its stack space with other temporaries or scoped local variables whose live range does not overlap with it. Legacy code extending local lifetime is likely to break with the stack reuse optimization.",1,6,function-tradeoff,gcc,0,0
2066,-fstack-usage,"Makes the compiler output stack usage information for the program, on a per-function basis. The filename for the dump is made by appending .su to the auxname. auxname is generated from the name of the output file, if explicitly specified and it is not an executable, otherwise it is the basename of the source file. An entry is made up of three fields:",1,6,function-tradeoff,gcc,0,0
2067,-fstats,"Emit statistics about front-end processing at the end of the compilation. This option is supported only by the C++ front end, and the information is generally only useful to the G++ development team.",1,6,function-tradeoff,gcc,0,1
2068,-fstdarg-opt,Optimize the prologue of variadic argument functions with respect to usage of those arguments.,1,4,limited-side-effect,gcc,0,1
2069,-fstore-merging,Perform merging of narrow stores to consecutive memory addresses. This pass merges contiguous stores of immediate values narrower than a word into fewer wider stores to reduce the number of instructions. This is enabled by default at -O2 and higher as well as -Os.,1,4,limited-side-effect,gcc,0,0
2070,-fstrict-aliasing,"Allow the compiler to assume the strictest aliasing rules applicable to the language being compiled. For C (and C++), this activates optimizations based on the type of expressions. In particular, an object of one type is assumed never to reside at the same address as an object of a different type, unless the types are almost the same. For example, an unsigned int can alias an int, but not a void* or a double. A character type may alias any other type.",1,4,limited-side-effect,gcc,0,0
2071,-fstrict-enums,"Allow the compiler to optimize using the assumption that a value of enumerated type can only be one of the values of the enumeration (as defined in the C++ standard; basically, a value that can be represented in the minimum number of bits needed to represent all the enumerators). This assumption may not be valid if the program uses a cast to convert an arbitrary integer value to the enumerated type.",1,4,limited-side-effect,gcc,0,0
2072,-fstrict-overflow,This option implies -fno-wrapv -fno-wrapv-pointer and when negated implies -fwrapv -fwrapv-pointer.,0,0,others,gcc,0,0
2073,-fstrict-volatile-bitfields,"This option should be used if accesses to volatile bit-fields (or other structure fields, although the compiler usually honors those types anyway) should use a single access of the width of the field's type, aligned to a natural alignment if possible. For example, targets with memory-mapped peripheral registers might require all such accesses to be 16 bits wide; with this flag you can declare all peripheral bit-fields as unsigned short (assuming short is 16 bits on these targets) to force GCC to use 16-bit accesses instead of, perhaps, a more efficient 32-bit access.",0,0,others,gcc,0,1
2074,-fstrong-eval-order,"Evaluate member access, array subscripting, and shift expressions in left-to-right order, and evaluate assignment in right-to-left order, as adopted for C++17. Enabled by default with -std=c++17. -fstrong-eval-order=some enables just the ordering of member access and shift expressions, and is the default without -std=c++17.",0,0,others,gcc,0,0
2075,-fsync-libcalls,This option controls whether any out-of-line instance of the __sync family of functions may be used to implement the C++11 __atomic family of functions.,0,0,others,gcc,0,0
2076,-fsyntax-only,"Check the code for syntax errors, but don't do anything beyond that.",0,0,others,gcc,0,0
2077,-ftabstop=width,"Set the distance between tab stops. This helps the preprocessor report correct column numbers in warnings or errors, even if tabs appear on the line. If the value is less than 1 or greater than 100, the option is ignored. The default is 8.",0,0,others,gcc,0,0
2078,-ftemplate-backtrace-limit=n,Set the maximum number of template instantiation notes for a single warning or error to n. The default value is 10.,1,5,workload-specific,gcc,0,0
2079,-ftemplate-depth=n,"Set the maximum instantiation depth for template classes to n. A limit on the template instantiation depth is needed to detect endless recursions during template class instantiation. ANSI/ISO C++ conforming programs must not rely on a maximum depth greater than 17 (changed to 1024 in C++11). The default value is 900, as the compiler can run out of stack space before hitting 1024 in some situations.",0,0,others,gcc,0,0
2080,-ftest-coverage,Produce a notes file that the gcov code-coverage utility (see gcov's Test Coverage Program) can use to show program coverage. Each source file's note file is called auxname.gcno. Refer to the -fprofile-arcs option above for a description of auxname and instructions on how to generate test coverage data. Coverage data matches the source files more closely if you do not optimize.,0,0,others,gcc,0,0
2081,-fthread-jumps,"Perform optimizations that check to see if a jump branches to a location where another comparison subsumed by the first is found. If so, the first branch is redirected to either the destination of the second branch or a point immediately following it, depending on whether the condition is known to be true or false.",1,4,limited-side-effect,gcc,0,0
2082,-ftime-report,Makes the compiler print some statistics about the time consumed by each pass when it finishes.,1,6,function-tradeoff,gcc,0,0
2083,-ftime-report-details,Record the time consumed by infrastructure parts separately for each pass.,0,0,others,gcc,0,1
2084,-ftls-model=model,"Alter the thread-local storage model to be used (see Thread-Local). The model argument should be one of global-dynamic local-dynamic initial-execor local-exec Note that the choice is subject to optimization: the compiler may use a more efficient model for symbols not visible outside of the translation unit, or if -fpic is not given on the command line.",0,0,others,gcc,0,0
2085,-ftracer,Perform tail duplication to enlarge superblock size. This transformation simplifies the control flow of the function allowing other optimizations to do a better job.,1,4,limited-side-effect,gcc,0,0
2086,-ftrack-macro-expansion[=level],"Track locations of tokens across macro expansions. This allows the compiler to emit diagnostic about the current macro expansion stack when a compilation error occurs in a macro expansion. Using this option makes the preprocessor and the compiler consume more memory. The level parameter can be used to choose the level of precision of token location tracking thus decreasing the memory consumption if necessary. Value of level de-activates this option. Value tracks tokens locations in a degraded mode for the sake of minimal memory overhead. In this mode all tokens resulting from the expansion of an argument of a function-like macro have the same location. Value tracks tokens locations completely. This value is the most memory hungry. When this option is given no argument, the default parameter value is",0,0,others,gcc,0,0
2087,-ftrampolines,"For targets that normally need trampolines for nested functions, always generate them instead of using descriptors. Otherwise, for targets that do not need them, like for example HP-PA or IA-64, do nothing.",0,0,others,gcc,0,0
2088,-ftrapv,"This option generates traps for signed overflow on addition, subtraction, multiplication operations. The options -ftrapv and -fwrapv override each other, so using -ftrapv -fwrapv on the command-line results in -fwrapv being effective. Note that only active options override, so using -ftrapv -fwrapv -fno-wrapv on the command-line results in -ftrapv being effective.",0,0,others,gcc,0,0
2089,-ftree-bit-ccp,"Perform sparse conditional bit constant propagation on trees and propagate pointer alignment information. This pass only operates on local scalar variables and is enabled by default at -O1 and higher, except for -Og. It requires that -ftree-ccp is enabled.",1,4,limited-side-effect,gcc,0,1
2090,-ftree-builtin-call-dce,Perform conditional dead code elimination (DCE) for calls to built-in functions that may set errno but are otherwise free of side effects. This flag is enabled by default at -O2 and higher if -Os is not also specified.,1,4,limited-side-effect,gcc,0,0
2091,-ftree-ccp,Perform sparse conditional constant propagation (CCP) on trees. This pass only operates on local scalar variables and is enabled by default at -O and higher.,1,4,limited-side-effect,gcc,0,0
2092,-ftree-ch,"Perform loop header copying on trees. This is beneficial since it increases effectiveness of code motion optimizations. It also saves one jump. This flag is enabled by default at -O and higher. It is not enabled for -Os, since it usually increases code size.",1,4,limited-side-effect,gcc,0,0
2093,-ftree-coalesce-vars,"While transforming the program out of the SSA representation, attempt to reduce copying by coalescing versions of different user-defined variables, instead of just compiler temporaries. This may severely limit the ability to debug an optimized program compiled with -fno-var-tracking-assignments. In the negated form, this flag prevents SSA coalescing of user variables. This option is enabled by default if optimization is enabled, and it does very little otherwise.",1,4,limited-side-effect,gcc,0,0
2094,-ftree-copy-prop,Perform copy propagation on trees. This pass eliminates unnecessary copy operations. This flag is enabled by default at -O and higher.,1,4,limited-side-effect,gcc,0,0
2095,-ftree-dce,Perform dead code elimination (DCE) on trees. This flag is enabled by default at -O and higher.,1,4,limited-side-effect,gcc,0,0
2096,-ftree-dominator-opts,"Perform a variety of simple scalar cleanups (constant/copy propagation, redundancy elimination, range propagation and expression simplification) based on a dominator tree traversal. This also performs jump threading (to reduce jumps to jumps). This flag is enabled by default at -O and higher.",1,4,limited-side-effect,gcc,0,1
2097,-ftree-dse,Perform dead store elimination (DSE) on trees. A dead store is a store into a memory location that is later overwritten by another store without any intervening loads. In this case the earlier store can be deleted. This flag is enabled by default at -O and higher.,1,4,limited-side-effect,gcc,0,0
2098,-ftree-forwprop,Perform forward propagation on trees. This flag is enabled by default at -O and higher.,1,4,limited-side-effect,gcc,0,0
2099,-ftree-fre,"Perform full redundancy elimination (FRE) on trees. The difference between FRE and PRE is that FRE only considers expressions that are computed on all paths leading to the redundant computation. This analysis is faster than PRE, though it exposes fewer redundancies. This flag is enabled by default at -O and higher.",1,4,limited-side-effect,gcc,0,0
2100,-ftree-loop-distribute-patterns,"Perform loop distribution of patterns that can be code generated with calls to a library. This flag is enabled by default at -O2 and higher, and by -fprofile-use and -fauto-profile.",1,4,limited-side-effect,gcc,0,1
2101,-ftree-loop-distribution,"Perform loop distribution. This flag can improve cache performance on big loop bodies and allow further loop optimizations, like parallelization or vectorization, to take place. For example, the loop",1,4,limited-side-effect,gcc,0,0
2102,-ftree-loop-if-convert,Attempt to transform conditional jumps in the innermost loops to branch-less equivalents. The intent is to remove control-flow from the innermost loops in order to improve the ability of the vectorization pass to handle these loops. This is enabled by default if vectorization is enabled.,1,4,limited-side-effect,gcc,0,0
2103,-ftree-loop-im,"Perform loop invariant motion on trees. This pass moves only invariants that are hard to handle at RTL level (function calls, operations that expand to nontrivial sequences of insns). With -funswitch-loops it also moves operands of conditions that are invariant out of the loop, so that we can use just trivial invariantness analysis in loop unswitching. The pass also includes store motion.",1,4,limited-side-effect,gcc,0,0
2104,-ftree-loop-ivcanon,Create a canonical counter for number of iterations in loops for which determining number of iterations requires complicated analysis. Later optimizations then may determine the number easily. Useful especially in connection with unrolling.,1,4,limited-side-effect,gcc,0,0
2105,-ftree-loop-optimize,Perform loop optimizations on trees. This flag is enabled by default at -O and higher.,1,4,limited-side-effect,gcc,0,0
2106,-ftree-loop-vectorize,"Perform loop vectorization on trees. This flag is enabled by default at -O3 and by -ftree-vectorize, -fprofile-use, and -fauto-profile.",1,4,limited-side-effect,gcc,0,0
2107,-ftree-parallelize-loops=n,"Parallelize loops, i.e., split their iteration space to run in n threads. This is only possible for loops whose iterations are independent and can be arbitrarily reordered. The optimization is only profitable on multiprocessor machines, for loops that are CPU-intensive, rather than constrained e.g. by memory bandwidth. This option implies -pthread, and thus is only supported on targets that have support for -pthread.",1,1,resource,gcc,0,0
2108,-ftree-partial-pre,Make partial redundancy elimination (PRE) more aggressive. This flag is enabled by default at -O3.,1,4,limited-side-effect,gcc,0,0
2109,-ftree-phiprop,Perform hoisting of loads from conditional pointers on trees. This pass is enabled by default at -O and higher.,1,4,limited-side-effect,gcc,0,0
2110,-ftree-pre,Perform partial redundancy elimination (PRE) on trees. This flag is enabled by default at -O2 and -O3.,1,4,limited-side-effect,gcc,0,0
2111,-ftree-pta,"Perform function-local points-to analysis on trees. This flag is enabled by default at -O1 and higher, except for -Og.",1,4,limited-side-effect,gcc,0,0
2112,-ftree-reassoc,Perform reassociation on trees. This flag is enabled by default at -O and higher.,1,6,function-tradeoff,gcc,0,0
2113,-ftree-scev-cprop,"Perform final value replacement. If a variable is modified in a loop in such a way that its value when exiting the loop can be determined using only its initial value and the number of loop iterations, replace uses of the final value by such a computation, provided it is sufficiently cheap. This reduces data dependencies and may allow further simplifications. Enabled by default at -O and higher.",1,4,limited-side-effect,gcc,0,0
2114,-ftree-sink,Perform forward store motion on trees. This flag is enabled by default at -O and higher.,1,4,limited-side-effect,gcc,0,0
2115,-ftree-slp-vectorize,"Perform basic block vectorization on trees. This flag is enabled by default at -O3 and by -ftree-vectorize, -fprofile-use, and -fauto-profile.",1,4,limited-side-effect,gcc,0,0
2116,-ftree-slsr,Perform straight-line strength reduction on trees. This recognizes related expressions involving multiplications and replaces them by less expensive calculations when possible. This is enabled by default at -O and higher.,1,4,limited-side-effect,gcc,0,1
2117,-ftree-sra,"Perform scalar replacement of aggregates. This pass replaces structure references with scalars to prevent committing structures to memory too early. This flag is enabled by default at -O1 and higher, except for -Og.",1,4,limited-side-effect,gcc,0,0
2118,-ftree-switch-conversion,Perform conversion of simple initializations in a switch to initializations from a scalar array. This flag is enabled by default at -O2 and higher.,1,4,limited-side-effect,gcc,0,0
2119,-ftree-tail-merge,"Look for identical code sequences. When found, replace one with a jump to the other. This optimization is known as tail merging or cross jumping. This flag is enabled by default at -O2 and higher. The compilation time in this pass can be limited using max-tail-merge-comparisons parameter and max-tail-merge-iterations parameter.",1,4,limited-side-effect,gcc,0,0
2120,-ftree-ter,"Perform temporary expression replacement during the SSA->normal phase. Single use/single def temporaries are replaced at their use location with their defining expression. This results in non-GIMPLE code, but gives the expanders much more complex trees to work on resulting in better RTL generation. This is enabled by default at -O and higher.",1,4,limited-side-effect,gcc,0,0
2121,-ftree-vectorize,Perform vectorization on trees. This flag enables -ftree-loop-vectorize and -ftree-slp-vectorize if not explicitly specified.,1,4,limited-side-effect,gcc,0,1
2122,-ftree-vrp,"Perform Value Range Propagation on trees. This is similar to the constant propagation pass, but instead of values, ranges of values are propagated. This allows the optimizers to remove unnecessary range checks like array bound checks and null pointer checks. This is enabled by default at -O2 and higher. Null pointer check elimination is only done if -fdelete-null-pointer-checks is enabled.",1,4,limited-side-effect,gcc,0,1
2123,-funconstrained-commons,This option tells the compiler that variables declared in common blocks (e.g. Fortran) may later be overridden with longer trailing arrays. This prevents certain optimizations that depend on knowing the array bounds.,1,4,limited-side-effect,gcc,0,0
2124,-funit-at-a-time,"This option is left for compatibility reasons. -funit-at-a-time has no effect, while -fno-unit-at-a-time implies -fno-toplevel-reorder and -fno-section-anchors.",1,4,limited-side-effect,gcc,0,1
2125,-funroll-all-loops,"Unroll all loops, even if their number of iterations is uncertain when the loop is entered. This usually makes programs run more slowly. -funroll-all-loops implies the same options as -funroll-loops.",1,4,limited-side-effect,gcc,0,0
2126,-funroll-loops,"Unroll loops whose number of iterations can be determined at compile time or upon entry to the loop. -funroll-loops implies -frerun-cse-after-loop, -fweb and -frename-registers. It also turns on complete loop peeling (i.e. complete removal of loops with a small constant number of iterations). This option makes code larger, and may or may not make it run faster.",1,4,limited-side-effect,gcc,0,0
2127,-funsafe-math-optimizations,"Allow optimizations for floating-point arithmetic that (a) assume that arguments and results are valid and (b) may violate IEEE or ANSI standards. When used at link time, it may include libraries or startup files that change the default FPU control word or other similar optimizations.",1,4,limited-side-effect,gcc,0,0
2128,-funsigned-char,"Let the type char be unsigned, like unsigned char.",0,0,others,gcc,0,0
2129,-funswitch-loops,"Move branches with loop invariant conditions out of the loop, with duplicates of the loop on both branches (modified according to result of the condition).",1,4,limited-side-effect,gcc,0,0
2130,-funwind-tables,"Similar to -fexceptions, except that it just generates any needed static data, but does not affect the generated code in any other way. You normally do not need to enable this option; instead, a language processor that needs this handling enables it on your behalf.",0,0,others,gcc,0,0
2131,-fuse-cxa-atexit,"Register destructors for objects with static storage duration with the __cxa_atexit function rather than the atexit function. This option is required for fully standards-compliant handling of static destructors, but only works if your C library supports __cxa_atexit.",0,0,others,gcc,0,0
2132,-fuse-ld=bfd,Use the bfd linker instead of the default linker.,0,0,others,gcc,0,0
2133,-fuse-ld=gold,Use the gold linker instead of the default linker.,0,0,others,gcc,0,0
2134,-fuse-ld=lld,Use the LLVM lld linker instead of the default linker.,0,0,others,gcc,0,0
2135,-fuse-linker-plugin,"Enables the use of a linker plugin during link-time optimization. This option relies on plugin support in the linker, which is available in gold or in GNU ld 2.21 or newer.",1,4,limited-side-effect,gcc,0,0
2136,-fvariable-expansion-in-unroller,"With this option, the compiler creates multiple copies of some local variables when unrolling a loop, which can result in superior code.",1,4,limited-side-effect,gcc,0,0
2137,-fvar-tracking,Run variable tracking pass. It computes where variables are stored at each position in code. Better debugging information is then generated (if the debugging information format supports this information).,0,0,others,gcc,0,0
2138,-fvar-tracking-assignments,"Annotate assignments to user variables early in the compilation and attempt to carry the annotations over throughout the compilation all the way to the end, in an attempt to improve debug information while optimizing. Use of -gdwarf-4 is recommended along with it.",0,0,others,gcc,0,0
2139,-fvar-tracking-assignments-toggle,"Toggle -fvar-tracking-assignments, in the same way that -gtoggle toggles -g.",1,6,function-tradeoff,gcc,0,0
2140,-fvect-cost-model=model,"Alter the cost model used for vectorization. The model argument should be one of unlimited dynamic cheapor very-cheap With the unlimitedmodel the vectorized code-path is assumed to be profitable while with the dynamicmodel a runtime check guards the vectorized code-path to enable it only for iteration counts that will likely execute faster than when executing the original scalar loop. The cheapmodel disables vectorization of loops where doing so would be cost prohibitive for example due to required runtime checks for data dependence or alignment but otherwise is equal to the dynamicmodel. The very-cheapmodel only allows vectorization if the vector code would entirely replace the scalar code that is being vectorized. For example, if each iteration of a vectorized loop would only be able to handle exactly four iterations of the scalar loop, the very-cheapmodel would only allow vectorization if the scalar iteration count is known to be a multiple of four.",1,4,limited-side-effect,gcc,0,0
2141,-fverbose-asm,Put extra commentary information in the generated assembly code to make it more readable. This option is generally only of use to those who actually need to read the generated assembly code (perhaps while debugging the compiler itself).,1,6,function-tradeoff,gcc,0,1
2142,-fversion-loops-for-strides,"If a loop iterates over an array with a variable stride, create another version of the loop that assumes the stride is always one. For example:",1,4,limited-side-effect,gcc,0,0
2143,-fvisibility=[default|internal|hidden|protected],"Set the default ELF image symbol visibility to the specified option ll symbols are marked with this unless overridden within the code. Using this feature can very substantially improve linking and load times of shared object libraries, produce more optimized code, provide near-perfect API export and prevent symbol clashes. It is strongly recommended that you use this in any shared objects you distribute.",0,0,others,gcc,0,0
2144,-fvisibility-inlines-hidden,This switch declares that the user does not attempt to compare pointers to inline functions or methods where the addresses of the two functions are taken in different shared objects.,0,0,others,gcc,0,0
2145,-fvisibility-ms-compat,This flag attempts to use visibility settings to make GCC C++ linkage model compatible with that of Microsoft Visual Studio.,0,0,others,gcc,0,0
2146,-fvpt,"If combined with -fprofile-arcs, this option instructs the compiler to add code to gather information about values of expressions.",1,4,limited-side-effect,gcc,0,0
2147,-fvtable-verify=[std|preinit|none],"This option is only available when compiling C++ code. It turns on (or off, if using -fvtable-verify=none) the security feature that verifies at run time, for every virtual call, that the vtable pointer through which the call is made is valid for the type of the object, and has not been corrupted or overwritten. If an invalid vtable pointer is detected at run time, an error is reported and execution of the program is immediately halted.",0,0,others,gcc,0,0
2148,-fvtv-counts,"This is a debugging flag. When used in conjunction with -fvtable-verify=std or -fvtable-verify=preinit, this causes the compiler to keep track of the total number of virtual calls it encounters and the number of verifications it inserts. It also counts the number of calls to certain run-time library functions that it inserts and logs this information for each compilation unit. The compiler writes this information to a file named vtv_count_data.log in the directory named by the environment variable VTV_LOGS_DIR if that is defined or the current working directory otherwise. It also counts the size of the vtable pointer sets for each class, and writes this information to vtv_class_set_sizes.log in the same directory.",1,6,function-tradeoff,gcc,0,0
2149,-fvtv-debug,"When used in conjunction with -fvtable-verify=std or -fvtable-verify=preinit, causes debug versions of the runtime functions for the vtable verification feature to be called. This flag also causes the compiler to log information about which vtable pointers it finds for each class. This information is written to a file named vtv_set_ptr_data.log in the directory named by the environment variable VTV_LOGS_DIR if that is defined or the current working directory otherwise.",0,0,others,gcc,0,0
2150,-fweb,"Constructs webs as commonly used for register allocation purposes and assign each web individual pseudo register. This allows the register allocation pass to operate on pseudos directly, but also strengthens several other optimization passes, such as CSE, loop optimizer and trivial dead code remover. It can, however, make debugging impossible, since variables no longer stay in a some register",1,4,limited-side-effect,gcc,0,0
2151,-fwhole-program,Assume that the current compilation unit represents the whole program being compiled. All public functions and variables with the exception of main and those merged by attribute externally_visible become static functions and in effect are optimized more aggressively by interprocedural optimizers.,1,4,limited-side-effect,gcc,0,1
2152,-fwide-exec-charset=charset,"Set the wide execution character set, used for wide string and character constants. The default is UTF-32 or UTF-16, whichever corresponds to the width of wchar_t. As with -fexec-charset, charset can be any encoding supported by the system iconv library routine; however, you will have problems with encodings that do not fit exactly in wchar_t.",0,0,others,gcc,0,0
2153,-fworking-directory,"Enable generation of linemarkers in the preprocessor output that let the compiler know the current working directory at the time of preprocessing. When this option is enabled, the preprocessor emits, after the initial linemarker, a second linemarker with the current working directory followed by two slashes. GCC uses this directory, when it's present in the preprocessed input, as the directory emitted as the current working directory in some debugging information formats. This option is implicitly enabled if debugging information is enabled, but this can be inhibited with the negated form -fno-working-directory. If the -P flag is present in the command line, this option has no effect, since no #line directives are emitted whatsoever.",0,0,others,gcc,0,0
2154,-fwrapv,"This option instructs the compiler to assume that signed arithmetic overflow of addition, subtraction and multiplication wraps around using twos-complement representation. This flag enables some optimizations and disables others. The options -ftrapv and -fwrapv override each other, so using -ftrapv -fwrapv on the command-line results in -fwrapv being effective. Note that only active options override, so using -ftrapv -fwrapv -fno-wrapv on the command-line results in -ftrapv being effective.",1,4,limited-side-effect,gcc,0,1
2155,-fwrapv-pointer,This option instructs the compiler to assume that pointer arithmetic overflow on addition and subtraction wraps around using twos-complement representation. This flag disables some optimizations which assume pointer overflow is invalid.,0,0,others,gcc,0,1
2156,-fzero-call-used-regs=choice,Zero call-used registers at function return to increase program security by either mitigating Return-Oriented Programming (ROP) attacks or preventing information leakage through registers.,1,4,limited-side-effect,gcc,0,0
2157,-fzero-link,"When compiling for the NeXT runtime, the compiler ordinarily replaces calls to objc_getClass("") (when the name of the class is known at compile time) with static class references that get initialized at load time which improves run-time performance. Specifying the -fzero-link flag suppresses this behavior and causes calls to objc_getClass("""""") to be retained. This is useful in Zero-Link debugging mode",0,0,others,gcc,0,0
2158,-g,"Produce debugging information in the operating system's native format (stabs, COFF, XCOFF, or DWARF). GDB can work with this debugging information.",1,6,function-tradeoff,gcc,0,0
2159,-gas-loc-support,Inform the compiler that the assembler supports .loc directives. It may then use them for the assembler to generate DWARF2+ line number tables.,0,0,others,gcc,0,1
2160,-gas-locview-support,Inform the compiler that the assembler supports view assignment and reset assertion checking in .loc directives.,0,0,others,gcc,0,0
2161,gcse-after-reload-critical-fraction,The threshold ratio of critical edges execution count that permit performing redundancy elimination after reload.,1,5,workload-specific,gcc,0,0
2162,gcse-after-reload-partial-fraction,The threshold ratio for performing partial redundancy elimination after reload.,1,5,workload-specific,gcc,0,1
2163,gcse-cost-distance-ratio,"Scaling factor in calculation of maximum distance an expression can be moved by GCSE optimizations. This is currently supported only in the code hoisting pass. The bigger the ratio, the more aggressive code hoisting is with simple expressions, i.e., the expressions that have cost less than gcse-unrestricted-cost. Specifying 0 disables hoisting of simple expressions.",1,4,limited-side-effect,gcc,0,0
2164,gcse-unrestricted-cost,"Cost, roughly measured as the cost of a single typical machine instruction, at which GCSE optimizations do not constrain the distance an expression can travel. This is currently supported only in the code hoisting pass. The lesser the cost, the more aggressive code hoisting is. Specifying 0 allows all expressions to travel unrestricted distances.",1,4,limited-side-effect,gcc,0,0
2166,-gdwarf64,"If DWARF debugging information is enabled, the -gdwarf32 selects the 32-bit DWARF format and the -gdwarf64 selects the 64-bit DWARF format. The default is target specific, on most targets it is -gdwarf32 though. The 32-bit DWARF format is smaller, but can't support more than 2GiB of debug information in any of the DWARF debug information sections. The 64-bit DWARF format allows larger debug information and might not be well supported by all consumers yet.",0,0,others,gcc,0,0
2167,-gdwarf-version,"Produce debugging information in DWARF format (if that is supported). The value of version may be either 2, 3, 4 or 5; the default version for most targets is 5 (with the exception of VxWorks, TPF and Darwin/Mac OS X, which default to version 2, and AIX, which defaults to version 4).",0,0,others,gcc,0,0
2168,-gen-decls,Dump interface declarations for all classes seen in the source file to a file named sourcename.decl.,1,6,function-tradeoff,gcc,0,0
2169,ggc-min-expand,GCC uses a garbage collector to manage its own memory allocation. This parameter specifies the minimum percentage by which the garbage collector heap should be allowed to expand between collections. Tuning this may improve compilation speed; it has no effect on code generation.,1,4,limited-side-effect,gcc,0,0
2170,ggc-min-heapsize,"Minimum size of the garbage collector heap before it begins bothering to collect garbage. The first collection occurs after the heap expands by ggc-min-expand% beyond ggc-min-heapsize. Again, tuning this may improve compilation speed, and has no effect on code generation.",1,4,limited-side-effect,gcc,0,1
2171,-ggdb,"Produce debugging information for use by GDB. This means to use the most expressive format available (DWARF, stabs, or the native format if neither of those are supported), including GDB extensions if at all possible.",0,0,others,gcc,0,1
2172,-ggnu-pubnames,Generate .debug_pubnames and .debug_pubtypes sections in a format suitable for conversion into a GDB index. This option is only useful with a linker that can produce GDB index version 7.,0,0,others,gcc,0,0
2173,gimple-fe-computed-hot-bb-threshold,The number of executions of a basic block which is considered hot. The parameter is used only in GIMPLE FE.,1,4,limited-side-effect,gcc,0,0
2174,-gno-as-loc-support,"Force GCC to generate DWARF2+ line number tables internally, if DWARF2+ line number tables are to be generated.",0,0,others,gcc,0,0
2175,-gno-as-locview-support,"Force GCC to assign view numbers internally, if -gvariable-location-views are explicitly requested.",0,0,others,gcc,0,0
2176,-gno-column-info,"Emit location column information into DWARF debugging information, rather than just file and line. This option is enabled by default.",0,0,others,gcc,0,0
2177,-gno-inline-points,"Generate extended debug information for inlined functions. Location view tracking markers are inserted at inlined entry points, so that address and view numbers can be computed and output in debug information. This can be enabled independently of location views, in which case the view numbers won't be output, but it can only be enabled along with statement frontiers, and it is only enabled by default if location views are enabled.",0,0,others,gcc,0,0
2178,-gno-internal-reset-location-views,"Attempt to determine location views that can be omitted from location view lists. This requires the compiler to have very accurate insn length estimates, which isn't always the case, and it may cause incorrect view lists to be generated silently when using an assembler that does not support location view lists. The GNU assembler will flag any such error as a view number mismatch. This is only enabled on ports that define a reliable estimation function.",0,0,others,gcc,0,0
2180,-gno-statement-frontiers,"This option causes GCC to create markers in the internal representation at the beginning of statements, and to keep them roughly in place throughout compilation, using them to guide the output of is_stmt markers in the line number table. This is enabled by default when compiling with optimization (-Os, -O, -O2, , and outputting DWARF 2 debug information at the normal level.",0,0,others,gcc,0,1
2181,-gno-strict-dwarf,Allow using extensions of later DWARF standard version than selected with -gdwarf-version.,0,0,others,gcc,0,0
2182,-gno-variable-location-views,"Augment variable location lists with progressive view numbers implied from the line number table. This enables debug information consumers to inspect state at certain points of the program, even if no instructions associated with the corresponding source locations are present at that point. If the assembler lacks support for view numbers in line number tables, this will cause the compiler to emit the line number table, which generally makes them somewhat less compact. The augmented line number tables and location lists are fully backward-compatible, so they can be consumed by debug information consumers that are not aware of these augmentations, but they won't derive any benefit from them either.",0,0,others,gcc,0,0
2183,gnu++03,GNU dialect of -std=c++98.,0,0,others,gcc,0,0
2187,gnu++23,"GNU dialect of -std=c++2b. Support is highly experimental, and will almost certainly change in incompatible ways in future releases.",0,0,others,gcc,0,0
2189,gnu18,GNU dialect of ISO C17. This is the default for C code.,0,0,others,gcc,0,0
2192,gnu89,GNU dialect of ISO C90 (including some C99 features).,0,0,others,gcc,0,0
2194,-gpubnames,Generate DWARF .debug_pubnames and .debug_pubtypes sections.,0,0,others,gcc,0,0
2195,graphite-allow-codegen-errors,Whether codegen errors should be ICEs when -fchecking.,1,4,limited-side-effect,gcc,0,0
2196,graphite-max-arrays-per-scop,Maximum number of arrays per scop.,1,5,workload-specific,gcc,0,0
2197,graphite-max-nb-scop-params,"To avoid exponential effects in the Graphite loop transforms, the number of parameters in a Static Control Part (SCoP) is bounded. A value of zero can be used to lift the bound. A variable whose value is unknown at compilation time and defined outside a SCoP is a parameter of the SCoP.",1,5,workload-specific,gcc,0,0
2199,-gstabs,"Produce debugging information in stabs format (if that is supported), without GDB extensions. This is the format used by DBX on most BSD systems. On MIPS, Alpha and System V Release 4 systems this option produces stabs debugging output that is not understood by DBX. On System V Release 4 systems this option requires the GNU assembler.",0,0,others,gcc,0,1
2200,-gstabs+,"Produce debugging information in stabs format (if that is supported), using GNU extensions understood only by the GNU debugger (GDB). The use of these extensions is likely to make other debuggers crash or refuse to read the program.",0,0,others,gcc,0,0
2201,-gstrict-dwarf,Disallow using extensions of later DWARF standard version than selected with -gdwarf-version. On most targets using non-conflicting DWARF extensions from later standard versions is allowed.,0,0,others,gcc,0,1
2202,-gtoggle,"Turn off generation of debug info, if leaving out this option generates it, or turn it on at level 2 otherwise. The position of this argument in the command line does not matter; it takes effect after all other options are processed, and it does so only once, no matter how many times it is given. This is mainly intended to be used with -fcompare-debug.",1,6,function-tradeoff,gcc,0,0
2203,-gvms,Produce debugging information in Alpha/VMS debug format (if that is supported). This is the format used by DEBUG on Alpha/VMS systems.,0,0,others,gcc,0,0
2204,-gvmslevel,Request debugging information and also use level to specify how much information. The default level is 2.,1,6,function-tradeoff,gcc,0,0
2205,-gxcoff,Produce debugging information in XCOFF format (if that is supported). This is the format used by the DBX debugger on IBM RS/6000 systems.,0,0,others,gcc,0,0
2206,-gxcoff+,"Produce debugging information in XCOFF format (if that is supported), using GNU extensions understood only by the GNU debugger (GDB). The use of these extensions is likely to make other debuggers crash or refuse to read the program, and may cause assemblers other than the GNU assembler (GAS) to fail with an error.",0,0,others,gcc,0,0
2207,-gz[=type],"Produce compressed debug sections in DWARF format, if that is supported. If type is not given, the default type depends on the capabilities of the assembler and linker used. type may be one of none(don't compress debug sections), zlib(use zlib compression in ELF gABI format), or zlib-gnu(use zlib compression in traditional GNU format). If the linker doesn't support writing compressed debug sections, the option is rejected. Otherwise, if the assembler does not support them, -gz is silently ignored when producing object files.",1,6,function-tradeoff,gcc,0,1
2208,-H,"Print the name of each header file used, in addition to other normal activities. Each name is indented to show how deep in the includestack it is. Precompiled header files are also printed, even if they are found to be invalid; an invalid precompiled header file is printed with ..xand a valid one with ..!.",1,6,function-tradeoff,gcc,0,0
2209,hash-table-verification-limit,The number of elements for which hash table verification is done for each searched element.,1,4,limited-side-effect,gcc,0,0
2210,--help,Print (on the standard output) a description of the command-line options understood by the compiler that fit into all specified classes and qualifiers. These are the supported classes:,1,6,function-tradeoff,gcc,0,0
2211,hot-bb-count-fraction,"The denominator n of fraction 1/n of the maximal execution count of a basic block in the entire program that a basic block needs to at least have in order to be considered hot. The default is 10000, which means that a basic block is considered hot if its execution count is greater than 1/10000 of the maximal execution count. 0 means that it is never considered hot. Used in non-LTO mode.",1,4,limited-side-effect,gcc,0,0
2212,hot-bb-count-ws-permille,"The number of most executed permilles, ranging from 0 to 1000, of the profiled execution of the entire program to which the execution count of a basic block must be part of in order to be considered hot. The default is 990, which means that a basic block is considered hot if its execution count contributes to the upper 990 permilles, or 99.0%, of the profiled execution of the entire program. 0 means that it is never considered hot. Used in LTO mode.",1,4,limited-side-effect,gcc,0,0
2213,hot-bb-frequency-fraction,"The denominator n of fraction 1/n of the execution frequency of the entry block of a function that a basic block of this function needs to at least have in order to be considered hot. The default is 1000, which means that a basic block is considered hot in a function if it is executed more frequently than 1/1000 of the frequency of the entry block of the function. 0 means that it is never considered hot.",1,4,limited-side-effect,gcc,0,0
2214,hwasan-instrument-allocas,"Enable hwasan instrumentation of dynamically sized stack-allocated variables. This kind of instrumentation is enabled by default when using -fsanitize=hwaddress and disabled by default when using -fsanitize=kernel-hwaddress. To disable instrumentation of such variables use --param hwasan-instrument-allocas=0, and to enable it use --param hwasan-instrument-allocas=1.",1,4,limited-side-effect,gcc,0,1
2215,hwasan-instrument-mem-intrinsics,Enable hwasan instrumentation of builtin functions. Instrumentation of these builtin functions is enabled by default for both -fsanitize=hwaddress and -fsanitize=kernel-hwaddress. To disable instrumentation of builtin functions use --param hwasan-instrument-mem-intrinsics=0.,1,4,limited-side-effect,gcc,0,0
2216,hwasan-instrument-reads,Enable hwasan checks on memory reads. Instrumentation of reads is enabled by default for both -fsanitize=hwaddress and -fsanitize=kernel-hwaddress. To disable checking memory reads use --param hwasan-instrument-reads=0.,1,4,limited-side-effect,gcc,0,1
2217,hwasan-instrument-stack,"Enable hwasan instrumentation of statically sized stack-allocated variables. This kind of instrumentation is enabled by default when using -fsanitize=hwaddress and disabled by default when using -fsanitize=kernel-hwaddress. To disable stack instrumentation use --param hwasan-instrument-stack=0, and to enable it use --param hwasan-instrument-stack=1.",1,4,limited-side-effect,gcc,0,0
2218,hwasan-instrument-writes,Enable hwasan checks on memory writes. Instrumentation of writes is enabled by default for both -fsanitize=hwaddress and -fsanitize=kernel-hwaddress. To disable checking memory writes use --param hwasan-instrument-writes=0.,1,4,limited-side-effect,gcc,0,0
2219,hwasan-random-frame-tag,"When using stack instrumentation, decide tags for stack variables using a deterministic sequence beginning at a random tag for each frame. With this parameter unset tags are chosen using the same sequence but beginning from 1. This is enabled by default for -fsanitize=hwaddress and unavailable for -fsanitize=kernel-hwaddress. To disable it use --param hwasan-random-frame-tag=0.",1,4,limited-side-effect,gcc,0,1
2222,-imacros file,"Exactly like -include, except that any output produced by scanning file is thrown away. Macros it defines remain defined. This allows you to acquire all the macros from a header without also processing its declarations.",0,0,others,gcc,0,0
2225,inline,Enable dumps from all inlining optimizations.,1,6,function-tradeoff,gcc,0,0
2226,inline-clone,"Only enable inlining and cloning optimizations, which includes inlining, cloning, interprocedural scalar replacement of aggregates and partial inlining. As a result, when patching a function, all its callers and its clonescallers are impacted, therefore need to be patched as well.",1,4,limited-side-effect,gcc,0,0
2227,inline-heuristics-hint-percent,"The scale (in percents) applied to inline-insns-single, inline-insns-single-O2, inline-insns-auto when inline heuristics hints that inlining is very profitable (will enable later optimizations).",1,4,limited-side-effect,gcc,0,1
2228,inline-min-speedup,"When estimated performance improvement of caller + callee runtime exceeds this threshold (in percent), the function can be inlined regardless of the limit on --param max-inline-insns-single and --param max-inline-insns-auto.",1,5,workload-specific,gcc,0,0
2229,inline-only-static,"Only enable inlining of static functions. As a result, when patching a static function, all its callers are impacted and so need to be patched as well.",1,4,limited-side-effect,gcc,0,0
2230,inline-unit-growth,"Specifies maximal overall growth of the compilation unit caused by inlining. For example, parameter value 20 limits unit growth to 1.2 times the original size. Cold functions (either marked cold via an attribute or by profile feedback) are not accounted into the unit size.",1,4,limited-side-effect,gcc,0,1
2231,integer-share-limit,"Small integer constants can use a shared data structure, reducing the compiler's memory usage and increasing its speed. This sets the maximum value of a shared integer constant.",1,4,limited-side-effect,gcc,0,0
2232,internals,"By default, only high-level messages are emitted. This option enables additional, more detailed, messages, which are likely to only be of interest to GCC developers.",1,6,function-tradeoff,gcc,0,0
2233,ipa,Enable dumps from all interprocedural optimizations.,1,6,function-tradeoff,gcc,0,0
2234,ipa-cp-eval-threshold,IPA-CP calculates its own score of cloning profitability heuristics and performs those cloning opportunities with scores that exceed ipa-cp-eval-threshold.,1,5,workload-specific,gcc,0,0
2235,ipa-cp-large-unit-insns,The size of translation unit that IPA-CP pass considers large.,1,4,limited-side-effect,gcc,0,0
2236,ipa-cp-loop-hint-bonus,"When IPA-CP determines that a cloning candidate would make the number of iterations of a loop known, it adds a bonus of ipa-cp-loop-hint-bonus to the profitability score of the candidate.",1,4,limited-side-effect,gcc,0,1
2237,ipa-cp-max-recursive-depth,Maximum depth of recursive cloning for self-recursive function.,1,3,reliability-tradeoff,gcc,0,1
2238,ipa-cp-min-recursive-probability,Recursive cloning only when the probability of call being executed exceeds the parameter.,1,4,limited-side-effect,gcc,0,0
2239,ipa-cp-recursion-penalty,Percentage penalty the recursive functions will receive when they are evaluated for cloning.,1,4,limited-side-effect,gcc,0,0
2240,ipa-cp-single-call-penalty,Percentage penalty functions containing a single call to another function will receive when they are evaluated for cloning.,1,4,limited-side-effect,gcc,0,0
2241,ipa-cp-unit-growth,"Specifies maximal overall growth of the compilation unit caused by interprocedural constant propagation. For example, parameter value 10 limits unit growth to 1.1 times the original size.",0,0,others,gcc,0,0
2242,ipa-cp-value-list-size,IPA-CP attempts to track all possible values and types passed to a function's parameter in order to propagate them and perform devirtualization. ipa-cp-value-list-size is the maximum number of values and types it stores per one formal parameter of a function.,1,4,limited-side-effect,gcc,0,0
2243,ipa-jump-function-lookups,Specifies number of statements visited during jump function offset discovery.,1,4,limited-side-effect,gcc,0,0
2244,ipa-max-aa-steps,"During its analysis of function bodies, IPA-CP employs alias analysis in order to track values pointed to by function parameters. In order not spend too much time analyzing huge functions, it gives up and consider all memory clobbered after examining ipa-max-aa-steps statements modifying memory.",1,4,limited-side-effect,gcc,0,1
2245,ipa-max-agg-items,IPA-CP is also capable to propagate a number of scalar values passed in an aggregate. ipa-max-agg-items controls the maximum number of such values per one parameter.,1,4,limited-side-effect,gcc,0,0
2246,ipa-max-loop-predicates,The maximum number of different predicates IPA will use to describe when loops in a function have known properties.,1,4,limited-side-effect,gcc,0,0
2247,ipa-max-param-expr-ops,"IPA-CP will analyze conditional statement that references some function parameter to estimate benefit for cloning upon certain constant value. But if number of operations in a parameter expression exceeds ipa-max-param-expr-ops, the expression is treated as complicated one, and is not handled by IPA analysis.",1,4,limited-side-effect,gcc,0,0
2248,ipa-max-switch-predicate-bounds,"Maximal number of boundary endpoints of case ranges of switch statement. For switch exceeding this limit, IPA-CP will not construct cloning cost predicate, which is used to estimate cloning benefit, for default case of the switch statement.",1,4,limited-side-effect,gcc,0,0
2249,ipa-sra-max-replacements,"Maximum pieces of an aggregate that IPA-SRA tracks. As a consequence, it is also the maximum number of replacements of a formal parameter.",1,5,workload-specific,gcc,0,0
2250,ipa-sra-ptr-growth-factor,IPA-SRA replaces a pointer to an aggregate with one or more new parameters only when their cumulative size is less or equal to ipa-sra-ptr-growth-factor times the size of the original pointer parameter.,1,4,limited-side-effect,gcc,0,1
2253,ira-loop-reserved-regs,IRA can be used to evaluate more accurate register pressure in loops for decisions to move loop invariants (see -O3). The number of available registers reserved for some other purposes is given by this parameter. Default of the parameter is the best found from numerous experiments.,1,4,limited-side-effect,gcc,0,0
2254,ira-max-conflict-table-size,"Although IRA uses a sophisticated algorithm to compress the conflict table, the table can still require excessive amounts of memory for huge functions. If the conflict table for a function could be more than the size in MB given by this parameter, the register allocator instead uses a faster, simpler, and lower-quality algorithm that does not require building a pseudo-register conflict table.",1,4,limited-side-effect,gcc,0,0
2255,ira-max-loops-num,"IRA uses regional register allocation by default. If a function contains more loops than the number given by this parameter, only at most the given number of the most frequently-executed loops form regions for regional register allocation.",1,5,workload-specific,gcc,0,0
2256,iso9899:1990,Support all ISO C90 programs (certain GNU extensions that conflict with ISO C90 are disabled). Same as -ansi for C code.,0,0,others,gcc,0,0
2257,iso9899:199409,ISO C90 as modified in amendment 1.,0,0,others,gcc,0,0
2258,iso9899:199x,"ISO C99. This standard is substantially completely supported, modulo bugs and floating-point issues (mainly but not entirely relating to optional C99 features from Annexes F and G). See http://gcc.gnu.org/c99status.html for more information. The names c9xand iso9899:199xare deprecated.",0,0,others,gcc,0,0
2259,iso9899:2011,"ISO C11, the 2011 revision of the ISO C standard. This standard is substantially completely supported, modulo bugs, floating-point issues (mainly but not entirely relating to optional C11 features from Annexes F and G) and the optional Annexes K (Bounds-checking interfaces) and L (Analyzability). The name c1xis deprecated.",0,0,others,gcc,0,0
2260,iso9899:2018,"ISO C17, the 2017 revision of the ISO C standard (published in 2018). This standard is same as C11 except for corrections of defects (all of which are also applied with -std=c11) and a new value of __STDC_VERSION__, and so is supported to the same extent as C11.",0,0,others,gcc,0,0
2262,iv-always-prune-cand-set-bound,"If the number of candidates in the set is smaller than this value, always try to remove unnecessary ivs from the set when adding a new one.",1,5,workload-specific,gcc,0,0
2263,iv-consider-all-candidates-bound,"Bound on number of candidates for induction variables, below which all candidates are considered for each use in induction variable optimizations. If there are more candidates than this, only the most relevant ones are considered to avoid quadratic time complexity.",1,4,limited-side-effect,gcc,0,0
2264,iv-max-considered-uses,The induction variable optimizations give up on loops that contain more induction variable uses.,1,4,limited-side-effect,gcc,0,0
2266,joined,"Display options taking an argument that appears after an equal sign in the same continuous piece of text, such as: -help=target",0,0,others,gcc,0,1
2267,jump-table-max-growth-ratio-for-size,The maximum code size growth ratio when expanding into a jump table (in percent). The parameter is used when optimizing for size.,1,4,limited-side-effect,gcc,0,0
2268,jump-table-max-growth-ratio-for-speed,The maximum code size growth ratio when expanding into a jump table (in percent). The parameter is used when optimizing for speed.,1,4,limited-side-effect,gcc,0,0
2270,l1-cache-line-size,"The size of cache line in L1 data cache, in bytes.",1,4,limited-side-effect,gcc,0,1
2271,l1-cache-size,"The size of L1 data cache, in kilobytes.",1,1,resource,gcc,0,0
2272,l2-cache-size,"The size of L2 data cache, in kilobytes.",1,1,resource,gcc,0,1
2274,large-function-growth,"Specifies maximal growth of large function caused by inlining in percents. For example, parameter value 100 limits large function growth to 2.0 times the original size.",1,4,limited-side-effect,gcc,0,1
2275,large-function-insns,"The limit specifying really large functions. For functions larger than this limit after inlining, inlining is constrained by --param large-function-growth. This parameter is useful primarily to avoid extreme compilation time caused by non-linear algorithms used by the back end.",1,4,limited-side-effect,gcc,0,0
2276,large-stack-frame,The limit specifying large stack frames. While inlining the algorithm is trying to not grow past this limit too much.,1,4,limited-side-effect,gcc,0,0
2277,large-stack-frame-growth,"Specifies maximal growth of large stack frames caused by inlining in percents. For example, parameter value 1000 limits large stack frame growth to 11 times the original size.",1,4,limited-side-effect,gcc,0,0
2278,large-unit-insns,"The limit specifying large translation unit. Growth caused by inlining of units larger than this limit is limited by --param inline-unit-growth. For small units this might be too tight. For example, consider a unit consisting of function A that is inline and B that just calls A three times. If B is small relative to A, the growth of unit is 300\% and yet such inlining is very sane. For very large units consisting of small inlineable functions, however, the overall unit growth limit is needed to avoid exponential explosion of code size. Thus for smaller units, the size is increased to --param large-unit-insns before applying --param inline-unit-growth.",1,4,limited-side-effect,gcc,0,0
2279,lazy-modules,Maximum number of concurrently open C++ module files when lazy loading.,1,4,limited-side-effect,gcc,0,1
2281,lim-expensive,The minimum cost of an expensive expression in the loop invariant motion.,1,4,limited-side-effect,gcc,0,0
2282,lineno,Enable showing line numbers for statements.,1,6,function-tradeoff,gcc,0,1
2283,-lobjc,You need this special case of the -l option in order to link an Objective-C or Objective-C++ program.,0,0,others,gcc,0,0
2285,loop,Enable dumps from all loop optimizations.,1,6,function-tradeoff,gcc,0,0
2286,loop-block-tile-size,"Loop blocking or strip mining transforms, enabled with -floop-block or -floop-strip-mine, strip mine each loop in the loop nest by a given number of iterations. The strip length can be changed using the loop-block-tile-size parameter.",1,4,limited-side-effect,gcc,0,0
2287,loop-interchange-max-num-stmts,The maximum number of stmts in a loop to be interchanged.,1,5,workload-specific,gcc,0,0
2288,loop-interchange-stride-ratio,The minimum ratio between stride of two loops for interchange to be profitable.,1,5,workload-specific,gcc,0,0
2289,loop-invariant-max-bbs-in-loop,"Loop invariant motion can be very expensive, both in compilation time and in amount of needed compile-time memory, with very large loops. Loops with more basic blocks than this parameter won have loop invariant motion optimization performed on them.",1,4,limited-side-effect,gcc,0,1
2290,loop-max-datarefs-for-datadeps,Building data dependencies is expensive for very large loops. This parameter limits the number of data references in loops that are considered for data dependence analysis. These large loops are no handled by the optimizations using loop data dependencies.,1,5,workload-specific,gcc,0,0
2291,loop-versioning-max-inner-insns,The maximum number of instructions that an inner loop can have before the loop versioning pass considers it too big to copy.,1,5,workload-specific,gcc,0,0
2292,loop-versioning-max-outer-insns,"The maximum number of instructions that an outer loop can have before the loop versioning pass considers it too big to copy, discounting any instructions in inner loops that directly benefit from versioning.",1,5,workload-specific,gcc,0,0
2293,lra-inheritance-ebb-probability-cutoff,LRA tries to reuse values reloaded in registers in subsequent insns. This optimization is called inheritance. EBB is used as a region to do this optimization. The parameter defines a minimal fall-through edge probability in percentage used to add BB to inheritance EBB in LRA. The default value was chosen from numerous runs of SPEC2000 on x86-64.,1,4,limited-side-effect,gcc,0,1
2294,lra-max-considered-reload-pseudos,The max number of reload pseudos which are considered during spilling a non-reload pseudo.,1,4,limited-side-effect,gcc,0,0
2295,lto-max-partition,Size of max partition for WHOPR (in estimated instructions). to provide an upper bound for individual size of partition. Meant to be used only with balanced partitioning.,1,4,limited-side-effect,gcc,0,1
2296,lto-max-streaming-parallelism,Maximal number of parallel processes used for LTO streaming.,1,4,limited-side-effect,gcc,0,0
2297,lto-min-partition,Size of minimal partition for WHOPR (in estimated instructions). This prevents expenses of splitting very small programs into too many partitions.,1,4,limited-side-effect,gcc,0,0
2298,lto-partitions,Specify desired number of partitions produced during WHOPR compilation. The number of partitions should exceed the number of CPUs used for compilation.,1,4,limited-side-effect,gcc,0,0
2300,max-average-unrolled-insns,"The maximum number of instructions biased by probabilities of their execution that a loop may have to be unrolled. If a loop is unrolled, this parameter also determines how many times the loop code is unrolled.",1,4,limited-side-effect,gcc,0,0
2301,max-combine-insns,The maximum number of instructions the RTL combiner tries to combine.,1,4,limited-side-effect,gcc,0,0
2302,max-completely-peeled-insns,The maximum number of insns of a completely peeled loop.,1,5,workload-specific,gcc,0,1
2303,max-completely-peel-loop-nest-depth,The maximum depth of a loop nest suitable for complete peeling.,1,5,workload-specific,gcc,0,0
2304,max-completely-peel-times,The maximum number of iterations of a loop to be suitable for complete peeling.,1,5,workload-specific,gcc,0,0
2305,max-crossjump-edges,"The maximum number of incoming edges to consider for cross-jumping. The algorithm used by -fcrossjumping is O(N^2) in the number of edges incoming to each block. Increasing values mean more aggressive optimization, making the compilation time increase with probably small improvement in executable size.",1,4,limited-side-effect,gcc,0,0
2306,max-cse-insns,The maximum number of instructions CSE processes before flushing.,1,4,limited-side-effect,gcc,0,0
2307,max-cselib-memory-locations,"The maximum number of memory locations cselib should take into account. Increasing values mean more aggressive optimization, making the compilation time increase with probably slightly better performance.",1,4,limited-side-effect,gcc,0,0
2308,max-cse-path-length,The maximum number of basic blocks on path that CSE considers.,1,4,limited-side-effect,gcc,0,0
2309,max-debug-marker-count,"Sets a threshold on the number of debug markers (e.g. begin stmt markers) to avoid complexity explosion at inlining or expanding to RTL. If a function has more such gimple stmts than the set limit, such stmts will be dropped from the inlined copy of a function, and from its RTL expansion.",1,5,workload-specific,gcc,0,0
2310,max-delay-slot-insn-search,"The maximum number of instructions to consider when looking for an instruction to fill a delay slot. If more than this arbitrary number of instructions are searched, the time savings from filling the delay slot are minimal, so stop searching. Increasing values mean more aggressive optimization, making the compilation time increase with probably small improvement in execution time.",1,4,limited-side-effect,gcc,0,1
2311,max-delay-slot-live-search,"When trying to fill delay slots, the maximum number of instructions to consider when searching for a block with valid live register information. Increasing this arbitrarily chosen value means more aggressive optimization, increasing the compilation time. This parameter should be removed when the delay slot code is rewritten to maintain the control-flow graph.",1,5,workload-specific,gcc,0,0
2312,max-dse-active-local-stores,Maximum number of active local stores in RTL dead store elimination.,1,4,limited-side-effect,gcc,0,1
2313,max-early-inliner-iterations,Limit of iterations of the early inliner. This basically bounds the number of nested indirect calls the early inliner can resolve. Deeper chains are still handled by late inlining.,1,4,limited-side-effect,gcc,0,1
2314,max-fields-for-field-sensitive,Maximum number of fields in a structure treated in a field sensitive manner during pointer analysis.,1,5,workload-specific,gcc,0,0
2315,max-find-base-term-values,Maximum number of VALUEs handled during a single find_base_term call.,1,5,workload-specific,gcc,0,0
2316,max-fsm-thread-length,Maximum number of basic blocks on a finite state automaton jump thread path.,1,4,limited-side-effect,gcc,0,0
2317,max-fsm-thread-path-insns,Maximum number of instructions to copy when duplicating blocks on a finite state automaton jump thread path.,1,4,limited-side-effect,gcc,0,1
2318,max-fsm-thread-paths,Maximum number of new jump thread paths to create for a finite state automaton.,1,1,resource,gcc,0,1
2319,max-gcse-insertion-ratio,"If the ratio of expression insertions to deletions is larger than this value for any expression, then RTL PRE inserts or removes the expression and thus leaves partially redundant computations in the instruction stream.",1,4,limited-side-effect,gcc,0,0
2320,max-gcse-memory,"The approximate maximum amount of memory in kB that can be allocated in order to perform the global common subexpression elimination optimization. If more memory than specified is required, the optimization is not done.",1,4,limited-side-effect,gcc,0,1
2321,max-goto-duplication-insns,"The maximum number of instructions to duplicate to a block that jumps to a computed goto. To avoid O(N^2) behavior in a number of passes, GCC factors computed gotos early in the compilation process, and unfactors them as late as possible. Only computed jumps at the end of a basic blocks with no more than max-goto-duplication-insns are unfactored.",1,4,limited-side-effect,gcc,0,0
2322,max-grow-copy-bb-insns,The maximum code size expansion factor when copying basic blocks instead of jumping. The expansion is relative to a jump instruction.,1,5,workload-specific,gcc,0,0
2323,max-hoist-depth,"The depth of search in the dominator tree for expressions to hoist. This is used to avoid quadratic behavior in hoisting algorithm. The value of 0 does not limit on the search, but may slow down compilation of huge functions.",1,4,limited-side-effect,gcc,0,1
2324,max-inline-insns-auto,"When you use -finline-functions (included in -O3), a lot of functions that would otherwise not be considered for inlining by the compiler are investigated. To those functions, a different (more restrictive) limit compared to functions declared inline can be applied (--param max-inline-insns-auto).",1,4,limited-side-effect,gcc,0,0
2325,max-inline-insns-recursive-auto,The maximum number of instructions non-inline function can grow to via recursive inlining.,1,4,limited-side-effect,gcc,0,0
2326,max-inline-insns-single,Several parameters control the tree inliner used in GCC. This number sets the maximum number of instructions (counted in GCC internal representation) in a single function that the tree inliner considers for inlining. This only affects functions declared inline and methods implemented in a class declaration (C++).,1,4,limited-side-effect,gcc,0,0
2327,max-inline-insns-size,This is bound applied to calls which are optimized for size. Small growth may be desirable to anticipate optimization oppurtunities exposed by inlining.,1,4,limited-side-effect,gcc,0,0
2328,max-inline-insns-small,This is bound applied to calls which are considered relevant with -finline-small-functions.,1,4,limited-side-effect,gcc,0,0
2329,max-inline-recursive-depth-auto,Specifies the maximum recursion depth used for recursive inlining.,1,3,reliability-tradeoff,gcc,0,0
2330,max-isl-operations,"Maximum number of isl operations, 0 means unlimited.",1,5,workload-specific,gcc,0,0
2331,max-iterations-computation-cost,Bound on the cost of an expression to compute the number of iterations.,1,5,workload-specific,gcc,0,0
2332,max-iterations-to-track,The maximum number of iterations of a loop the brute-force algorithm for analysis of the number of iterations of the loop tries to evaluate.,1,5,workload-specific,gcc,0,0
2333,max-jump-thread-duplication-stmts,Maximum number of statements allowed in a block that needs to be duplicated when threading jumps.,1,5,workload-specific,gcc,0,0
2334,max-last-value-rtl,The maximum size measured as number of RTLs that can be recorded in an expression in combiner for a pseudo register as last known value of that register.,1,4,limited-side-effect,gcc,0,0
2335,max-loop-header-insns,The maximum number of insns in loop header duplicated by the copy loop headers pass.,1,4,limited-side-effect,gcc,0,0
2336,max-modulo-backtrack-attempts,The maximum number of backtrack attempts the scheduler should make when modulo scheduling a loop. Larger values can exponentially increase compilation time.,1,5,workload-specific,gcc,0,1
2337,max-partial-antic-length,"Maximum length of the partial antic set computed during the tree partial redundancy elimination optimization (-ftree-pre) when optimizing at -O3 and above. For some sorts of source code the enhanced partial redundancy elimination optimization can run away, consuming all of the memory available on the host machine. This parameter sets a limit on the length of the sets that are computed, which prevents the runaway behavior. Setting a value of 0 for this parameter allows an unlimited set length.",1,4,limited-side-effect,gcc,0,1
2338,max-peel-branches,The maximum number of branches on the hot path through the peeled sequence.,1,5,workload-specific,gcc,0,0
2339,max-peeled-insns,"The maximum number of instructions that a loop may have to be peeled. If a loop is peeled, this parameter also determines how many times the loop code is peeled.",1,4,limited-side-effect,gcc,0,0
2340,max-peel-times,The maximum number of peelings of a single loop.,1,5,workload-specific,gcc,0,0
2341,max-pending-list-length,The maximum number of pending dependencies scheduling allows before flushing the current state and starting over. Large functions with few branches or calls can create excessively large lists which needlessly consume memory and resources.,1,4,limited-side-effect,gcc,0,0
2342,max-pipeline-region-blocks,The maximum number of blocks in a region to be considered for pipelining in the selective scheduler.,1,5,workload-specific,gcc,0,0
2343,max-pipeline-region-insns,The maximum number of insns in a region to be considered for pipelining in the selective scheduler.,1,5,workload-specific,gcc,0,0
2344,max-pow-sqrt-depth,Maximum depth of sqrt chains to use when synthesizing exponentiation by a real constant.,1,5,workload-specific,gcc,0,0
2345,max-predicted-iterations,"The maximum number of loop iterations we predict statically. This is useful in cases where a function contains a single loop with known bound and another loop with unknown bound. The known number of iterations is predicted correctly, while the unknown number of iterations average to roughly 10. This means that the loop without bounds appears artificially cold relative to the other one.",1,5,workload-specific,gcc,0,0
2346,max-reload-search-insns,"The maximum number of instruction reload should look backward for equivalent register. Increasing values mean more aggressive optimization, making the compilation time increase with probably slightly better performance.",1,5,workload-specific,gcc,0,0
2347,max-rtl-if-conversion-insns,RTL if-conversion tries to remove conditional branches around a block and replace them with conditionally executed instructions. This parameter gives the maximum number of instructions in a block which should be considered for if-conversion. The compiler will also use other heuristics to decide whether if-conversion is likely to be profitable.,1,4,limited-side-effect,gcc,0,1
2348,max-rtl-if-conversion-unpredictable-cost,Maximum permissible cost for the sequence that would be generated by the RTL if-conversion pass for a branch that is considered unpredictable.,1,5,workload-specific,gcc,0,0
2349,max-sched-extend-regions-iters,The maximum number of iterations through CFG to extend regions. A value of 0 disables region extensions.,1,5,workload-specific,gcc,0,0
2350,max-sched-insn-conflict-delay,The maximum conflict delay for an insn to be considered for speculative motion.,1,5,workload-specific,gcc,0,0
2351,max-sched-ready-insns,"The maximum number of instructions ready to be issued the scheduler should consider at any given time during the first scheduling pass. Increasing values mean more thorough searches, making the compilation time increase with probably little benefit.",1,5,workload-specific,gcc,0,0
2352,max-sched-region-blocks,The maximum number of blocks in a region to be considered for interblock scheduling.,1,5,workload-specific,gcc,0,0
2353,max-sched-region-insns,The maximum number of insns in a region to be considered for interblock scheduling.,1,5,workload-specific,gcc,0,0
2354,max-slsr-cand-scan,Set the maximum number of existing candidates that are considered when seeking a basis for a new straight-line strength reduction candidate.,1,5,workload-specific,gcc,0,0
2355,max-speculative-devirt-maydefs,The maximum number of may-defs we analyze when looking for a must-def specifying the dynamic type of an object that invokes a virtual call we may be able to devirtualize speculatively.,1,4,limited-side-effect,gcc,0,0
2356,max-ssa-name-query-depth,Maximum depth of recursion when querying properties of SSA names in things like fold routines. One level of recursion corresponds to following a use-def chain.,1,4,limited-side-effect,gcc,0,0
2357,max-store-chains-to-track,The maximum number of store chains to track at the same time in the attempt to merge them into wider stores in the store merging pass.,1,4,limited-side-effect,gcc,0,1
2358,max-stores-to-merge,The maximum number of stores to attempt to merge into wider stores in the store merging pass.,1,5,workload-specific,gcc,0,0
2359,max-stores-to-sink,The maximum number of conditional store pairs that can be sunk. Set to 0 if either vectorization (-ftree-vectorize) or if-conversion (-ftree-loop-if-convert) is disabled.,1,5,workload-specific,gcc,0,0
2360,max-stores-to-track,The maximum number of stores to track at the same time in the attemt to to merge them into wider stores in the store merging pass.,1,5,workload-specific,gcc,0,0
2361,max-tail-merge-comparisons,The maximum amount of similar bbs to compare a bb with. This is used to avoid quadratic behavior in tree tail merging.,1,4,limited-side-effect,gcc,0,0
2362,max-tail-merge-iterations,The maximum amount of iterations of the pass over the function. This is used to limit compilation time in tree tail merging.,1,5,workload-specific,gcc,0,0
2363,max-tracked-strlens,Maximum number of strings for which strlen optimization pass will track string lengths.,1,4,limited-side-effect,gcc,0,0
2364,max-tree-if-conversion-phi-args,Maximum number of arguments in a PHI supported by TREE if conversion unless the loop is marked with simd pragma.,1,4,limited-side-effect,gcc,0,0
2365,max-unrolled-insns,"The maximum number of instructions that a loop may have to be unrolled. If a loop is unrolled, this parameter also determines how many times the loop code is unrolled.",1,5,workload-specific,gcc,0,0
2366,max-unroll-times,The maximum number of unrollings of a single loop.,1,5,workload-specific,gcc,0,0
2367,max-unswitch-insns,The maximum number of insns of an unswitched loop.,1,5,workload-specific,gcc,0,0
2368,max-unswitch-level,The maximum number of branches unswitched in a single loop.,1,5,workload-specific,gcc,0,0
2369,max-variable-expansions-in-unroller,"If -fvariable-expansion-in-unroller is used, the maximum number of times that an individual variable will be expanded during loop unrolling.",1,4,limited-side-effect,gcc,0,0
2370,max-vartrack-expr-depth,"Sets a maximum number of recursion levels when attempting to map variable names or debug temporaries to value expressions. This trades compilation time for more complete debug information. If this is set too low, value expressions that are available and could be represented in debug information may end up not being used; setting this higher may enable the compiler to find more complex debug expressions, but compile time and memory use may grow.",1,4,limited-side-effect,gcc,0,0
2371,max-vartrack-reverse-op-size,Max. size of loc list for which reverse ops should be added.,1,4,limited-side-effect,gcc,0,0
2372,max-vartrack-size,"Sets a maximum number of hash table slots to use during variable tracking dataflow analysis of any function. If this limit is exceeded with variable tracking at assignments enabled, analysis for that function is retried without it, after removing all debug insns from the function. If the limit is exceeded even without debug insns, var tracking analysis is completely disabled for the function. Setting the parameter to zero makes it unlimited.",1,4,limited-side-effect,gcc,0,0
2373,max-vrp-switch-assertions,The maximum number of assertions to add along the default edge of a switch statement during VRP.,1,5,workload-specific,gcc,0,0
2377,min-crossjump-insns,The minimum number of instructions that must be matched at the end of two blocks before cross-jumping is performed on them. This value is ignored in the case where all instructions in the block being cross-jumped from are matched.,1,4,limited-side-effect,gcc,0,1
2378,min-inline-recursive-probability,Recursive inlining is profitable only for function having deep recursion in average and can hurt for function having little recursion depth by increasing the prologue size or complexity of function body to other optimizers.,1,4,limited-side-effect,gcc,0,0
2379,min-insn-to-prefetch-ratio,The minimum ratio between the number of instructions and the number of prefetches to enable prefetching in a loop.,1,5,workload-specific,gcc,0,0
2380,min-loop-cond-split-prob,"When FDO profile information is available, min-loop-cond-split-prob specifies minimum threshold for probability of semi-invariant condition statement to trigger loop split.",1,5,workload-specific,gcc,0,0
2381,min-nondebug-insn-uid,"Use uids starting at this parameter for nondebug insns. The range below the parameter is reserved exclusively for debug insns created by -fvar-tracking-assignments, but debug insns may get (non-overlapping) uids above it if the reserved range is exhausted.",1,4,limited-side-effect,gcc,0,0
2382,min-size-for-stack-sharing,The minimum size of variables taking part in stack slot sharing when not optimizing.,1,4,limited-side-effect,gcc,0,0
2383,min-spec-prob,The minimum probability (in percents) of reaching a source block for interblock speculative scheduling.,1,5,workload-specific,gcc,0,0
2384,min-vect-loop-bound,The minimum number of iterations under which loops are not vectorized when -ftree-vectorize is used. The number of iterations after vectorization needs to be greater than the value specified by this option to allow vectorization.,1,5,workload-specific,gcc,0,0
2385,-MM,"Like -M but do not mention header files that are found in system header directories, nor header files that are included, directly or indirectly, from such a header.",0,0,others,gcc,0,1
2386,-MMD,"Like -MD except mention only user header files, not system header files.",0,0,others,gcc,0,0
2387,-Mno-modules,Disable dependency generation for compiled module interfaces.,0,0,others,gcc,0,0
2388,modref-max-accesses,"Specifies the maximal number of base pointers, references and accesses stored for a single function by mod/ref analysis.",1,4,limited-side-effect,gcc,0,0
2389,modref-max-depth,Specifies the maximum depth of DFS walk used by modref escape analysis. Setting to 0 disables the analysis completely.,1,4,limited-side-effect,gcc,0,0
2390,modref-max-escape-points,Specifies the maximum number of escape points tracked by modref per SSA-name.,1,4,limited-side-effect,gcc,0,0
2391,modref-max-tests,Specifies the maxmal number of tests alias oracle can perform to disambiguate memory locations using the mod/ref information. This parameter ought to be bigger than --param modref-max-bases and --param modref-max-refs.,1,4,limited-side-effect,gcc,0,1
2392,module,"Dump module information. Options lineno (locations), graph (reachability), blocks (clusters), uid (serialization), alias (mergeable), asmname (Elrond), eh (mapper) & vops (macros) may provide additional information. This option is applicable to C++ only.",1,6,function-tradeoff,gcc,0,0
2393,-MP,"This option instructs CPP to add a phony target for each dependency other than the main file, causing each to depend on nothing. These dummy rules work around errors make gives if you remove header files without updating the Makefile to match.",0,0,others,gcc,0,0
2394,-MQ target,"Same as -MT, but it quotes any characters which are special to Make. -MQ'$(objpfx)foo.o' gives",0,0,others,gcc,0,0
2395,-MT target,"Change the target of the rule emitted by dependency generation. By default CPP takes the name of the main input file, deletes any directory components and any file suffix such as c and appends the platform usual object suffix. The result is the target.",0,0,others,gcc,0,0
2397,-nodefaultlibs,"Do not use the standard system libraries when linking. Only the libraries you specify are passed to the linker, and options specifying linkage of the system libraries, such as -static-libgcc or -shared-libgcc, are ignored. The standard startup files are used normally, unless -nostartfiles is used.",0,0,others,gcc,0,0
2398,-no-integrated-cpp,"Perform preprocessing as a separate pass before compilation. By default, GCC performs preprocessing as an integrated part of input tokenization and parsing. If this option is provided, the appropriate language front end (cc1, cc1plus, or cc1obj for C, C++, and Objective-C, respectively) is instead invoked twice, once for preprocessing only and once for actual compilation of the preprocessed input. This option may be useful in conjunction with the -B or -wrapper options to specify an alternate preprocessor or perform additional processing of the program source between normal preprocessing and compilation.",0,0,others,gcc,0,0
2399,-nolibc,"Do not use the C library or system libraries tightly coupled with it when linking. Still link with the startup files, libgcc or toolchain provided language support libraries such as libgnat, libgfortran or libstdc++ unless options preventing their inclusion are used as well. This typically removes -lc from the link command line, as well as system libraries that normally go with it and become meaningless when absence of a C library is assumed, for example -lpthread or -lm in some configurations. This is intended for bare-board targets when there is indeed no C library available.",0,0,others,gcc,0,0
2400,-no-pie,Don't produce a dynamically linked position independent executable.,0,0,others,gcc,0,0
2401,-nostartfiles,"Do not use the standard system startup files when linking. The standard system libraries are used normally, unless -nostdlib, -nolibc, or -nodefaultlibs is used.",0,0,others,gcc,0,0
2403,-nostdinc++,"Do not search for header files in the C++-specific standard directories, but do still search the other standard directories. (This option is used when building the C++ library.)",0,0,others,gcc,0,1
2404,-nostdlib,"Do not use the standard system startup files or libraries when linking. No startup files and only the libraries you specify are passed to the linker, and options specifying linkage of the system libraries, such as -static-libgcc or -shared-libgcc, are ignored.",0,0,others,gcc,0,1
2407,-O0,Reduce compilation time and make debugging produce the expected results. This is the default.,1,4,limited-side-effect,gcc,0,1
2408,-O1,"Optimize. Optimizing compilation takes somewhat more time, and a lot more memory for a large function.",1,4,limited-side-effect,gcc,0,0
2409,-O2,"Optimize even more. GCC performs nearly all supported optimizations that do not involve a space-speed tradeoff. As compared to -O, this option increases both compilation time and the performance of the generated code.",1,4,limited-side-effect,gcc,0,0
2410,-O3,Optimize yet more. -O3 turns on all optimizations specified by -O2 and also turns on the following optimization flags:,1,4,limited-side-effect,gcc,0,0
2412,-Ofast,"Disregard strict standards compliance. -Ofast enables all -O3 optimizations. It also enables optimizations that are not valid for all standard-compliant programs. It turns on -ffast-math, -fallow-store-data-races and the Fortran-specific -fstack-arrays, unless -fmax-stack-var-size is specified, and -fno-protect-parens.",1,4,limited-side-effect,gcc,0,0
2413,-Og,"Optimize debugging experience. -Og should be the optimization level of choice for the standard edit-compile-debug cycle, offering a reasonable level of optimization while maintaining fast compilation and a good debugging experience. It is a better choice than -O0 for producing debuggable code because some compiler passes that collect debug information are disabled at -O0.",1,4,limited-side-effect,gcc,0,0
2414,-Os,Optimize for size. -Os enables all -O2 optimizations except those that often increase code size:,1,4,limited-side-effect,gcc,0,0
2415,-P,"Inhibit generation of linemarkers in the output from the preprocessor. This might be useful when running the preprocessor on something that is not C code, and will be sent to a program which might be confused by the linemarkers.",0,0,others,gcc,0,0
2416,--param name=value,"In some places, GCC uses various constants to control the amount of optimization that is done. For example, GCC does not inline functions that contain more than a certain number of instructions. You can control some of these constants on the command line using the --param option.",1,4,limited-side-effect,gcc,0,0
2417,params,Display the values recognized by the --param option.,1,6,function-tradeoff,gcc,0,0
2418,parloops-chunk-size,Chunk size of omp schedule for loops parallelized by parloops.,1,4,limited-side-effect,gcc,0,0
2419,parloops-min-per-thread,The minimum number of iterations per thread of an innermost parallelized loop for which the parallelized variant is preferred over the single threaded one. Note that for a parallelized loop nest the minimum number of iterations of the outermost loop per thread is two.,1,4,limited-side-effect,gcc,0,0
2420,parloops-schedule,"Schedule type of omp schedule for loops parallelized by parloops (static, dynamic, guided, auto, runtime).",1,4,limited-side-effect,gcc,0,0
2421,partial-inlining-entry-probability,Maximum probability of the entry BB of split region (in percent relative to entry BB of the function) to make partial inlining happen.,1,4,limited-side-effect,gcc,0,0
2422,-pass-exit-codes,"Normally the gcc program exits with the code of 1 if any phase of the compiler returns a non-success return code. If you specify -pass-exit-codes, the gcc program instead returns with the numerically highest error produced by any phase returning an error indication. The C, C++, and Fortran front ends return 4 if an internal compiler error is encountered.",0,0,others,gcc,0,0
2423,path=,"SGR substring for colorizing paths of control-flow events as printed via -fdiagnostics-path-format=, such as the identifiers of individual events and lines indicating interprocedural calls and returns.",0,0,others,gcc,0,0
2426,-pg,"Generate extra code to write profile information suitable for the analysis program prof (for -p) or gprof (for -pg). You must use this option when compiling the source files you want data about, and you must also use it when linking.",0,0,others,gcc,0,0
2427,-pie,"Produce a dynamically linked position independent executable on targets that support it. For predictable results, you must also specify the same set of options used for compilation (-fpie, -fPIE, or model suboptions) when you specify this linker option.",0,0,others,gcc,0,0
2428,-pipe,Use pipes rather than temporary files for communication between the various stages of compilation. This fails to work on some systems where the assembler is unable to read from a pipe; but the GNU assembler has no trouble.,0,0,others,gcc,0,0
2429,predictable-branch-outcome,"When branch is predicted to be taken with probability lower than this threshold (in percent), then it is considered well predictable.",1,5,workload-specific,gcc,0,0
2430,prefetch-dynamic-strides,"Whether the loop array prefetch pass should issue software prefetch hints for strides that are non-constant. In some cases this may be beneficial, though the fact the stride is non-constant may make it hard to predict when there is clear benefit to issuing these hints.",1,4,limited-side-effect,gcc,0,1
2431,prefetch-latency,Estimate on average number of instructions that are executed before prefetch finishes. The distance prefetched ahead is proportional to this constant. Increasing this number may also lead to less streams being prefetched (see simultaneous-prefetches).,1,4,limited-side-effect,gcc,0,0
2432,prefetch-minimum-stride,"Minimum constant stride, in bytes, to start using prefetch hints for. If the stride is less than this threshold, prefetch hints will not be issued.",1,5,workload-specific,gcc,0,0
2433,prefetch-min-insn-to-mem-ratio,The minimum ratio between the number of instructions and the number of memory references to enable prefetching in a loop.,1,4,limited-side-effect,gcc,0,0
2434,-print-file-name=library,"Print the full absolute name of the library file library that would be used when linking and don't do anything else. With this option, GCC does not compile or link anything; it just prints the file name.",1,6,function-tradeoff,gcc,0,0
2435,-print-libgcc-file-name,Same as -print-file-name=libgcc.a.,1,6,function-tradeoff,gcc,0,0
2436,-print-multiarch,"Print the path to OS libraries for the selected multiarch, relative to some lib subdirectory.",1,6,function-tradeoff,gcc,0,0
2437,-print-multi-directory,Print the directory name corresponding to the multilib selected by any other switches present in the command line. This directory is supposed to exist in GCC_EXEC_PREFIX.,1,6,function-tradeoff,gcc,0,0
2438,-print-multi-lib,Print the mapping from multilib directory names to compiler switches that enable them. The directory name is separated from the switches by and each switch starts with an @instead of the without spaces between multiple switches. This is supposed to ease shell processing.,1,6,function-tradeoff,gcc,0,1
2439,-print-multi-os-directory,"Print the path to OS libraries for the selected multilib, relative to some lib subdirectory. If OS libraries are present in the lib subdirectory and no multilibs are used, this is usually just ., if OS libraries are present in libsuffix sibling directories this prints e.g. ../lib64, ../lib or ../lib32, or if OS libraries are present in lib/subdir subdirectories it prints e.g. amd64, sparcv9 or ev6.",1,6,function-tradeoff,gcc,0,0
2440,-print-objc-runtime-info,"Generate C header describing the largest structure that is passed by value, if any.",0,0,others,gcc,0,0
2441,-print-prog-name=program,"Like -print-file-name, but searches for a program such as cpp.",1,6,function-tradeoff,gcc,0,0
2442,-print-search-dirs,Print the name of the configured installation directory and a list of program and library directories gcc searches and don't do anything else.,1,6,function-tradeoff,gcc,0,0
2443,-print-sysroot,"Print the target sysroot directory that is used during compilation. This is the target sysroot specified either at configure time or using the --sysroot option, possibly with an extra suffix that depends on compilation options. If no target sysroot is specified, the option prints nothing.",1,6,function-tradeoff,gcc,0,0
2444,-print-sysroot-headers-suffix,"Print the suffix added to the target sysroot when searching for headers, or give an error if the compiler is not configured with such a suffix and don't do anything else.",1,6,function-tradeoff,gcc,0,0
2445,profile-func-internal-id,"A parameter to control whether to use function internal id in profile database lookup. If the value is 0, the compiler uses an id that is based on function assembler name and filename, which makes old profile data more tolerant to source changes such as function reordering etc.",1,4,limited-side-effect,gcc,0,0
2446,-pthread,"Define additional macros required for using the POSIX threads library. You should use this option consistently for both compilation and linking. This option is supported on GNU/Linux targets, most other Unix derivatives, and also on x86 Cygwin and MinGW targets.",0,0,others,gcc,0,1
2447,-Q,"Makes the compiler print out each function name as it is compiled, and print some statistics about each pass when it finishes.",1,6,function-tradeoff,gcc,0,0
2448,quote=,SGR substring for information printed within quotes.,0,0,others,gcc,0,0
2449,-r,Produce a relocatable object as output. This is also known as partial linking.,1,6,function-tradeoff,gcc,0,0
2450,range1=,SGR substring for first additional range.,0,0,others,gcc,0,0
2452,-rdynamic,"Pass the flag -export-dynamic to the ELF linker, on targets that support it. This instructs the linker to add all symbols, not only used ones, to the dynamic symbol table. This option is needed for some uses of dlopen or to allow obtaining backtraces from within a program.",0,0,others,gcc,0,1
2454,rpo-vn-max-loop-depth,Maximum loop depth that is value-numbered optimistically. When the limit hits the innermost rpo-vn-max-loop-depth loops and the outermost loop in the loop nest are value-numbered optimistically and the remaining ones not.,1,5,workload-specific,gcc,0,0
2455,-s,Remove all symbol table and relocation information from the executable.,0,0,others,gcc,0,1
2458,-save-temps=cwd,Equivalent to -save-temps -dumpdir ./.,0,0,others,gcc,0,0
2460,sccvn-max-alias-queries-per-access,Maximum number of alias-oracle queries we perform when looking for redundancies for loads and stores. If this limit is hit the search is aborted and the load or store is not considered redundant. The number of queries is algorithmically limited to the number of stores on all paths from the load to the function entry.,1,4,limited-side-effect,gcc,0,0
2461,scev,Enable showing scalar evolution analysis details.,1,6,function-tradeoff,gcc,0,0
2462,scev-max-expr-complexity,Bound on the complexity of the expressions in the scalar evolutions analyzer. Complex expressions slow the analyzer.,1,5,workload-specific,gcc,0,0
2463,scev-max-expr-size,Bound on size of expressions used in the scalar evolutions analyzer. Large expressions slow the analyzer.,1,4,limited-side-effect,gcc,0,1
2464,sched-autopref-queue-depth,Hardware autoprefetcher scheduler model control flag. Number of lookahead cycles the model looks into; at only enable instruction sorting heuristic.,1,4,limited-side-effect,gcc,0,0
2465,sched-mem-true-dep-cost,Minimal distance (in CPU cycles) between store and load targeting same memory locations.,1,4,limited-side-effect,gcc,0,0
2466,sched-pressure-algorithm,Choose between the two available implementations of -fsched-pressure. Algorithm 1 is the original implementation and is the more likely to prevent instructions from being reordered. Algorithm 2 was designed to be a compromise between the relatively conservative approach taken by algorithm 1 and the rather aggressive approach taken by the default scheduler. It relies more heavily on having a regular register file and accurate register pressure classes. See haifa-sched.c in the GCC sources for more details.,1,4,limited-side-effect,gcc,0,0
2467,sched-spec-prob-cutoff,"The minimal probability of speculation success (in percents), so that speculative insns are scheduled.",1,5,workload-specific,gcc,0,0
2468,sched-state-edge-prob-cutoff,The minimum probability an edge must have for the scheduler to save its state across it.,1,5,workload-specific,gcc,0,0
2469,selsched-insns-to-rename,The maximum number of best instructions in the ready list that are considered for renaming in the selective scheduler.,1,4,limited-side-effect,gcc,0,1
2470,selsched-max-lookahead,The maximum size of the lookahead window of selective scheduling. It is a depth of search for available instructions.,1,4,limited-side-effect,gcc,0,1
2471,selsched-max-sched-times,The maximum number of times that an instruction is scheduled during selective scheduling. This is the limit on the number of iterations through which the instruction may be pipelined.,1,4,limited-side-effect,gcc,0,0
2473,-shared,"On systems that support dynamic linking, this overrides -pie and prevents linking with the shared libraries. On other systems, this option has no effect.",0,0,others,gcc,0,0
2474,-shared-libgcc,"Produce a shared object which can then be linked with other objects to form an executable. Not all systems support this option. For predictable results, you must also specify the same set of options used for compilation (-fpic, -fPIC, or model suboptions) when you specify this linker option.1",0,0,others,gcc,0,0
2475,simultaneous-prefetches,Maximum number of prefetches that can run at the same time.,1,5,workload-specific,gcc,0,1
2476,sink-frequency-threshold,The maximum relative execution frequency (in percents) of the target block relative to a statement original block to allow statement sinking of a statement. Larger numbers result in more aggressive statement sinking. A small positive adjustment is applied for statements with memory operands as those are even more profitable so sink.,1,1,resource,gcc,0,0
2477,sms-dfa-history,The number of cycles the swing modulo scheduler considers when checking conflicts using DFA.,1,4,limited-side-effect,gcc,0,0
2478,sms-loop-average-count-threshold,A threshold on the average loop count considered by the swing modulo scheduler.,1,5,workload-specific,gcc,0,0
2479,sms-max-ii-factor,A factor for tuning the upper bound that swing modulo scheduler uses for scheduling a loop.,1,5,workload-specific,gcc,0,1
2480,sms-min-sc,The minimum value of stage count that swing modulo scheduler generates.,1,5,workload-specific,gcc,0,1
2482,sra-max-propagations,"The maximum number of artificial accesses that Scalar Replacement of Aggregates (SRA) will track, per one local variable, in order to facilitate copy propagation.",1,4,limited-side-effect,gcc,0,0
2483,sra-max-scalarization-size-Osize,"Maximum size, in storage units, of an aggregate which should be considered for scalarization when compiling for size.",1,5,workload-specific,gcc,0,0
2484,ssa-name-def-chain-limit,The maximum number of SSA_NAME assignments to follow in determining a property of a variable such as its value. This limits the number of iterations or recursive calls GCC performs when optimizing certain statements or when determining their validity prior to issuing diagnostics.,1,5,workload-specific,gcc,0,0
2485,ssp-buffer-size,The minimum size of buffers (i.e. arrays) that receive stack smashing protection when -fstack-protection is used.,1,4,limited-side-effect,gcc,0,0
2486,stack-clash-protection-guard-size,"Specify the size of the operating system provided stack guard as 2 raised to num bytes. Higher values may reduce the number of explicit probes, but a value larger than the operating system provided guard will leave code vulnerable to stack clash style attacks.",1,4,limited-side-effect,gcc,0,0
2487,stack-clash-protection-probe-interval,"Stack clash protection involves probing stack space as it is allocated. This param controls the maximum distance between probes into the stack as 2 raised to num bytes. Higher values may reduce the number of explicit probes, but a value larger than the operating system provided guard will leave code vulnerable to stack clash style attacks.",1,4,limited-side-effect,gcc,0,1
2492,-static-libtsan,"When the -fsanitize=thread option is used to link a program, the GCC driver automatically links against libtsan. If libtsan is available as a shared library, and the -static option is not used, then this links against the shared version of libtsan. The -static-libtsan option directs the GCC driver to link libtsan statically, without necessarily linking other libraries statically.",0,0,others,gcc,0,0
2495,stats,Enable dumping various statistics about the pass (not honored by every dump option).,1,6,function-tradeoff,gcc,0,0
2496,-std=,"Determine the language standard. See Language Standards Supported by GCC, for details of these standard versions. This option is currently only supported when compiling C or C++.",0,0,others,gcc,0,1
2497,"-stdlib=libstdc++,libc++","When G++ is configured to support this option, it allows specification of alternate C++ runtime libraries. Two options are available: libstdc++ (the default, native C++ runtime for G++) and libc++ which is the C++ runtime installed on some operating systems (e.g. Darwin versions from Darwin11 onwards). The option switches G++ to use the headers from the specified library and to emit -lstdc++ or -lc++ respectively, when a C++ runtime is required for linking.",0,0,others,gcc,0,1
2498,store-merging-allow-unaligned,Allow the store merging pass to introduce unaligned stores if it is legal to do so.,1,4,limited-side-effect,gcc,0,1
2499,store-merging-max-size,Maximum size of a single store merging region in bytes.,1,1,resource,gcc,0,0
2500,switch-conversion-max-branch-ratio,Switch initialization conversion refuses to create arrays that are bigger than switch-conversion-max-branch-ratio times the number of branches in the switch.,1,4,limited-side-effect,gcc,0,0
2501,-symbolic,Bind references to global symbols when building a shared object. Warn about any unresolved references (unless overridden by the link editor option -Xlinker -z -Xlinker defs). Only a few systems support this option.,1,6,function-tradeoff,gcc,0,1
2503,-T script,"Use script as the linker script. This option is supported by most systems using the GNU linker. On some targets, such as bare-board targets without an operating system, the -T option may be required when linking to avoid references to undefined symbols.",0,0,others,gcc,0,0
2504,target,"Display target-specific options. Unlike the --target-help option however, target-specific options of the linker and assembler are not displayed. This is because those tools do not currently support the extended --help= syntax.",1,6,function-tradeoff,gcc,0,1
2505,--target-help,Print (on the standard output) a description of target-specific command-line options for each tool. For some targets extra target-specific information may also be printed.,1,6,function-tradeoff,gcc,0,0
2506,-time[=file],"Report the CPU time taken by each subprocess in the compilation sequence. For C source files, this is the compiler proper and assembler (plus the linker if linking is done).",0,0,others,gcc,0,0
2507,tm-max-aggregate-size,"When making copies of thread-local variables in a transaction, this parameter specifies the size in bytes after which variables are saved with the logging functions as opposed to save/restore code sequence pairs. This option only applies when using -fgnu-tm.",1,4,limited-side-effect,gcc,0,1
2508,tracer-dynamic-coverage-feedback,"The percentage of function, weighted by execution frequency, that must be covered by trace formation. Used when profile feedback is available.",1,4,limited-side-effect,gcc,0,0
2509,tracer-max-code-growth,"Stop tail duplication once code growth has reached given percentage. This is a rather artificial limit, as most of the duplicates are eliminated later in cross jumping, so it may be set to much higher values than is the desired code growth.",1,4,limited-side-effect,gcc,0,0
2510,tracer-min-branch-probability-feedback,Stop forward growth if the probability of best edge is less than this threshold (in percent). Used when profile feedback is available.,1,5,workload-specific,gcc,0,1
2511,tracer-min-branch-ratio,Stop reverse growth when the reverse probability of best edge is less than this threshold (in percent).,1,5,workload-specific,gcc,0,0
2512,-traditional-cpp,"Try to imitate the behavior of pre-standard C preprocessors, as opposed to ISO C preprocessors. See the GNU CPP manual for details.",0,0,others,gcc,0,0
2513,tree-reassoc-width,Set the maximum number of instructions executed in parallel in reassociated tree. This parameter overrides target dependent heuristics used by default if has non zero value.,1,5,workload-specific,gcc,0,0
2514,-trigraphs,"Support ISO C trigraphs. These are three-character sequences, all starting with ? that are defined by ISO C to stand for single characters. For example, ?/stands for \ so ??/n'is a character constant for a newline.",0,0,others,gcc,0,0
2515,tsan-distinguish-volatile,Emit special instrumentation for accesses to volatiles.,1,6,function-tradeoff,gcc,0,0
2516,tsan-instrument-func-entry-exit,Emit instrumentation calls to __tsan_func_entry() and __tsan_func_exit().,1,6,function-tradeoff,gcc,0,0
2517,type-diff=,SGR substring for highlighting mismatching types within template arguments in the C++ frontend.,0,0,others,gcc,0,0
2519,-u symbol,"Pretend the symbol symbol is undefined, to force linking of library modules to define it. You can use -u multiple times with different symbols to force loading of additional library modules.",0,0,others,gcc,0,0
2521,-undef,Do not predefine any system-specific or GCC-specific macros. The standard predefined macros remain defined.,0,0,others,gcc,0,0
2522,undocumented,Display only those options that are undocumented.,1,6,function-tradeoff,gcc,0,0
2523,uninit-control-dep-attempts,Maximum number of nested calls to search for control dependencies during uninitialized variable analysis.,1,5,workload-specific,gcc,0,0
2524,uninlined-function-insns,Number of instructions accounted by inliner for function overhead such as function prologue and epilogue.,1,4,limited-side-effect,gcc,0,1
2525,uninlined-function-time,Extra time accounted by inliner for function overhead such as time needed to execute function prologue and epilogue,1,4,limited-side-effect,gcc,0,0
2526,uninlined-thunk-time,Same as --param uninlined-function-insns and --param uninlined-function-time but applied to function thunks,1,4,limited-side-effect,gcc,0,1
2527,unlikely-bb-count-fraction,"The denominator n of fraction 1/n of the number of profiled runs of the entire program below which the execution count of a basic block must be in order for the basic block to be considered unlikely executed. The default is 20, which means that a basic block is considered unlikely executed if it is executed in fewer than 1/20, or 5%, of the runs of the program. 0 means that it is always considered unlikely executed.",1,4,limited-side-effect,gcc,0,0
2528,unroll-jam-max-unroll,The maximum number of times the outer loop should be unrolled by the unroll-and-jam transformation.,1,5,workload-specific,gcc,0,0
2529,unroll-jam-min-percent,The minimum percentage of memory references that must be optimized away for the unroll-and-jam transformation to be considered profitable.,1,5,workload-specific,gcc,0,0
2530,use-after-scope-direct-emission-threshold,"If the size of a local variable in bytes is smaller or equal to this number, directly poison (or unpoison) shadow memory instead of using run-time callbacks.",1,4,limited-side-effect,gcc,0,0
2531,use-canonical-types,"Whether the compiler should use the anonicaltype system. Should always be 1, which uses a more efficient internal mechanism for comparing types in C++ and Objective-C++. However, if bugs in the canonical type system are causing compilation failures, set this value to 0 to disable canonical types.",1,4,limited-side-effect,gcc,0,0
2532,-v,Print (on standard error output) the commands executed to run the stages of compilation. Also print the version number of the compiler driver program and of the preprocessor and the compiler proper.,1,6,function-tradeoff,gcc,0,0
2533,vec,Enable dumps from all vectorization optimizations.,1,6,function-tradeoff,gcc,0,0
2534,vect-epilogues-nomask,Enable loop epilogue vectorization using smaller vector size.,1,4,limited-side-effect,gcc,0,0
2535,vect-max-peeling-for-alignment,The maximum number of loop peels to enhance access alignment for vectorizer. Value -1 means no limit.,1,4,limited-side-effect,gcc,0,0
2536,vect-max-version-for-alias-checks,The maximum number of run-time checks that can be performed when doing loop versioning for alias in the vectorizer.,1,5,workload-specific,gcc,0,0
2537,vect-max-version-for-alignment-checks,The maximum number of run-time checks that can be performed when doing loop versioning for alignment in the vectorizer.,1,5,workload-specific,gcc,0,0
2538,vect-partial-vector-usage,Controls when the loop vectorizer considers using partial vector loads and stores as an alternative to falling back to scalar code. 0 stops the vectorizer from ever using partial vector loads and stores. 1 allows partial vector loads and stores if vectorization removes the need for the code to iterate. 2 allows partial vector loads and stores in all loops. The parameter only has an effect on targets that support partial vector loads and stores.,1,4,limited-side-effect,gcc,0,1
2539,verbose,Enable showing the tree dump for each statement.,1,6,function-tradeoff,gcc,0,0
2540,--version,Display the version number and copyrights of the invoked GCC.,0,0,others,gcc,0,0
2541,vops,Enable showing virtual operands for every statement.,0,0,others,gcc,0,0
2542,-w,Inhibit all warning messages.,1,6,function-tradeoff,gcc,0,0
2543,"-Wa,option","Pass option as an option to the assembler. If option contains commas, it is split into multiple options at the commas.",0,0,others,gcc,0,0
2544,"-Wabi (C, Objective-C, C++ and Objective-C++ only)",Warn about code affected by ABI changes. This includes code that may not be compatible with the vendor-neutral C++ ABI as well as the psABI for the particular target.,1,6,function-tradeoff,gcc,0,0
2545,-Wabi-tag (C++ and Objective-C++ only),Warn when a type with an ABI tag is used in a context that does not have that ABI tag. See C++ Attributes for more information about ABI tags.,1,6,function-tradeoff,gcc,0,0
2546,-Wabsolute-value (C and Objective-C only),"Warn for calls to standard functions that compute the absolute value of an argument when a more appropriate standard function is available. For example, calling abs(3.14) triggers the warning because the appropriate function to call to compute the absolute value of a double argument is fabs. The option also triggers warnings when the argument in a call to such a function has an unsigned type. This warning can be suppressed with an explicit type cast and it is also enabled by -Wextra.",1,6,function-tradeoff,gcc,0,0
2547,-Waddress,"Warn about suspicious uses of memory addresses. These include using the address of a function in a conditional expression, such as void func(void); if (func), and comparisons against the memory address of a string literal, such as if (x == ""abc""). Such uses typically indicate a programmer error: the address of a function always evaluates to true, so their use in a conditional usually indicate that the programmer forgot the parentheses in a function call; and comparisons against string literals result in unspecified behavior and are not portable in C, so they usually indicate that the programmer intended to use strcmp. This warning is enabled by -Wall.",1,6,function-tradeoff,gcc,0,0
2548,-Waggregate-return,"Warn if any functions that return structures or unions are defined or called. (In languages where you can return an array, this also elicits a warning.)",0,0,others,gcc,0,0
2549,-Waligned-new,Warn about a new-expression of a type that requires greater alignment than the alignof(std::max_align_t) but uses an allocation function without an explicit alignment parameter. This option is enabled by -Wall.,1,6,function-tradeoff,gcc,0,0
2550,-Wall,"This enables all the warnings about constructions that some users consider questionable, and that are easy to avoid (or modify to prevent the warning), even in conjunction with macros. This also enables some language-specific warnings described in C++ Dialect Options and Objective-C and Objective-C++ Dialect Options.",0,0,others,gcc,0,1
2551,-Walloca,This option warns on all uses of alloca in the source.,0,0,others,gcc,0,1
2552,-Walloca-larger-than=byte-size,"This option warns on calls to alloca with an integer argument whose value is either zero, or that is not bounded by a controlling predicate that limits its value to at most byte-size. It also warns for calls to alloca where the bound value is unknown. Arguments of non-integer types are considered unbounded even if they appear to be constrained to the expected range.",0,0,others,gcc,0,0
2553,-Walloc-size-larger-than=byte-size,"Warn about calls to functions decorated with attribute alloc_size that attempt to allocate objects larger than the specified number of bytes, or where the result of the size computation in an integer type with infinite precision would exceed the value of PTRDIFF_MAXon the target. -Walloc-size-larger-than=PTRDIFF_MAXis enabled by default. Warnings controlled by the option can be disabled either by specifying byte-size of SIZE_MAXor more or by -Wno-alloc-size-larger-than. See Function Attributes.",1,6,function-tradeoff,gcc,0,0
2554,-Walloc-zero,"Warn about calls to allocation functions decorated with attribute alloc_size that specify zero bytes, including those to the built-in forms of the functions aligned_alloc, alloca, calloc, malloc, and realloc. Because the behavior of these functions when called with a zero size differs among implementations (and in the case of realloc has been deprecated) relying on it may result in subtle portability bugs and should be avoided.",1,6,function-tradeoff,gcc,0,0
2555,-Warith-conversion,"Do warn about implicit conversions from arithmetic operations even when conversion of the operands to the same type cannot change their values. This affects warnings from -Wconversion, -Wfloat-conversion, and -Wsign-conversion.",1,6,function-tradeoff,gcc,0,0
2556,-Warray-bounds=1,"This is the warning level of -Warray-bounds and is enabled by -Wall; higher levels are not, and must be explicitly requested.",0,0,others,gcc,0,1
2557,-Warray-bounds=2,This warning level also warns about out of bounds access for arrays at the end of a struct and for arrays accessed through pointers. This warning level may give a larger number of false positives and is deactivated by default.,0,0,others,gcc,0,0
2558,-Warray-bounds=n,This option is only active when -ftree-vrp is active (default for -O2 and above). It warns about subscripts to arrays that are always out of bounds. This warning is enabled by -Wall.,0,0,others,gcc,0,0
2559,-Warray-parameter=n,"Warn about redeclarations of functions involving arguments of array or pointer types of inconsistent kinds or forms, and enable the detection of out-of-bounds accesses to such parameters by warnings such as -Warray-bounds.",1,6,function-tradeoff,gcc,0,1
2560,-Wassign-intercept (Objective-C and Objective-C++ only),Warn whenever an Objective-C assignment is being intercepted by the garbage collector.,1,6,function-tradeoff,gcc,0,0
2561,-Wattribute-alias=1,The default warning level of the -Wattribute-alias option diagnoses incompatibilities between the type of the alias declaration and that of its target. Such incompatibilities are typically indicative of bugs.,0,0,others,gcc,0,0
2563,-Wbad-function-cast (C and Objective-C only),"Warn when a function call is cast to a non-matching type. For example, warn if a call to a function returning an integer type is cast to a pointer type.",1,6,function-tradeoff,gcc,0,1
2564,-Wbool-compare,"Warn about boolean expression compared with an integer value different from true/false. For instance, the following comparison is always false:",1,6,function-tradeoff,gcc,0,1
2566,-Wc++11-compat (C++ and Objective-C++ only),"Warn about C++ constructs whose meaning differs between ISO C++ 1998 and ISO C++ 2011, e.g., identifiers in ISO C++ 1998 that are keywords in ISO C++ 2011. This warning turns on -Wnarrowing and is enabled by -Wall.",1,6,function-tradeoff,gcc,0,0
2567,-Wc++14-compat (C++ and Objective-C++ only),Warn about C++ constructs whose meaning differs between ISO C++ 2011 and ISO C++ 2014. This warning is enabled by -Wall.,1,6,function-tradeoff,gcc,0,1
2568,-Wc++17-compat (C++ and Objective-C++ only),Warn about C++ constructs whose meaning differs between ISO C++ 2014 and ISO C++ 2017. This warning is enabled by -Wall.,1,6,function-tradeoff,gcc,0,0
2569,-Wc++20-compat (C++ and Objective-C++ only),Warn about C++ constructs whose meaning differs between ISO C++ 2017 and ISO C++ 2020. This warning is enabled by -Wall.,1,6,function-tradeoff,gcc,0,0
2570,-Wc++-compat (C and Objective-C only),"Warn about ISO C constructs that are outside of the common subset of ISO C and ISO C++, e.g. request for implicit conversion from void * to a pointer to non-void type.",1,6,function-tradeoff,gcc,0,0
2571,-Wc11-c2x-compat (C and Objective-C only),"Warn about features not present in ISO C11, but present in ISO C2X. For instance, warn about omitting the string in _Static_assert, use of [[]]syntax for attributes, use of decimal floating-point types, and so on. This option is independent of the standards mode. Warnings are disabled in the expression that follows __extension__.",1,6,function-tradeoff,gcc,0,0
2572,-Wc90-c99-compat (C and Objective-C only),"Warn about features not present in ISO C90, but present in ISO C99. For instance, warn about use of variable length arrays, long long type, bool type, compound literals, designated initializers, and so on. This option is independent of the standards mode. Warnings are disabled in the expression that follows __extension__.",1,6,function-tradeoff,gcc,0,1
2573,-Wc99-c11-compat (C and Objective-C only),"Warn about features not present in ISO C99, but present in ISO C11. For instance, warn about use of anonymous structures and unions, _Atomic type qualifier, _Thread_local storage-class specifier, _Alignas specifier, Alignof operator, _Generic keyword, and so on. This option is independent of the standards mode. Warnings are disabled in the expression that follows __extension__.",1,6,function-tradeoff,gcc,0,0
2575,-Wcast-align=strict,"Warn whenever a pointer is cast such that the required alignment of the target is increased. For example, warn if a char * is cast to an int * regardless of the target machine.",1,6,function-tradeoff,gcc,0,0
2576,-Wcast-function-type,"Warn when a function pointer is cast to an incompatible function pointer. In a cast involving function types with a variable argument list only the types of initial arguments that are provided are considered. Any parameter of pointer-type matches any other pointer-type. Any benign differences in integral types are ignored, like int vs. long on ILP32 targets. Likewise type qualifiers are ignored. The function type void (*) (void) is special and matches everything, which can be used to suppress this warning. In a cast involving pointer to member types this warning warns whenever the type cast is changing the pointer to member type. This warning is enabled by -Wextra.",1,6,function-tradeoff,gcc,0,1
2577,-Wcast-qual,"Warn whenever a pointer is cast so as to remove a type qualifier from the target type. For example, warn if a const char * is cast to an ordinary char *.",1,6,function-tradeoff,gcc,0,0
2578,-Wcatch-value=n (C++ and Objective-C++ only),Warn about catch handlers that do not catch via reference. With -Wcatch-value=1 (or -Wcatch-value for short) warn about polymorphic class types that are caught by value. With -Wcatch-value=2 warn about all class types that are caught by value. With -Wcatch-value=3 warn about all types that are not caught by reference. -Wcatch-value is enabled by -Wall.,1,6,function-tradeoff,gcc,0,0
2579,-Wchar-subscripts,"Warn if an array subscript has type char. This is a common cause of error, as programmers often forget that this type is signed on some machines. This warning is enabled by -Wall.",1,6,function-tradeoff,gcc,0,0
2580,-Wclass-memaccess (C++ and Objective-C++ only),"Warn when the destination of a call to a raw memory function such as memset or memcpy is an object of class type, and when writing into such an object might bypass the class non-trivial or deleted constructor or copy assignment, violate const-correctness or encapsulation, or corrupt virtual table pointers. Modifying the representation of such objects may violate invariants maintained by member functions of the class. For example, the call to memset below is undefined because it modifies a non-trivial class object and is, therefore, diagnosed. The safe way to either initialize or clear the storage of objects of such types is by using the appropriate constructor or assignment operator, if one is available.",1,6,function-tradeoff,gcc,0,1
2581,-Wclobbered,Warn for variables that might be changed by longjmp or vfork. This warning is also enabled by -Wextra.,1,6,function-tradeoff,gcc,0,0
2583,-Wcomments,"Warn whenever a comment-start sequence *appears in a *comment, or whenever a backslash-newline appears in a /comment. This warning is enabled by -Wall.",1,6,function-tradeoff,gcc,0,0
2584,-Wconditionally-supported (C++ and Objective-C++ only),Warn for conditionally-supported (C++11 [intro.defs]) constructs.,1,6,function-tradeoff,gcc,0,1
2585,-Wconversion,"Warn for implicit conversions that may alter a value. This includes conversions between real and integer, like abs (x) when x is double; conversions between signed and unsigned, like unsigned ui = -1; and conversions to smaller types, like sqrtf (M_PI). Do not warn for explicit casts like abs ((int) x) and ui = (unsigned) -1, or if the value is not changed by the conversion like in abs (2.0). Warnings about conversions between signed and unsigned integers can be disabled by using -Wno-sign-conversion.",1,6,function-tradeoff,gcc,0,1
2587,-Wctor-dtor-privacy (C++ and Objective-C++ only),"Warn when a class seems unusable because all the constructors or destructors in that class are private, and it has neither friends nor public static member functions. Also warn if there are no non-private methods, and there's at least one private member function that isn't a constructor or destructor.",1,6,function-tradeoff,gcc,0,0
2589,-Wdate-time,"Warn when macros __TIME__, __DATE__ or __TIMESTAMP__ are encountered as they might prevent bit-wise-identical reproducible compilations.",1,6,function-tradeoff,gcc,0,0
2590,-Wdeclaration-after-statement (C and Objective-C only),"Warn when a declaration is found after a statement in a block. This construct, known from C++, was introduced with ISO C99 and is by default allowed in GCC. It is not supported by ISO C90. See Mixed Labels and Declarations.",1,6,function-tradeoff,gcc,0,1
2591,-Wdelete-non-virtual-dtor (C++ and Objective-C++ only),Warn when delete is used to destroy an instance of a class that has virtual functions and non-virtual destructor. It is unsafe to delete an instance of a derived class through a pointer to a base class if the base class does not have a virtual destructor. This warning is enabled by -Wall.,1,6,function-tradeoff,gcc,0,0
2593,-Wdisabled-optimization,"Warn if a requested optimization pass is disabled. This warning does not generally indicate that there is anything wrong with your code; it merely indicates that GCC optimizers are unable to handle the code effectively. Often, the problem is that your code is too big or too complex; GCC refuses to optimize programs when the optimization itself is likely to take inordinate amounts of time.",1,6,function-tradeoff,gcc,0,0
2594,"-Wdouble-promotion (C, C++, Objective-C and Objective-C++ only)","Give a warning when a value of type float is implicitly promoted to double. CPUs with a 32-bit single-precisionfloating-point unit implement float in hardware, but emulate double in software. On such a machine, doing computations using double values is much more expensive because of the overhead required for software emulation.",0,0,others,gcc,0,1
2595,-Wduplicated-branches,Warn when an if-else has identical branches. This warning detects cases like,1,6,function-tradeoff,gcc,0,0
2596,-Wduplicated-cond,"Warn about duplicated conditions in an if-else-if chain. For instance, warn for the following code:",1,6,function-tradeoff,gcc,0,0
2597,-Wduplicate-decl-specifier (C and Objective-C only),"Warn if a declaration has duplicate const, volatile, restrict or _Atomic specifier. This warning is enabled by -Wall.",1,6,function-tradeoff,gcc,0,1
2598,-Weffc++ (C++ and Objective-C++ only),Warn about violations of the following style guidelines from Scott MeyersEffective C++ series of books:,1,6,function-tradeoff,gcc,0,0
2599,-Wempty-body,"Warn if an empty body occurs in an if, else or do while statement. This warning is also enabled by -Wextra.",1,6,function-tradeoff,gcc,0,0
2600,-Wenum-compare,Warn about a comparison between values of different enumerated types. In C++ enumerated type mismatches in conditional expressions are also diagnosed and the warning is enabled by default. In C this warning is enabled by -Wall.,1,6,function-tradeoff,gcc,0,0
2601,-Wenum-conversion,Warn when a value of enumerated type is implicitly converted to a different enumerated type. This warning is enabled by -Wextra in C.,1,6,function-tradeoff,gcc,0,0
2602,-Werror,Make all warnings into errors.,0,0,others,gcc,0,0
2603,-Werror=,"Make the specified warning into an error. The specifier for a warning is appended; for example -Werror=switch turns the warnings controlled by -Wswitch into errors. This switch takes a negative form, to be used to negate -Werror for specific warnings; for example -Wno-error=switch makes -Wswitch warnings not be errors, even when -Werror is in effect.",0,0,others,gcc,0,0
2606,"-Wextra-semi (C++, Objective-C++ only)",Warn about redundant semicolons after in-class function definitions.,1,6,function-tradeoff,gcc,0,0
2607,-Wfatal-errors,This option causes the compiler to abort compilation on the first error occurred rather than trying to keep going and printing further error messages.,0,0,others,gcc,0,0
2608,-Wfloat-conversion,"Warn for implicit conversions that reduce the precision of a real value. This includes conversions from real to integer, and from higher precision real to lower precision real values. This option is also enabled by -Wconversion.",1,6,function-tradeoff,gcc,0,1
2609,-Wfloat-equal,Warn if floating-point values are used in equality comparisons.,1,6,function-tradeoff,gcc,0,0
2610,-Wformat,"Option -Wformat is equivalent to -Wformat=1, and -Wno-format is equivalent to -Wformat=0. Since -Wformat also checks for null format arguments for several functions, -Wformat also implies -Wnonnull. Some aspects of this level of format checking can be disabled by the options: -Wno-format-contains-nul, -Wno-format-extra-args, and -Wno-format-zero-length. -Wformat is enabled by -Wall.",0,0,others,gcc,0,0
2611,-Wformat=2,Enable -Wformat plus additional format checks. Currently equivalent to -Wformat -Wformat-nonliteral -Wformat-security -Wformat-y2k.,0,0,others,gcc,0,0
2612,-Wformat=n,"Check calls to printf and scanf, etc., to make sure that the arguments supplied have types appropriate to the format string specified, and that the conversions specified in the format string make sense. This includes standard functions, and others specified by format attributes (see Function Attributes), in the printf, scanf, strftime and strfmon (an X/Open extension, not in the C standard) families (or other target-specific families). Which functions are checked without format attributes having been specified depends on the standard version selected, and such checks of functions without the attribute specified are disabled by -ffreestanding or -fno-builtin.",1,6,function-tradeoff,gcc,0,0
2613,-Wformat-nonliteral,"If -Wformat is specified, also warn about uses of format functions that represent possible security problems. At present, this warns about calls to printf and scanf functions where the format string is not a string literal and there are no format arguments, as in printf (foo);. This may be a security hole if the format string came from untrusted input and contains n (This is currently a subset of what -Wformat-nonliteral warns about, but in future warnings may be added to -Wformat-security that are not included in -Wformat-nonliteral.)",1,6,function-tradeoff,gcc,0,0
2614,-Wformat-overflow,"Level 1 of -Wformat-overflow enabled by -Wformat employs a conservative approach that warns only about calls that most likely overflow the buffer. At this level, numeric arguments to format directives with unknown values are assumed to have the value of one, and strings of unknown length to be empty. Numeric arguments that are known to be bounded to a subrange of their type, or string arguments whose output is bounded either by their directive precision or by a finite set of string literals, are assumed to take on the value within the range that results in the most bytes on output. For example, the call to sprintf below is diagnosed because even with both a and b equal to zero, the terminating NUL character ('\0') appended by the function to the destination buffer will be written past its end. Increasing the size of the buffer by a single byte is sufficient to avoid the warning, though it may not be sufficient to avoid the overflow.",1,3,reliability-tradeoff,gcc,0,0
2615,-Wformat-security,"If -Wformat is specified, also warn if the format string requires an unsigned argument and the argument is signed and vice versa.",1,6,function-tradeoff,gcc,0,1
2617,-Wformat-y2k,Warn about passing a null pointer for arguments marked as requiring a non-null value by the nonnull function attribute.,1,6,function-tradeoff,gcc,0,1
2618,-Wframe-address,Warn when the __builtin_frame_addressor __builtin_return_addressis called with an argument greater than 0. Such calls may return indeterminate values or crash the program. The warning is included in -Wall.,1,6,function-tradeoff,gcc,0,0
2619,-Wframe-larger-than=byte-size,"Warn if the size of a function frame exceeds byte-size. The computation done to determine the stack frame size is approximate and not conservative. The actual requirements may be somewhat greater than byte-size even if you do not get a warning. In addition, any space allocated via alloca, variable-length arrays, or related constructs is not included by the compiler when determining whether or not to issue a warning. -Wframe-larger-than=PTRDIFF_MAXis enabled by default. Warnings controlled by the option can be disabled either by specifying byte-size of SIZE_MAXor more or by -Wno-frame-larger-than.",1,6,function-tradeoff,gcc,0,1
2620,-Wignored-qualifiers (C and C++ only),"This option controls warnings when an attribute is ignored. This is different from the -Wattributes option in that it warns whenever the compiler decides to drop an attribute, not that the attribute is either unknown, used in a wrong place, etc. This warning is enabled by default.",0,0,others,gcc,0,0
2621,-Wimplicit (C and Objective-C only),-Wimplicit-fallthrough is the same as -Wimplicit-fallthrough=3 and -Wno-implicit-fallthrough is the same as -Wimplicit-fallthrough=0.,0,0,others,gcc,0,0
2622,-Wimplicit-fallthrough,Warn when a switch case falls through. For example:,1,6,function-tradeoff,gcc,0,0
2623,-Wimplicit-fallthrough=n,Control if warnings triggered by the warn_if_not_aligned attribute should be issued. These warnings are enabled by default.,0,0,others,gcc,0,1
2624,"-Winit-self (C, C++, Objective-C and Objective-C++ only)","This option controls warnings when a declaration does not specify a type. This warning is enabled by default in C99 and later dialects of C, and also by -Wall.",0,0,others,gcc,0,1
2625,-Winline,"Warn if a function that is declared as inline cannot be inlined. Even with this option, the compiler does not warn about failures to inline functions declared in system headers.",1,6,function-tradeoff,gcc,0,0
2627,-Winvalid-imported-macros,"Verify all imported macro definitions are valid at the end of compilation. This is not enabled by default, as it requires additional processing to determine. It may be useful when preparing sets of header-units to ensure consistent macros.",0,0,others,gcc,0,1
2629,"-Wjump-misses-init (C, Objective-C only)","Warn if a goto statement or a switch statement jumps forward across the initialization of a variable, or jumps backward to a label after the variable has been initialized. This only warns about variables that are initialized when they are declared. This warning is only supported for C and Objective-C; in C++ this sort of branch is an error in any case.",1,6,function-tradeoff,gcc,0,1
2630,"-Wl,option","Pass option as an option to the linker. If option contains commas, it is split into multiple options at the commas. You can use this syntax to pass an argument to the option. For example, -Wl,-Map,output.map passes -Map output.map to the linker. When using the GNU linker, you can also get the same effect with -Wl,-Map=output.map.",0,0,others,gcc,0,0
2631,-Wlarger-than=byte-size,Warn whenever an object is defined whose size exceeds byte-size. -Wlarger-than=PTRDIFF_MAXis enabled by default. Warnings controlled by the option can be disabled either by specifying byte-size of SIZE_MAXor more or by -Wno-larger-than.,1,6,function-tradeoff,gcc,0,1
2632,-Wlogical-not-parentheses,Warn about logical not used on the left hand side operand of a comparison. This option does not warn if the right operand is considered to be a boolean expression. Its purpose is to detect suspicious code like the following:,1,6,function-tradeoff,gcc,0,0
2634,-Wlong-long,"Warn if long long type is used. This is enabled by either -Wpedantic or -Wtraditional in ISO C90 and C++98 modes. To inhibit the warning messages, use -Wno-long-long.",1,6,function-tradeoff,gcc,0,0
2635,-Wmain,"Warn when the indentation of the code does not reflect the block structure. Specifically, a warning is issued for if, else, while, and for clauses with a guarded statement that does not use braces, followed by an unguarded statement with the same indentation.",1,6,function-tradeoff,gcc,0,0
2636,-Wmaybe-uninitialized,"For an object with automatic or allocated storage duration, if there exists a path from the function entry to a use of the object that is initialized, but there exist some other paths for which the object is not initialized, the compiler emits a warning if it cannot prove the uninitialized paths are not executed at run time.",1,5,workload-specific,gcc,0,0
2637,-Wmemset-elt-size,"Warn for suspicious calls to the memset built-in function, if the first argument references an array, and the third argument is a number equal to the number of elements, but not equal to the size of the array in memory. This indicates that the user has omitted a multiplication by the element size. This warning is enabled by -Wall.",1,6,function-tradeoff,gcc,0,0
2638,-Wmemset-transposed-args,"Warn for suspicious calls to the memset built-in function where the second argument is not zero and the third argument is zero. For example, the call memset (buf, sizeof buf, 0) is diagnosed because memset (buf, 0, sizeof buf) was meant instead. The diagnostic is only emitted if the third argument is a literal zero. Otherwise, if it is an expression that is folded to zero, or a cast of zero to some type, it is far less likely that the arguments have been mistakenly transposed and no warning is emitted. This warning is enabled by -Wall.",1,6,function-tradeoff,gcc,0,0
2639,-Wmisleading-indentation (C and C++ only),"Warn when a declaration of a function is missing one or more attributes that a related function is declared with and whose absence may adversely affect the correctness or efficiency of generated code. For example, the warning is issued for declarations of aliases that use attributes to specify less restrictive requirements than those of their targets. This typically represents a potential optimization opportunity. By contrast, the -Wattribute-alias=2 option controls warnings issued when the alias is more restrictive than the target, which could lead to incorrect code generation. Attributes considered include alloc_align, alloc_size, cold, const, hot, leaf, malloc, nonnull, noreturn, nothrow, pure, returns_nonnull, and returns_twice.",1,6,function-tradeoff,gcc,0,0
2640,-Wmismatched-tags (C++ and Objective-C++ only),"Warn for declarations of structs, classes, and class templates and their specializations with a class-key that does not match either the definition or the first declaration if no definition is provided.",1,6,function-tradeoff,gcc,0,0
2641,-Wmissing-attributes,"Warn if an aggregate or union initializer is not fully bracketed. In the following example, the initializer for a is not fully bracketed, but that for b is fully bracketed.",1,6,function-tradeoff,gcc,0,0
2643,-Wmissing-declarations,"Warn if a global function is defined without a previous declaration. Do so even if the definition itself provides a prototype. Use this option to detect global functions that are not declared in header files. In C, no warnings are issued for functions with previous non-prototype declarations; use -Wmissing-prototypes to detect missing prototypes. In C++, no warnings are issued for function templates, or for inline functions, or for functions in anonymous namespaces.",1,6,function-tradeoff,gcc,0,0
2644,-Wmissing-field-initializers,"Warn if a structure initializer has some fields missing. For example, the following code causes such a warning, because x.h is implicitly zero:",1,6,function-tradeoff,gcc,0,1
2645,-Wmissing-format-attribute,"Warn about function pointers that might be candidates for format attributes. Note these are only possible candidates, not absolute ones. GCC guesses that function pointers with format attributes that are used in assignment, initialization, parameter passing or return statements should have a corresponding format attribute in the resulting type. I.e. the left-hand side of the assignment or initialization, the type of the parameter variable, or the return type of the containing function respectively should also have a format attribute to avoid the warning.",1,6,function-tradeoff,gcc,0,0
2646,"-Wmissing-include-dirs (C, C++, Objective-C and Objective-C++ only)","This option controls warnings if feedback profiles are missing when using the -fprofile-use option. This option diagnoses those cases where a new function or a new file is added between compiling with -fprofile-generate and with -fprofile-use, without regenerating the profiles. In these cases, the profile feedback data files do not contain any profile feedback information for the newly added function or file respectively. Also, in the case when profile count data (.gcda) files are removed, GCC cannot use any profile feedback information. In all these cases, warnings are issued to inform you that a profile generation step is due. Ignoring the warning can result in poorly optimized code. -Wno-missing-profile can be used to disable the warning, but this is not recommended and should be done only when non-existent profile data is justified.",0,0,others,gcc,0,0
2647,-Wmissing-parameter-type (C and Objective-C only),A function parameter is declared without a type specifier in K&R-style functions:,0,0,others,gcc,0,0
2648,-Wmissing-prototypes (C and Objective-C only),Warn if a global function is defined without a previous prototype declaration. This warning is issued even if the definition itself provides a prototype. Use this option to detect global functions that do not have a matching prototype declaration in a header file. This option is not valid for C++ because all function declarations provide prototypes and a non-matching declaration declares an overload rather than conflict with an earlier declaration. Use -Wmissing-declarations to detect missing declarations in C++.,1,6,function-tradeoff,gcc,0,0
2652,-Wnested-externs (C and Objective-C only),Warn if an extern declaration is encountered within a function.,1,6,function-tradeoff,gcc,0,0
2654,-Wno-aggressive-loop-optimizations,Warn if in a loop with constant number of iterations the compiler detects undefined behavior in some statement during one or more of the iterations.,1,6,function-tradeoff,gcc,0,0
2655,-Wno-alloca-larger-than,Disable -Walloca-larger-than= warnings. The option is equivalent to -Walloca-larger-than=SIZE_MAXor larger.,0,0,others,gcc,0,0
2656,-Wno-alloc-size-larger-than,Disable -Walloc-size-larger-than= warnings. The option is equivalent to -Walloc-size-larger-than=SIZE_MAXor larger.,0,0,others,gcc,0,0
2657,-Wno-attribute-alias,Warn about declarations using the alias and similar attributes whose target is incompatible with the type of the alias. See Declaring Attributes of Functions.,1,6,function-tradeoff,gcc,0,0
2658,-Wno-attributes,"Do not warn if an unexpected __attribute__ is used, such as unrecognized attributes, function attributes applied to variables, etc. This does not stop errors for incorrect use of supported attributes.",1,6,function-tradeoff,gcc,0,0
2659,-Wno-attribute-warning,"Do not warn about usage of functions (see Function Attributes) declared with warning attribute. By default, this warning is enabled. -Wno-attribute-warning can be used to disable the warning or -Wno-error=attribute-warning can be used to disable the error when compiled with -Werror flag.",1,6,function-tradeoff,gcc,0,0
2660,-Wno-builtin-declaration-mismatch,"Warn if a built-in function is declared with an incompatible signature or as a non-function, or when a built-in function declared with a type that does not include a prototype is called with arguments whose promoted types do not match those expected by the function. When -Wextra is specified, also warn when a built-in function that takes arguments is declared without a prototype. The -Wbuiltin-declaration-mismatch warning is enabled by default. To avoid the warning include the appropriate header to bring the prototypes of built-in functions into scope.",1,6,function-tradeoff,gcc,0,0
2661,-Wno-builtin-macro-redefined,"Do not warn if certain built-in macros are redefined. This suppresses warnings for redefinition of __TIMESTAMP__, __TIME__, __DATE__, __FILE__, and __BASE_FILE__.",1,6,function-tradeoff,gcc,0,0
2662,-Wno-class-conversion (C++ and Objective-C++ only),"Do not warn when a conversion function converts an object to the same type, to a base class of that type, or to void; such a conversion function will never be called.",1,6,function-tradeoff,gcc,0,0
2663,-Wno-conversion-null (C++ and Objective-C++ only),Do not warn for conversions between NULL and non-pointer types. -Wconversion-null is enabled by default.,1,6,function-tradeoff,gcc,0,0
2664,-Wno-coverage-mismatch,"Warn if feedback profiles do not match when using the -fprofile-use option. If a source file is changed between compiling with -fprofile-generate and with -fprofile-use, the files with the profile feedback can fail to match the source file and GCC cannot use the profile feedback information. By default, this warning is enabled and is treated as an error. -Wno-coverage-mismatch can be used to disable the warning or -Wno-error=coverage-mismatch can be used to disable the error. Disabling the error for this warning can result in poorly optimized code and is useful only in the case of very minor changes such as bug fixes to an existing code-base. Completely disabling the warning is not recommended.",1,6,function-tradeoff,gcc,0,0
2665,-Wno-cpp,"(C, Objective-C, C++, Objective-C++ and Fortran only) Suppress warning messages emitted by #warning directives.",0,0,others,gcc,0,0
2666,-Wno-delete-incomplete (C++ and Objective-C++ only),"Do not warn when deleting a pointer to incomplete type, which may cause undefined behavior at runtime. This warning is enabled by default.",1,6,function-tradeoff,gcc,0,1
2667,-Wno-deprecated,Do not warn about usage of deprecated features. See Deprecated Features.,0,0,others,gcc,0,0
2668,-Wno-deprecated-declarations,"Do not warn about uses of functions (see Function Attributes), variables (see Variable Attributes), and types (see Type Attributes) marked as deprecated by using the deprecated attribute.",1,6,function-tradeoff,gcc,0,0
2671,-Wno-designated-init (C and Objective-C only),Suppress warnings when a positional initializer is used to initialize a structure that has been marked with the designated_init attribute.,0,0,others,gcc,0,0
2672,-Wno-discarded-array-qualifiers (C and Objective-C only),"Do not warn if type qualifiers on arrays which are pointer targets are being discarded. Typically, the compiler warns if a const int (*)[] variable is passed to a function that takes a int (*)[] parameter. This option can be used to suppress such a warning.",1,6,function-tradeoff,gcc,0,0
2673,-Wno-discarded-qualifiers (C and Objective-C only),"Do not warn if type qualifiers on pointers are being discarded. Typically, the compiler warns if a const char * variable is passed to a function that takes a char * parameter. This option can be used to suppress such a warning.",1,6,function-tradeoff,gcc,0,0
2674,-Wno-div-by-zero,"Do not warn about compile-time integer division by zero. Floating-point division by zero is not warned about, as it can be a legitimate way of obtaining infinities and NaNs.",0,0,others,gcc,0,0
2675,-Wno-endif-labels,Do not warn about stray tokens after #else and #endif.,1,6,function-tradeoff,gcc,0,0
2676,-Wnoexcept (C++ and Objective-C++ only),Warn when a noexcept-expression evaluates to false because of a call to a function that does not have a non-throwing exception specification (i.e. throw() or noexcept) but is known by the compiler to never throw an exception.,1,6,function-tradeoff,gcc,0,0
2677,-Wno-exceptions (C++ and Objective-C++ only),"Disable the warning about the case when an exception handler is shadowed by another handler, which can point out a wrong ordering of exception handlers.",1,6,function-tradeoff,gcc,0,0
2678,-Wnoexcept-type (C++ and Objective-C++ only),Warn if the C++17 feature making noexcept part of a function type changes the mangled name of a symbol relative to C++14. Enabled by -Wabi and -Wc++17-compat.,1,6,function-tradeoff,gcc,0,0
2679,-Wno-format-contains-nul,"If -Wformat is specified, do not warn about format strings that contain NUL bytes.",1,6,function-tradeoff,gcc,0,0
2680,-Wno-format-extra-args,"If -Wformat is specified, do not warn about excess arguments to a printf or scanf format function. The C standard specifies that such arguments are ignored.",1,6,function-tradeoff,gcc,0,1
2681,-Wno-format-zero-length,"If -Wformat is specified, also warn if the format string is not a string literal and so cannot be checked, unless the format function takes its format arguments as a va_list.",1,6,function-tradeoff,gcc,0,0
2682,-Wno-frame-larger-than,Disable -Wframe-larger-than= warnings. The option is equivalent to -Wframe-larger-than=SIZE_MAXor larger.,0,0,others,gcc,0,0
2683,-Wno-free-nonheap-object,"Warn when attempting to deallocate an object that was either not allocated on the heap, or by using a pointer that was not returned from a prior call to the corresponding allocation function. For example, because the call to stpcpy returns a pointer to the terminating nul character and not to the begginning of the object, the call to free below is diagnosed.",1,6,function-tradeoff,gcc,0,1
2684,"-Wno-if-not-aligned (C, C++, Objective-C and Objective-C++ only)","Warn if the return type of a function has a type qualifier such as const. For ISO C such a type qualifier has no effect, since the value returned by a function is not an lvalue. For C++, the warning is only emitted for scalar types or void. ISO C prohibits qualified void return types on function definitions, so such return types always receive a warning even without this option.",1,6,function-tradeoff,gcc,0,0
2685,-Wno-ignored-attributes (C and C++ only),"Warn if the type of main is suspicious. main should be a function with external linkage, returning int, taking either zero arguments, two, or three arguments of appropriate types. This warning is enabled by default in C++ and is enabled by either -Wall or -Wpedantic.",1,6,function-tradeoff,gcc,0,0
2686,-Wno-implicit-function-declaration (C and Objective-C only),Same as -Wimplicit-int and -Wimplicit-function-declaration. This warning is enabled by -Wall.,0,0,others,gcc,0,0
2687,-Wno-implicit-int (C and Objective-C only),"This option controls warnings when a function is used before being declared. This warning is enabled by default in C99 and later dialects of C, and also by -Wall. The warning is made into an error by -pedantic-errors.",0,0,others,gcc,0,0
2688,"-Wno-inaccessible-base (C++, Objective-C++ only)",This option controls warnings when a base class is inaccessible in a class derived from it due to ambiguity. The warning is enabled by default. Note that the warning for ambiguous virtual bases is enabled by the -Wextra option.,1,6,function-tradeoff,gcc,0,0
2689,-Wno-incompatible-pointer-types (C and Objective-C only),"Do not warn when there is a conversion between pointers that have incompatible types. This warning is for cases not covered by -Wno-pointer-sign, which warns for pointer argument passing or assignment with different signedness.",1,6,function-tradeoff,gcc,0,0
2690,-Wno-inherited-variadic-ctor,Suppress warnings about use of C++11 inheriting constructors when the base class inherited from has a C variadic constructor; the warning is on by default because the ellipsis is not inherited.,1,4,limited-side-effect,gcc,0,1
2691,-Wno-init-list-lifetime (C++ and Objective-C++ only),"Do not warn about uses of std::initializer_list that are likely to result in dangling pointers. Since the underlying array for an initializer_list is handled like a normal C++ temporary object, it is easy to inadvertently keep a pointer to the array past the end of the array's lifetime. For example:",1,6,function-tradeoff,gcc,0,0
2692,-Wno-int-conversion (C and Objective-C only),Do not warn about incompatible integer to pointer and pointer to integer conversions. This warning is about implicit conversions; for explicit conversions the warnings -Wno-int-to-pointer-cast and -Wno-pointer-to-int-cast may be used.,1,6,function-tradeoff,gcc,0,0
2693,-Wno-int-to-pointer-cast,"Suppress warnings from casts to pointer type of an integer of a different size. In C++, casting to a pointer type of smaller size is an error. Wint-to-pointer-cast is enabled by default.",0,0,others,gcc,0,0
2694,-Wno-invalid-memory-model,"This option controls warnings for invocations of __atomic Builtins, __sync Builtins, and the C11 atomic generic functions with a memory consistency argument that is either invalid for the operation or outside the range of values of the memory_order enumeration. For example, since the __atomic_store and __atomic_store_n built-ins are only defined for the relaxed, release, and sequentially consistent memory orders the following code is diagnosed:",0,0,others,gcc,0,0
2695,-Wno-invalid-offsetof (C++ and Objective-C++ only),"Suppress warnings from applying the offsetof macro to a non-POD type. According to the 2014 ISO C++ standard, applying offsetof to a non-standard-layout type is undefined. In existing C++ implementations, however, offsetof typically gives meaningful results. This flag is for users who are aware that they are writing nonportable code and who have deliberately chosen to ignore the warning about it.",1,4,limited-side-effect,gcc,0,0
2696,-Wno-larger-than,Disable -Wlarger-than= warnings. The option is equivalent to -Wlarger-than=SIZE_MAXor larger.,0,0,others,gcc,0,1
2697,-Wno-literal-suffix (C++ and Objective-C++ only),"Do not warn when a string or character literal is followed by a ud-suffix which does not begin with an underscore. As a conforming extension, GCC treats such suffixes as separate preprocessing tokens in order to maintain backwards compatibility with code that uses formatting macros from <inttypes.h>. For example:",1,6,function-tradeoff,gcc,0,0
2698,-Wno-lto-type-mismatch,"During the link-time optimization, do not warn about type mismatches in global declarations from different compilation units. Requires -flto to be enabled. Enabled by default.",1,6,function-tradeoff,gcc,0,1
2699,-Wno-mismatched-dealloc,"Warn about unsafe multiple statement macros that appear to be guarded by a clause such as if, else, for, switch, or while, in which only the first statement is actually guarded after the macro is expanded.",1,6,function-tradeoff,gcc,0,0
2700,-Wno-mismatched-new-delete (C++ and Objective-C++ only),"Warn for mismatches between calls to operator new or operator delete and the corresponding call to the allocation or deallocation function. This includes invocations of C++ operator delete with pointers returned from either mismatched forms of operator new, or from other functions that allocate objects for which the operator delete isn't a suitable deallocator, as well as calls to other deallocation functions with pointers returned from operator new for which the deallocation function isn't suitable.",1,6,function-tradeoff,gcc,0,0
2701,-Wno-missing-profile,"Warn for calls to deallocation functions with pointer arguments returned from from allocations functions for which the former isn't a suitable deallocator. A pair of functions can be associated as matching allocators and deallocators by use of attribute malloc. Unless disabled by the -fno-builtin option the standard functions calloc, malloc, realloc, and free, as well as the corresponding forms of C++ operator new and operator delete are implicitly associated as matching allocators and deallocators. In the following example mydealloc is the deallocator for pointers returned from myalloc.",1,6,function-tradeoff,gcc,0,0
2702,-Wno-multichar,"Do not warn if a multicharacter constant (FOOF' is used. Usually they indicate a typo in the user's code, as they have implementation-defined values, and should not be used in portable code.",1,6,function-tradeoff,gcc,0,1
2703,-Wno-narrowing (C++ and Objective-C++ only),"For C++11 and later standards, narrowing conversions are diagnosed by default, as required by the standard. A narrowing conversion from a constant produces an error, and a narrowing conversion from a non-constant produces a warning, but -Wno-narrowing suppresses the diagnostic. Note that this does not affect the meaning of well-formed code; narrowing conversions are still considered ill-formed in SFINAE contexts.",0,0,others,gcc,0,0
2704,-Wnonnull,Warn when comparing an argument marked with the nonnull function attribute against null inside the function.,1,6,function-tradeoff,gcc,0,1
2705,-Wnonnull-compare,"Warn if the compiler detects paths that trigger erroneous or undefined behavior due to dereferencing a null pointer. This option is only active when -fdelete-null-pointer-checks is active, which is enabled by optimizations in most targets. The precision of the warnings depends on the optimization options used.",1,6,function-tradeoff,gcc,0,0
2706,-Wno-non-template-friend (C++ and Objective-C++ only),"Disable warnings when non-template friend functions are declared within a template. In very old versions of GCC that predate implementation of the ISO standard, declarations such as friend int foo(int) where the name of the friend is an unqualified-id, could be interpreted as a particular specialization of a template function; the warning exists to diagnose compatibility problems, and is enabled by default.",1,6,function-tradeoff,gcc,0,0
2707,-Wnon-virtual-dtor (C++ and Objective-C++ only),"Warn when a class has virtual functions and an accessible non-virtual destructor itself or in an accessible polymorphic base class, in which case it is possible but unsafe to delete an instance of a derived class through a pointer to the class itself or base class. This warning is automatically enabled if -Weffc++ is specified.",1,6,function-tradeoff,gcc,0,0
2708,-Wno-odr,Warn about One Definition Rule violations during link-time optimization. Enabled by default.,1,6,function-tradeoff,gcc,0,1
2709,-Wno-overflow,Do not warn about compile-time overflow in constant expressions.,0,0,others,gcc,0,0
2710,-Wno-override-init-side-effects (C and Objective-C only),Do not warn if an initialized field with side effects is overridden when using designated initializers (see Designated Initializers). This warning is enabled by default.,1,6,function-tradeoff,gcc,0,1
2711,-Wnopacked-bitfield-compat,"The 4.1, 4.2 and 4.3 series of GCC ignore the packed attribute on bit-fields of type char. This was fixed in GCC 4.4 but the change can lead to differences in the structure layout. GCC informs you when the offset of such a field has changed in GCC 4.4. For example there is no longer a 4-bit padding between field a and b in this structure:",0,0,others,gcc,0,1
2712,-Wno-pedantic-ms-format (MinGW targets only),"When used in combination with -Wformat and -pedantic without GNU extensions, this option disables the warnings about non-ISO printf / scanf format width specifiers I32, I64, and I used on Windows targets, which depend on the MS runtime.",0,0,others,gcc,0,0
2713,-Wno-pessimizing-move (C++ and Objective-C++ only),"This warning warns when a call to std::move prevents copy elision. A typical scenario when copy elision can occur is when returning in a function with a class return type, when the expression being returned is the name of a non-volatile automatic object, and is not a function parameter, and has the same type as the function return type.",1,6,function-tradeoff,gcc,0,0
2714,-Wno-pmf-conversions (C++ and Objective-C++ only),Disable the diagnostic for converting a bound pointer to member function to a plain pointer.,1,6,function-tradeoff,gcc,0,0
2715,-Wno-pointer-compare,Do not warn if a pointer is compared with a zero character constant. This usually means that the pointer was meant to be dereferenced. For example:,1,6,function-tradeoff,gcc,0,1
2716,-Wno-pointer-to-int-cast (C and Objective-C only),Suppress warnings from casts from a pointer to an integer type of a different size.,0,0,others,gcc,0,0
2717,-Wno-pragmas,"Do not warn about misuses of pragmas, such as incorrect parameters, invalid syntax, or conflicts between pragmas. See also -Wunknown-pragmas.",1,6,function-tradeoff,gcc,0,0
2718,-Wno-prio-ctor-dtor,Do not warn if a priority from 0 to 100 is used for constructor or destructor. The use of constructor and destructor attributes allow you to assign a priority to the constructor/destructor to control its order of execution before main is called or after it returns. The priority values must be greater than 100 as the compiler reserves priority values between 000 for the implementation.,1,6,function-tradeoff,gcc,0,1
2719,-Wno-property-assign-default (Objective-C and Objective-C++ only),Do not warn if a property for an Objective-C object has no assign semantics specified.,1,6,function-tradeoff,gcc,0,0
2720,-Wno-protocol (Objective-C and Objective-C++ only),"If a class is declared to implement a protocol, a warning is issued for every method in the protocol that is not implemented by the class. The default behavior is to issue a warning for every method not explicitly implemented in the class, even if a method implementation is inherited from the superclass. If you use the -Wno-protocol option, then methods inherited from the superclass are considered to be implemented, and no warning is issued for them.",1,2,security-tradeoff,gcc,0,0
2722,-Wno-return-local-addr,Warn whenever a function is defined with a return type that defaults to int. Also warn about any return statement with no return value in a function whose return type is not void (falling off the end of the function body is considered returning without a value).,1,6,function-tradeoff,gcc,0,0
2723,-Wnormalized=[none|id|nfc|nfkc],"In ISO C and ISO C++, two identifiers are different if they are different sequences of characters. However, sometimes when characters outside the basic ASCII character set are used, you can have two different character sequences that look the same. To avoid confusion, the ISO 10646 standard sets out some normalization rules which when applied ensure that two sequences that look the same are turned into the same sequence. GCC can warn you if you are using identifiers that have not been normalized; this option controls that warning.",1,6,function-tradeoff,gcc,0,0
2724,-Wno-scalar-storage-order,Do not warn on suspicious constructs involving reverse scalar storage order.,1,6,function-tradeoff,gcc,0,0
2725,-Wno-shadow-ivar (Objective-C only),Do not warn whenever a local variable shadows an instance variable in an Objective-C method.,1,6,function-tradeoff,gcc,0,1
2726,-Wno-shift-count-negative,Controls warnings if a shift count is greater than or equal to the bit width of the type. This warning is enabled by default.,0,0,others,gcc,0,0
2727,-Wno-shift-count-overflow,Warn if left shifting a negative value. This warning is enabled by -Wextra in C99 and C++11 modes (and newer).,1,6,function-tradeoff,gcc,0,0
2728,-Wno-shift-overflow,These options control warnings about left shift overflows.,0,0,others,gcc,0,0
2729,-Wno-sizeof-array-argument,Do not warn when the sizeof operator is applied to a parameter that is declared as an array in a function definition. This warning is enabled by default for C and C++ programs.,1,6,function-tradeoff,gcc,0,0
2730,-Wno-stack-usage,Disable -Wstack-usage= warnings. The option is equivalent to -Wstack-usage=SIZE_MAXor larger.,0,0,others,gcc,0,0
2731,-Wno-stringop-overread,"Warn for calls to string manipulation functions such as memchr, or strcpy that are determined to read past the end of the source sequence.",1,6,function-tradeoff,gcc,0,0
2732,-Wno-stringop-truncation,"Do not warn for calls to bounded string manipulation functions such as strncat, strncpy, and stpncpy that may either truncate the copied string or leave the destination unchanged.",1,6,function-tradeoff,gcc,0,0
2733,-Wno-subobject-linkage (C++ and Objective-C++ only),"Do not warn if a class type has a base or a field whose type uses the anonymous namespace or depends on a type with no linkage. If a type A depends on a type B with no or internal linkage, defining it in multiple translation units would be an ODR violation because the meaning of B is different in each translation unit. If A only appears in a single translation unit, the best way to silence the warning is to give it internal linkage by putting it in an anonymous namespace as well. The compiler doesn妾?give this warning for types defined in the main .C file, as those are unlikely to have multiple definitions. -Wsubobject-linkage is enabled by default.",1,6,function-tradeoff,gcc,0,0
2734,-Wno-switch-bool,Do not warn when a switch statement has an index of boolean type and the case values are outside the range of a boolean type. It is possible to suppress this warning by casting the controlling expression to a type other than bool. For example:,1,6,function-tradeoff,gcc,0,0
2735,-Wno-switch-outside-range,This option controls warnings when a switch case has a value that is outside of its respective type range. This warning is enabled by default for C and C++ programs.,0,0,others,gcc,0,1
2736,-Wno-switch-unreachable,"Do not warn when a switch statement contains statements between the controlling expression and the first case label, which will never be executed. For example:",1,6,function-tradeoff,gcc,0,0
2737,-Wno-terminate (C++ and Objective-C++ only),Disable the warning about a throw-expression that will immediately result in a call to terminate.,1,6,function-tradeoff,gcc,0,0
2738,-Wno-unused-result,Do not warn if a caller of a function marked with attribute warn_unused_result (see Function Attributes) does not use its return value. The default is -Wunused-result.,1,6,function-tradeoff,gcc,0,0
2739,-Wno-varargs,Do not warn upon questionable usage of the macros used to handle variable arguments like va_start. These warnings are enabled by default.,1,6,function-tradeoff,gcc,0,0
2740,-Wno-vexing-parse (C++ and Objective-C++ only),"Warn about the most vexing parse syntactic ambiguity. This warns about the cases when a declaration looks like a variable definition, but the C++ language requires it to be interpreted as a function declaration. For instance:",1,6,function-tradeoff,gcc,0,0
2741,-Wno-virtual-move-assign,"Suppress warnings about inheriting from a virtual base with a non-trivial C++11 move assignment operator. This is dangerous because if the virtual base is reachable along more than one path, it is moved multiple times, which can mean both objects end up in the moved-from state. If the move assignment operator is written to avoid moving from a moved-from object, this warning can be disabled.",1,6,function-tradeoff,gcc,0,0
2742,-Wno-vla-larger-than,Disable -Wvla-larger-than= warnings. The option is equivalent to -Wvla-larger-than=SIZE_MAXor larger.,0,0,others,gcc,0,0
2743,-Wnull-dereference,Warn about uninitialized variables that are initialized with themselves. Note this option can only be used with the -Wuninitialized option.,1,6,function-tradeoff,gcc,0,0
2745,-Wold-style-cast (C++ and Objective-C++ only),"Warn if an old-style (C-style) cast to a non-void type is used within a C++ program. The new-style casts (dynamic_cast, static_cast, reinterpret_cast, and const_cast) are less vulnerable to unintended effects and much easier to search for.",1,6,function-tradeoff,gcc,0,0
2746,-Wold-style-declaration (C and Objective-C only),"Warn for obsolescent usages, according to the C Standard, in a declaration. For example, warn if storage-class specifiers like static are not the first things in a declaration. This warning is also enabled by -Wextra.",1,6,function-tradeoff,gcc,0,1
2747,-Wold-style-definition (C and Objective-C only),"Warn if an old-style function definition is used. A warning is given even if there is a previous prototype. A definition using )is not considered an old-style definition in C2X mode, because it is equivalent to void)in that case, but is considered an old-style definition for older standards.",1,6,function-tradeoff,gcc,0,0
2748,-Wopenmp-simd,Warn if the vectorizer cost model overrides the OpenMP simd directive set by user. The -fsimd-cost-model=unlimited option can be used to relax the cost model.,1,6,function-tradeoff,gcc,0,1
2749,-Woverlength-strings,"Warn about string constants that are longer than the minimum maximumlength specified in the C standard. Modern compilers generally allow string constants that are much longer than the standard's minimum limit, but very portable programs should avoid using longer strings.",1,6,function-tradeoff,gcc,0,0
2750,-Woverloaded-virtual (C++ and Objective-C++ only),"Warn when a function declaration hides virtual functions from a base class. For example, in:",1,6,function-tradeoff,gcc,0,0
2751,-Woverride-init (C and Objective-C only),Warn if an initialized field without side effects is overridden when using designated initializers (see Designated Initializers).,1,6,function-tradeoff,gcc,0,0
2753,-Wpacked,"Warn if a structure is given the packed attribute, but the packed attribute has no effect on the layout or size of the structure. Such structures may be mis-aligned for little benefit. For instance, in this code, the variable f.x in struct bar is misaligned even though struct bar does not itself have the packed attribute:",1,6,function-tradeoff,gcc,0,0
2754,"-Wpacked-not-aligned (C, C++, Objective-C and Objective-C++ only)","Warn if a structure field with explicitly specified alignment in a packed struct or union is misaligned. For example, a warning will be issued on struct S, like, warning: alignment 1 of 'struct S' is less than 8, in this code:",1,6,function-tradeoff,gcc,0,0
2755,-Wpadded,"Warn if padding is included in a structure, either to align an element of the structure or to align the whole structure. Sometimes when this happens it is possible to rearrange the fields of the structure to reduce the padding and so make the structure smaller.",1,6,function-tradeoff,gcc,0,0
2756,-Wparentheses,Warn about code that may have undefined semantics because of violations of sequence point rules in the C and C++ standards.,1,6,function-tradeoff,gcc,0,0
2757,-Wplacement-new=1,"This is the default warning level of -Wplacement-new. At this level the warning is not issued for some strictly undefined constructs that GCC allows as extensions for compatibility with legacy code. For example, the following new expression is not diagnosed at this level even though it has undefined behavior according to the C++ standard because it writes past the end of the one-element array.",1,6,function-tradeoff,gcc,0,0
2758,-Wplacement-new=2,"At this level, in addition to diagnosing all the same constructs as at level 1, a diagnostic is also issued for placement new expressions that construct an object in the last member of structure whose type is an array of a single element and whose size is less than the size of the object being constructed. While the previous example would be diagnosed, the following construct makes use of the flexible member array extension to avoid the warning at level 2.",1,6,function-tradeoff,gcc,0,0
2759,-Wplacement-new=n,"Warn about placement new expressions with undefined behavior, such as constructing an object in a buffer that is smaller than the type of the object. For example, the placement new expression below is diagnosed because it attempts to construct an array of 64 integers in a buffer only 64 bytes large.",1,6,function-tradeoff,gcc,0,0
2760,-Wpointer-arith,"Warn about anything that depends on the size ofa function type or of void. GNU C assigns these types a size of 1, for convenience in calculations with void * pointers and pointers to functions. In C++, warn also when an arithmetic operation involves NULL. This warning is also enabled by -Wpedantic.",1,6,function-tradeoff,gcc,0,0
2761,-Wpointer-sign (C and Objective-C only),"Warn for pointer argument passing or assignment with different signedness. This option is only supported for C and Objective-C. It is implied by -Wall and by -Wpedantic, which can be disabled with -Wno-pointer-sign.",1,6,function-tradeoff,gcc,0,0
2762,-Wrange-loop-construct (C++ and Objective-C++ only),"This warning warns when a C++ range-based for-loop is creating an unnecessary copy. This can happen when the range declaration is not a reference, but probably should be. For example:",1,6,function-tradeoff,gcc,0,0
2767,-Wreorder (C++ and Objective-C++ only),Warn when the order of member initializers given in the code does not match the order in which they must be executed. For instance:,1,6,function-tradeoff,gcc,0,0
2768,-Wrestrict,"Warn when an object referenced by a restrict-qualified parameter (or, in C++, a __restrict-qualified parameter) is aliased by another argument, or when copies between such objects overlap. For example, the call to the strcpy function below attempts to truncate the string by replacing its initial characters with the last four. However, because the call writes the terminating NUL into a[4], the copies overlap and the call is diagnosed.",1,6,function-tradeoff,gcc,0,0
2769,-Wreturn-type,Controls warnings if a shift count is negative. This warning is enabled by default.,0,0,others,gcc,0,0
2770,-Wselector (Objective-C and Objective-C++ only),"Warn if multiple methods of different types for the same selector are found during compilation. The check is performed on the list of methods in the final stage of compilation. Additionally, a check is performed for each selector appearing in a @selector( expression, and a corresponding method for that selector has been found during compilation. Because these checks scan the method table only at the end of compilation, these warnings are not produced if the final stage of compilation is not reached, for example because an error is found during compilation, or because the -fsyntax-only option is being used.",1,6,function-tradeoff,gcc,0,0
2771,-Wsequence-point,"Do not warn about returning a pointer (or in C++, a reference) to a variable that goes out of scope after the function returns.",1,6,function-tradeoff,gcc,0,1
2772,-Wshadow,"Warn whenever a local variable or type declaration shadows another variable, parameter, type, class member (in C++), or instance variable (in Objective-C) or whenever a built-in function is shadowed. Note that in C++, the compiler warns if a local variable shadows an explicit typedef, but not if it shadows a struct/class/enum. If this warning is enabled, it includes also all instances of local shadowing. This means that -Wno-shadow=local and -Wno-shadow=compatible-local are ignored when -Wshadow is used. Same as -Wshadow=global.",1,6,function-tradeoff,gcc,0,1
2773,-Wshadow=compatible-local,"Warn when a local variable shadows another local variable or parameter whose type is compatible with that of the shadowing variable. In C++, type compatibility here means the type of the shadowing variable can be converted to that of the shadowed variable. The creation of this flag (in addition to -Wshadow=local) is based on the idea that when a local variable shadows another one of incompatible type, it is most likely intentional, not a bug or typo, as shown in the following example:",1,6,function-tradeoff,gcc,0,1
2774,-Wshadow=global,Warn for any shadowing. Same as -Wshadow.,1,6,function-tradeoff,gcc,0,1
2775,-Wshadow=local,Warn when a local variable shadows another local variable or parameter.,1,6,function-tradeoff,gcc,0,0
2776,-Wshift-overflow=1,"This is the warning level of -Wshift-overflow and is enabled by default in C99 and C++11 modes (and newer). This warning level does not warn about left-shifting 1 into the sign bit. (However, in C, such an overflow is still rejected in contexts where an integer constant expression is required.) No warning is emitted in C++20 mode (and newer), as signed left shifts always wrap.",0,0,others,gcc,0,0
2777,-Wshift-overflow=2,"This warning level also warns about left-shifting 1 into the sign bit, unless C++14 mode (or newer) is active.",0,0,others,gcc,0,1
2778,-Wsign-compare,"Warn when a comparison between signed and unsigned values could produce an incorrect result when the signed value is converted to unsigned. In C++, this warning is also enabled by -Wall. In C, it is also enabled by -Wextra.",1,6,function-tradeoff,gcc,0,0
2779,-Wsign-conversion,"Warn for implicit conversions that may change the sign of an integer value, like assigning a signed integer expression to an unsigned integer variable. An explicit cast silences the warning. In C, this option is enabled also by -Wconversion.",1,6,function-tradeoff,gcc,0,0
2780,-Wsign-promo (C++ and Objective-C++ only),"Warn when overload resolution chooses a promotion from unsigned or enumerated type to a signed type, over a conversion to an unsigned type of the same size. Previous versions of G++ tried to preserve unsignedness, but the standard mandates the current behavior.",1,6,function-tradeoff,gcc,0,0
2781,-Wsized-deallocation (C++ and Objective-C++ only),Warn about a definition of an unsized deallocation function,1,6,function-tradeoff,gcc,0,0
2782,-Wsizeof-array-div,"Warn about divisions of two sizeof operators when the first one is applied to an array and the divisor does not equal the size of the array element. In such a case, the computation will not yield the number of elements in the array, which is likely what the user intended. This warning warns e.g. about",1,6,function-tradeoff,gcc,0,0
2783,-Wsizeof-pointer-div,"Warn for suspicious divisions of two sizeof expressions that divide the pointer size by the element size, which is the usual way to compute the array size but won't work out correctly with pointers. This warning warns e.g. about sizeof (ptr) / sizeof (ptr[0]) if ptr is not an array, but a pointer. This warning is enabled by -Wall.",1,6,function-tradeoff,gcc,0,0
2784,-Wsizeof-pointer-memaccess,"Warn for suspicious length parameters to certain string and memory built-in functions if the argument uses sizeof. This warning triggers for example for memset (ptr, 0, sizeof (ptr)); if ptr is not an array, but a pointer, and suggests a possible fix, or about memcpy (&foo, ptr, sizeof (&foo));. -Wsizeof-pointer-memaccess also warns about calls to bounded string copy functions like strncat or strncpy that specify as the bound a sizeof expression of the source array. For example, in the following function the call to strncat specifies the size of the source string as the bound. That is almost certainly a mistake and so the call is diagnosed.",1,6,function-tradeoff,gcc,0,0
2785,-Wstack-protector,This option is only active when -fstack-protector is active. It warns about functions that are not protected against stack smashing.,0,0,others,gcc,0,0
2786,-Wstack-usage=byte-size,"Warn if the stack usage of a function might exceed byte-size. The computation done to determine the stack usage is conservative. Any space allocated via alloca, variable-length arrays, or related constructs is included by the compiler when determining whether or not to issue a warning.",1,6,function-tradeoff,gcc,0,0
2787,-Wstrict-aliasing,"This option is only active when -fstrict-aliasing is active. It warns about code that might break the strict aliasing rules that the compiler is using for optimization. The warning does not catch all cases, but does attempt to catch the more common pitfalls. It is included in -Wall. It is equivalent to -Wstrict-aliasing=3",0,0,others,gcc,0,0
2788,-Wstrict-aliasing=n,"This option is only active when -fstrict-aliasing is active. It warns about code that might break the strict aliasing rules that the compiler is using for optimization. Higher levels correspond to higher accuracy (fewer false positives). Higher levels also correspond to more effort, similar to the way -O works. -Wstrict-aliasing is equivalent to -Wstrict-aliasing=3.",0,0,others,gcc,0,0
2789,-Wstrict-null-sentinel (C++ and Objective-C++ only),"Warn about the use of an uncasted NULL as sentinel. When compiling only with GCC this is a valid sentinel, as NULL is defined to __null. Although it is a null pointer constant rather than a null pointer, it is guaranteed to be of the same size as a pointer. But this use is not portable across different compilers.",1,6,function-tradeoff,gcc,0,1
2790,-Wstrict-overflow=1,"Warn about cases that are both questionable and easy to avoid. For example the compiler simplifies x + 1 > x to 1. This level of -Wstrict-overflow is enabled by -Wall; higher levels are not, and must be explicitly requested.",1,6,function-tradeoff,gcc,0,0
2793,-Wstrict-overflow=4,Also warn about other simplifications not covered by the above cases. For example: (x * 10) / 5 is simplified to x * 2.,1,6,function-tradeoff,gcc,0,0
2794,-Wstrict-overflow=5,"Also warn about cases where the compiler reduces the magnitude of a constant involved in a comparison. For example: x + 2 > y is simplified to x + 1 >= y. This is reported only at the highest warning level because this simplification applies to many comparisons, so this warning level gives a very large number of false positives.",1,6,function-tradeoff,gcc,0,0
2795,-Wstrict-overflow=n,This option is only active when signed overflow is undefined. It warns about cases where the compiler optimizes based on the assumption that signed overflow does not occur. Note that it does not warn about all cases where the code might overflow: it only warns about cases where the compiler implements some optimization. Thus this warning depends on the optimization level.,1,6,function-tradeoff,gcc,0,0
2796,-Wstrict-prototypes (C and Objective-C only),Warn if a function is declared or defined without specifying the argument types. (An old-style function definition is permitted without a warning if preceded by a declaration that specifies the argument types.),1,6,function-tradeoff,gcc,0,0
2797,-Wstrict-selector-match (Objective-C and Objective-C++ only),"Warn if multiple methods with differing argument and/or return types are found for a given selector when attempting to send a message using this selector to a receiver of type id or Class. When this flag is off (which is the default behavior), the compiler omits such warnings if any differences found are confined to types that share the same size and alignment.",1,6,function-tradeoff,gcc,0,0
2798,-Wstring-compare,"Warn for calls to strcmp and strncmp whose result is determined to be either zero or non-zero in tests for such equality owing to the length of one argument being greater than the size of the array the other argument is stored in (or the bound in the case of strncmp). Such calls could be mistakes. For example, the call to strcmp below is diagnosed because its result is necessarily non-zero irrespective of the contents of the array a.",1,6,function-tradeoff,gcc,0,0
2799,-Wstringop-overflow=1,"The -Wstringop-overflow=1 option uses type-zero Object Size Checking to determine the sizes of destination objects. At this setting the option does not warn for writes past the end of subobjects of larger objects accessed by pointers unless the size of the largest surrounding object is known. When the destination may be one of several objects it is assumed to be the largest one of them. On Linux systems, when optimization is enabled at this setting the option warns for the same code as when the _FORTIFY_SOURCE macro is defined to a non-zero value.",1,6,function-tradeoff,gcc,0,0
2800,-Wstringop-overflow=2,"The -Wstringop-overflow=2 option uses type-one Object Size Checking to determine the sizes of destination objects. At this setting the option warns about overflows when writing to members of the largest complete objects whose exact size is known. However, it does not warn for excessive writes to the same members of unknown objects referenced by pointers since they may point to arrays containing unknown numbers of elements. This is the default setting of the option.",1,6,function-tradeoff,gcc,0,0
2801,-Wstringop-overflow=3,The -Wstringop-overflow=3 option uses type-two Object Size Checking to determine the sizes of destination objects. At this setting the option warns about overflowing the smallest object or data member. This is the most restrictive setting of the option that may result in warnings for safe code.,0,0,others,gcc,0,0
2802,-Wstringop-overflow=4,"The -Wstringop-overflow=4 option uses type-three Object Size Checking to determine the sizes of destination objects. At this setting the option warns about overflowing any data members, and when the destination is one of several objects it uses the size of the largest of them to decide whether to issue a warning. Similarly to -Wstringop-overflow=3 this setting of the option may result in warnings for benign code.",0,0,others,gcc,0,0
2803,-Wstringop-overflow=type,"Warn for calls to string manipulation functions such as memcpy and strcpy that are determined to overflow the destination buffer. The optional argument is one greater than the type of Object Size Checking to perform to determine the size of the destination. See Object Size Checking. The argument is meaningful only for functions that operate on character arrays but not for raw memory functions like memcpy which always make use of Object Size type-0. The option also warns for calls that specify a size in excess of the largest possible object or at most SIZE_MAX / 2 bytes. The option produces the best results with optimization enabled but can detect a small subset of simple buffer overflows even without optimization in calls to the GCC built-in functions like __builtin_memcpy that correspond to the standard functions. In any case, the option warns about just a subset of buffer overflows detected by the corresponding overflow checking built-ins. For example, the option issues a warning for the strcpy call below because it copies at least 5 characters (the string ""blue"" including the terminating NUL) into the buffer of size 4.",1,6,function-tradeoff,gcc,0,0
2805,-Wsuggest-attribute=cold,Warn about functions that might be candidates for cold attribute. This is based on static detection and generally only warns about functions which always leads to a call to another cold function such as wrappers of C++ throw or fatal error reporting functions leading to abort.,1,6,function-tradeoff,gcc,0,0
2806,-Wsuggest-attribute=malloc,"Warn about functions that might be candidates for attributes pure, const or noreturn or malloc. The compiler only warns for functions visible in other compilation units or (in the case of pure and const) if it cannot prove that the function returns normally. A function returns normally if it doesn't contain an infinite loop or return abnormally by throwing, calling abort or trapping. This analysis requires option -fipa-pure-const, which is enabled by default at -O and higher. Higher optimization levels improve the accuracy of the analysis.",1,6,function-tradeoff,gcc,0,0
2807,-Wsuggest-final-methods,"Warn about virtual methods where code quality would be improved if the method were declared with the C++11 final specifier, or, if possible, its type were declared in an anonymous namespace or with the final specifier. This warning is more effective with link-time optimization, where the information about the class hierarchy graph is more complete. It is recommended to first consider suggestions of -Wsuggest-final-types and then rebuild with new annotations.",1,6,function-tradeoff,gcc,0,1
2808,-Wsuggest-final-types,"Warn about types with virtual methods where code quality would be improved if the type were declared with the C++11 final specifier, or, if possible, declared in an anonymous namespace. This allows GCC to more aggressively devirtualize the polymorphic calls. This warning is more effective with link-time optimization, where the information about the class hierarchy graph is more complete.",1,6,function-tradeoff,gcc,0,0
2809,-Wsuggest-override,Warn about overriding virtual functions that are not marked with the override keyword.,1,6,function-tradeoff,gcc,0,0
2810,-Wswitch,Warn whenever a switch statement has an index of enumerated type and lacks a case for one or more of the named codes of that enumeration. (The presence of a default label prevents this warning.) case labels outside the enumeration range also provoke warnings when this option is used (even if there is a default label). This warning is enabled by -Wall.,1,6,function-tradeoff,gcc,0,0
2811,-Wswitch-default,Warn whenever a switch statement does not have a default case.,1,6,function-tradeoff,gcc,0,0
2812,-Wswitch-enum,Warn whenever a switch statement has an index of enumerated type and lacks a case for one or more of the named codes of that enumeration. case labels outside the enumeration range also provoke warnings when this option is used. The only difference between -Wswitch and this option is that this option gives a warning about an omitted enumeration code even if there is a default label.,1,6,function-tradeoff,gcc,0,0
2813,-Wsync-nand (C and C++ only),Warn when __sync_fetch_and_nand and __sync_nand_and_fetch built-in functions are used. These functions changed semantics in GCC 4.4.,1,6,function-tradeoff,gcc,0,0
2814,-Wsystem-headers,"Print warning messages for constructs found in system header files. Warnings from system headers are normally suppressed, on the assumption that they usually do not indicate real problems and would only make the compiler output harder to read. Using this command-line option tells GCC to emit warnings from system headers as if they occurred in user code. However, note that using -Wall in conjunction with this option does not warn about unknown pragmas in system headers or that, -Wunknown-pragmas must also be used.",1,6,function-tradeoff,gcc,0,0
2815,-Wtautological-compare,Warn if a self-comparison always evaluates to true or false. This warning detects various mistakes such as:,1,6,function-tradeoff,gcc,0,0
2817,-Wtraditional (C and Objective-C only),"Warn about certain constructs that behave differently in traditional and ISO C. Also warn about ISO C constructs that have no traditional C equivalent, and/or problematic constructs that should be avoided.",1,6,function-tradeoff,gcc,0,1
2818,-Wtraditional-conversion (C and Objective-C only),"Warn if a prototype causes a type conversion that is different from what would happen to the same argument in the absence of a prototype. This includes conversions of fixed point to floating and vice versa, and conversions changing the width or signedness of a fixed-point argument except when the same as the default promotion.",1,6,function-tradeoff,gcc,0,0
2819,-Wtrampolines,"Warn about trampolines generated for pointers to nested functions. A trampoline is a small piece of data or code that is created at run time on the stack when the address of a nested function is taken, and is used to call the nested function indirectly. For some targets, it is made up of data only and thus requires no special treatment. But, for most targets, it is made up of code and thus requires the stack to be made executable in order for the program to work properly.",1,6,function-tradeoff,gcc,0,0
2820,-Wtrigraphs,"Warn if any trigraphs are encountered that might change the meaning of the program. Trigraphs within comments are not warned about, except those that would form escaped newlines.",1,6,function-tradeoff,gcc,0,0
2821,-Wtsan,Warn about unsupported features in ThreadSanitizer.,1,6,function-tradeoff,gcc,0,0
2822,-Wtype-limits,"Warn if a comparison is always true or always false due to the limited range of the data type, but do not warn for constant expressions. For example, warn if an unsigned variable is compared against zero with < or >=. This warning is also enabled by -Wextra.",1,6,function-tradeoff,gcc,0,0
2823,-Wundeclared-selector (Objective-C and Objective-C++ only),"Warn if a @selector( expression referring to an undeclared selector is found. A selector is considered undeclared if no method with that name has been declared before the @selector( expression, either explicitly in an @interface or @protocol declaration, or implicitly in an @implementation section. This option always performs its checks as soon as a @selector( expression is found, while -Wselector only performs its checks in the final stage of compilation. This also enforces the coding style convention that methods and selectors must be declared before being used.",0,0,others,gcc,0,0
2824,-Wundef,Warn if an undefined identifier is evaluated in an #if directive. Such identifiers are replaced with zero.,1,6,function-tradeoff,gcc,0,0
2825,-Wunknown-pragmas,"Warn when a #pragma directive is encountered that is not understood by GCC. If this command-line option is used, warnings are even issued for unknown pragmas in system header files. This is not the case if the warnings are only enabled by the -Wall command-line option.",1,6,function-tradeoff,gcc,0,0
2826,-Wunsafe-loop-optimizations,Warn if the loop cannot be optimized because the compiler cannot assume anything on the bounds of the loop indices. With -funsafe-loop-optimizations warn if the compiler makes such assumptions.,1,6,function-tradeoff,gcc,0,1
2827,-Wunsuffixed-float-constants (C and Objective-C only),Issue a warning for any floating constant that does not have a suffix. When used together with -Wsystem-headers it warns about such constants in system header files. This can be useful when preparing code to use with the FLOAT_CONST_DECIMAL64 pragma from the decimal floating-point extension to C99.,0,0,others,gcc,0,1
2829,-Wunused-but-set-parameter,"Warn whenever a function parameter is assigned to, but otherwise unused (aside from its declaration).",1,6,function-tradeoff,gcc,0,1
2832,-Wunused-function,Warn whenever a static function is declared but not defined or a non-inline static function is unused. This warning is enabled by -Wall.,1,6,function-tradeoff,gcc,0,0
2833,-Wunused-label,Warn whenever a label is declared but not used. This warning is enabled by -Wall.,1,6,function-tradeoff,gcc,0,0
2834,"-Wunused-local-typedefs (C, Objective-C, C++ and Objective-C++ only)",Warn when a typedef locally defined in a function is not used. This warning is enabled by -Wall.,1,6,function-tradeoff,gcc,0,1
2836,-Wunused-parameter,Warn whenever a function parameter is unused aside from its declaration.,1,6,function-tradeoff,gcc,0,0
2839,-Wuseless-cast (C++ and Objective-C++ only),Warn when an expression is casted to its own type.,1,6,function-tradeoff,gcc,0,0
2840,-Wvariadic-macros,"Warn if variadic macros are used in ISO C90 mode, or if the GNU alternate syntax is used in ISO C99 mode. This is enabled by either -Wpedantic or -Wtraditional. To inhibit the warning messages, use -Wno-variadic-macros.",1,6,function-tradeoff,gcc,0,0
2841,-Wvector-operation-performance,"Warn if vector operation is not implemented via SIMD capabilities of the architecture. Mainly useful for the performance tuning. Vector operation can be implemented piecewise, which means that the scalar operation is performed on every vector element; in parallel, which means that the vector operation is implemented using scalars of wider type, which normally is more performance efficient; and as a single scalar, which means that vector fits into a scalar type.",1,6,function-tradeoff,gcc,0,0
2843,-Wvla,Warn if a variable-length array is used in the code. -Wno-vla prevents the -Wpedantic warning of the variable-length array.,1,6,function-tradeoff,gcc,0,0
2844,-Wvla-larger-than=byte-size,"If this option is used, the compiler warns for declarations of variable-length arrays whose size is either unbounded, or bounded by an argument that allows the array size to exceed byte-size bytes. This is similar to how -Walloca-larger-than=byte-size works, but with variable-length arrays.",0,0,others,gcc,0,0
2845,-Wvla-parameter,"Warn about redeclarations of functions involving arguments of Variable Length Array types of inconsistent kinds or forms, and enable the detection of out-of-bounds accesses to such parameters by warnings such as -Warray-bounds.",1,6,function-tradeoff,gcc,0,1
2846,-Wvolatile (C++ and Objective-C++ only),"Warn about deprecated uses of the volatile qualifier. This includes postfix and prefix ++ and -- expressions of volatile-qualified types, using simple assignments where the left operand is a volatile-qualified non-class type for their value, compound assignments where the left operand is a volatile-qualified non-class type, volatile-qualified function return type, volatile-qualified parameter type, and structured bindings of a volatile-qualified type. This usage was deprecated in C++20.",1,6,function-tradeoff,gcc,0,1
2847,-Wvolatile-register-var,Warn if a register variable is declared volatile. The volatile modifier does not inhibit all optimizations that may eliminate reads and/or writes to register variables. This warning is enabled by -Wall.,1,6,function-tradeoff,gcc,0,0
2849,-Wzero-as-null-pointer-constant (C++ and Objective-C++ only),Warn when a literal is used as null pointer constant. This can be useful to facilitate the conversion to nullptr in C++11.,1,6,function-tradeoff,gcc,0,0
2850,-Wzero-length-bounds,Warn about accesses to elements of zero-length array members that might overlap other members of the same object. Declaring interior zero-length arrays is discouraged because accesses to them are undefined. See See Zero Length.,1,6,function-tradeoff,gcc,0,0
2853,-Xassembler option,Pass option as an option to the assembler. You can use this to supply system-specific assembler options that GCC does not recognize.,0,0,others,gcc,0,0
2854,-Xlinker option,Pass option as an option to the linker. You can use this to supply system-specific linker options that GCC does not recognize.,0,0,others,gcc,0,1
2855,-Xpreprocessor option,Pass option as an option to the preprocessor. You can use this to supply system-specific preprocessor options that GCC does not recognize.,0,0,others,gcc,0,0
2857,dfs.balancer.address,The hostname used for a keytab based Kerberos login. Keytab based login can be enabled with dfs.balancer.keytab.enabled.,0,0,others,hdfs,0,0
2858,dfs.balancer.block-move.timeout,"Maximum amount of time in milliseconds for a block to move. If this is set greater than 0, Balancer will stop waiting for a block move completion after this time. In typical clusters, a 3 to 5 minute timeout is reasonable. If timeout happens to a large proportion of block moves, this needs to be increased. It could also be that too much work is dispatched and many nodes are constantly exceeding the bandwidth limit as a result. In that case, other balancer parameters might need to be adjusted. It is disabled (0) by default.",0,0,others,hdfs,0,0
2859,dfs.balancer.dispatcherThreads,Size of the thread pool for the HDFS balancer block mover. dispatchExecutor,1,1,resource,hdfs,0,0
2860,dfs.balancer.getBlocks.min-block-size,Minimum block threshold size in bytes to ignore when fetching a source's block list.,0,0,others,hdfs,0,0
2861,dfs.balancer.getBlocks.size,Total size in bytes of Datanode blocks to get when fetching a source's block list.,0,0,others,hdfs,0,1
2862,dfs.balancer.kerberos.principal,The Balancer principal. This is typically set to balancer/_HOST@REALM.TLD. The Balancer will substitute _HOST with its own fully qualified hostname at startup. The _HOST placeholder allows using the same configuration setting on different servers. Keytab based login can be enabled with dfs.balancer.keytab.enabled.,0,0,others,hdfs,0,1
2863,dfs.balancer.keytab.enabled,Set to true to enable login using a keytab for Kerberized Hadoop.,0,0,others,hdfs,0,0
2865,dfs.balancer.max-iteration-time,"Maximum amount of time while an iteration can be run by the Balancer. After this time the Balancer will stop the iteration, and reevaluate the work needs to be done to Balance the cluster. The default value is 20 minutes.",0,0,others,hdfs,0,1
2866,dfs.balancer.max-no-move-interval,"If this specified amount of time has elapsed and no block has been moved out of a source DataNode, on more effort will be made to move blocks out of this DataNode in the current Balancer iteration.",0,0,others,hdfs,0,0
2867,dfs.balancer.max-size-to-move,Maximum number of bytes that can be moved by the balancer in a single thread.,1,5,workload-specific,hdfs,0,0
2868,dfs.balancer.movedWinWidth,Window of time in ms for the HDFS balancer tracking blocks and its locations.,0,0,others,hdfs,0,0
2869,dfs.balancer.moverThreads,Thread pool size for executing block moves. moverThreadAllocator,1,1,resource,hdfs,0,0
2870,dfs.balancer.service.interval,The schedule interval of balancer when running as a long service.,0,0,others,hdfs,0,0
2871,dfs.balancer.service.retries.on.exception,"When the balancer is executed as a long-running service, it will retry upon encountering an exception. This configuration determines how many times it will retry before considering the exception to be fatal and quitting.",0,0,others,hdfs,0,1
2872,dfs.batched.ls.limit,"Limit the number of paths that can be listed in a single batched listing call. printed by ls. If less or equal to zero, at most DFS_LIST_LIMIT_DEFAULT (= 1000) will be printed.",0,0,others,hdfs,0,0
2873,dfs.block.access.key.update.interval,Interval in minutes at which namenode updates its access keys.,0,0,others,hdfs,0,0
2874,dfs.block.access.token.enable,"If ""true"", access tokens are used as capabilities for accessing datanodes. If ""false"", no access tokens are checked on accessing datanodes.",0,0,others,hdfs,0,1
2875,dfs.block.access.token.lifetime,The lifetime of access tokens in minutes.,0,0,others,hdfs,0,0
2876,dfs.block.access.token.protobuf.enable,"If ""true"", block tokens are written using Protocol Buffers. If ""false"", block tokens are written using Legacy format.",1,4,limited-side-effect,hdfs,0,0
2877,dfs.block.invalidate.limit,"The maximum number of invalidate blocks sent by namenode to a datanode per heartbeat deletion command. This property works with ""dfs.namenode.invalidate.work.pct.per.iteration"" to throttle block deletions.",1,5,workload-specific,hdfs,0,0
2879,dfs.block.misreplication.processing.limit,Maximum number of blocks to process for initializing replication queues.,0,0,others,hdfs,0,0
2880,dfs.block.placement.ec.classname,Placement policy class for striped files. Defaults to BlockPlacementPolicyRackFaultTolerant.class,0,0,others,hdfs,0,1
2881,dfs.block.replicator.classname,"Class representing block placement policy for non-striped files. There are four block placement policies currently being supported: BlockPlacementPolicyDefault, BlockPlacementPolicyWithNodeGroup, BlockPlacementPolicyRackFaultTolerant and BlockPlacementPolicyWithUpgradeDomain. BlockPlacementPolicyDefault chooses the desired number of targets for placing block replicas in a default way. BlockPlacementPolicyWithNodeGroup places block replicas on environment with node-group layer. BlockPlacementPolicyRackFaultTolerant places the replicas to more racks. BlockPlacementPolicyWithUpgradeDomain places block replicas that honors upgrade domain policy. The details of placing replicas are documented in the javadoc of the corresponding policy classes. The default policy is BlockPlacementPolicyDefault, and the corresponding class is org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.",0,0,others,hdfs,0,1
2882,dfs.block.scanner.volume.bytes.per.second,"If this is 0, the DataNode's block scanner will be disabled. If this is positive, this is the number of bytes per second that the DataNode's block scanner will try to scan from each volume.",1,6,function-tradeoff,hdfs,0,1
2883,dfs.blockreport.incremental.intervalMsec,"If set to a positive integer, the value in ms to wait between sending incremental block reports from the Datanode to the Namenode.",0,0,others,hdfs,0,0
2884,dfs.blockreport.initialDelay,"Delay for first block report in seconds. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.If no time unit is specified then seconds is assumed",0,0,others,hdfs,0,1
2885,dfs.blockreport.intervalMsec,Determines block reporting interval in milliseconds.,0,0,others,hdfs,0,0
2886,dfs.blockreport.split.threshold,If the number of blocks on the DataNode is below this threshold then it will send block reports for all Storage Directories in a single message. If the number of blocks exceeds this threshold then the DataNode will send block reports for each Storage Directory in separate messages. Set to zero to always split.,1,5,workload-specific,hdfs,0,0
2887,dfs.blocksize,"The default block size for new files, in bytes. You can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.), Or provide complete size in bytes (such as 134217728 for 128 MB).",0,0,others,hdfs,0,0
2888,dfs.bytes-per-checksum,The number of bytes per checksum. Must not be larger than dfs.stream-buffer-size,0,0,others,hdfs,0,0
2889,dfs.cachereport.intervalMsec,"Determines cache reporting interval in milliseconds. After this amount of time, the DataNode sends a full report of its cache state to the NameNode. The NameNode uses the cache report to update its map of cached blocks to DataNode locations. This configuration has no effect if in-memory caching has been disabled by setting dfs.datanode.max.locked.memory to 0 (which is the default). If the native libraries are not available to the DataNode, this configuration has no effect.",1,6,function-tradeoff,hdfs,0,0
2891,dfs.checksum.type,Checksum type,0,0,others,hdfs,0,1
2892,dfs.client.block.reader.remote.buffer.size,"The output stream buffer size of a DFSClient remote read. The buffer default value is 512B. The buffer includes only some request parameters that are: block, blockToken, clientName, startOffset, len, verifyChecksum, cachingStrategy.",1,1,resource,hdfs,0,0
2893,dfs.client.block.write.locateFollowingBlock.initial.delay.ms,"The initial delay (unit is ms) for locateFollowingBlock, the delay time will increase exponentially(double) for each retry until dfs.client.block.write.locateFollowingBlock.max.delay.ms is reached, after that the delay for each retry will be dfs.client.block.write.locateFollowingBlock.max.delay.ms.",0,0,others,hdfs,0,0
2894,dfs.client.block.write.locateFollowingBlock.max.delay.ms,The maximum delay (unit is ms) before retrying locateFollowingBlock.,0,0,others,hdfs,0,0
2895,dfs.client.block.write.locateFollowingBlock.retries,Number of retries to use when finding the next block during HDFS writes.,0,0,others,hdfs,0,0
2896,dfs.client.block.write.replace-datanode-on-failure.best-effort,"This property is used only if the value of dfs.client.block.write.replace-datanode-on-failure.enable is true. Best effort means that the client will try to replace a failed datanode in write pipeline (provided that the policy is satisfied), however, it continues the write operation in case that the datanode replacement also fails. Suppose the datanode replacement fails. false: An exception should be thrown so that the write will fail. true : The write should be resumed with the remaining datandoes. Note that setting this property to true allows writing to a pipeline with a smaller number of datanodes. As a result, it increases the probability of data loss.",1,3,reliability-tradeoff,hdfs,0,0
2897,dfs.client.block.write.replace-datanode-on-failure.enable,"If there is a datanode/network failure in the write pipeline, DFSClient will try to remove the failed datanode from the pipeline and then continue writing with the remaining datanodes. As a result, the number of datanodes in the pipeline is decreased. The feature is to add new datanodes to the pipeline. This is a site-wide property to enable/disable the feature. When the cluster size is extremely small, e.g. 3 nodes or less, cluster administrators may want to set the policy to NEVER in the default configuration file or disable this feature. Otherwise, users may experience an unusually high rate of pipeline failures since it is impossible to find new datanodes for replacement. See also dfs.client.block.write.replace-datanode-on-failure.policy",1,3,reliability-tradeoff,hdfs,0,0
2898,dfs.client.block.write.replace-datanode-on-failure.min-replication,"The minimum number of replications that are needed to not to fail the write pipeline if new datanodes can not be found to replace failed datanodes (could be due to network failure) in the write pipeline. If the number of the remaining datanodes in the write pipeline is greater than or equal to this property value, continue writing to the remaining nodes. Otherwise throw exception. If this is set to 0, an exception will be thrown, when a replacement can not be found. See also dfs.client.block.write.replace-datanode-on-failure.policy",1,3,reliability-tradeoff,hdfs,0,0
2899,dfs.client.block.write.replace-datanode-on-failure.policy,This property is used only if the value of dfs.client.block.write.replace-datanode-on-failure.enable is true. ALWAYS: always add a new datanode when an existing datanode is removed. NEVER: never add a new datanode. DEFAULT: Let r be the replication number. Let n be the number of existing datanodes. Add a new datanode only if r is greater than or equal to 3 and either (1) floor(r/2) is greater than or equal to n; or (2) r is greater than n and the block is hflushed/appended.,1,3,reliability-tradeoff,hdfs,0,0
2900,dfs.client.block.write.retries,"The number of retries for writing blocks to the data nodes, before we signal failure to the application.",0,0,others,hdfs,0,0
2901,dfs.client.cache.drop.behind.reads,"Just like dfs.datanode.drop.cache.behind.reads, this setting causes the page cache to be dropped behind HDFS reads, potentially freeing up more memory for other uses. Unlike dfs.datanode.drop.cache.behind.reads, this is a client-side setting rather than a setting for the entire datanode. If present, this setting will override the DataNode default. If the native libraries are not available to the DataNode, this configuration has no effect.",1,4,limited-side-effect,hdfs,0,0
2902,dfs.client.cache.drop.behind.writes,"Just like dfs.datanode.drop.cache.behind.writes, this setting causes the page cache to be dropped behind HDFS writes, potentially freeing up more memory for other uses. Unlike dfs.datanode.drop.cache.behind.writes, this is a client-side setting rather than a setting for the entire datanode. If present, this setting will override the DataNode default. If the native libraries are not available to the DataNode, this configuration has no effect.",1,4,limited-side-effect,hdfs,0,1
2903,dfs.client.cache.readahead,"When using remote reads, this setting causes the datanode to read ahead in the block file using posix_fadvise, potentially decreasing I/O wait times. Unlike dfs.datanode.readahead.bytes, this is a client-side setting rather than a setting for the entire datanode. If present, this setting will override the DataNode default. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize. When using local reads, this setting determines how much readahead we do in BlockReaderLocal. If the native libraries are not available to the DataNode, this configuration has no effect.",1,4,limited-side-effect,hdfs,0,0
2904,dfs.client.cached.conn.retry,"The number of times the HDFS client will pull a socket from the cache. Once this number is exceeded, the client will try to create a new socket.",0,0,others,hdfs,0,0
2905,dfs.client.context,"The name of the DFSClient context that we should use. Clients that share a context share a socket cache and short-circuit cache, among other things. You should only change this if you don't want to share with another set of threads.",0,0,others,hdfs,0,1
2906,dfs.client.datanode-restart.timeout,"Expert only. The time to wait, in seconds, from reception of an datanode shutdown notification for quick restart, until declaring the datanode dead and invoking the normal recovery mechanisms. The notification is sent by a datanode when it is being shutdown using the shutdownDatanode admin command with the upgrade option. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.If no time unit is specified then seconds is assumed.",0,0,others,hdfs,0,0
2907,dfs.client.deadnode.detection.deadnode.queue.max,The max queue size of probing dead node.,1,5,workload-specific,hdfs,0,0
2908,dfs.client.deadnode.detection.enabled,Set to true to enable dead node detection in client side. Then all the DFSInputStreams of the same client can share the dead node information.,1,3,reliability-tradeoff,hdfs,0,0
2909,dfs.client.deadnode.detection.probe.connection.timeout.ms,Connection timeout for probing dead node in milliseconds.,0,0,others,hdfs,0,0
2910,dfs.client.deadnode.detection.probe.deadnode.interval.ms,Interval time in milliseconds for probing dead node behavior.,0,0,others,hdfs,0,0
2911,dfs.client.deadnode.detection.probe.deadnode.threads,The maximum number of threads to use for probing dead node.,1,1,resource,hdfs,0,0
2912,dfs.client.deadnode.detection.probe.suspectnode.interval.ms,Interval time in milliseconds for probing suspect node behavior.,0,0,others,hdfs,0,0
2913,dfs.client.deadnode.detection.probe.suspectnode.threads,The maximum number of threads to use for probing suspect node.,1,1,resource,hdfs,0,0
2914,dfs.client.deadnode.detection.rpc.threads,The maximum number of threads to use for calling RPC call to recheck the liveness of dead node.,1,1,resource,hdfs,0,0
2915,dfs.client.deadnode.detection.suspectnode.queue.max,The max queue size of probing suspect node.,1,5,workload-specific,hdfs,0,0
2917,dfs.client.failover.connection.retries,Expert only. Indicates the number of retries a failover IPC client will make to establish a server connection.,0,0,others,hdfs,0,1
2918,dfs.client.failover.connection.retries.on.timeouts,Expert only. The number of retry attempts a failover IPC client will make on socket timeout when establishing a server connection.,0,0,others,hdfs,0,1
2919,dfs.client.failover.max.attempts,Expert only. The number of client failover attempts that should be made before the failover is considered failed.,0,0,others,hdfs,0,0
2921,dfs.client.failover.random.order,"Determines if the failover proxies are picked in random order instead of the configured order. Random order may be enabled for better load balancing or to avoid always hitting failed ones first if the failed ones appear in the beginning of the configured or resolved list. For example, In the case of multiple RBF routers or ObserverNameNodes, it is recommended to be turned on for load balancing. The config name can be extended with an optional nameservice ID (of form dfs.client.failover.random.order[.nameservice]) in case multiple nameservices exist and random order should be enabled for specific nameservices.",1,4,limited-side-effect,hdfs,0,0
2924,dfs.client.failover.resolver.useFQDN,"Determines whether the resolved result is fully qualified domain name instead of pure IP address(es). The config name can be extended with an optional nameservice ID (of form dfs.client.failover.resolver.impl[.nameservice]) to configure specific nameservices when multiple nameservices exist. In secure environment, this has to be enabled since Kerberos is using fqdn in machine's principal therefore accessing servers by IP won't be recognized by the KDC.",0,0,others,hdfs,0,0
2925,dfs.client.failover.sleep.base.millis,"Expert only. The time to wait, in milliseconds, between failover attempts increases exponentially as a function of the number of attempts made so far, with a random factor of +/- 50%. This option specifies the base value used in the failover calculation. The first failover will retry immediately. The 2nd failover attempt will delay at least dfs.client.failover.sleep.base.millis milliseconds. And so on.",0,0,others,hdfs,0,1
2926,dfs.client.failover.sleep.max.millis,"Expert only. The time to wait, in milliseconds, between failover attempts increases exponentially as a function of the number of attempts made so far, with a random factor of +/- 50%. This option specifies the maximum value to wait between failovers. Specifically, the time between two failover attempts will not exceed +/- 50% of dfs.client.failover.sleep.max.millis milliseconds.",0,0,others,hdfs,0,0
2927,dfs.client.hedged.read.threadpool.size,"Support 'hedged' reads in DFSClient. To enable this feature, set the parameter to a positive number. The threadpool size is how many threads to dedicate to the running of these 'hedged', concurrent reads in your client.",1,1,resource,hdfs,0,0
2928,dfs.client.hedged.read.threshold.millis,Configure 'hedged' reads in DFSClient. This is the number of milliseconds to wait before starting up a 'hedged' read.,0,0,others,hdfs,0,0
2929,dfs.client.https.keystore.resource,Resource file from which ssl client keystore information will be extracted,0,0,others,hdfs,0,0
2930,dfs.client.https.need-auth,Whether SSL client certificate authentication is required,1,2,security-tradeoff,hdfs,0,1
2931,dfs.client.key.provider.cache.expiry,DFS client security key cache expiration in milliseconds.,1,2,security-tradeoff,hdfs,0,1
2933,dfs.client.max.block.acquire.failures,Maximum failures allowed when trying to get block information from a specific datanode.,0,0,others,hdfs,0,0
2934,dfs.client.mmap.cache.size,"When zero-copy reads are used, the DFSClient keeps a cache of recently used memory mapped regions. This parameter controls the maximum number of entries that we will keep in that cache. The larger this number is, the more file descriptors we will potentially use for memory-mapped files. mmaped files also use virtual address space. You may need to increase your ulimit virtual address space limits before increasing the client mmap cache size. Note that you can still do zero-copy reads when this size is set to 0.",1,1,resource,hdfs,0,1
2935,dfs.client.mmap.cache.timeout.ms,"The minimum length of time that we will keep an mmap entry in the cache between uses. If an entry is in the cache longer than this, and nobody uses it, it will be removed by a background thread.",1,5,workload-specific,hdfs,0,0
2936,dfs.client.mmap.enabled,"If this is set to false, the client won't attempt to perform memory-mapped reads.",1,4,limited-side-effect,hdfs,0,0
2937,dfs.client.mmap.retry.timeout.ms,The minimum amount of time that we will wait before retrying a failed mmap operation.,0,0,others,hdfs,0,0
2938,dfs.client.read.prefetch.size,The number of bytes for the DFSClient will fetch from the Namenode during a read operation. Defaults to 10 * ${dfs.blocksize}.,1,1,resource,hdfs,0,0
2939,dfs.client.read.short.circuit.replica.stale.threshold.ms,Threshold in milliseconds for read entries during short-circuit local reads.,0,0,others,hdfs,0,0
2940,dfs.client.read.shortcircuit,This configuration parameter turns on short-circuit local reads.,1,4,limited-side-effect,hdfs,0,0
2941,dfs.client.read.shortcircuit.buffer.size,Buffer size in bytes for short-circuit local reads.,1,1,resource,hdfs,0,0
2942,dfs.client.read.shortcircuit.skip.checksum,"If this configuration parameter is set, short-circuit local reads will skip checksums. This is normally not recommended, but it may be useful for special setups. You might consider using this if you are doing your own checksumming outside of HDFS.",1,4,limited-side-effect,hdfs,0,0
2943,dfs.client.read.shortcircuit.streams.cache.expiry.ms,This controls the minimum amount of time file descriptors need to sit in the client cache context before they can be closed for being inactive for too long.,1,5,workload-specific,hdfs,0,0
2944,dfs.client.read.shortcircuit.streams.cache.size,"The DFSClient maintains a cache of recently opened file descriptors. This parameter controls the maximum number of file descriptors in the cache. Setting this higher will use more file descriptors, but potentially provide better performance on workloads involving lots of seeks.",1,1,resource,hdfs,0,0
2945,dfs.client.read.striped.threadpool.size,The maximum number of threads used for parallel reading in striped layout.,1,1,resource,hdfs,0,0
2946,dfs.client.refresh.read-block-locations.ms,Refreshing LocatedBlocks period. A value of 0 disables the feature.,0,0,others,hdfs,0,0
2947,dfs.client.replica.accessor.builder.classes,"Comma-separated classes for building ReplicaAccessor. If the classes are specified, client will use external BlockReader that uses the ReplicaAccessor built by the builder.",0,0,others,hdfs,0,0
2948,dfs.client.retry.interval-ms.get-last-block-length,Retry interval in milliseconds to wait between retries in getting block lengths from the datanodes.,0,0,others,hdfs,0,0
2949,dfs.client.retry.max.attempts,Max retry attempts for DFSClient talking to namenodes.,0,0,others,hdfs,0,0
2950,dfs.client.retry.policy.enabled,"If true, turns on DFSClient retry policy.",1,3,reliability-tradeoff,hdfs,0,0
2951,dfs.client.retry.policy.spec,Set to pairs of timeouts and retries for DFSClient.,0,0,others,hdfs,0,1
2952,dfs.client.retry.times.get-last-block-length,Number of retries for calls to fetchLocatedBlocksAndGetLastBlockLength().,0,0,others,hdfs,0,0
2954,dfs.client.server-defaults.validity.period.ms,"The amount of milliseconds after which cached server defaults are updated. By default this parameter is set to 1 hour. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.",0,0,others,hdfs,0,0
2955,dfs.client.short.circuit.replica.stale.threshold.ms,"The maximum amount of time that we will consider a short-circuit replica to be valid, if there is no communication from the DataNode. After this time has elapsed, we will re-fetch the short-circuit replica even if it is in the cache.",0,0,others,hdfs,0,1
2956,dfs.client.slow.io.warning.threshold.ms,"The threshold in milliseconds at which we will log a slow io warning in a dfsclient. By default, this parameter is set to 30000 milliseconds (30 seconds).",0,0,others,hdfs,0,1
2957,dfs.client.socket.send.buffer.size,"Socket send buffer size for a write pipeline in DFSClient side. This may affect TCP connection throughput. If it is set to zero or negative value, no buffer size will be set explicitly, thus enable tcp auto-tuning on some system. The default value is 0.",1,1,resource,hdfs,0,0
2958,dfs.client.socketcache.capacity,"Socket cache capacity (in entries) for short-circuit reads. If this value is set to 0, the client socket cache is disabled.",1,1,resource,hdfs,0,0
2959,dfs.client.socketcache.expiryMsec,Socket cache expiration for short-circuit reads in msec.,1,5,workload-specific,hdfs,0,0
2960,dfs.client.socket-timeout,Default timeout value in milliseconds for all sockets.,0,0,others,hdfs,0,0
2961,dfs.client.test.drop.namenode.response.number,The number of Namenode responses dropped by DFSClient for each RPC call. Used for testing the NN retry cache.,0,0,others,hdfs,0,0
2962,dfs.client.use.datanode.hostname,Whether clients should use datanode hostnames when connecting to datanodes.,0,0,others,hdfs,0,1
2964,dfs.client.write.byte-array-manager.count-limit,The maximum number of arrays allowed for each array length.,1,5,workload-specific,hdfs,0,0
2965,dfs.client.write.byte-array-manager.count-reset-time-period-ms,The time period in milliseconds that the allocation count for each array length is reset to zero if there is no increment.,0,0,others,hdfs,0,0
2966,dfs.client.write.byte-array-manager.count-threshold,"The count threshold for each array length so that a manager is created only after the allocation count exceeds the threshold. In other words, the particular array length is not managed until the allocation count exceeds the threshold.",1,5,workload-specific,hdfs,0,0
2967,dfs.client.write.byte-array-manager.enabled,"If true, enables byte array manager used by DFSOutputStream.",1,6,function-tradeoff,hdfs,0,1
2968,dfs.client.write.exclude.nodes.cache.expiry.interval.millis,"The maximum period to keep a DN in the excluded nodes list at a client. After this period, in milliseconds, the previously excluded node(s) will be removed automatically from the cache and will be considered good for block allocations again. Useful to lower or raise in situations where you keep a file open for very long periods (such as a Write-Ahead-Log (WAL) file) to make the writer tolerant to cluster maintenance restarts. Defaults to 10 minutes.",1,5,workload-specific,hdfs,0,0
2969,dfs.client.write.max-packets-in-flight,The maximum number of DFSPackets allowed in flight.,1,3,reliability-tradeoff,hdfs,0,0
2970,dfs.client-write-packet-size,Packet size for clients to write,1,1,resource,hdfs,0,0
2972,dfs.content-summary.limit,The maximum content summary counts allowed in one locking period. 0 or a negative number means no limit (i.e. no yielding).,1,5,workload-specific,hdfs,0,0
2973,dfs.content-summary.sleep-microsec,"The length of time in microseconds to put the thread to sleep, between reaquiring the locks in content summary computation.",0,0,others,hdfs,0,0
2974,dfs.data.transfer.client.tcpnodelay,"If true, set TCP_NODELAY to sockets for transferring data from DFS client.",1,4,limited-side-effect,hdfs,0,1
2975,dfs.data.transfer.protection,"A comma-separated list of SASL protection values used for secured connections to the DataNode when reading or writing block data. Possible values are authentication, integrity and privacy. authentication means authentication only and no integrity or privacy; integrity implies authentication and integrity are enabled; and privacy implies all of authentication, integrity and privacy are enabled. If dfs.encrypt.data.transfer is set to true, then it supersedes the setting for dfs.data.transfer.protection and enforces that all connections must use a specialized encrypted SASL handshake. This property is ignored for connections to a DataNode listening on a privileged port. In this case, it is assumed that the use of a privileged port establishes sufficient trust.",1,2,security-tradeoff,hdfs,0,0
2976,dfs.data.transfer.saslproperties.resolver.class,"SaslPropertiesResolver used to resolve the QOP used for a connection to the DataNode when reading or writing block data. If not specified, the value of hadoop.security.saslproperties.resolver.class is used as the default value.",0,0,others,hdfs,0,0
2977,dfs.data.transfer.server.tcpnodelay,"If true, set TCP_NODELAY to sockets for transferring data between Datanodes.",1,4,limited-side-effect,hdfs,0,0
2979,dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction,"Only used when the dfs.datanode.fsdataset.volume.choosing.policy is set to org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy. This setting controls what percentage of new block allocations will be sent to volumes with more available disk space than others. This setting should be in the range 0.0 - 1.0, though in practice 0.5 - 1.0, since there should be no reason to prefer that volumes with less available disk space receive more block allocations.",1,5,workload-specific,hdfs,0,0
2980,dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold,"Only used when the dfs.datanode.fsdataset.volume.choosing.policy is set to org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy. This setting controls how much DN volumes are allowed to differ in terms of bytes of free disk space before they are considered imbalanced. If the free space of all the volumes are within this range of each other, the volumes will be considered balanced and block assignments will be done on a pure round robin basis. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize.",1,5,workload-specific,hdfs,0,0
2981,dfs.datanode.balance.bandwidthPerSec,"Specifies the maximum amount of bandwidth that each datanode can utilize for the balancing purpose in term of the number of bytes per second. You can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa)to specify the size (such as 128k, 512m, 1g, etc.). Or provide complete size in bytes (such as 134217728 for 128 MB).",1,1,resource,hdfs,0,0
2982,dfs.datanode.balance.max.concurrent.moves,"Maximum number of threads for Datanode balancer pending moves. This value is reconfigurable via the ""dfsadmin -reconfig"" command.",1,1,resource,hdfs,0,0
2983,dfs.datanode.block.id.layout.upgrade.threads,The number of threads to use when creating hard links from current to previous blocks during upgrade of a DataNode to block ID-based block layout (see HDFS-6482 for details on the layout).,1,1,resource,hdfs,0,0
2984,dfs.datanode.block-pinning.enabled,Whether pin blocks on favored DataNode.,0,0,others,hdfs,0,0
2985,dfs.datanode.bp-ready.timeout,"The maximum wait time for datanode to be ready before failing the received request. Setting this to 0 fails requests right away if the datanode is not yet registered with the namenode. This wait time reduces initial request failures after datanode restart. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.If no time unit is specified then seconds is assumed.",0,0,others,hdfs,0,0
2986,dfs.datanode.cache.revocation.polling.ms,How often the DataNode should poll to see if the clients have stopped using a replica that the DataNode wants to uncache.,1,5,workload-specific,hdfs,0,0
2987,dfs.datanode.cache.revocation.timeout.ms,"When the DFSClient reads from a block file which the DataNode is caching, the DFSClient can skip verifying checksums. The DataNode will keep the block file in cache until the client is done. If the client takes an unusually long time, though, the DataNode may need to evict the block file from the cache anyway. This value controls how long the DataNode will wait for the client to release a replica that it is reading without checksums.",1,5,workload-specific,hdfs,0,1
2988,dfs.datanode.cached-dfsused.check.interval.ms,"The interval check time of loading DU_CACHE_FILE in each volume. When the cluster doing the rolling upgrade operations, it will usually lead dfsUsed cache file of each volume expired and redo the du operations in datanode and that makes datanode start slowly. Adjust this property can make cache file be available for the time as you want.",1,6,function-tradeoff,hdfs,0,0
2991,dfs.datanode.data.transfer.bandwidthPerSec,"Specifies the maximum amount of bandwidth that the data transfering can utilize for transfering block when BlockConstructionStage is PIPELINE_SETUP_CREATE and clientName is empty. When the bandwidth value is zero, there is no limit.",1,1,resource,hdfs,0,0
2992,dfs.datanode.data.write.bandwidthPerSec,"Specifies the maximum amount of bandwidth that the data transfering can utilize for writing block or pipeline recovery when BlockConstructionStage is PIPELINE_SETUP_APPEND_RECOVERY or PIPELINE_SETUP_STREAMING_RECOVERY. When the bandwidth value is zero, there is no limit.",1,1,resource,hdfs,0,1
2993,dfs.datanode.directoryscan.interval,"Interval in seconds for Datanode to scan data directories and reconcile the difference between blocks in memory and on the disk. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.If no time unit is specified then seconds is assumed.",0,0,others,hdfs,0,0
2994,dfs.datanode.directoryscan.threads,How many threads should the threadpool used to compile reports for volumes in parallel have.,1,1,resource,hdfs,0,1
2995,dfs.datanode.directoryscan.throttle.limit.ms.per.sec,"The report compilation threads are limited to only running for a given number of milliseconds per second, as configured by the property. The limit is taken per thread, not in aggregate, e.g. setting a limit of 100ms for 4 compiler threads will result in each thread being limited to 100ms, not 25ms. Note that the throttle does not interrupt the report compiler threads, so the actual running time of the threads per second will typically be somewhat higher than the throttle limit, usually by no more than 20%. Setting this limit to 1000 disables compiler thread throttling. Only values between 1 and 1000 are valid. Setting an invalid value will result in the throttle being disabled and an error message being logged. 1000 is the default setting.",1,5,workload-specific,hdfs,0,1
2996,dfs.datanode.disk.check.min.gap,The minimum gap between two successive checks of the same DataNode volume. This setting supports multiple time unit suffixes as described in dfs.heartbeat.interval. If no suffix is specified then milliseconds is assumed.,0,0,others,hdfs,0,0
2997,dfs.datanode.disk.check.timeout,Maximum allowed time for a disk check to complete. If the check does not complete within this time interval then the disk is declared as failed. This setting supports multiple time unit suffixes as described in dfs.heartbeat.interval. If no suffix is specified then milliseconds is assumed.,0,0,others,hdfs,0,0
2998,dfs.datanode.dns.interface,The name of the Network Interface from which a data node should report its IP address. e.g. eth2. This setting may be required for some multi-homed nodes where the DataNodes are assigned multiple hostnames and it is desirable for the DataNodes to use a non-default hostname. Prefer using hadoop.security.dns.interface over dfs.datanode.dns.interface.,0,0,others,hdfs,0,0
3000,dfs.datanode.drop.cache.behind.reads,"In some workloads, the data read from HDFS is known to be significantly large enough that it is unlikely to be useful to cache it in the operating system buffer cache. In this case, the DataNode may be configured to automatically purge all data from the buffer cache after it is delivered to the client. This behavior is automatically disabled for workloads which read only short sections of a block (e.g HBase random-IO workloads). This may improve performance for some workloads by freeing buffer cache space usage for more cacheable data. If the Hadoop native libraries are not available, this configuration has no effect.",1,4,limited-side-effect,hdfs,0,0
3001,dfs.datanode.drop.cache.behind.writes,"In some workloads, the data written to HDFS is known to be significantly large enough that it is unlikely to be useful to cache it in the operating system buffer cache. In this case, the DataNode may be configured to automatically purge all data from the buffer cache after it is written to disk. This may improve performance for some workloads by freeing buffer cache space usage for more cacheable data. If the Hadoop native libraries are not available, this configuration has no effect.",1,4,limited-side-effect,hdfs,0,0
3002,dfs.datanode.du.reserved,"Reserved space in bytes per volume. Always leave this much space free for non dfs use. Specific storage type based reservation is also supported. The property can be followed with corresponding storage types ([ssd]/[disk]/[archive]/[ram_disk]) for cluster with heterogeneous storage. For example, reserved space for RAM_DISK storage can be configured using property 'dfs.datanode.du.reserved.ram_disk'. If specific storage type reservation is not configured then dfs.datanode.du.reserved will be used. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize. Note: In case of using tune2fs to set reserved-blocks-percentage, or other filesystem tools, then you can possibly run into out of disk errors because hadoop will not check those external tool configurations.",1,5,workload-specific,hdfs,0,0
3003,dfs.datanode.du.reserved.calculator,"Determines the class of ReservedSpaceCalculator to be used for calculating disk space reserved for non-HDFS data. The default calculator is ReservedSpaceCalculatorAbsolute which will use dfs.datanode.du.reserved for a static reserved number of bytes. ReservedSpaceCalculatorPercentage will use dfs.datanode.du.reserved.pct to calculate the reserved number of bytes based on the size of the storage. ReservedSpaceCalculatorConservative and ReservedSpaceCalculatorAggressive will use their combination, Conservative will use maximum, Aggressive minimum. For more details see ReservedSpaceCalculator.",1,5,workload-specific,hdfs,0,0
3004,dfs.datanode.du.reserved.pct,"Reserved space in percentage. Read dfs.datanode.du.reserved.calculator to see when this takes effect. The actual number of bytes reserved will be calculated by using the total capacity of the data directory in question. Specific storage type based reservation is also supported. The property can be followed with corresponding storage types ([ssd]/[disk]/[archive]/[ram_disk]) for cluster with heterogeneous storage. For example, reserved percentage space for RAM_DISK storage can be configured using property 'dfs.datanode.du.reserved.pct.ram_disk'. If specific storage type reservation is not configured then dfs.datanode.du.reserved.pct will be used.",1,5,workload-specific,hdfs,0,1
3005,dfs.datanode.ec.reconstruction.stripedread.buffer.size,Datanode striped read buffer size.,1,1,resource,hdfs,0,0
3006,dfs.datanode.ec.reconstruction.stripedread.timeout.millis,Datanode striped read timeout in milliseconds.,0,0,others,hdfs,0,0
3007,dfs.datanode.ec.reconstruction.threads,Number of threads used by the Datanode for background reconstruction work.,1,1,resource,hdfs,0,0
3008,dfs.datanode.ec.reconstruction.xmits.weight,"Datanode uses xmits weight to calculate the relative cost of EC recovery tasks comparing to replicated block recovery, of which xmits is always 1. Namenode then uses xmits reported from datanode to throttle recovery tasks for EC and replicated blocks. The xmits of an erasure coding recovery task is calculated as the maximum value between the number of read streams and the number of write streams.",0,0,others,hdfs,0,0
3009,dfs.datanode.failed.volumes.tolerated,"The number of volumes that are allowed to fail before a datanode stops offering service. By default any volume failure will cause a datanode to shutdown. The value should be greater than or equal to -1 , -1 represents minimum 1 valid volume.",0,0,others,hdfs,0,0
3010,dfs.datanode.fileio.profiling.sampling.percentage,This setting controls the percentage of file I/O events which will be profiled for DataNode disk statistics. The default value of 0 disables disk statistics. Set to an integer value between 1 and 100 to enable disk statistics.,1,5,workload-specific,hdfs,0,0
3011,dfs.datanode.fixed.volume.size,"If false, call function getTotalSpace of File to get capacity of volume during every heartbeat. If true, cache the capacity when when the first call, and reuse it later.",1,4,limited-side-effect,hdfs,0,0
3014,dfs.datanode.fsdatasetcache.max.threads.per.volume,The maximum number of threads per volume to use for caching new data on the datanode. These threads consume both I/O and CPU. This can affect normal datanode operations.,1,1,resource,hdfs,0,0
3015,dfs.datanode.handler.count,The number of server threads for the datanode.,1,1,resource,hdfs,0,0
3019,dfs.datanode.https.address,The datanode secure http server address and port.,0,0,others,hdfs,0,0
3022,dfs.datanode.kerberos.principal,The DataNode service principal. This is typically set to dn/_HOST@REALM.TLD. Each DataNode will substitute _HOST with its own fully qualified hostname at startup. The _HOST placeholder allows using the same configuration setting on all DataNodes.,0,0,others,hdfs,0,0
3024,dfs.datanode.lazywriter.interval.sec,Interval in seconds for Datanodes for lazy persist writes.,0,0,others,hdfs,0,0
3026,dfs.datanode.lock.fair,"If this is true, the Datanode FsDataset lock will be used in Fair mode, which will help to prevent writer threads from being starved, but can lower lock throughput. See java.util.concurrent.locks.ReentrantReadWriteLock for more information on fair/non-fair locks.",1,3,reliability-tradeoff,hdfs,0,0
3027,dfs.datanode.lock-reporting-threshold-ms,"When thread waits to obtain a lock, or a thread holds a lock for more than the threshold, a log message will be written. Note that dfs.lock.suppress.warning.interval ensures a single log message is emitted per interval for waiting threads and a single message for holding threads to avoid excessive logging.",0,0,others,hdfs,0,0
3028,dfs.datanode.max.locked.memory,"The amount of memory in bytes to use for caching of block replicas in memory on the datanode. The datanode's maximum locked memory soft ulimit (RLIMIT_MEMLOCK) must be set to at least this value, else the datanode will abort on startup. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize. By default, this parameter is set to 0, which disables in-memory caching. If the native libraries are not available to the DataNode, this configuration has no effect.",1,1,resource,hdfs,0,0
3029,dfs.datanode.max.transfer.threads,Specifies the maximum number of threads to use for transferring data in and out of the DN.,1,1,resource,hdfs,0,0
3030,dfs.datanode.metrics.logger.period.seconds,This setting controls how frequently the DataNode logs its metrics. The logging configuration must also define one or more appenders for DataNodeMetricsLog for the metrics to be logged. DataNode metrics logging is disabled if this value is set to zero or less than zero.,0,0,others,hdfs,0,0
3031,dfs.datanode.network.counts.cache.max.size,The maximum number of entries the datanode per-host network error count cache may contain.,1,1,resource,hdfs,0,1
3032,dfs.datanode.oob.timeout-ms,"Timeout value when sending OOB response for each OOB type, which are OOB_RESTART, OOB_RESERVED1, OOB_RESERVED2, and OOB_RESERVED3, respectively. Currently, only OOB_RESTART is used.",0,0,others,hdfs,0,0
3033,dfs.datanode.outliers.report.interval,This setting controls how frequently DataNodes will report their peer latencies to the NameNode via heartbeats. This setting supports multiple time unit suffixes as described in dfs.heartbeat.interval. If no suffix is specified then milliseconds is assumed. It is ignored if dfs.datanode.peer.stats.enabled is false.,0,0,others,hdfs,0,0
3034,dfs.datanode.parallel.volumes.load.threads.num,Maximum number of threads to use for upgrading data directories. The default value is the number of storage directories in the DataNode.,1,1,resource,hdfs,0,1
3035,dfs.datanode.peer.metrics.min.outlier.detection.samples,Minimum number of packet send samples which are required to qualify for outlier detection. If the number of samples is below this then outlier detection is skipped.,1,5,workload-specific,hdfs,0,1
3036,dfs.datanode.peer.stats.enabled,A switch to turn on/off tracking DataNode peer statistics.,1,6,function-tradeoff,hdfs,0,0
3037,dfs.datanode.plugins,Comma-separated list of datanode plug-ins to be activated.,0,0,others,hdfs,0,0
3038,dfs.datanode.pmem.cache.dirs,"This value specifies the persistent memory directory used for caching block replica. Multiple directories separated by "","" are acceptable.",0,0,others,hdfs,0,0
3039,dfs.datanode.pmem.cache.recovery,This value specifies whether previous cache on persistent memory will be recovered. This configuration can take effect only if persistent memory cache is enabled by specifying value for 'dfs.datanode.pmem.cache.dirs'.,0,0,others,hdfs,0,1
3040,dfs.datanode.processcommands.threshold,"The threshold in milliseconds at which we will log a slow command processing in BPServiceActor. By default, this parameter is set to 2 seconds.",0,0,others,hdfs,0,1
3042,dfs.datanode.readahead.bytes,"While reading block files, if the Hadoop native libraries are available, the datanode can use the posix_fadvise system call to explicitly page data into the operating system buffer cache ahead of the current reader's position. This can improve performance especially when disks are highly contended. This configuration specifies the number of bytes ahead of the current read position which the datanode will attempt to read ahead. This feature may be disabled by configuring this property to 0. If the native libraries are not available, this configuration has no effect.",1,4,limited-side-effect,hdfs,0,0
3043,dfs.datanode.replica.cache.expiry.time,Living time of replica cached files in milliseconds.,1,5,workload-specific,hdfs,0,0
3044,dfs.datanode.replica.cache.root.dir,Use this key to change root dir of replica cache. The default root dir is currentDir.,0,0,others,hdfs,0,0
3045,dfs.datanode.restart.replica.expiration,"During shutdown for restart, the amount of time in seconds budgeted for datanode restart.",0,0,others,hdfs,0,0
3046,dfs.datanode.scan.period.hours,"If this is positive, the DataNode will not scan any individual block more than once in the specified scan period. If this is negative, the block scanner is disabled. If this is set to zero, then the default value of 504 hours or 3 weeks is used. Prior versions of HDFS incorrectly documented that setting this key to zero will disable the block scanner.",0,0,others,hdfs,0,0
3047,dfs.datanode.shared.file.descriptor.paths,"A comma-separated list of paths to use when creating file descriptors that will be shared between the DataNode and the DFSClient. Typically we use /dev/shm, so that the file descriptors will not be written to disk. It tries paths in order until creation of shared memory segment succeeds.",0,0,others,hdfs,0,1
3048,dfs.datanode.slow.io.warning.threshold.ms,"The threshold in milliseconds at which we will log a slow io warning in a datanode. By default, this parameter is set to 300 milliseconds.",0,0,others,hdfs,0,1
3050,dfs.datanode.socket.write.timeout,Timeout in ms for clients socket writes to DataNodes.,0,0,others,hdfs,0,0
3051,dfs.datanode.sync.behind.writes,"If this configuration is enabled, the datanode will instruct the operating system to enqueue all written data to the disk immediately after it is written. This differs from the usual OS policy which may wait for up to 30 seconds before triggering writeback. This may improve performance for some workloads by smoothing the IO profile for data written to disk. If the Hadoop native libraries are not available, this configuration has no effect.",1,5,workload-specific,hdfs,0,0
3052,dfs.datanode.sync.behind.writes.in.background,"If set to true, then sync_file_range() system call will occur asynchronously. This property is only valid when the property dfs.datanode.sync.behind.writes is true.",1,4,limited-side-effect,hdfs,0,0
3053,dfs.datanode.transfer.socket.recv.buffer.size,"Socket receive buffer size for DataXceiver (receiving packets from client during block writing). This may affect TCP connection throughput. If it is set to zero or negative value, no buffer size will be set explicitly, thus enable tcp auto-tuning on some system. The default value is 0.",1,1,resource,hdfs,0,0
3054,dfs.datanode.transfer.socket.send.buffer.size,"Socket send buffer size for DataXceiver (mirroring packets to downstream in pipeline). This may affect TCP connection throughput. If it is set to zero or negative value, no buffer size will be set explicitly, thus enable tcp auto-tuning on some system. The default value is 0.",1,1,resource,hdfs,0,0
3055,dfs.datanode.transferTo.allowed,"If false, break block transfers on 32-bit machines greater than or equal to 2GB into smaller chunks.",1,4,limited-side-effect,hdfs,0,0
3056,dfs.datanode.use.datanode.hostname,Whether datanodes should use datanode hostnames when connecting to other datanodes for data transfer.,0,0,others,hdfs,0,0
3057,dfs.datanode.volumes.replica-add.threadpool.size,"Specifies the maximum number of threads to use for adding block in volume. Default value for this configuration is max of (volume * number of bp_service, number of processor).",1,1,resource,hdfs,0,0
3058,dfs.default.chunk.view.size,The number of bytes to view for a file on the browser.,0,0,others,hdfs,0,1
3059,dfs.disk.balancer.block.tolerance.percent,"When a disk balancer copy operation is proceeding, the datanode is still active. So it might not be possible to move the exactly specified amount of data. So tolerance allows us to define a percentage which defines a good enough move.",1,5,workload-specific,hdfs,0,0
3060,dfs.disk.balancer.enabled,"This enables the diskbalancer feature on a cluster. By default, disk balancer is enabled.",1,4,limited-side-effect,hdfs,0,0
3061,dfs.disk.balancer.max.disk.errors,"During a block move from a source to destination disk, we might encounter various errors. This defines how many errors we can tolerate before we declare a move between 2 disks (or a step) has failed.",1,3,reliability-tradeoff,hdfs,0,1
3062,dfs.disk.balancer.max.disk.throughputInMBperSec,Maximum disk bandwidth used by diskbalancer during read from a source disk. The unit is MB/sec.,1,1,resource,hdfs,0,0
3063,dfs.disk.balancer.plan.threshold.percent,"The percentage threshold value for volume Data Density in a plan. If the absolute value of volume Data Density which is out of threshold value in a node, it means that the volumes corresponding to the disks should do the balancing in the plan. The default value is 10.",1,5,workload-specific,hdfs,0,1
3064,dfs.disk.balancer.plan.valid.interval,Maximum amount of time disk balancer plan is valid. This setting supports multiple time unit suffixes as described in dfs.heartbeat.interval. If no suffix is specified then milliseconds is assumed.,0,0,others,hdfs,0,0
3065,dfs.domain.socket.disable.interval.seconds,"The interval that a DataNode is disabled for future Short-Circuit Reads, after an error happens during a Short-Circuit Read. Setting this to 0 will not disable Short-Circuit Reads at all after errors happen. Negative values are invalid.",0,0,others,hdfs,0,0
3067,dfs.edit.log.transfer.bandwidthPerSec,"Maximum bandwidth used for transferring edit log to between journal nodes for syncing, in bytes per second. A default value of 0 indicates that throttling is disabled.",1,1,resource,hdfs,0,0
3068,dfs.edit.log.transfer.timeout,Socket timeout for edit log transfer in milliseconds. This timeout should be configured such that normal edit log transfer for journal node syncing can complete successfully.,0,0,others,hdfs,0,0
3070,dfs.encrypt.data.overwrite.downstream.new.qop,"When dfs.datanode.overwrite.downstream.derived.qop is set to true, this configuration specifies the new QOP to be used to overwrite inter-DN QOP.",0,0,others,hdfs,0,0
3071,dfs.encrypt.data.transfer,"Whether or not actual block data that is read/written from/to HDFS should be encrypted on the wire. This only needs to be set on the NN and DNs, clients will deduce this automatically. It is possible to override this setting per connection by specifying custom logic via dfs.trustedchannel.resolver.class.",1,2,security-tradeoff,hdfs,0,0
3072,dfs.encrypt.data.transfer.algorithm,"This value may be set to either ""3des"" or ""rc4"". If nothing is set, then the configured JCE default on the system is used (usually 3DES.) It is widely believed that 3DES is more cryptographically secure, but RC4 is substantially faster. Note that if AES is supported by both the client and server then this encryption algorithm will only be used to initially transfer keys for AES. (See dfs.encrypt.data.transfer.cipher.suites.)",1,2,security-tradeoff,hdfs,0,1
3073,dfs.encrypt.data.transfer.cipher.key.bitlength,"The key bitlength negotiated by dfsclient and datanode for encryption. This value may be set to either 128, 192 or 256.",1,2,security-tradeoff,hdfs,0,1
3074,dfs.encrypt.data.transfer.cipher.suites,"This value may be either undefined or AES/CTR/NoPadding. If defined, then dfs.encrypt.data.transfer uses the specified cipher suite for data encryption. If not defined, then only the algorithm specified in dfs.encrypt.data.transfer.algorithm is used. By default, the property is not defined.",1,2,security-tradeoff,hdfs,0,0
3075,dfs.ha.automatic-failover.enabled,Whether automatic failover is enabled. See the HDFS High Availability documentation for details on automatic HA configuration.,0,0,others,hdfs,0,0
3076,dfs.ha.fencing.methods,A list of scripts or Java classes which will be used to fence the Active NameNode during a failover. See the HDFS High Availability documentation for details on automatic HA configuration.,0,0,others,hdfs,0,0
3077,dfs.ha.log-roll.period,"How often, in seconds, the StandbyNode should ask the active to roll edit logs. Since the StandbyNode only reads from finalized log segments, the StandbyNode will only be as up-to-date as how often the logs are rolled. Note that failover triggers a log roll so the StandbyNode will be up to date before it becomes active. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.If no time unit is specified then seconds is assumed.",0,0,others,hdfs,0,0
3080,dfs.ha.nn.not-become-active-in-safemode,This will prevent safe mode namenodes to become active while other standby namenodes might be ready to serve requests when it is set to true.,0,0,others,hdfs,0,0
3081,dfs.ha.standby.checkpoints,"If true, a NameNode in Standby state periodically takes a checkpoint of the namespace, saves it to its local storage and then upload to the remote NameNode.",1,3,reliability-tradeoff,hdfs,0,0
3082,dfs.ha.tail-edits.in-progress,"Whether enable standby namenode to tail in-progress edit logs. Clients might want to turn it on when they want Standby NN to have more up-to-date data. When using the QuorumJournalManager, this enables tailing of edit logs via the RPC-based mechanism, rather than streaming, which allows for much fresher data.",1,3,reliability-tradeoff,hdfs,0,0
3083,dfs.ha.tail-edits.namenode-retries,Number of retries to use when contacting the namenode when tailing the log.,0,0,others,hdfs,0,0
3084,dfs.ha.tail-edits.period,"How often, the StandbyNode and ObserverNode should check if there are new edit log entries ready to be consumed. This is the minimum period between checking; exponential backoff will be applied if no edits are found and dfs.ha.tail-edits.period.backoff-max is configured. By default, no backoff is applied. Supports multiple time unit suffix (case insensitive), as described in dfs.heartbeat.interval.",0,0,others,hdfs,0,0
3085,dfs.ha.tail-edits.period.backoff-max,"The maximum time the tailer should wait between checking for new edit log entries. Exponential backoff will be applied when an edit log tail is performed but no edits are available to be read. Values less than or equal to zero disable backoff entirely; this is the default behavior. Supports multiple time unit suffix (case insensitive), as described in dfs.heartbeat.interval.",0,0,others,hdfs,0,0
3086,dfs.ha.tail-edits.rolledits.timeout,The timeout in seconds of calling rollEdits RPC on Active NN.,0,0,others,hdfs,0,0
3087,dfs.ha.zkfc.nn.http.timeout.ms,"The HTTP connection and read timeout value (unit is ms ) when DFS ZKFC tries to get local NN thread dump after local NN becomes SERVICE_NOT_RESPONDING or SERVICE_UNHEALTHY. If it is set to zero, DFS ZKFC won't get local NN thread dump.",0,0,others,hdfs,0,0
3088,dfs.ha.zkfc.port,The port number that the zookeeper failover controller RPC server binds to.,0,0,others,hdfs,0,0
3089,dfs.heartbeat.interval,"Determines datanode heartbeat interval in seconds. Can use the following suffix (case insensitive): ms(millis), s(sec), m(min), h(hour), d(day) to specify the time (such as 2s, 2m, 1h, etc.). Or provide complete number in seconds (such as 30 for 30 seconds). If no time unit is specified then seconds is assumed.",0,0,others,hdfs,0,1
3092,dfs.http.client.failover.max.attempts,Specify the max number of failover attempts for WebHDFS client in case of network exception.,0,0,others,hdfs,0,0
3093,dfs.http.client.failover.sleep.base.millis,Specify the base amount of time in milliseconds upon which the exponentially increased sleep time between retries or failovers is calculated for WebHDFS client.,0,0,others,hdfs,0,0
3094,dfs.http.client.failover.sleep.max.millis,Specify the upper bound of sleep time in milliseconds between retries or failovers for WebHDFS client.,0,0,others,hdfs,0,0
3095,dfs.http.client.retry.max.attempts,"Specify the max number of retry attempts for WebHDFS client, if the difference between retried attempts and failovered attempts is larger than the max number of retry attempts, there will be no more retries.",0,0,others,hdfs,0,1
3096,dfs.http.client.retry.policy.enabled,"If ""true"", enable the retry policy of WebHDFS client. If ""false"", retry policy is turned off. Enabling the retry policy can be quite useful while using WebHDFS to copy large files between clusters that could timeout, or copy files between HA clusters that could failover during the copy.",1,3,reliability-tradeoff,hdfs,0,1
3097,dfs.http.client.retry.policy.spec,"Specify a policy of multiple linear random retry for WebHDFS client, e.g. given pairs of number of retries and sleep time (n0, t0), (n1, t1), ..., the first n0 retries sleep t0 milliseconds on average, the following n1 retries sleep t1 milliseconds on average, and so on.",0,0,others,hdfs,0,0
3098,dfs.http.policy,Decide if HTTPS(SSL) is supported on HDFS This configures the HTTP endpoint for HDFS daemons: The following values are supported: - HTTP_ONLY : Service is provided only on http - HTTPS_ONLY : Service is provided only on https - HTTP_AND_HTTPS : Service is provided both on http and https,1,2,security-tradeoff,hdfs,0,0
3099,dfs.https.server.keystore.resource,Resource file from which ssl server keystore information will be extracted,0,0,others,hdfs,0,0
3100,dfs.image.compress,"When this value is true, the dfs image will be compressed. Enabling this will be very helpful if dfs image is large since it can avoid consuming a lot of network bandwidth when SBN uploads a new dfs image to ANN. The compressed codec is specified by the setting dfs.image.compression.codec.",1,4,limited-side-effect,hdfs,0,1
3101,dfs.image.compression.codec,"If the dfs image is compressed, how should they be compressed? This has to be a codec defined in io.compression.codecs.",1,6,function-tradeoff,hdfs,0,0
3102,dfs.image.parallel.inode.threshold,"If the image contains less inodes than this setting, then do not write sub-sections and hence disable parallel loading. This is because small images load very quickly in serial and parallel loading is not needed.",1,5,workload-specific,hdfs,0,0
3103,dfs.image.parallel.load,"If true, write sub-section entries to the fsimage index so it can be loaded in parallel. Also controls whether parallel loading will be used for an image previously created with sub-sections. If the image contains sub-sections and this is set to false, parallel loading will not be used. Parallel loading is not compatible with image compression, so if dfs.image.compress is set to true this setting will be ignored and no parallel loading will occur. Enabling this feature may impact rolling upgrades and downgrades if the previous version does not support this feature. If the feature was enabled and a downgrade is required, first set this parameter to false and then save the namespace to create a fsimage with no sub-sections and then perform the downgrade.",1,4,limited-side-effect,hdfs,0,1
3104,dfs.image.parallel.target.sections,"Controls the number of sub-sections that will be written to fsimage for each section. This should be larger than dfs.image.parallel.threads, otherwise all threads will not be used when loading. Ideally, have at least twice the number of target sections as threads, so each thread must load more than one section to avoid one long running section affecting the load time.",0,0,others,hdfs,0,0
3105,dfs.image.parallel.threads,The number of threads to use when dfs.image.parallel.load is enabled. This setting should be less than dfs.image.parallel.target.sections. The optimal number of threads will depend on the hardware and environment.,1,1,resource,hdfs,0,0
3106,dfs.image.transfer.bandwidthPerSec,"Maximum bandwidth used for regular image transfers (instead of bootstrapping the standby namenode), in bytes per second. This can help keep normal namenode operations responsive during checkpointing. A default value of 0 indicates that throttling is disabled. The maximum bandwidth used for bootstrapping standby namenode is configured with dfs.image.transfer-bootstrap-standby.bandwidthPerSec. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize.",1,1,resource,hdfs,0,0
3107,dfs.image.transfer.chunksize,"Chunksize in bytes to upload the checkpoint. Chunked streaming is used to avoid internal buffering of contents of image file of huge size. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize.",1,5,workload-specific,hdfs,0,0
3108,dfs.image.transfer.timeout,"Socket timeout for the HttpURLConnection instance used in the image transfer. This is measured in milliseconds. This timeout prevents client hangs if the connection is idle for this configured timeout, during image transfer.",0,0,others,hdfs,0,0
3109,dfs.image.transfer-bootstrap-standby.bandwidthPerSec,"Maximum bandwidth used for transferring image to bootstrap standby namenode, in bytes per second. A default value of 0 indicates that throttling is disabled. This default value should be used in most cases, to ensure timely HA operations. The maximum bandwidth used for regular image transfers is configured with dfs.image.transfer.bandwidthPerSec. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize.",1,1,resource,hdfs,0,0
3110,dfs.internal.nameservices,Comma-separated list of nameservices that belong to this cluster. Datanode will report to all the nameservices in this list. By default this is set to the value of dfs.nameservices.,0,0,others,hdfs,0,1
3111,dfs.journalnode.edit-cache-size.bytes,"The size, in bytes, of the in-memory cache of edits to keep on the JournalNode. This cache is used to serve edits for tailing via the RPC-based mechanism, and is only enabled when dfs.ha.tail-edits.in-progress is true. Transactions range in size but are around 200 bytes on average, so the default of 1MB can store around 5000 transactions.",1,1,resource,hdfs,0,0
3114,dfs.journalnode.enable.sync,"If true, the journal nodes wil sync with each other. The journal nodes will periodically gossip with other journal nodes to compare edit log manifests and if they detect any missing log segment, they will download it from the other journal nodes.",1,3,reliability-tradeoff,hdfs,0,0
3119,dfs.journalnode.kerberos.internal.spnego.principal,"The server principal used by the JournalNode HTTP Server for SPNEGO authentication when Kerberos security is enabled. This is typically set to HTTP/_HOST@REALM.TLD. The SPNEGO server principal begins with the prefix HTTP/ by convention. If the value is '*', the web server will attempt to login with every principal specified in the keytab file dfs.web.authentication.kerberos.keytab. For most deployments this can be set to ${dfs.web.authentication.kerberos.principal} i.e use the value of dfs.web.authentication.kerberos.principal.",0,0,others,hdfs,0,0
3120,dfs.journalnode.kerberos.principal,The JournalNode service principal. This is typically set to jn/_HOST@REALM.TLD. Each JournalNode will substitute _HOST with its own fully qualified hostname at startup. The _HOST placeholder allows using the same configuration setting on all JournalNodes.,0,0,others,hdfs,0,0
3125,dfs.journalnode.sync.interval,"Time interval, in milliseconds, between two Journal Node syncs. This configuration takes effect only if the journalnode sync is enabled by setting the configuration parameter dfs.journalnode.enable.sync to true.",0,0,others,hdfs,0,0
3126,dfs.lock.suppress.warning.interval,Instrumentation reporting long critical sections will suppress consecutive warnings within this interval.,0,0,others,hdfs,0,1
3127,dfs.ls.limit,"Limit the number of files printed by ls. If less or equal to zero, at most DFS_LIST_LIMIT_DEFAULT (= 1000) will be printed.",0,0,others,hdfs,0,0
3128,dfs.metrics.percentiles.intervals,"Comma-delimited set of integers denoting the desired rollover intervals (in seconds) for percentile latency metrics on the Namenode and Datanode. By default, percentile latency metrics are disabled.",0,0,others,hdfs,0,0
3129,dfs.mover.address,The hostname used for a keytab based Kerberos login. Keytab based login can be enabled with dfs.mover.keytab.enabled.,0,0,others,hdfs,0,0
3130,dfs.mover.kerberos.principal,The Mover principal. This is typically set to mover/_HOST@REALM.TLD. The Mover will substitute _HOST with its own fully qualified hostname at startup. The _HOST placeholder allows using the same configuration setting on different servers. Keytab based login can be enabled with dfs.mover.keytab.enabled.,0,0,others,hdfs,0,0
3131,dfs.mover.keytab.enabled,Set to true to enable login using a keytab for Kerberized Hadoop.,0,0,others,hdfs,0,0
3133,dfs.mover.max-no-move-interval,"If this specified amount of time has elapsed and no block has been moved out of a source DataNode, on more effort will be made to move blocks out of this DataNode in the current Mover iteration.",0,0,others,hdfs,0,1
3135,dfs.mover.moverThreads,Configure the balancer's mover thread pool size.,1,1,resource,hdfs,0,0
3136,dfs.mover.retry.max.attempts,The maximum number of retries before the mover consider the move failed.,0,0,others,hdfs,0,1
3138,dfs.namenode.acls.enabled,"Set to true to enable support for HDFS ACLs (Access Control Lists). By default, ACLs are enabled. When ACLs are disabled, the NameNode rejects all RPCs related to setting or getting ACLs.",0,0,others,hdfs,0,0
3139,dfs.namenode.audit.log.async,"If true, enables asynchronous audit log.",1,4,limited-side-effect,hdfs,0,0
3140,dfs.namenode.audit.log.debug.cmdlist,A comma separated list of NameNode commands that are written to the HDFS namenode audit log only if the audit log level is debug.,0,0,others,hdfs,0,0
3141,dfs.namenode.audit.log.token.tracking.id,"If true, adds a tracking ID for all audit log events.",0,0,others,hdfs,0,1
3142,dfs.namenode.audit.loggers,"List of classes implementing audit loggers that will receive audit events. These should be implementations of org.apache.hadoop.hdfs.server.namenode.AuditLogger. The special value ""default"" can be used to reference the default audit logger, which uses the configured log system. Installing custom audit loggers may affect the performance and stability of the NameNode. Refer to the custom logger's documentation for more details.",1,6,function-tradeoff,hdfs,0,0
3143,dfs.namenode.available-space-block-placement-policy.balanced-space-preference-fraction,"Only used when the dfs.block.replicator.classname is set to org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy. Special value between 0 and 1, noninclusive. Increases chance of placing blocks on Datanodes with less disk space used.",1,3,reliability-tradeoff,hdfs,0,0
3144,dfs.namenode.available-space-block-placement-policy.balance-local-node,"Only used when the dfs.block.replicator.classname is set to org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy. If true, balances the local node too.",1,4,limited-side-effect,hdfs,0,0
3145,dfs.namenode.avoid.read.stale.datanode,"Indicate whether or not to avoid reading from ""stale"" datanodes whose heartbeat messages have not been received by the namenode for more than a specified time interval. Stale datanodes will be moved to the end of the node list returned for reading. See dfs.namenode.avoid.write.stale.datanode for a similar setting for writes.",1,3,reliability-tradeoff,hdfs,0,0
3146,dfs.namenode.avoid.write.stale.datanode,"Indicate whether or not to avoid writing to ""stale"" datanodes whose heartbeat messages have not been received by the namenode for more than a specified time interval. Writes will avoid using stale datanodes unless more than a configured ratio (dfs.namenode.write.stale.datanode.ratio) of datanodes are marked as stale. See dfs.namenode.avoid.read.stale.datanode for a similar setting for reads.",1,3,reliability-tradeoff,hdfs,0,0
3147,dfs.namenode.backup.address,The backup node server address and port. If the port is 0 then the server will start on a free port.,0,0,others,hdfs,0,0
3148,dfs.namenode.backup.dnrpc-address,Service RPC address for the backup Namenode.,0,0,others,hdfs,0,0
3149,dfs.namenode.backup.http-address,The backup node http server address and port. If the port is 0 then the server will start on a free port.,0,0,others,hdfs,0,0
3150,dfs.namenode.block.deletion.increment,The number of block deletion increment. This setting will control the block increment deletion rate to ensure that other waiters on the lock can get in.,1,5,workload-specific,hdfs,0,1
3151,dfs.namenode.block-placement-policy.default.prefer-local-node,"Controls how the default block placement policy places the first replica of a block. When true, it will prefer the node where the client is running. When false, it will prefer a node in the same rack as the client. Setting to false avoids situations where entire copies of large files end up on a single node, thus creating hotspots.",1,4,limited-side-effect,hdfs,0,1
3152,dfs.namenode.blockreport.max.lock.hold.time,The BlockReportProcessingThread max write lock hold time in ms.,0,0,others,hdfs,0,0
3153,dfs.namenode.blockreport.queue.size,The queue size of BlockReportProcessingThread in BlockManager.,1,5,workload-specific,hdfs,0,0
3154,dfs.namenode.blocks.per.postponedblocks.rescan,Number of blocks to rescan for each iteration of postponedMisreplicatedBlocks.,1,6,function-tradeoff,hdfs,0,0
3155,dfs.namenode.caching.enabled,"Set to true to enable block caching. This flag enables the NameNode to maintain a mapping of cached blocks to DataNodes via processing DataNode cache reports. Based on these reports and addition and removal of caching directives, the NameNode will schedule caching and uncaching work.",1,4,limited-side-effect,hdfs,0,0
3156,dfs.namenode.checkpoint.check.period,"The SecondaryNameNode and CheckpointNode will poll the NameNode every 'dfs.namenode.checkpoint.check.period' seconds to query the number of uncheckpointed transactions. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.If no time unit is specified then seconds is assumed.",0,0,others,hdfs,0,0
3157,dfs.namenode.checkpoint.check.quiet-multiplier,"Used to calculate the amount of time between retries when in the 'quiet' period for creating checkpoints (active namenode already has an up-to-date image from another checkpointer), so we wait a multiplier of the dfs.namenode.checkpoint.check.period before retrying the checkpoint because another node likely is already managing the checkpoints, allowing us to save bandwidth to transfer checkpoints that don't need to be used.",0,0,others,hdfs,0,0
3158,dfs.namenode.checkpoint.dir,Determines where on the local filesystem the DFS secondary name node should store the temporary images to merge. If this is a comma-delimited list of directories then the image is replicated in all of the directories for redundancy.,0,0,others,hdfs,0,1
3159,dfs.namenode.checkpoint.edits.dir,Determines where on the local filesystem the DFS secondary name node should store the temporary edits to merge. If this is a comma-delimited list of directories then the edits is replicated in all of the directories for redundancy. Default value is same as dfs.namenode.checkpoint.dir,0,0,others,hdfs,0,0
3160,dfs.namenode.checkpoint.max-retries,"The SecondaryNameNode retries failed checkpointing. If the failure occurs while loading fsimage or replaying edits, the number of retries is limited by this variable.",0,0,others,hdfs,0,1
3161,dfs.namenode.checkpoint.period,"The number of seconds between two periodic checkpoints. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.If no time unit is specified then seconds is assumed.",0,0,others,hdfs,0,0
3162,dfs.namenode.checkpoint.txns,"The Secondary NameNode or CheckpointNode will create a checkpoint of the namespace every 'dfs.namenode.checkpoint.txns' transactions, regardless of whether 'dfs.namenode.checkpoint.period' has expired.",0,0,others,hdfs,0,0
3163,dfs.namenode.corrupt.block.delete.immediately.enabled,"Whether the corrupt replicas should be deleted immediately, irrespective of other replicas on stale storages.",1,4,limited-side-effect,hdfs,0,0
3164,dfs.namenode.datanode.registration.ip-hostname-check,"If true (the default), then the namenode requires that a connecting datanode's address must be resolved to a hostname. If necessary, a reverse DNS lookup is performed. All attempts to register a datanode from an unresolvable address are rejected. It is recommended that this setting be left on to prevent accidental registration of datanodes listed by hostname in the excludes file during a DNS outage. Only set this to false in environments where there is no infrastructure to support reverse DNS lookup.",1,2,security-tradeoff,hdfs,0,1
3165,dfs.namenode.decommission.backoff.monitor.pending.blocks.per.lock,"When loading blocks into the replication queue, release the namenode write lock after the defined number of blocks have been processed.",0,0,others,hdfs,0,1
3166,dfs.namenode.decommission.backoff.monitor.pending.limit,"When the Backoff monitor is enabled, determines the maximum number of blocks related to decommission and maintenance operations that can be loaded into the replication queue at any given time. Every dfs.namenode.decommission.interval seconds, the list is checked to see if the blocks have become fully replicated and then further blocks are added to reach the limit defined in this parameter.",0,0,others,hdfs,0,0
3167,dfs.namenode.decommission.blocks.per.interval,"The approximate number of blocks to process per decommission or maintenance interval, as defined in dfs.namenode.decommission.interval.",1,1,resource,hdfs,0,0
3168,dfs.namenode.decommission.interval,"Namenode periodicity in seconds to check if decommission or maintenance is complete. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval. If no time unit is specified then seconds is assumed.",0,0,others,hdfs,0,0
3169,dfs.namenode.decommission.max.concurrent.tracked.nodes,The maximum number of decommission-in-progress or entering-maintenance datanodes nodes that will be tracked at one time by the namenode. Tracking these datanode consumes additional NN memory proportional to the number of blocks on the datnode. Having a conservative limit reduces the potential impact of decommissioning or maintenance of a large number of nodes at once. A value of 0 means no limit will be enforced.,1,1,resource,hdfs,0,0
3171,dfs.namenode.delegation.key.update-interval,The update interval for master key for delegation tokens in the namenode in milliseconds.,0,0,others,hdfs,0,0
3172,dfs.namenode.delegation.token.always-use,"For testing. Setting to true always allows the DT secret manager to be used, even if security is disabled.",0,0,others,hdfs,0,0
3173,dfs.namenode.delegation.token.max-lifetime,The maximum lifetime in milliseconds for which a delegation token is valid.,0,0,others,hdfs,0,0
3174,dfs.namenode.delegation.token.renew-interval,The renewal interval for delegation token in milliseconds.,0,0,others,hdfs,0,0
3175,dfs.namenode.ec.policies.max.cellsize,The maximum cell size of erasure coding policy. Default is 4MB.,0,0,others,hdfs,0,0
3177,dfs.namenode.ec.userdefined.policy.allowed,"If set to false, doesn't allow addition of user defined erasure coding policies.",0,0,others,hdfs,0,1
3178,dfs.namenode.edekcacheloader.initial.delay.ms,"When KeyProvider is configured, the time delayed until the first attempt to warm up edek cache on NN start up / become active.",1,5,workload-specific,hdfs,0,0
3179,dfs.namenode.edekcacheloader.interval.ms,"When KeyProvider is configured, the interval time of warming up edek cache on NN starts up / becomes active. All edeks will be loaded from KMS into provider cache. The edek cache loader will try to warm up the cache until succeed or NN leaves active state.",1,5,workload-specific,hdfs,0,0
3180,dfs.namenode.edit.log.autoroll.check.interval.ms,"How often an active namenode will check if it needs to roll its edit log, in milliseconds.",0,0,others,hdfs,0,0
3181,dfs.namenode.edit.log.autoroll.multiplier.threshold,"Determines when an active namenode will roll its own edit log. The actual threshold (in number of edits) is determined by multiplying this value by dfs.namenode.checkpoint.txns. This prevents extremely large edit files from accumulating on the active namenode, which can cause timeouts during namenode startup and pose an administrative hassle. This behavior is intended as a failsafe for when the standby or secondary namenode fail to roll the edit log by the normal checkpoint threshold.",1,5,workload-specific,hdfs,0,0
3182,dfs.namenode.edits.asynclogging,"If set to true, enables asynchronous edit logs in the Namenode. If set to false, the Namenode uses the traditional synchronous edit logs.",1,4,limited-side-effect,hdfs,0,0
3184,dfs.namenode.edits.dir.minimum,"dfs.namenode.edits.dir includes both required directories (specified by dfs.namenode.edits.dir.required) and optional directories. The number of usable optional directories must be greater than or equal to this property. If the number of usable optional directories falls below dfs.namenode.edits.dir.minimum, HDFS will issue an error. This property defaults to 1.",0,0,others,hdfs,0,0
3187,dfs.namenode.edits.noeditlogchannelflush,"Specifies whether to flush edit log file channel. When set, expensive FileChannel#force calls are skipped and synchronous disk writes are enabled instead by opening the edit log file with RandomAccessFile(""rws"") flags. This can significantly improve the performance of edit log writes on the Windows platform. Note that the behavior of the ""rws"" flags is platform and hardware specific and might not provide the same level of guarantees as FileChannel#force. For example, the write will skip the disk-cache on SAS and SCSI devices while it might not on SATA devices. This is an expert level setting, change with caution.",1,4,limited-side-effect,hdfs,0,0
3188,dfs.namenode.enable.log.stale.datanode,Enable and disable logging datanode staleness. Disabled by default.,0,0,others,hdfs,0,0
3189,dfs.namenode.enable.retrycache,"This enables the retry cache on the namenode. Namenode tracks for non-idempotent requests the corresponding response. If a client retries the request, the response from the retry cache is sent. Such operations are tagged with annotation @AtMostOnce in namenode protocols. It is recommended that this flag be set to true. Setting it to false, will result in clients getting failure responses to retried request. This flag must be enabled in HA setup for transparent fail-overs. The entries in the cache have expiration time configurable using dfs.namenode.retrycache.expirytime.millis.",1,4,limited-side-effect,hdfs,0,0
3190,dfs.namenode.file.close.num-committed-allowed,"Normally a file can only be closed with all its blocks are committed. When this value is set to a positive integer N, a file can be closed when N blocks are committed and the rest complete.",1,3,reliability-tradeoff,hdfs,0,1
3191,dfs.namenode.fs-limits.max-blocks-per-file,"Maximum number of blocks per file, enforced by the Namenode on write. This prevents the creation of extremely large files which can degrade performance.",0,0,others,hdfs,0,0
3192,dfs.namenode.fs-limits.max-component-length,"Defines the maximum number of bytes in UTF-8 encoding in each component of a path. A value of 0 will disable the check. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize.",1,5,workload-specific,hdfs,0,0
3193,dfs.namenode.fs-limits.max-directory-items,Defines the maximum number of items that a directory may contain. Cannot set the property to a value less than 1 or more than 6400000.,0,0,others,hdfs,0,0
3194,dfs.namenode.fs-limits.max-xattr-size,"The maximum combined size of the name and value of an extended attribute in bytes. It should be larger than 0, and less than or equal to maximum size hard limit which is 32768. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize.",0,0,others,hdfs,0,0
3195,dfs.namenode.fs-limits.max-xattrs-per-inode,Maximum number of extended attributes per inode.,0,0,others,hdfs,0,1
3196,dfs.namenode.fs-limits.min-block-size,"Minimum block size in bytes, enforced by the Namenode at create time. This prevents the accidental creation of files with tiny block sizes (and thus many blocks), which can degrade performance. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize.",1,5,workload-specific,hdfs,0,1
3197,dfs.namenode.fslock.fair,"If this is true, the FS Namesystem lock will be used in Fair mode, which will help to prevent writer threads from being starved, but can provide lower lock throughput. See java.util.concurrent.locks.ReentrantReadWriteLock for more information on fair/non-fair locks.",1,3,reliability-tradeoff,hdfs,0,0
3198,dfs.namenode.full.block.report.lease.length.ms,The number of milliseconds that the NameNode will wait before invalidating a full block report lease. This prevents a crashed DataNode from permanently using up a full block report lease.,0,0,others,hdfs,0,0
3199,dfs.namenode.gc.time.monitor.enable,Enable the GcTimePercentage metrics in NameNode's JvmMetrics. It will start a thread(GcTimeMonitor) computing the metric.,1,6,function-tradeoff,hdfs,0,0
3200,dfs.namenode.gc.time.monitor.observation.window.ms,Determines the windows size of GcTimeMonitor. A window is a period of time starts at now-windowSize and ends at now. The GcTimePercentage is the gc time proportion of the window.,0,0,others,hdfs,0,0
3201,dfs.namenode.gc.time.monitor.sleep.interval.ms,Determines the sleep interval in the window. The GcTimeMonitor wakes up in the sleep interval periodically to compute the gc time proportion. The shorter the interval the preciser the GcTimePercentage. The sleep interval must be shorter than the window size.,0,0,others,hdfs,0,0
3202,dfs.namenode.get-blocks.max-qps,"The maximum number of getBlocks RPCs data movement utilities can make to a NameNode per second. Values less than or equal to 0 disable throttling. This affects anything that uses a NameNodeConnector, i.e., the Balancer, Mover, and StoragePolicySatisfier.",1,5,workload-specific,hdfs,0,1
3203,dfs.namenode.handler.count,The number of Namenode RPC server threads that listen to requests from clients. If dfs.namenode.servicerpc-address is not configured then Namenode RPC server threads listen to requests from all nodes.,1,1,resource,hdfs,0,0
3204,dfs.namenode.heartbeat.recheck-interval,"This time decides the interval to check for expired datanodes. With this value and dfs.heartbeat.interval, the interval of deciding the datanode is stale or not is also calculated. The unit of this configuration is millisecond.",0,0,others,hdfs,0,0
3208,dfs.namenode.https-address,The namenode secure http server address and port.,0,0,others,hdfs,0,0
3210,dfs.namenode.inode.attributes.provider.bypass.users,A list of user principals (in secure cluster) or user names (in insecure cluster) for whom the external attributes provider will be bypassed for all operations. This means file attributes stored in HDFS instead of the external provider will be used for permission checking and be returned when requested.,0,0,others,hdfs,0,0
3211,dfs.namenode.inode.attributes.provider.class,Name of class to use for delegating HDFS authorization.,0,0,others,hdfs,0,0
3212,dfs.namenode.inotify.max.events.per.rpc,Maximum number of events that will be sent to an inotify client in a single RPC response. The default value attempts to amortize away the overhead for this RPC while avoiding huge memory requirements for the client and NameNode (1000 events should consume no more than 1 MB.),1,5,workload-specific,hdfs,0,0
3213,dfs.namenode.invalidate.work.pct.per.iteration,"*Note*: Advanced property. Change with caution. This determines the percentage amount of block invalidations (deletes) to do over a single DN heartbeat deletion command. The final deletion count is determined by applying this percentage to the number of live nodes in the system. The resultant number is the number of blocks from the deletion list chosen for proper invalidation over a single heartbeat of a single DN. Value should be a positive, non-zero percentage in float notation (X.Yf), with 1.0f meaning 100%.",1,5,workload-specific,hdfs,0,0
3214,dfs.namenode.kerberos.internal.spnego.principal,"The server principal used by the NameNode for web UI SPNEGO authentication when Kerberos security is enabled. This is typically set to HTTP/_HOST@REALM.TLD The SPNEGO server principal begins with the prefix HTTP/ by convention. If the value is '*', the web server will attempt to login with every principal specified in the keytab file dfs.web.authentication.kerberos.keytab.",0,0,others,hdfs,0,1
3215,dfs.namenode.kerberos.principal,The NameNode service principal. This is typically set to nn/_HOST@REALM.TLD. Each NameNode will substitute _HOST with its own fully qualified hostname at startup. The _HOST placeholder allows using the same configuration setting on both NameNodes in an HA setup.,0,0,others,hdfs,0,0
3216,dfs.namenode.kerberos.principal.pattern,A client-side RegEx that can be configured to control allowed realms to authenticate with (useful in cross-realm env.),0,0,others,hdfs,0,0
3218,dfs.namenode.lazypersist.file.scrub.interval.sec,"The NameNode periodically scans the namespace for LazyPersist files with missing blocks and unlinks them from the namespace. This configuration key controls the interval between successive scans. If this value is set to 0, the file scrubber is disabled.",0,0,others,hdfs,0,0
3219,dfs.namenode.lease-hard-limit-sec,Determines the namenode automatic lease recovery interval in seconds.,0,0,others,hdfs,0,0
3221,dfs.namenode.legacy-oiv-image.dir,"Determines where to save the namespace in the old fsimage format during checkpointing by standby NameNode or SecondaryNameNode. Users can dump the contents of the old format fsimage by oiv_legacy command. If the value is not specified, old format fsimage will not be saved in checkpoint.",0,0,others,hdfs,0,0
3222,dfs.namenode.lifeline.handler.count,"Sets an absolute number of RPC server threads the NameNode runs for handling the DataNode Lifeline Protocol and HA health check requests from ZKFC. If this property is defined, then it overrides the behavior of dfs.namenode.lifeline.handler.ratio. By default, it is not defined. This property has no effect if dfs.namenode.lifeline.rpc-address is not defined.",1,1,resource,hdfs,0,0
3223,dfs.namenode.lifeline.handler.ratio,"A ratio applied to the value of dfs.namenode.handler.count, which then provides the number of RPC server threads the NameNode runs for handling the lifeline RPC server. For example, if dfs.namenode.handler.count is 100, and dfs.namenode.lifeline.handler.factor is 0.10, then the NameNode starts 100 * 0.10 = 10 threads for handling the lifeline RPC server. It is common to tune the value of dfs.namenode.handler.count as a function of the number of DataNodes in a cluster. Using this property allows for the lifeline RPC server handler threads to be tuned automatically without needing to touch a separate property. Lifeline message processing is lightweight, so it is expected to require many fewer threads than the main NameNode RPC server. This property is not used if dfs.namenode.lifeline.handler.count is defined, which sets an absolute thread count. This property has no effect if dfs.namenode.lifeline.rpc-address is not defined.",1,1,resource,hdfs,0,1
3224,dfs.namenode.lifeline.rpc-address,"NameNode RPC lifeline address. This is an optional separate RPC address that can be used to isolate health checks and liveness to protect against resource exhaustion in the main RPC handler pool. In the case of HA/Federation where multiple NameNodes exist, the name service ID is added to the name e.g. dfs.namenode.lifeline.rpc-address.ns1. The value of this property will take the form of nn-host1:rpc-port. If this property is not defined, then the NameNode will not start a lifeline RPC server. By default, the property is not defined.",0,0,others,hdfs,0,0
3226,dfs.namenode.list.cache.directives.num.responses,This value controls the number of cache directives that the NameNode will send over the wire in response to a listDirectives RPC.,1,1,resource,hdfs,0,0
3227,dfs.namenode.list.cache.pools.num.responses,This value controls the number of cache pools that the NameNode will send over the wire in response to a listPools RPC.,1,1,resource,hdfs,0,1
3228,dfs.namenode.list.encryption.zones.num.responses,"When listing encryption zones, the maximum number of zones that will be returned in a batch. Fetching the list incrementally in batches improves namenode performance.",1,1,resource,hdfs,0,0
3229,dfs.namenode.list.openfiles.num.responses,"When listing open files, the maximum number of open files that will be returned in a single batch. Fetching the list incrementally in batches improves namenode performance.",1,1,resource,hdfs,0,0
3230,dfs.namenode.list.reencryption.status.num.responses,"When listing re-encryption status, the maximum number of zones that will be returned in a batch. Fetching the list incrementally in batches improves namenode performance.",1,5,workload-specific,hdfs,0,0
3231,dfs.namenode.lock.detailed-metrics.enabled,"If true, the namenode will keep track of how long various operations hold the Namesystem lock for and emit this as metrics. These metrics have names of the form FSN(Read|Write)LockNanosOperationName, where OperationName denotes the name of the operation that initiated the lock hold (this will be OTHER for certain uncategorized operations) and they export the hold time values in nanoseconds.",1,6,function-tradeoff,hdfs,0,0
3232,dfs.namenode.maintenance.replication.min,Minimal live block replication in existence of maintenance mode.,0,0,others,hdfs,0,0
3233,dfs.namenode.max.extra.edits.segments.retained,"The maximum number of extra edit log segments which should be retained beyond what is minimally necessary for a NN restart. When used in conjunction with dfs.namenode.num.extra.edits.retained, this configuration property serves to cap the number of extra edits files to a reasonable value.",1,1,resource,hdfs,0,0
3234,dfs.namenode.max.full.block.report.leases,The maximum number of leases for full block reports that the NameNode will issue at any given time. This prevents the NameNode from being flooded with full block reports that use up all the RPC handler threads. This number should never be more than the number of RPC handler threads or less than 1.,0,0,others,hdfs,0,1
3235,dfs.namenode.max.objects,"The maximum number of files, directories and blocks dfs supports. A value of zero indicates no limit to the number of objects that dfs supports.",0,0,others,hdfs,0,0
3236,dfs.namenode.max.op.size,Maximum opcode size in bytes.,0,0,others,hdfs,0,0
3237,dfs.namenode.max-corrupt-file-blocks-returned,"The maximum number of corrupt file blocks listed by NameNode Web UI, JMX and other client request.",0,0,others,hdfs,0,0
3239,dfs.namenode.max-num-blocks-to-log,Puts a limit on the number of blocks printed to the log by the Namenode after a block report.,0,0,others,hdfs,0,1
3240,dfs.namenode.metrics.logger.period.seconds,This setting controls how frequently the NameNode logs its metrics. The logging configuration must also define one or more appenders for NameNodeMetricsLog for the metrics to be logged. NameNode metrics logging is disabled if this value is set to zero or less than zero.,0,0,others,hdfs,0,1
3241,dfs.namenode.missing.checkpoint.periods.before.shutdown,The number of checkpoint period windows (as defined by the property dfs.namenode.checkpoint.period) allowed by the Namenode to perform saving the namespace before shutdown.,0,0,others,hdfs,0,0
3242,dfs.namenode.name.cache.threshold,Frequently accessed files that are accessed more times than this threshold are cached in the FSDirectory nameCache.,1,4,limited-side-effect,hdfs,0,0
3244,dfs.namenode.name.dir.restore,"Set to true to enable NameNode to attempt recovering a previously failed dfs.namenode.name.dir. When enabled, a recovery of any failed directory is attempted during checkpoint.",0,0,others,hdfs,0,0
3245,dfs.namenode.num.checkpoints.retained,The number of image checkpoint files (fsimage_*) that will be retained by the NameNode and Secondary NameNode in their storage directories. All edit logs (stored on edits_* files) necessary to recover an up-to-date namespace from the oldest retained checkpoint will also be retained.,1,3,reliability-tradeoff,hdfs,0,0
3246,dfs.namenode.num.extra.edits.retained,"The number of extra transactions which should be retained beyond what is minimally necessary for a NN restart. It does not translate directly to file's age, or the number of files kept, but to the number of transactions (here ""edits"" means transactions). One edit file may contain several transactions (edits). During checkpoint, NameNode will identify the total number of edits to retain as extra by checking the latest checkpoint transaction value, subtracted by the value of this property. Then, it scans edits files to identify the older ones that don't include the computed range of retained transactions that are to be kept around, and purges them subsequently. The retainment can be useful for audit purposes or for an HA setup where a remote Standby Node may have been offline for some time and need to have a longer backlog of retained edits in order to start again. Typically each edit is on the order of a few hundred bytes, so the default of 1 million edits should be on the order of hundreds of MBs or low GBs. NOTE: Fewer extra edits may be retained than value specified for this setting if doing so would mean that more segments would be retained than the number configured by dfs.namenode.max.extra.edits.segments.retained.",1,5,workload-specific,hdfs,0,0
3247,dfs.namenode.path.based.cache.block.map.allocation.percent,The percentage of the Java heap which we will allocate to the cached blocks map. The cached blocks map is a hash map which uses chained hashing. Smaller maps may be accessed more slowly if the number of cached blocks is large; larger maps will consume more memory.,1,5,workload-specific,hdfs,0,0
3248,dfs.namenode.path.based.cache.refresh.interval.ms,"The amount of milliseconds between subsequent path cache rescans. Path cache rescans are when we calculate which blocks should be cached, and on what datanodes. By default, this parameter is set to 30 seconds.",1,5,workload-specific,hdfs,0,0
3249,dfs.namenode.path.based.cache.retry.interval.ms,"When the NameNode needs to uncache something that is cached, or cache something that is not cached, it must direct the DataNodes to do so by sending a DNA_CACHE or DNA_UNCACHE command in response to a DataNode heartbeat. This parameter controls how frequently the NameNode will resend these commands.",0,0,others,hdfs,0,0
3250,dfs.namenode.plugins,Comma-separated list of namenode plug-ins to be activated.,0,0,others,hdfs,0,0
3252,dfs.namenode.provided.enabled,Enables the Namenode to handle provided storages.,0,0,others,hdfs,0,0
3253,dfs.namenode.quota.init-threads,"The number of concurrent threads to be used in quota initialization. The speed of quota initialization also affects the namenode fail-over latency. If the size of name space is big, try increasing this.",1,1,resource,hdfs,0,0
3254,dfs.namenode.read.considerLoad,Decide if sort block locations considers the target's load or not when read. Turn off by default.,1,4,limited-side-effect,hdfs,0,1
3255,dfs.namenode.read-lock-reporting-threshold-ms,"When a read lock is held on the namenode for a long time, this will be logged as the lock is released. This sets how long the lock must be held for logging to occur.",0,0,others,hdfs,0,0
3256,dfs.namenode.reconstruction.pending.timeout-sec,"Timeout in seconds for block reconstruction. If this value is 0 or less, then it will default to 5 minutes.",0,0,others,hdfs,0,0
3257,dfs.namenode.redundancy.considerLoad,Decide if chooseTarget considers the target's load or not when write. Turn on by default.,1,4,limited-side-effect,hdfs,0,0
3258,dfs.namenode.redundancy.considerLoad.factor,"The factor by which a node's load can exceed the average before being rejected for writes, only if considerLoad is true.",1,5,workload-specific,hdfs,0,0
3259,dfs.namenode.redundancy.interval.seconds,"The periodicity in seconds with which the namenode computes low redundancy work for datanodes. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.",0,0,others,hdfs,0,0
3260,dfs.namenode.redundancy.queue.restart.iterations,"When picking blocks from the low redundancy queues, reset the bookmarked iterator after the set number of iterations to ensure any blocks which were not processed on the first pass are retried before the iterators would naturally reach their end point. This ensures blocks are retried more frequently when there are many pending blocks or blocks are continuously added to the queues preventing the iterator reaching its natural endpoint. The default setting of 2400 combined with the default of dfs.namenode.redundancy.interval.seconds means the iterators will be reset approximately every 2 hours. Setting this parameter to zero disables the feature and the iterators will be reset only when the end of all queues has been reached.",1,3,reliability-tradeoff,hdfs,0,0
3261,dfs.namenode.reencrypt.batch.size,How many EDEKs should the re-encrypt thread process in one batch.,0,0,others,hdfs,0,0
3262,dfs.namenode.reencrypt.edek.threads,Maximum number of re-encrypt threads to contact the KMS and re-encrypt the edeks.,1,1,resource,hdfs,0,0
3263,dfs.namenode.reencrypt.sleep.interval,"Interval the re-encrypt EDEK thread sleeps in the main loop. The interval accepts units. If none given, millisecond is assumed.",0,0,others,hdfs,0,0
3264,dfs.namenode.reencrypt.throttle.limit.handler.ratio,"Throttling ratio for the re-encryption, indicating what fraction of time should the re-encrypt handler thread work under NN read lock. Larger than 1.0 values are interpreted as 1.0. Negative value or 0 are invalid values and will fail NN startup.",1,5,workload-specific,hdfs,0,0
3265,dfs.namenode.reencrypt.throttle.limit.updater.ratio,"Throttling ratio for the re-encryption, indicating what fraction of time should the re-encrypt updater thread work under NN write lock. Larger than 1.0 values are interpreted as 1.0. Negative value or 0 are invalid values and will fail NN startup.",1,5,workload-specific,hdfs,0,1
3266,dfs.namenode.reject-unresolved-dn-topology-mapping,"If the value is set to true, then namenode will reject datanode registration if the topology mapping for a datanode is not resolved and NULL is returned (script defined by net.topology.script.file.name fails to execute). Otherwise, datanode will be registered and the default rack will be assigned as the topology path. Topology paths are important for data resiliency, since they define fault domains. Thus it may be unwanted behavior to allow datanode registration with the default rack if the resolving topology failed.",0,0,others,hdfs,0,0
3267,dfs.namenode.replication.max-streams,Hard limit for the number of highest-priority replication streams.,1,3,reliability-tradeoff,hdfs,0,1
3268,dfs.namenode.replication.max-streams-hard-limit,Hard limit for all replication streams.,0,0,others,hdfs,0,1
3269,dfs.namenode.replication.min,Minimal block replication.,1,3,reliability-tradeoff,hdfs,0,1
3270,dfs.namenode.replication.work.multiplier.per.iteration,"*Note*: Advanced property. Change with caution. This determines the total amount of block transfers to begin in parallel at a DN, for replication, when such a command list is being sent over a DN heartbeat by the NN. The actual number is obtained by multiplying this multiplier with the total number of live nodes in the cluster. The result number is the number of blocks to begin transfers immediately for, per DN heartbeat. This number can be any positive, non-zero integer.",0,0,others,hdfs,0,1
3271,dfs.namenode.resource.check.interval,"The interval in milliseconds at which the NameNode resource checker runs. The checker calculates the number of the NameNode storage volumes whose available spaces are more than dfs.namenode.resource.du.reserved, and enters safemode if the number becomes lower than the minimum value specified by dfs.namenode.resource.checked.volumes.minimum.",0,0,others,hdfs,0,0
3272,dfs.namenode.resource.checked.volumes,A list of local directories for the NameNode resource checker to check in addition to the local edits directories.,0,0,others,hdfs,0,0
3273,dfs.namenode.resource.checked.volumes.minimum,The minimum number of redundant NameNode storage volumes required.,1,3,reliability-tradeoff,hdfs,0,1
3274,dfs.namenode.resource.du.reserved,"The amount of space to reserve/require for a NameNode storage directory in bytes. The default is 100MB. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize.",1,1,resource,hdfs,0,1
3275,dfs.namenode.retrycache.expirytime.millis,The time for which retry cache entries are retained.,1,5,workload-specific,hdfs,0,0
3276,dfs.namenode.retrycache.heap.percent,"This parameter configures the heap size allocated for retry cache (excluding the response cached). This corresponds to approximately 4096 entries for every 64MB of namenode process java heap size. Assuming retry cache entry expiration time (configured using dfs.namenode.retrycache.expirytime.millis) of 10 minutes, this enables retry cache to support 7 operations per second sustained for 10 minutes. As the heap size is increased, the operation rate linearly increases.",1,1,resource,hdfs,0,0
3280,dfs.namenode.safemode.extension,"Determines extension of safe mode in milliseconds after the threshold level is reached. Support multiple time unit suffix (case insensitive), as described in dfs.heartbeat.interval.",0,0,others,hdfs,0,0
3281,dfs.namenode.safemode.min.datanodes,Specifies the number of datanodes that must be considered alive before the name node exits safemode. Values less than or equal to 0 mean not to take the number of live datanodes into account when deciding whether to remain in safe mode during startup. Values greater than the number of datanodes in the cluster will make safe mode permanent.,0,0,others,hdfs,0,0
3282,dfs.namenode.safemode.replication.min,a separate minimum replication factor for calculating safe block count. This is an expert level setting. Setting this lower than the dfs.namenode.replication.min is not recommend and/or dangerous for production setups. When it's not set it takes value from dfs.namenode.replication.min,1,3,reliability-tradeoff,hdfs,0,0
3283,dfs.namenode.safemode.threshold-pct,Specifies the percentage of blocks that should satisfy the minimal replication requirement defined by dfs.namenode.replication.min. Values less than or equal to 0 mean not to wait for any particular percentage of blocks before exiting safemode. Values greater than 1 will make safe mode permanent.,1,5,workload-specific,hdfs,0,0
3286,dfs.namenode.send.qop.enabled,"A boolean specifies whether NameNode should encrypt the established QOP and include it in block token. The encrypted QOP will be used by DataNode as target QOP, overwriting DataNode configuration. This ensures DataNode will use exactly the same QOP NameNode and client has already agreed on.",1,2,security-tradeoff,hdfs,0,0
3287,dfs.namenode.service.handler.count,The number of Namenode RPC server threads that listen to requests from DataNodes and from all other non-client nodes. dfs.namenode.service.handler.count will be valid only if dfs.namenode.servicerpc-address is configured.,1,1,resource,hdfs,0,0
3290,dfs.namenode.shared.edits.dir,A directory on shared storage between the multiple namenodes in an HA cluster. This directory will be written by the active and read by the standby in order to keep the namespaces synchronized. This directory does not need to be listed in dfs.namenode.edits.dir above. It should be left empty in a non-HA cluster.,0,0,others,hdfs,0,0
3291,dfs.namenode.snapshot.capture.openfiles,"If true, snapshots taken will have an immutable shared copy of the open files that have valid leases. Even after the open files grow or shrink in size, snapshot will always have the previous point-in-time version of the open files, just like all other closed files. Default is false. Note: The file length captured for open files in snapshot is whats recorded in NameNode at the time of snapshot and it may be shorter than what the client has written till then. In order to capture the latest length, the client can call hflush/hsync with the flag SyncFlag.UPDATE_LENGTH on the open files handles.",0,0,others,hdfs,0,0
3292,dfs.namenode.snapshot.max.limit,"Limits the maximum number of snapshots allowed per snapshottable directory.If the configuration is not set, the default limit for maximum no of snapshots allowed is 65536.",0,0,others,hdfs,0,1
3297,dfs.namenode.snapshotdiff.listing.limit,"Limit the number of entries generated by getSnapshotDiffReportListing within one rpc call to the namenode.If less or equal to zero, at most DFS_NAMENODE_SNAPSHOT_DIFF_LISTING_LIMIT_DEFAULT (= 1000) will be sent across to the client within one rpc call.",0,0,others,hdfs,0,0
3298,dfs.namenode.stale.datanode.interval,"Default time interval in milliseconds for marking a datanode as ""stale"", i.e., if the namenode has not received heartbeat msg from a datanode for more than this time interval, the datanode will be marked and treated as ""stale"" by default. The stale interval cannot be too small since otherwise this may cause too frequent change of stale states. We thus set a minimum stale interval value (the default value is 3 times of heartbeat interval) and guarantee that the stale interval cannot be less than the minimum value. A stale data node is avoided during lease/block recovery. It can be conditionally avoided for reads (see dfs.namenode.avoid.read.stale.datanode) and for writes (see dfs.namenode.avoid.write.stale.datanode).",0,0,others,hdfs,0,1
3299,dfs.namenode.stale.datanode.minimum.interval,"Minimum number of missed heartbeats intervals for a datanode to be marked stale by the Namenode. The actual interval is calculated as (dfs.namenode.stale.datanode.minimum.interval * dfs.heartbeat.interval) in seconds. If this value is greater than the property dfs.namenode.stale.datanode.interval, then the calculated value above is used.",0,0,others,hdfs,0,1
3300,dfs.namenode.startup.delay.block.deletion.sec,"The delay in seconds at which we will pause the blocks deletion after Namenode startup. By default it's disabled. In the case a directory has large number of directories and files are deleted, suggested delay is one hour to give the administrator enough time to notice large number of pending deletion blocks and take corrective action.",0,0,others,hdfs,0,0
3301,dfs.namenode.state.context.enabled,"Whether enable namenode sending back its current txnid back to client. Setting this to true is required by Consistent Read from Standby feature. But for regular cases, this should be set to false to avoid the overhead of updating and maintaining this state.",1,3,reliability-tradeoff,hdfs,0,0
3303,dfs.namenode.storageinfo.defragment.interval.ms,The thread for checking the StorageInfo for defragmentation will run periodically. The time between runs is determined by this property.,0,0,others,hdfs,0,0
3304,dfs.namenode.storageinfo.defragment.ratio,The defragmentation threshold for the StorageInfo.,1,5,workload-specific,hdfs,0,1
3305,dfs.namenode.storageinfo.defragment.timeout.ms,Timeout value in ms for the StorageInfo compaction run.,0,0,others,hdfs,0,0
3306,dfs.namenode.support.allow.format,"Does HDFS namenode allow itself to be formatted? You may consider setting this to false for any production cluster, to avoid any possibility of formatting a running DFS.",1,5,workload-specific,hdfs,0,0
3307,dfs.namenode.top.enabled,Enable nntop: reporting top users on namenode,0,0,others,hdfs,0,0
3308,dfs.namenode.top.num.users,Number of top users returned by the top tool,0,0,others,hdfs,0,1
3309,dfs.namenode.top.window.num.buckets,Number of buckets in the rolling window implementation of nntop,0,0,others,hdfs,0,0
3310,dfs.namenode.top.windows.minutes,comma separated list of nntop reporting periods in minutes,0,0,others,hdfs,0,0
3311,dfs.namenode.upgrade.domain.factor,"This is valid only when block placement policy is set to BlockPlacementPolicyWithUpgradeDomain. It defines the number of unique upgrade domains any block's replicas should have. When the number of replicas is less or equal to this value, the policy ensures each replica has an unique upgrade domain. When the number of replicas is greater than this value, the policy ensures the number of unique domains is at least this value.",0,0,others,hdfs,0,0
3312,dfs.namenode.write.stale.datanode.ratio,"When the ratio of number stale datanodes to total datanodes marked is greater than this ratio, stop avoiding writing to stale nodes so as to prevent causing hotspots.",1,5,workload-specific,hdfs,0,0
3313,dfs.namenode.write-lock-reporting-threshold-ms,"When a write lock is held on the namenode for a long time, this will be logged as the lock is released. This sets how long the lock must be held for logging to occur.",0,0,others,hdfs,0,0
3314,dfs.namenode.xattrs.enabled,Whether support for extended attributes is enabled on the NameNode.,0,0,others,hdfs,0,0
3316,dfs.nameservices,Comma-separated list of nameservices.,0,0,others,hdfs,0,0
3317,dfs.net.topology.impl,"The implementation class of NetworkTopology used in HDFS. By default, the class org.apache.hadoop.hdfs.net.DFSNetworkTopology is specified and used in block placement. This property only works when dfs.use.dfs.network.topology is true.",0,0,others,hdfs,0,0
3319,dfs.permissions.ContentSummary.subAccess,"If ""true"", the ContentSummary permission checking will use subAccess. If ""false"", the ContentSummary permission checking will NOT use subAccess. subAccess means using recursion to check the access of all descendants.",1,2,security-tradeoff,hdfs,0,0
3320,dfs.permissions.enabled,"If ""true"", enable permission checking in HDFS. If ""false"", permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories.",0,0,others,hdfs,0,0
3322,dfs.pipeline.ecn,"If true, allows ECN (explicit congestion notification) from the Datanode.",1,4,limited-side-effect,hdfs,0,1
3323,dfs.provided.acls.import.enabled,"Set to true to inherit ACLs (Access Control Lists) from remote stores during mount. Disabled by default, i.e., ACLs are not inherited from remote stores. Note had HDFS ACLs have to be enabled (dfs.namenode.acls.enabled must be set to true) for this to take effect.",0,0,others,hdfs,0,0
3325,dfs.provided.aliasmap.inmemory.batch-size,The batch size when iterating over the database backing the aliasmap,1,5,workload-specific,hdfs,0,0
3327,dfs.provided.aliasmap.inmemory.enabled,Don't use the aliasmap by default. Some tests will fail because they try to start the namenode twice with the same parameters if you turn it on.,1,6,function-tradeoff,hdfs,0,0
3329,dfs.provided.aliasmap.inmemory.rpc.bind-host,"The actual address the in-memory aliasmap server will bind to. If this optional address is set, it overrides the hostname portion of dfs.provided.aliasmap.inmemory.rpc.address. This is useful for making the name node listen on all interfaces by setting it to 0.0.0.0.",0,0,others,hdfs,0,1
3330,dfs.provided.aliasmap.inmemory.server.log,Ensures that InMemoryAliasMap server logs every call to it. Set to false by default.,1,6,function-tradeoff,hdfs,0,0
3332,dfs.provided.aliasmap.load.retries,The number of retries on the Datanode to load the provided aliasmap; defaults to 0.,0,0,others,hdfs,0,0
3333,dfs.provided.aliasmap.text.codec,The codec used to de-compress the provided block map.,1,6,function-tradeoff,hdfs,0,0
3338,dfs.qjm.operations.timeout,Common key to set timeout for related operations in QuorumJournalManager. This setting supports multiple time unit suffixes as described in dfs.heartbeat.interval. If no suffix is specified then milliseconds is assumed.,0,0,others,hdfs,0,0
3339,dfs.qjournal.accept-recovery.timeout.ms,Quorum timeout in milliseconds during accept phase of recovery/synchronization for a specific segment.,0,0,others,hdfs,0,0
3340,dfs.qjournal.finalize-segment.timeout.ms,Quorum timeout in milliseconds during finalizing for a specific segment.,0,0,others,hdfs,0,0
3341,dfs.qjournal.get-journal-state.timeout.ms,Timeout in milliseconds when calling getJournalState(). JournalNodes.,0,0,others,hdfs,0,0
3342,dfs.qjournal.http.open.timeout.ms,Timeout in milliseconds when open a new HTTP connection to remote journals.,0,0,others,hdfs,0,0
3343,dfs.qjournal.http.read.timeout.ms,Timeout in milliseconds when reading from a HTTP connection from remote journals.,0,0,others,hdfs,0,0
3344,dfs.qjournal.new-epoch.timeout.ms,Timeout in milliseconds when getting an epoch number for write access to JournalNodes.,0,0,others,hdfs,0,0
3345,dfs.qjournal.parallel-read.num-threads,Number of threads per JN to be used for tailing edits.,1,1,resource,hdfs,0,0
3346,dfs.qjournal.prepare-recovery.timeout.ms,Quorum timeout in milliseconds during preparation phase of recovery/synchronization for a specific segment.,0,0,others,hdfs,0,0
3347,dfs.qjournal.queued-edits.limit.mb,Queue size in MB for quorum journal edits.,1,5,workload-specific,hdfs,0,0
3348,dfs.qjournal.select-input-streams.timeout.ms,Timeout in milliseconds for accepting streams from JournalManagers.,0,0,others,hdfs,0,1
3349,dfs.qjournal.start-segment.timeout.ms,Quorum timeout in milliseconds for starting a log segment.,0,0,others,hdfs,0,1
3350,dfs.qjournal.write-txns.timeout.ms,Write timeout in milliseconds when writing to a quorum of remote journals.,0,0,others,hdfs,0,0
3351,dfs.quota.by.storage.type.enabled,"If true, enables quotas based on storage type.",1,4,limited-side-effect,hdfs,0,1
3352,dfs.reformat.disabled,"Disable reformat of NameNode. If it's value is set to ""true"" and metadata directories already exist then attempt to format NameNode will throw NameNodeFormatException.",0,0,others,hdfs,0,0
3353,dfs.replication,Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time.,1,3,reliability-tradeoff,hdfs,0,0
3354,dfs.replication.max,Maximal block replication.,1,3,reliability-tradeoff,hdfs,0,0
3355,dfs.secondary.namenode.kerberos.internal.spnego.principal,"The server principal used by the Secondary NameNode for web UI SPNEGO authentication when Kerberos security is enabled. Like all other Secondary NameNode settings, it is ignored in an HA setup. If the value is '*', the web server will attempt to login with every principal specified in the keytab file dfs.web.authentication.kerberos.keytab.",0,0,others,hdfs,0,0
3358,dfs.short.circuit.shared.memory.watcher.interrupt.check.ms,The length of time in milliseconds that the short-circuit shared memory watcher will go between checking for java interruptions sent from other threads. This is provided mainly for unit tests.,0,0,others,hdfs,0,0
3359,dfs.storage.policy.enabled,Allow users to change the storage policy on files and directories.,0,0,others,hdfs,0,0
3360,dfs.storage.policy.permissions.superuser-only,Allow only superuser role to change the storage policy on files and directories.,0,0,others,hdfs,0,0
3361,dfs.storage.policy.satisfier.address,The hostname used for a keytab based Kerberos login. Keytab based login is required when dfs.storage.policy.satisfier.mode is external.,0,0,others,hdfs,0,0
3362,dfs.storage.policy.satisfier.datanode.cache.refresh.interval.ms,"How often to refresh the datanode storages cache in milliseconds. This cache keeps live datanode storage reports fetched from namenode. After elapsed time, it will again fetch latest datanodes from namenode. By default, this parameter is set to 5 minutes.",1,5,workload-specific,hdfs,0,0
3363,dfs.storage.policy.satisfier.enabled,"Following values are supported - external, none. If external, StoragePolicySatisfier will be enabled and started as an independent service outside namenode. If none, StoragePolicySatisfier is disabled. By default, StoragePolicySatisfier is disabled. Administrator can dynamically change StoragePolicySatisfier mode by using reconfiguration option. Dynamic mode change can be achieved in the following way. 1. Edit/update this configuration property values in hdfs-site.xml 2. Execute the reconfig command on hadoop command line prompt. For example:$hdfs -reconfig namenode nn_host:port start",0,0,others,hdfs,0,1
3364,dfs.storage.policy.satisfier.kerberos.principal,The StoragePolicySatisfier principal. This is typically set to satisfier/_HOST@REALM.TLD. The StoragePolicySatisfier will substitute _HOST with its own fully qualified hostname at startup. The _HOST placeholder allows using the same configuration setting on different servers. Keytab based login is required when dfs.storage.policy.satisfier.mode is external.,0,0,others,hdfs,0,1
3366,dfs.storage.policy.satisfier.max.outstanding.paths,Defines the maximum number of paths to satisfy that can be queued up in the Satisfier call queue in a period of time. Default value is 10000.,1,5,workload-specific,hdfs,0,0
3367,dfs.storage.policy.satisfier.queue.limit,Storage policy satisfier queue size. This queue contains the currently scheduled file's inode ID for statisfy the policy. Default value is 1000.,0,0,others,hdfs,0,0
3368,dfs.storage.policy.satisfier.recheck.timeout.millis,Blocks storage movements monitor re-check interval in milliseconds. This check will verify whether any blocks storage movement results arrived from DN and also verify if any of file blocks movements not at all reported to DN since dfs.storage.policy.satisfier.self.retry.timeout. The default value is 1 * 60 * 1000 (1 mins),0,0,others,hdfs,0,0
3369,dfs.storage.policy.satisfier.retry.max.attempts,Max retry to satisfy the block storage policy. After this retry block will be removed from the movement needed queue.,0,0,others,hdfs,0,0
3370,dfs.storage.policy.satisfier.self.retry.timeout.millis,"If any of file related block movements not at all reported by datanode, then after this timeout(in milliseconds), the item will be added back to movement needed list at namenode which will be retried for block movements. The default value is 5 * 60 * 1000 (5 mins)",0,0,others,hdfs,0,0
3371,dfs.storage.policy.satisfier.work.multiplier.per.iteration,"*Note*: Advanced property. Change with caution. This determines the total amount of block transfers to begin in one iteration, for satisfy the policy. The actual number is obtained by multiplying this multiplier with the total number of live nodes in the cluster. The result number is the number of blocks to begin transfers immediately. This number can be any positive, non-zero integer.",1,5,workload-specific,hdfs,0,1
3372,dfs.stream-buffer-size,"The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",1,1,resource,hdfs,0,0
3373,dfs.trustedchannel.resolver.class,"TrustedChannelResolver is used to determine whether a channel is trusted for plain data transfer. The TrustedChannelResolver is invoked on both client and server side. If the resolver indicates that the channel is trusted, then the data transfer will not be encrypted even if dfs.encrypt.data.transfer is set to true. The default implementation returns false indicating that the channel is not trusted.",0,0,others,hdfs,0,0
3374,dfs.use.dfs.network.topology,"Enables DFSNetworkTopology to choose nodes for placing replicas. When enabled, NetworkTopology will be instantiated as class defined in property dfs.net.topology.impl, otherwise NetworkTopology will be instantiated as class defined in property net.topology.impl.",1,4,limited-side-effect,hdfs,0,0
3377,dfs.web.authentication.kerberos.principal,The server principal used by the NameNode for WebHDFS SPNEGO authentication. Required when WebHDFS and security are enabled. In most secure clusters this setting is also used to specify the values for dfs.namenode.kerberos.internal.spnego.principal and dfs.journalnode.kerberos.internal.spnego.principal.,0,0,others,hdfs,0,1
3378,dfs.web.authentication.simple.anonymous.allowed,"If true, allow anonymous user to access WebHDFS. Set to false to disable anonymous authentication.",0,0,others,hdfs,0,1
3379,dfs.web.ugi,dfs.web.ugi is deprecated. Use hadoop.http.staticuser.user instead.,0,0,others,hdfs,0,0
3380,dfs.webhdfs.acl.provider.permission.pattern,"Valid pattern for user and group names in webhdfs acl operations, it must be a valid java regex.",0,0,others,hdfs,0,0
3381,dfs.webhdfs.netty.high.watermark,High watermark configuration to Netty for Datanode WebHdfs.,1,5,workload-specific,hdfs,0,0
3382,dfs.webhdfs.netty.low.watermark,Low watermark configuration to Netty for Datanode WebHdfs.,1,5,workload-specific,hdfs,0,0
3383,dfs.webhdfs.oauth2.access.token.provider,Access token provider class for WebHDFS using OAuth2. Defaults to org.apache.hadoop.hdfs.web.oauth2.ConfCredentialBasedAccessTokenProvider.,0,0,others,hdfs,0,0
3385,dfs.webhdfs.oauth2.enabled,"If true, enables OAuth2 in WebHDFS",1,2,security-tradeoff,hdfs,0,0
3387,dfs.webhdfs.rest-csrf.browser-useragents-regex,"A comma-separated list of regular expressions used to match against an HTTP request's User-Agent header when protection against cross-site request forgery (CSRF) is enabled for WebHDFS by setting dfs.webhdfs.reset-csrf.enabled to true. If the incoming User-Agent matches any of these regular expressions, then the request is considered to be sent by a browser, and therefore CSRF prevention is enforced. If the request's User-Agent does not match any of these regular expressions, then the request is considered to be sent by something other than a browser, such as scripted automation. In this case, CSRF is not a potential attack vector, so the prevention is not enforced. This helps achieve backwards-compatibility with existing automation that has not been updated to send the CSRF prevention header.",0,0,others,hdfs,0,1
3388,dfs.webhdfs.rest-csrf.custom-header,The name of a custom header that HTTP requests must send when protection against cross-site request forgery (CSRF) is enabled for WebHDFS by setting dfs.webhdfs.rest-csrf.enabled to true. The WebHDFS client also uses this property to determine whether or not it needs to send the custom CSRF prevention header in its HTTP requests.,0,0,others,hdfs,0,1
3389,dfs.webhdfs.rest-csrf.enabled,"If true, then enables WebHDFS protection against cross-site request forgery (CSRF). The WebHDFS client also uses this property to determine whether or not it needs to send the custom CSRF prevention header in its HTTP requests.",0,0,others,hdfs,0,0
3390,dfs.webhdfs.rest-csrf.methods-to-ignore,A comma-separated list of HTTP methods that do not require HTTP requests to include a custom header when protection against cross-site request forgery (CSRF) is enabled for WebHDFS by setting dfs.webhdfs.rest-csrf.enabled to true. The WebHDFS client also uses this property to determine whether or not it needs to send the custom CSRF prevention header in its HTTP requests.,1,2,security-tradeoff,hdfs,0,0
3393,dfs.webhdfs.ugi.expire.after.access,"How long in milliseconds after the last access the cached UGI will expire. With 0, never expire.",0,0,others,hdfs,0,1
3394,dfs.webhdfs.use.ipc.callq,Enables routing of webhdfs calls through rpc call queue,1,6,function-tradeoff,hdfs,0,1
3395,dfs.webhdfs.user.provider.user.pattern,"Valid pattern for user and group names for webhdfs, it must be a valid java regex.",0,0,others,hdfs,0,1
3396,dfs.xframe.enabled,"If true, then enables protection against clickjacking by returning X_FRAME_OPTIONS header value set to SAMEORIGIN. Clickjacking protection prevents an attacker from using transparent or opaque layers to trick a user into clicking on a button or link on another page.",0,0,others,hdfs,0,0
3397,dfs.xframe.value,"This configration value allows user to specify the value for the X-FRAME-OPTIONS. The possible values for this field are DENY, SAMEORIGIN and ALLOW-FROM. Any other value will throw an exception when namenode and datanodes are starting up.",1,3,reliability-tradeoff,hdfs,0,0
3398,hadoop.fuse.connection.timeout,The minimum number of seconds that we'll cache libhdfs connection objects in fuse_dfs. Lower values will result in lower memory consumption; higher values may speed up access by avoiding the overhead of creating new connection objects.,0,0,others,hdfs,0,1
3399,hadoop.fuse.timer.period,The number of seconds between cache expiry checks in fuse_dfs. Lower values will result in fuse_dfs noticing changes to Kerberos ticket caches more quickly.,0,0,others,hdfs,0,1
3401,hadoop.user.group.metrics.percentiles.intervals,"A comma-separated list of the granularity in seconds for the metrics which describe the 50/75/90/95/99th percentile latency for group resolution in milliseconds. By default, percentile latency metrics are disabled.",0,0,others,hdfs,0,1
3402,httpfs.buffer.size,The size buffer to be used when creating or opening httpfs filesystem IO stream.,1,1,resource,hdfs,0,0
3404,nfs.dump.dir,"This directory is used to temporarily save out-of-order writes before writing to HDFS. For each file, the out-of-order writes are dumped after they are accumulated to exceed certain threshold (e.g., 1MB) in memory. One needs to make sure the directory has enough space.",0,0,others,hdfs,0,0
3407,nfs.mountd.port,Specify the port number used by Hadoop mount daemon.,0,0,others,hdfs,0,0
3408,nfs.rtmax,"This is the maximum size in bytes of a READ request supported by the NFS gateway. If you change this, make sure you also update the nfs mount's rsize(add rsize= # of bytes to the mount directive).",1,1,resource,hdfs,0,0
3409,nfs.server.port,Specify the port number used by Hadoop NFS.,0,0,others,hdfs,0,1
3410,nfs.wtmax,"This is the maximum size in bytes of a WRITE request supported by the NFS gateway. If you change this, make sure you also update the nfs mount's wsize(add wsize= # of bytes to the mount directive).",1,1,resource,hdfs,0,0
3415,ssl.server.truststore.password,Truststore password for HTTPS SSL configuration,0,0,others,hdfs,0,1
3416,AcceptFilter,"This directive enables operating system specific optimizations for a listening socket by the Protocol type. The basic premise is for the kernel to not send a socket to the server process until either data is received or an entire HTTP Request is buffered. Only FreeBSD's Accept Filters, Linux's more primitive TCP_DEFER_ACCEPT, and Windows' optimized AcceptEx() are currently supported.",1,4,limited-side-effect,httpd,0,1
3420,AddAlt,"AddAlt provides the alternate text to display for a file, instead of an icon, for FancyIndexing. File is a file extension, partial filename, wild-card expression or full filename for files to describe. If String contains any whitespace, you have to enclose it in quotes ("" or '). This alternate text is displayed if the client is image-incapable, has image loading disabled, or fails to retrieve the icon.",0,0,others,httpd,0,0
3421,AddAltByEncoding,"AddAltByEncoding provides the alternate text to display for a file, instead of an icon, for FancyIndexing. MIME-encoding is a valid content-encoding, such as x-compress. If String contains any whitespace, you have to enclose it in quotes ("" or '). This alternate text is displayed if the client is image-incapable, has image loading disabled, or fails to retrieve the icon.",0,0,others,httpd,0,1
3422,AddAltByType,"AddAltByType sets the alternate text to display for a file, instead of an icon, for FancyIndexing. MIME-type is a valid content-type, such as text/html. If String contains any whitespace, you have to enclose it in quotes ("" or '). This alternate text is displayed if the client is image-incapable, has image loading disabled, or fails to retrieve the icon.",0,0,others,httpd,0,0
3426,AddEncoding,"The AddEncoding directive maps the given filename extensions to the specified HTTP content-encoding. encoding is the HTTP content coding to append to the value of the Content-Encoding header field for documents named with the extension. This mapping is added to any already in force, overriding any mappings that already exist for the same extension.",0,0,others,httpd,0,0
3431,AddInputFilter,"AddInputFilter maps the filename extension extension to the filters which will process client requests and POST input when they are received by the server. This is in addition to any filters defined elsewhere, including the SetInputFilter directive. This mapping is merged over any already in force, overriding any mappings that already exist for the same extension.",0,0,others,httpd,0,0
3432,AddLanguage,The AddLanguage directive maps the given filename extension to the specified content language. Files with the filename extension are assigned an HTTP Content-Language value of language-tag corresponding to the language identifiers defined by RFC 3066. This directive overrides any mappings that already exist for the same extension.,0,0,others,httpd,0,0
3434,AddOutputFilter,"The AddOutputFilter directive maps the filename extension extension to the filters which will process responses from the server before they are sent to the client. This is in addition to any filters defined elsewhere, including SetOutputFilter and AddOutputFilterByType directive. This mapping is merged over any already in force, overriding any mappings that already exist for the same extension.",0,0,others,httpd,0,0
3435,AddOutputFilterByType,This directive activates a particular output filter for a request depending on the response media-type.,0,0,others,httpd,0,0
3436,AddType,"The AddType directive maps the given filename extensions onto the specified content type. media-type is the media type to use for filenames containing extension. This mapping is added to any already in force, overriding any mappings that already exist for the same extension.",0,0,others,httpd,0,0
3439,AllowCONNECT,The AllowCONNECT directive specifies a list of port numbers or ranges to which the proxy CONNECT method may connect. Today's browsers use this method when a https connection is requested and proxy tunneling over HTTP is in effect.,0,0,others,httpd,0,1
3441,AsyncRequestWorkerFactor,"The event MPM handles some connections in an asynchronous way, where request worker threads are only allocated for short periods of time as needed, and other connections with one request worker thread reserved per connection. This can lead to situations where all workers are tied up and no worker thread is available to handle new work on established async connections.",1,3,reliability-tradeoff,httpd,0,1
3442,AuthDigestShmemSize,"The AuthDigestShmemSize directive defines the amount of shared memory, that will be allocated at the server startup for keeping track of clients. Note that the shared memory segment cannot be set less than the space that is necessary for tracking at least one client. This value is dependent on your system. If you want to find out the exact value, you may simply set AuthDigestShmemSize to the value of 0 and read the error message after trying to start the server.",1,1,resource,httpd,0,0
3444,AuthnCacheEnable,"This directive is not normally necessary: it is implied if authentication caching is enabled anywhere in httpd.conf. However, if it is not enabled anywhere in httpd.conf it will by default not be initialised, and is therefore not available in a .htaccess context. This directive ensures it is initialised so it can be used in .htaccess.",0,0,others,httpd,0,0
3445,AuthnCacheSOCache,"This is a server-wide setting to select a provider for the shared object cache, followed by optional arguments for that provider. Some possible values for provider-name are ""dbm"", ""dc"", ""memcache"", or ""shmcb"", each subject to the appropriate module being loaded. If not set, your platform's default will be used.",0,0,others,httpd,0,0
3446,AuthnProviderAlias,<AuthnProviderAlias> and </AuthnProviderAlias> are used to enclose a group of authentication directives that can be referenced by the alias name using one of the directives AuthBasicProvider or AuthDigestProvider.,0,0,others,httpd,0,1
3447,AuthnzFcgiDefineProvider,This directive is used to define a FastCGI application as a provider for a particular phase of authentication or authorization.,0,0,others,httpd,0,0
3448,AuthzProviderAlias,<AuthzProviderAlias> and </AuthzProviderAlias> are used to enclose a group of authorization directives that can be referenced by the alias name using the directive Require.,0,0,others,httpd,0,1
3449,BalancerGrowth,This directive allows for growth potential in the number of Balancers available for a virtualhost in addition to the number pre-configured. It only takes effect if there is at least one pre-configured Balancer.,0,0,others,httpd,0,0
3450,BalancerInherit,"This directive will cause the current server/vhost to ""inherit"" ProxyPass Balancers and Workers defined in the main server. This can cause issues and inconsistent behavior if using the Balancer Manager and so should be disabled if using that feature.",0,0,others,httpd,0,0
3451,BalancerPersist,This directive will cause the shared memory storage associated with the balancers and balancer members to be persisted across restarts. This allows these local changes to not be lost during the normal restart/graceful state transitions.,1,4,limited-side-effect,httpd,0,0
3452,BrotliAlterETag,The BrotliAlterETag directive specifies how the ETag hader should be altered when a response is compressed.,0,0,others,httpd,0,0
3453,BrotliCompressionMaxInputBlock,"The BrotliCompressionMaxInputBlock directive specifies the maximum input block size between 16 and 24, with the caveat that larger block sizes require more memory.",1,1,resource,httpd,0,0
3454,BrotliCompressionQuality,"The BrotliCompressionQuality directive specifies the compression quality (a value between 0 and 11). Higher quality values result in better, but also slower compression.",1,6,function-tradeoff,httpd,0,1
3455,BrotliCompressionWindow,"The BrotliCompressionWindow directive specifies the brotli sliding compression window size (a value between 10 and 24). Larger window sizes can improve compression quality, but require more memory.",1,6,function-tradeoff,httpd,0,0
3456,BrotliFilterNote,The BrotliFilterNote directive specifies that a note about compression ratios should be attached to the request. The name of the note is the value specified for the directive. You can use that note for statistical purposes by adding the value to your access log.,0,0,others,httpd,0,0
3457,BrowserMatch,The BrowserMatch is a special cases of the SetEnvIf directive that sets environment variables conditional on the User-Agent HTTP request header. The following two lines have the same effect:,0,0,others,httpd,0,1
3458,BrowserMatchNoCase,"The BrowserMatchNoCase directive is semantically identical to the BrowserMatch directive. However, it provides for case-insensitive matching. For example:",0,0,others,httpd,0,0
3459,BufferedLogs,"The BufferedLogs directive causes mod_log_config to store several log entries in memory and write them together to disk, rather than writing them after each request. On some systems, this may result in more efficient disk access and hence higher performance. It may be set only once for the entire server; it cannot be configured per virtual-host.",1,4,limited-side-effect,httpd,0,0
3460,BufferSize,The BufferSize directive specifies the amount of data in bytes that will be buffered before being read from or written to each request. The default is 128 kilobytes.,1,1,resource,httpd,0,1
3461,CacheDefaultExpire,"The CacheDefaultExpire directive specifies a default time, in seconds, to cache a document if neither an expiry date nor last-modified date are provided with the document. The value specified with the CacheMaxExpire directive does not override this setting.",1,5,workload-specific,httpd,0,0
3462,CacheDetailHeader,"When the CacheDetailHeader directive is switched on, an X-Cache-Detail header will be added to the response containing the detailed reason for a particular caching decision.",1,4,limited-side-effect,httpd,0,0
3463,CacheDirLength,The CacheDirLength directive sets the number of characters for each subdirectory name in the cache hierarchy. It can be used in conjunction with CacheDirLevels to determine the approximate structure of your cache hierarchy.,1,1,resource,httpd,0,0
3464,CacheDirLevels,The CacheDirLevels directive sets the number of subdirectory levels in the cache. Cached data will be saved this many directory levels below the CacheRoot directory.,1,1,resource,httpd,0,1
3465,CacheDisable,The CacheDisable directive instructs mod_cache to not cache urls at or below url-string.,1,4,limited-side-effect,httpd,0,0
3466,CacheEnable,The CacheEnable directive instructs mod_cache to cache urls at or below url-string. The cache storage manager is specified with the cache_type argument. The CacheEnable directive can alternatively be placed inside either <Location> or <LocationMatch> sections to indicate the content is cacheable. cache_type disk instructs mod_cache to use the disk based storage manager implemented by mod_cache_disk. cache_type socache instructs mod_cache to use the shared object cache based storage manager implemented by mod_cache_socache.,1,4,limited-side-effect,httpd,0,0
3467,CacheFile,"The CacheFile directive opens handles to one or more files (given as whitespace separated arguments) and places these handles into the cache at server startup time. Handles to cached files are automatically closed on a server shutdown. When the files have changed on the filesystem, the server should be restarted to re-cache them.",0,0,others,httpd,0,0
3468,CacheHeader,"When the CacheHeader directive is switched on, an X-Cache header will be added to the response with the cache status of this response. If the normal handler is used, this directive may appear within a <Directory> or <Location> directive. If the quick handler is used, this directive must appear within a server or virtual host context, otherwise the setting will be ignored.",1,4,limited-side-effect,httpd,0,0
3469,CacheIgnoreCacheControl,"Ordinarily, requests containing a Cache-Control: no-cache or Pragma: no-cache header value will not be served from the cache. The CacheIgnoreCacheControl directive allows this behavior to be overridden. CacheIgnoreCacheControl On tells the server to attempt to serve the resource from the cache even if the request contains no-cache header values. Resources requiring authorization will never be cached.",1,4,limited-side-effect,httpd,0,1
3470,CacheIgnoreHeaders,"According to RFC 2616, hop-by-hop HTTP headers are not stored in the cache. The following HTTP headers are hop-by-hop headers and thus do not get stored in the cache in any case regardless of the setting of CacheIgnoreHeaders:",1,4,limited-side-effect,httpd,0,0
3471,CacheIgnoreNoLastMod,"Ordinarily, documents without a last-modified date are not cached. Under some circumstances the last-modified date is removed (during mod_include processing for example) or not provided at all. The CacheIgnoreNoLastMod directive provides a way to specify that documents without last-modified dates should be considered for caching, even without a last-modified date. If neither a last-modified date nor an expiry date are provided with the document then the value specified by the CacheDefaultExpire directive will be used to generate an expiration date.",1,4,limited-side-effect,httpd,0,0
3472,CacheIgnoreQueryString,"Ordinarily, requests with query string parameters are cached separately for each unique query string. This is according to RFC 2616/13.9 done only if an expiration time is specified. The CacheIgnoreQueryString directive tells the cache to cache requests even if no expiration time is specified, and to reply with a cached reply even if the query string differs. From a caching point of view the request is treated as if having no query string when this directive is enabled.",1,4,limited-side-effect,httpd,0,0
3473,CacheIgnoreURLSessionIdentifiers,"This causes cacheable resources to be stored separately for each session, which is often not desired. CacheIgnoreURLSessionIdentifiers lets define a list of identifiers that are removed from the key that is used to identify an entity in the cache, such that cacheable resources are not stored separately for each session.",1,6,function-tradeoff,httpd,0,0
3474,CacheKeyBaseURL,"When the CacheKeyBaseURL directive is specified, the URL provided will be used as the base URL to calculate the URL of the cache keys in the reverse proxy configuration. When not specified, the scheme, hostname and port of the current virtual host is used to construct the cache key. When a cluster of machines is present, and all cached entries should be cached beneath the same cache key, a new base URL can be specified with this directive.",0,0,others,httpd,0,0
3477,CacheLockMaxAge,The CacheLockMaxAge directive specifies the maximum age of any cache lock.,0,0,others,httpd,0,0
3478,CacheLockPath,"The CacheLockPath directive allows you to specify the directory in which the locks are created. By default, the system's temporary folder is used. Locks consist of empty files that only exist for stale URLs in flight, so is significantly less resource intensive than the traditional disk cache.",0,0,others,httpd,0,0
3479,CacheMaxExpire,"The CacheMaxExpire directive specifies the maximum number of seconds for which cacheable HTTP documents will be retained without checking the origin server. Thus, documents will be out of date at most this number of seconds. This maximum value is enforced even if an expiry date was supplied with the document.",1,5,workload-specific,httpd,0,0
3480,CacheMaxFileSize,"The CacheMaxFileSize directive sets the maximum size, in bytes, for a document to be considered for storage in the cache.",1,1,resource,httpd,0,0
3481,CacheMinExpire,The CacheMinExpire directive specifies the minimum number of seconds for which cacheable HTTP documents will be retained without checking the origin server. This is only used if no valid expire time was supplied with the document.,1,5,workload-specific,httpd,0,0
3482,CacheMinFileSize,"The CacheMinFileSize directive sets the minimum size, in bytes, for a document to be considered for storage in the cache.",1,1,resource,httpd,0,0
3483,CacheNegotiatedDocs,"If set, this directive allows content-negotiated documents to be cached by proxy servers. This could mean that clients behind those proxys could retrieve versions of the documents that are not the best match for their abilities, but it will make caching more efficient.",1,4,limited-side-effect,httpd,0,0
3484,CacheQuickHandler,The CacheQuickHandler directive controls the phase in which the cache is handled.,0,0,others,httpd,0,1
3485,CacheReadSize,"The CacheReadSize directive sets the minimum amount of data, in bytes, to be read from the backend before the data is sent to the client. The default of zero causes all data read of any size to be passed downstream to the client immediately as it arrives. Setting this to a higher value causes the disk cache to buffer at least this amount before sending the result to the client. This can improve performance when caching content from a reverse proxy.",1,1,resource,httpd,0,0
3486,CacheReadTime,"The CacheReadTime directive sets the minimum amount of elapsed time that should pass before making an attempt to send data downstream to the client. During the time period, data will be buffered before sending the result to the client. This can improve performance when caching content from a reverse proxy.",1,5,workload-specific,httpd,0,0
3487,CacheRoot,"The CacheRoot directive defines the name of the directory on the disk to contain cache files. If the mod_cache_disk module has been loaded or compiled in to the Apache server, this directive must be defined. Failing to provide a value for CacheRoot will result in a configuration file processing error. The CacheDirLevels and CacheDirLength directives define the structure of the directories under the specified root directory.",0,0,others,httpd,0,1
3488,CacheSocache,"The CacheSocache directive defines the name of the shared object cache implementation to use, followed by optional arguments for that implementation. A number of implementations of shared object caches are available to choose from.",0,0,others,httpd,0,0
3489,CacheSocacheMaxSize,"The CacheSocacheMaxSize directive sets the maximum size, in bytes, for the combined headers and body of a document to be considered for storage in the cache. The larger the headers that are stored alongside the body, the smaller the body may be.",1,1,resource,httpd,0,1
3490,CacheSocacheMaxTime,"The CacheSocacheMaxTime directive sets the maximum freshness lifetime, in seconds, for a document to be stored in the cache. This value overrides the freshness lifetime defined for the document by the HTTP protocol.",1,5,workload-specific,httpd,0,0
3491,CacheSocacheMinTime,"The CacheSocacheMinTime directive sets the amount of seconds beyond the freshness lifetime of the response that the response should be cached for in the shared object cache. If a response is only stored for its freshness lifetime, there will be no opportunity to revalidate the response to make it fresh again.",1,5,workload-specific,httpd,0,0
3492,CacheSocacheReadSize,"The CacheSocacheReadSize directive sets the minimum amount of data, in bytes, to be read from the backend before the data is sent to the client. The default of zero causes all data read of any size to be passed downstream to the client immediately as it arrives. Setting this to a higher value causes the disk cache to buffer at least this amount before sending the result to the client. This can improve performance when caching content from a slow reverse proxy.",1,1,resource,httpd,0,0
3493,CacheSocacheReadTime,"The CacheSocacheReadTime directive sets the minimum amount of elapsed time that should pass before making an attempt to send data downstream to the client. During the time period, data will be buffered before sending the result to the client. This can improve performance when caching content from a reverse proxy.",1,5,workload-specific,httpd,0,0
3494,CacheStaleOnError,"When the CacheStaleOnError directive is switched on, and when stale data is available in the cache, the cache will respond to 5xx responses from the backend by returning the stale data instead of the 5xx response. While the Cache-Control headers sent by clients will be respected, and the raw 5xx responses returned to the client on request, the 5xx response so returned to the client will not invalidate the content in the cache.",1,3,reliability-tradeoff,httpd,0,0
3495,CacheStoreExpired,"Since httpd 2.2.4, responses which have already expired are not stored in the cache. The CacheStoreExpired directive allows this behavior to be overridden. CacheStoreExpired On tells the server to attempt to cache the resource if it is stale. Subsequent requests would trigger an If-Modified-Since request of the origin server, and the response may be fulfilled from cache if the backend resource has not changed.",1,5,workload-specific,httpd,0,1
3496,CacheStoreNoStore,"Ordinarily, requests or responses with Cache-Control: no-store header values will not be stored in the cache. The CacheStoreNoStore directive allows this behavior to be overridden. CacheStoreNoStore On tells the server to attempt to cache the resource even if it contains no-store header values. Resources requiring authorization will never be cached.",0,0,others,httpd,0,1
3497,CacheStorePrivate,"Ordinarily, responses with Cache-Control: private header values will not be stored in the cache. The CacheStorePrivate directive allows this behavior to be overridden. CacheStorePrivate On tells the server to attempt to cache the resource even if it contains private header values. Resources requiring authorization will never be cached.",0,0,others,httpd,0,0
3498,CGIDScriptTimeout,"This directive limits the length of time to wait for more output from the CGI program. If the time is exceeded, the request and CGI are terminated.",0,0,others,httpd,0,1
3499,CharsetDefault,The CharsetDefault directive specifies the charset that content in the associated container should be translated to.,0,0,others,httpd,0,0
3500,CharsetOptions,The CharsetOptions directive configures certain behaviors of mod_charset_lite. Option can be one of,0,0,others,httpd,0,1
3501,CharsetSourceEnc,The CharsetSourceEnc directive specifies the source charset of files in the associated container.,0,0,others,httpd,0,1
3502,CheckCaseOnly,"When set, this directive limits the action of the spelling correction to lower/upper case changes. Other potential corrections are not performed.",0,0,others,httpd,0,0
3503,CheckSpelling,This directive enables or disables the spelling module.,1,6,function-tradeoff,httpd,0,0
3505,ContentDigest,This directive enables the generation of Content-MD5 headers as defined in RFC1864 respectively RFC2616.,0,0,others,httpd,0,0
3507,CookieExpires,"When used, this directive sets an expiry time on the cookie generated by the usertrack module. The expiry-period can be given either as a number of seconds, or in the format such as ""2 weeks 3 days 7 hours"". Valid denominations are: years, months, weeks, days, hours, minutes and seconds. If the expiry time is in any format other than one number indicating the number of seconds, it must be enclosed by double quotes.",0,0,others,httpd,0,0
3508,CookieHTTPOnly,"When set to 'ON', the 'HTTPOnly' cookie attribute is added to this modules tracking cookie. This attribute instructs browsers to block javascript from reading the value of the cookie.",0,0,others,httpd,0,1
3510,CookieSameSite,"When set to 'None', 'Lax', or 'Strict', the 'SameSite' cookie attribute is added to this modules tracking cookie with the corresponding value. This attribute instructs browser on how to treat the cookie when it is requested in a cross-site context.",0,0,others,httpd,0,0
3511,CookieSecure,"When set to 'ON', the 'Secure' cookie attribute is added to this modules tracking cookie. This attribute instructs browsers to only transmit the cookie over HTTPS.",1,2,security-tradeoff,httpd,0,1
3512,CookieStyle,This directive controls the format of the cookie header field. The three formats allowed are:,0,0,others,httpd,0,1
3513,CookieTracking,"When mod_usertrack is loaded, and CookieTracking on is set, Apache will send a user-tracking cookie for all new requests. This directive can be used to turn this behavior on or off on a per-server or per-directory basis. By default, enabling mod_usertrack will not activate cookies.",1,6,function-tradeoff,httpd,0,1
3514,CoreDumpDirectory,"This controls the directory to which Apache httpd attempts to switch before dumping core. If your operating system is configured to create core files in the working directory of the crashing process, CoreDumpDirectory is necessary to change working directory from the default ServerRoot directory, which should not be writable by the user the server runs as.",0,0,others,httpd,0,1
3515,CustomLog,"The CustomLog directive is used to log requests to the server. A log format is specified, and the logging can optionally be made conditional on request characteristics using environment variables.",0,0,others,httpd,0,0
3516,DavDepthInfinity,"Use the DavDepthInfinity directive to allow the processing of PROPFIND requests containing the header 'Depth: Infinity'. Because this type of request could constitute a denial-of-service attack, by default it is not allowed.",0,0,others,httpd,0,0
3517,DavGenericLockDB,"Use the DavGenericLockDB directive to specify the full path to the lock database, excluding an extension. If the path is not absolute, it will be interpreted relative to ServerRoot. The implementation of mod_dav_lock uses a SDBM database to track user locks.",0,0,others,httpd,0,1
3518,DavLockDB,"Use the DavLockDB directive to specify the full path to the lock database, excluding an extension. If the path is not absolute, it will be taken relative to ServerRoot. The implementation of mod_dav_fs uses a SDBM database to track user locks.",0,0,others,httpd,0,0
3520,DBDExptime,Set the time to keep idle connections alive when the number of connections specified in DBDKeep has been exceeded (threaded platforms only).,0,0,others,httpd,0,0
3521,DBDInitSQL,"Modules, that wish it, can have one or more SQL statements executed when a connection to a database is created. Example usage could be initializing certain values or adding a log entry when a new connection is made to the database.",0,0,others,httpd,0,0
3522,DBDKeep,"Set the maximum number of connections per process to be sustained, other than for handling peak demand (threaded platforms only).",1,1,resource,httpd,0,1
3523,DBDMax,Set the hard maximum number of connections per process (threaded platforms only).,1,1,resource,httpd,0,1
3524,DBDMin,Set the minimum number of connections per process (threaded platforms only).,0,0,others,httpd,0,0
3525,DBDParams,"As required by the underlying driver. Typically this will be used to pass whatever cannot be defaulted amongst username, password, database name, hostname and port number for connection.",0,0,others,httpd,0,0
3526,DBDPersist,"If set to Off, persistent and pooled connections are disabled. A new database connection is opened when requested by a client, and closed immediately on release. This option is for debugging and low-usage servers.",1,3,reliability-tradeoff,httpd,0,1
3527,DBDPrepareSQL,"For modules such as authentication that repeatedly use a single SQL statement, optimum performance is achieved by preparing the statement at startup rather than every time it is used. This directive prepares an SQL statement and assigns it a label.",0,0,others,httpd,0,1
3531,DefaultRuntimeDir,"The DefaultRuntimeDir directive sets the directory in which the server will create various run-time files (shared memory, locks, etc.). If set as a relative path, the full path will be relative to ServerRoot.",0,0,others,httpd,0,0
3532,DefaultType,"This directive has been disabled. For backwards compatibility of configuration files, it may be specified with the value none, meaning no default media type. For example:",0,0,others,httpd,0,0
3533,Define,"In its one parameter form, Define is equivalent to passing the -D argument to httpd. It can be used to toggle the use of <IfDefine> sections without needing to alter -D arguments in any startup scripts.",0,0,others,httpd,0,1
3534,DeflateBufferSize,"The DeflateBufferSize directive specifies the size in bytes of the fragments that zlib should compress at one time. If the compressed response size is bigger than the one specified by this directive then httpd will switch to chunked encoding (HTTP header Transfer-Encoding set to Chunked), with the side effect of not setting any Content-Length HTTP header. This is particularly important when httpd works behind reverse caching proxies or when httpd is configured with mod_cache and mod_cache_disk because HTTP responses without any Content-Length header might not be cached.",1,1,resource,httpd,0,0
3535,DeflateCompressionLevel,"The DeflateCompressionLevel directive specifies what level of compression should be used, the higher the value, the better the compression, but the more CPU time is required to achieve this.",1,6,function-tradeoff,httpd,0,0
3536,DeflateFilterNote,The DeflateFilterNote directive specifies that a note about compression ratios should be attached to the request. The name of the note is the value specified for the directive. You can use that note for statistical purposes by adding the value to your access log.,0,0,others,httpd,0,0
3537,DeflateInflateLimitRequestBody,"The DeflateInflateLimitRequestBody directive specifies the maximum size of an inflated request body. If it is unset, LimitRequestBody is applied to the inflated body.",1,1,resource,httpd,0,1
3538,DeflateInflateRatioBurst,The DeflateInflateRatioBurst directive specifies the maximum number of times the DeflateInflateRatioLimit can be crossed before terminating the request.,0,0,others,httpd,0,0
3539,DeflateInflateRatioLimit,"The DeflateInflateRatioLimit directive specifies the maximum ratio of deflated to inflated size of an inflated request body. This ratio is checked as the body is streamed in, and if crossed more than DeflateInflateRatioBurst times, the request will be terminated.",0,0,others,httpd,0,0
3540,DeflateMemLevel,The DeflateMemLevel directive specifies how much memory should be used by zlib for compression (a value between 1 and 9).,1,6,function-tradeoff,httpd,0,0
3541,DeflateWindowSize,"The DeflateWindowSize directive specifies the zlib compression window size (a value between 1 and 15). Generally, the higher the window size, the higher can the compression ratio be expected.",1,6,function-tradeoff,httpd,0,1
3543,DirectoryCheckHandler,"The DirectoryCheckHandler directive determines whether mod_dir should check for directory indexes or add trailing slashes when some other handler has been configured for the current URL. Handlers can be set by directives such as SetHandler or by other modules, such as mod_rewrite during per-directory substitutions.",0,0,others,httpd,0,0
3545,DirectoryIndexRedirect,"By default, the DirectoryIndex is selected and returned transparently to the client. DirectoryIndexRedirect causes an external redirect to instead be issued.",0,0,others,httpd,0,0
3549,DTracePrivileges,"This server-wide directive determines whether Apache will run with the privileges required to run dtrace. Note that DTracePrivileges On will not in itself activate DTrace, but DTracePrivileges Off will prevent it working.",1,6,function-tradeoff,httpd,0,0
3550,DumpIOInput,Enable dumping of all input.,1,6,function-tradeoff,httpd,0,0
3551,DumpIOOutput,Enable dumping of all output.,1,6,function-tradeoff,httpd,0,0
3552,Else,The <Else> applies the enclosed directives if and only if the most recent <If> or <ElseIf> section in the same scope has not been applied. For example: In,0,0,others,httpd,0,1
3553,ElseIf,The <ElseIf> applies the enclosed directives if and only if both the given condition evaluates to true and the most recent <If> or <ElseIf> section in the same scope has not been applied. For example: In,0,0,others,httpd,0,0
3554,EnableExceptionHook,For safety reasons this directive is only available if the server was configured with the --enable-exception-hook option. It enables a hook that allows external modules to plug in and do something after a child crashed.,0,0,others,httpd,0,0
3555,EnableMMAP,"This directive controls whether the httpd may use memory-mapping if it needs to read the contents of a file during delivery. By default, when the handling of a request requires access to the data within a file -- for example, when delivering a server-parsed file using mod_include -- Apache httpd memory-maps the file if the OS supports it.",1,4,limited-side-effect,httpd,0,0
3556,EnableSendfile,"This directive controls whether httpd may use the sendfile support from the kernel to transmit file contents to the client. By default, when the handling of a request requires no access to the data within a file -- for example, when delivering a static file -- Apache httpd uses sendfile to deliver the file contents without ever reading the file if the OS supports it.",1,6,function-tradeoff,httpd,0,0
3557,Error,"If an error can be detected within the configuration, this directive can be used to generate a custom error message, and halt configuration parsing. The typical use is for reporting required modules which are missing from the configuration.",0,0,others,httpd,0,0
3558,ErrorDocument,"In the event of a problem or error, Apache httpd can be configured to do one of four things,",0,0,others,httpd,0,0
3559,ErrorLog,The ErrorLog directive sets the name of the file to which the server will log any errors it encounters. If the file-path is not absolute then it is assumed to be relative to the ServerRoot.,0,0,others,httpd,0,0
3560,ErrorLogFormat,ErrorLogFormat allows to specify what supplementary information is logged in the error log in addition to the actual log message.,0,0,others,httpd,0,1
3562,ExpiresActive,"This directive enables or disables the generation of the Expires and Cache-Control headers for the document realm in question. (That is, if found in an .htaccess file, for instance, it applies only to documents generated from that directory.) If set to Off, the headers will not be generated for any document in the realm (unless overridden at a lower level, such as an .htaccess file overriding a server config file). If set to On, the headers will be added to served documents according to the criteria defined by the ExpiresByType and ExpiresDefault directives (q.v.).",0,0,others,httpd,0,1
3563,ExpiresByType,"This directive defines the value of the Expires header and the max-age directive of the Cache-Control header generated for documents of the specified type (e.g., text/html). The second argument sets the number of seconds that will be added to a base time to construct the expiration date. The Cache-Control: max-age is calculated by subtracting the request time from the expiration date and expressing the result in seconds.",0,0,others,httpd,0,0
3564,ExpiresDefault,"This directive sets the default algorithm for calculating the expiration time for all documents in the affected realm. It can be overridden on a type-by-type basis by the ExpiresByType directive. See the description of that directive for details about the syntax of the argument, and the alternate syntax description as well.",0,0,others,httpd,0,0
3565,ExtendedStatus,This option tracks additional data per worker about the currently executing request and creates a utilization summary. You can see these variables during runtime by configuring mod_status. Note that other modules may rely on this scoreboard.,0,0,others,httpd,0,1
3566,ExtFilterDefine,"The ExtFilterDefine directive defines the characteristics of an external filter, including the program to run and its arguments.",0,0,others,httpd,0,0
3568,FileETag,The FileETag directive configures the file attributes that are used to create the ETag (entity tag) response header field when the document is based on a static file. (The ETag value is used in cache management to save network bandwidth.) The FileETag directive allows you to choose which of these -- if any -- should be used. The recognized keywords are:,0,0,others,httpd,0,0
3570,FilesMatch,"The <FilesMatch> directive limits the scope of the enclosed directives by filename, just as the <Files> directive does. However, it accepts a regular expression. For example:",0,0,others,httpd,0,0
3571,FilterChain,"This configures an actual filter chain, from declared filters. FilterChain takes any number of arguments, each optionally preceded with a single-character control that determines what to do:",0,0,others,httpd,0,0
3573,FilterProtocol,"This directs mod_filter to deal with ensuring the filter doesn't run when it shouldn't, and that the HTTP response headers are correctly set taking into account the effects of the filter.",0,0,others,httpd,0,0
3574,FilterProvider,This directive registers a provider for the smart filter. The provider will be called if and only if the expression declared evaluates to true when the harness is first called.,0,0,others,httpd,0,0
3575,FilterTrace,"This directive generates debug information from mod_filter. It is designed to help test and debug providers (filter modules), although it may also help with mod_filter itself.",0,0,others,httpd,0,0
3577,ForensicLog,"The ForensicLog directive is used to log requests to the server for forensic analysis. Each log entry is assigned a unique ID which can be associated with the request using the normal CustomLog directive. mod_log_forensic creates a token called forensic-id, which can be added to the transfer log using the %{forensic-id}n format string.",0,0,others,httpd,0,0
3578,GlobalLog,The GlobalLog directive defines a log shared by the main server configuration and all defined virtual hosts.,0,0,others,httpd,0,1
3579,GprofDir,"When the server has been compiled with gprof profiling support, GprofDir causes gmon.out files to be written to the specified directory when the process exits. If the argument ends with a percent symbol ('%'), subdirectories are created for each process id.",0,0,others,httpd,0,0
3581,Group,"The Group directive sets the group under which the server will answer requests. In order to use this directive, the server must be run initially as root. If you start the server as a non-root user, it will fail to change to the specified group, and will instead continue to run as the group of the original user. Unix-group is one of:",0,0,others,httpd,0,0
3584,H2EarlyHints,"This setting controls if HTTP status 103 interim responses are forwarded to the client or not. By default, this is currently not the case since a range of clients still have trouble with unexpected interim responses.",0,0,others,httpd,0,1
3585,H2MaxSessionStreams,This directive sets the maximum number of active streams per HTTP/2 session (e.g. connection) that the server allows. A stream is active if it is not idle or closed according to RFC 7540.,1,1,resource,httpd,0,0
3586,H2MaxWorkerIdleSeconds,This directive sets the maximum number of seconds a h2 worker may idle until it shuts itself down. This only happens while the number of h2 workers exceeds H2MinWorkers.,0,0,others,httpd,0,0
3587,H2MaxWorkers,"This directive sets the maximum number of worker threads to spawn per child process for HTTP/2 processing. If this directive is not used, mod_http2 will chose a value suitable for the mpm module loaded.",1,1,resource,httpd,0,1
3588,H2MinWorkers,"This directive sets the minimum number of worker threads to spawn per child process for HTTP/2 processing. If this directive is not used, mod_http2 will chose a value suitable for the mpm module loaded.",1,1,resource,httpd,0,0
3589,H2ModernTLSOnly,This directive toggles the security checks on HTTP/2 connections in TLS mode (https:). This can be used server wide or for specific <VirtualHost>s.,1,2,security-tradeoff,httpd,0,0
3590,H2Padding,"With the default 0, no padding bytes are added to any payload frames, e.g. HEADERS, DATA and PUSH_PROMISE. This is the behaviour of previous versions. It means that under certain conditions, an observer of network traffic can see the length of those frames in the TLS stream.",0,0,others,httpd,0,1
3591,H2Push,This directive toggles the usage of the HTTP/2 server push protocol feature.,0,0,others,httpd,0,0
3592,H2PushDiarySize,This directive toggles the maximum number of HTTP/2 server pushes that are remembered per HTTP/2 connection. This can be used inside the <VirtualHost> section to influence the number for all connections to that virtual host.,0,0,others,httpd,0,0
3595,H2SerializeHeaders,This directive toggles if HTTP/2 requests shall be serialized in HTTP/1.1 format for processing by httpd core or if received binary data shall be passed into the request_recs directly.,0,0,others,httpd,0,0
3596,H2StreamMaxMemSize,This directive sets the maximum number of outgoing data bytes buffered in memory for an active streams. This memory is not allocated per stream as such. Allocations are counted against this limit when they are about to be done. Stream processing freezes when the limit has been reached and will only continue when buffered data has been sent out to the client.,1,1,resource,httpd,0,0
3597,H2TLSCoolDownSecs,This directive sets the number of seconds of idle time on a TLS connection before the TLS write size falls back to small (~1300 bytes) length. This can be used server wide or for specific <VirtualHost>s.,0,0,others,httpd,0,0
3598,H2TLSWarmUpSize,This directive sets the number of bytes to be sent in small TLS records (~1300 bytes) until doing maximum sized writes (16k) on https: HTTP/2 connections. This can be used server wide or for specific <VirtualHost>s.,1,1,resource,httpd,0,0
3600,H2WindowSize,This directive sets the size of the window that is used for flow control from client to server and limits the amount of data the server has to buffer. The client will stop sending on a stream once the limit has been reached until the server announces more available space (as it has processed some of the data).,1,1,resource,httpd,0,0
3601,Header,"This directive can replace, merge or remove HTTP response headers. The header is modified just after the content handler and output filters are run, allowing outgoing headers to be modified.",0,0,others,httpd,0,0
3602,HeaderName,The HeaderName directive sets the name of the file that will be inserted at the top of the index listing. Filename is the name of the file to include.,0,0,others,httpd,0,0
3605,HeartbeatMaxServers,The HeartbeatMaxServers directive specifies the maximum number of servers that will be sending requests to this monitor server. It is used to control the size of the shared memory allocated to store the heartbeat info when mod_slotmem_shm is in use.,1,1,resource,httpd,0,1
3608,HostnameLookups,"This directive enables DNS lookups so that host names can be logged (and passed to CGIs/SSIs in REMOTE_HOST). The value Double refers to doing double-reverse DNS lookup. That is, after a reverse lookup is performed, a forward lookup is then performed on that result. At least one of the IP addresses in the forward lookup must match the original address. (In ""tcpwrappers"" terminology this is called PARANOID.)",0,0,others,httpd,0,0
3609,HttpProtocolOptions,"This directive changes the rules applied to the HTTP Request Line (RFC 7230 v.1.1) and the HTTP Request Header Fields (RFC 7230 v.2), which are now applied by default or using the Strict option. Due to legacy modules, applications or custom user-agents which must be deprecated the Unsafe option has been added to revert to the legacy behaviors.",0,0,others,httpd,0,0
3610,IdentityCheck,"This directive enables RFC 1413-compliant logging of the remote user name for each connection, where the client machine runs identd or something similar. This information is logged in the access log using the %...l format string.",0,0,others,httpd,0,1
3612,If,"The <If> directive evaluates an expression at runtime, and applies the enclosed directives if and only if the expression evaluates to true. For example:",0,0,others,httpd,0,0
3613,IfDefine,"The <IfDefine test>...</IfDefine> section is used to mark directives that are conditional. The directives within an <IfDefine> section are only processed if the test is true. If test is false, everything between the start and end markers is ignored.",0,0,others,httpd,0,0
3614,IfDirective,"The <IfDirective test>...</IfDirective> section is used to mark directives that are conditional on the presence of a specific directive. The directives within an <IfDirective> section are only processed if the test is true. If test is false, everything between the start and end markers is ignored.",0,0,others,httpd,0,0
3616,IfModule,"The <IfModule test>...</IfModule> section is used to mark directives that are conditional on the presence of a specific module. The directives within an <IfModule> section are only processed if the test is true. If test is false, everything between the start and end markers is ignored.",0,0,others,httpd,0,0
3618,IfVersion,"The <IfVersion> section encloses configuration directives which are executed only if the httpd version matches the desired criteria. For normal (numeric) comparisons the version argument has the format major[.minor[.patch]], e.g. 2.1.0 or 2.2. minor and patch are optional. If these numbers are omitted, they are assumed to be zero. The following numerical operators are possible:",0,0,others,httpd,0,0
3624,IndexHeadInsert,The IndexHeadInsert directive specifies a string to insert in the <head> section of the HTML generated for the index page.,0,0,others,httpd,0,0
3626,IndexIgnoreReset,The IndexIgnoreReset directive removes any files ignored by IndexIgnore otherwise inherited from other configuration sections.,0,0,others,httpd,0,0
3630,ISAPIAppendLogToErrors,Record HSE_APPEND_LOG_PARAMETER requests from ISAPI extensions to the server error log.,0,0,others,httpd,0,0
3631,ISAPIAppendLogToQuery,Record HSE_APPEND_LOG_PARAMETER requests from ISAPI extensions to the query field (appended to the CustomLog %q component).,0,0,others,httpd,0,0
3633,ISAPIFakeAsync,"While set to on, asynchronous support for ISAPI callbacks is simulated.",1,4,limited-side-effect,httpd,0,0
3634,ISAPILogNotSupported,"Logs all requests for unsupported features from ISAPI extensions in the server error log. This may help administrators to track down problems. Once set to on and all desired ISAPI modules are functioning, it should be set back to off.",0,0,others,httpd,0,0
3635,ISAPIReadAheadBuffer,Defines the maximum size of the Read Ahead Buffer sent to ISAPI extensions when they are initially invoked. All remaining data must be retrieved using the ReadClient callback; some ISAPI extensions may not support the ReadClient function. Refer questions to the ISAPI extension's author.,1,1,resource,httpd,0,0
3636,KeepAlive,"The Keep-Alive extension to HTTP/1.0 and the persistent connection feature of HTTP/1.1 provide long-lived HTTP sessions which allow multiple requests to be sent over the same TCP connection. In some cases this has been shown to result in an almost 50% speedup in latency times for HTML documents with many images. To enable Keep-Alive connections, set KeepAlive On.",1,5,workload-specific,httpd,0,0
3637,KeepAliveTimeout,"The number of seconds Apache httpd will wait for a subsequent request before closing the connection. By adding a postfix of ms the timeout can be also set in milliseconds. Once a request has been received, the timeout value specified by the Timeout directive applies.",0,0,others,httpd,0,0
3639,LDAPCacheEntries,Specifies the maximum size of the primary LDAP cache. This cache contains successful search/binds. Set it to 0 to turn off search/bind caching. The default size is 1024 cached searches.,1,1,resource,httpd,0,1
3640,LDAPCacheTTL,Specifies the time (in seconds) that an item in the search/bind cache remains valid. The default is 600 seconds (10 minutes).,1,5,workload-specific,httpd,0,0
3641,LDAPConnectionPoolTTL,"Specifies the maximum age, in seconds, that a pooled LDAP connection can remain idle and still be available for use. Connections are cleaned up when they are next needed, not asynchronously.",1,5,workload-specific,httpd,0,0
3643,LDAPLibraryDebug,Turns on SDK-specific LDAP debug options that generally cause the LDAP SDK to log verbose trace information to the main Apache error log. The trace messages from the LDAP SDK provide gory details that can be useful during debugging of connectivity problems with backend LDAP servers,0,0,others,httpd,0,0
3644,LDAPOpCacheEntries,This specifies the number of entries mod_ldap will use to cache LDAP compare operations. The default is 1024 entries. Setting it to 0 disables operation caching.,1,1,resource,httpd,0,0
3645,LDAPOpCacheTTL,Specifies the time (in seconds) that entries in the operation cache remain valid. The default is 600 seconds.,1,5,workload-specific,httpd,0,0
3646,LDAPRetries,The server will retry failed LDAP requests up to LDAPRetries times. Setting this directive to 0 disables retries.,0,0,others,httpd,0,0
3647,LDAPRetryDelay,"If LDAPRetryDelay is set to a non-zero value, the server will delay retrying an LDAP request for the specified amount of time. Setting this directive to 0 will result in any retry to occur without delay.",0,0,others,httpd,0,0
3648,LDAPSharedCacheFile,"Specifies the directory path and file name of the shared memory cache file. If not set, anonymous shared memory will be used if the platform supports it.",0,0,others,httpd,0,1
3649,LDAPSharedCacheSize,"Specifies the number of bytes to allocate for the shared memory cache. The default is 500kb. If set to 0, shared memory caching will not be used and every HTTPD process will create its own cache.",1,1,resource,httpd,0,1
3650,LDAPTimeout,"This directive configures the timeout for bind and search operations, as well as the LDAP_OPT_TIMEOUT option in the underlying LDAP client library, when available.",0,0,others,httpd,0,0
3651,LDAPTrustedGlobalCert,"It specifies the directory path and file name of the trusted CA certificates and/or system wide client certificates mod_ldap should use when establishing an SSL or TLS connection to an LDAP server. Note that all certificate information specified using this directive is applied globally to the entire server installation. Some LDAP toolkits (notably Novell) require all client certificates to be set globally using this directive. Most other toolkits require clients certificates to be set per Directory or per Location using LDAPTrustedClientCert. If you get this wrong, an error may be logged when an attempt is made to contact the LDAP server, or the connection may silently fail (See the SSL/TLS certificate guide above for details). The type specifies the kind of certificate parameter being set, depending on the LDAP toolkit being used. Supported types are:",0,0,others,httpd,0,1
3652,LDAPTrustedMode,The following modes are supported:,0,0,others,httpd,0,0
3653,LDAPVerifyServerCert,Specifies whether to force the verification of a server certificate when establishing an SSL connection to the LDAP server.,0,0,others,httpd,0,0
3655,LimitRequestBody,This directive specifies the number of bytes from 0 (meaning unlimited) to 2147483647 (2GB) that are allowed in a request body. See the note below for the limited applicability to proxy requests.,1,5,workload-specific,httpd,0,0
3656,LimitRequestFields,Number is an integer from 0 (meaning unlimited) to 32767. The default value is defined by the compile-time constant DEFAULT_LIMIT_REQUEST_FIELDS (100 as distributed).,0,0,others,httpd,0,0
3657,LimitRequestFieldSize,This directive specifies the number of bytes that will be allowed in an HTTP request header.,1,5,workload-specific,httpd,0,0
3658,LimitRequestLine,This directive sets the number of bytes that will be allowed on the HTTP request-line.,1,5,workload-specific,httpd,0,0
3659,LimitXMLRequestBody,Limit (in bytes) on maximum size of an XML-based request body. A value of 0 will disable any checking.,1,5,workload-specific,httpd,0,0
3660,Listen,"The Listen directive instructs Apache httpd to listen to only specific IP addresses or ports; by default it responds to requests on all IP interfaces. Listen is now a required directive. If it is not in the config file, the server will fail to start. This is a change from previous versions of Apache httpd.",0,0,others,httpd,0,1
3661,ListenBackLog,"The maximum length of the queue of pending connections. Generally no tuning is needed or desired; however on some systems, it is desirable to increase this when under a TCP SYN flood attack. See the backlog parameter to the listen(2) system call.",1,1,resource,httpd,0,1
3662,ListenCoresBucketsRatio,"A ratio between the number of (online) CPU cores and the number of listeners' buckets can be used to make Apache HTTP Server create num_cpu_cores / ratio listening buckets, each containing its own Listen-ing socket(s) on the same port(s), and then make each child handle a single bucket (with round-robin distribution of the buckets at children creation time).",1,5,workload-specific,httpd,0,1
3667,LogFormat,This directive specifies the format of the access log file.,0,0,others,httpd,0,1
3668,LogIOTrackTTFB,This directive configures whether this module tracks the delay between the request being read and the first byte of the response headers being written. The resulting value may be logged with the %^FB format.,0,0,others,httpd,0,0
3669,LogLevel,"LogLevel adjusts the verbosity of the messages recorded in the error logs (see ErrorLog directive). The following levels are available, in order of decreasing significance:",0,0,others,httpd,0,0
3670,LuaAuthzProvider,"After a lua function has been registered as authorization provider, it can be used with the Require directive:",0,0,others,httpd,0,0
3671,LuaCodeCache,"Specify the behavior of the in-memory code cache. The default is stat, which stats the top level script (not any included ones) each time that file is needed, and reloads it if the modified time indicates it is newer than the one it has already loaded. The other values cause it to keep the file cached forever (don't stat and replace) or to never cache the file.",1,4,limited-side-effect,httpd,0,0
3672,LuaHookAccessChecker,"Add your hook to the access_checker phase. An access checker hook function usually returns OK, DECLINED, or HTTP_FORBIDDEN.",0,0,others,httpd,0,0
3673,LuaHookAuthChecker,Invoke a lua function in the auth_checker phase of processing a request. This can be used to implement arbitrary authentication and authorization checking. A very simple example:,0,0,others,httpd,0,1
3674,LuaHookFixups,"Just like LuaHookTranslateName, but executed at the fixups phase",0,0,others,httpd,0,0
3675,LuaHookInsertFilter,Not Yet Implemented,0,0,others,httpd,0,0
3676,LuaHookLog,"This simple logging hook allows you to run a function when httpd enters the logging phase of a request. With it, you can append data to your own logs, manipulate data before the regular log is written, or prevent a log entry from being created. To prevent the usual logging from happening, simply return apache2.DONE in your logging handler, otherwise return apache2.OK to tell httpd to log as normal.",0,0,others,httpd,0,1
3677,LuaHookMapToStorage,"Like LuaHookTranslateName but executed at the map-to-storage phase of a request. Modules like mod_cache run at this phase, which makes for an interesting example on what to do here:",0,0,others,httpd,0,0
3678,LuaHookTranslateName,"Add a hook (at APR_HOOK_MIDDLE) to the translate name phase of request processing. The hook function receives a single argument, the request_rec, and should return a status code, which is either an HTTP error code, or the constants defined in the apache2 module: apache2.OK, apache2.DECLINED, or apache2.DONE.",0,0,others,httpd,0,0
3681,LuaInputFilter,"Provides a means of adding a Lua function as an input filter. As with output filters, input filters work as coroutines, first yielding before buffers are sent, then yielding whenever a bucket needs to be passed down the chain, and finally (optionally) yielding anything that needs to be appended to the input data. The global variable bucket holds the buckets as they are passed onto the Lua script:",0,0,others,httpd,0,1
3682,LuaMapHandler,"This directive matches a uri pattern to invoke a specific handler function in a specific file. It uses PCRE regular expressions to match the uri, and supports interpolating match groups into both the file path and the function name. Be careful writing your regular expressions to avoid security issues.",0,0,others,httpd,0,0
3683,LuaOutputFilter,"Provides a means of adding a Lua function as an output filter. As with input filters, output filters work as coroutines, first yielding before buffers are sent, then yielding whenever a bucket needs to be passed down the chain, and finally (optionally) yielding anything that needs to be appended to the input data. The global variable bucket holds the buckets as they are passed onto the Lua script:",0,0,others,httpd,0,0
3690,MaxConnectionsPerChild,"The MaxConnectionsPerChild directive sets the limit on the number of connections that an individual child server process will handle. After MaxConnectionsPerChild connections, the child process will die. If MaxConnectionsPerChild is 0, then the process will never expire.",1,1,resource,httpd,0,0
3691,MaxKeepAliveRequests,"The MaxKeepAliveRequests directive limits the number of requests allowed per connection when KeepAlive is on. If it is set to 0, unlimited requests will be allowed. We recommend that this setting be kept to a high value for maximum server performance.",1,1,resource,httpd,0,0
3692,MaxMemFree,"The MaxMemFree directive sets the maximum number of free Kbytes that every allocator is allowed to hold without calling free(). In threaded MPMs, every thread has its own allocator. When set to zero, the threshold will be set to unlimited.",1,1,resource,httpd,0,0
3693,MaxRangeOverlaps,"The MaxRangeOverlaps directive limits the number of overlapping HTTP ranges the server is willing to return to the client. If more overlapping ranges than permitted are requested, the complete resource is returned instead.",1,5,workload-specific,httpd,0,1
3694,MaxRangeReversals,"The MaxRangeReversals directive limits the number of HTTP Range reversals the server is willing to return to the client. If more ranges reversals than permitted are requested, the complete resource is returned instead.",0,0,others,httpd,0,0
3695,MaxRanges,"The MaxRanges directive limits the number of HTTP ranges the server is willing to return to the client. If more ranges than permitted are requested, the complete resource is returned instead.",0,0,others,httpd,0,0
3696,MaxRequestWorkers,"The MaxRequestWorkers directive sets the limit on the number of simultaneous requests that will be served. Any connection attempts over the MaxRequestWorkers limit will normally be queued, up to a number based on the ListenBacklog directive. Once a child process is freed at the end of a different request, the connection will then be serviced.",1,1,resource,httpd,0,1
3697,MaxSpareServers,"The MaxSpareServers directive sets the desired maximum number of idle child server processes. An idle process is one which is not handling a request. If there are more than MaxSpareServers idle, then the parent process will kill off the excess processes.",1,1,resource,httpd,0,1
3698,MaxSpareThreads,Maximum number of idle threads. Different MPMs deal with this directive differently.,1,1,resource,httpd,0,1
3699,MaxThreads,"The MaxThreads directive sets the desired maximum number worker threads allowable. The default value is also the compiled in hard limit. Therefore it can only be lowered, for example:",1,1,resource,httpd,0,0
3701,MDCAChallenges,"Sets challenge types (in order of preference) when proving domain ownership. Supported by the module are the challenge methods 'tls-alpn-01', 'dns-01' and 'http-01'. The module will look at the overall configuration of the server to find out which methods can be used.",0,0,others,httpd,0,1
3702,MDCertificateAgreement,"When you use mod_md to obtain a certificate, you become a customer of the CA (e.g. Let's Encrypt). That means you need to read and agree to their Terms of Service, so that you understand what they offer and what they might exclude or require from you. mod_md cannot, by itself, agree to such a thing.",0,0,others,httpd,0,0
3706,MDCertificateMonitor,This is part of the 'server-status' HTML user interface and has nothing to do with the core functioning itself. It defines the link offered on that page for easy checking of a certificate monitor. The SHA256 fingerprint of the certificate is appended to the configured url.,0,0,others,httpd,0,1
3707,MDCertificateProtocol,"Specifies the protocol to use. Currently, only ACME is supported.",0,0,others,httpd,0,0
3710,MDContactEmail,"The ACME protocol requires you to give a contact url when you sign up. Currently, Let's Encrypt wants an email address (and it will use it to inform you about renewals or changed terms of service). mod_md uses the MDContactEmail directive email in your Apache configuration, so please specify the correct address there. If MDContactEmail is not present, mod_md will use the ServerAdmin directive.",0,0,others,httpd,0,1
3712,MDHttpProxy,Use a http proxy to connect to the MDCertificateAuthority. Define this if your webserver can only reach the internet with a forward proxy.,0,0,others,httpd,0,0
3719,MDomainSetsection,"This is the directive MDomain with the added possibility to add setting just for this MD. In fact, you may also use ""<MDomain ..>"" as a shortcut.",0,0,others,httpd,0,0
3720,MDPortMap,"The ACME protocol provides two methods to verify domain ownership via HTTP: one that uses 'http:' urls (port 80) and one for 'https:' urls (port 443). If your server is not reachable by at least one of the two, ACME may only work by configuring your DNS server, see MDChallengeDns01.",0,0,others,httpd,0,0
3721,MDPrivateKeys,Defines what kind of private keys are generated for a managed domain and with what parameters. The only supported type right now is 'RSA' and the only parameter it takes is the number of bits used for the key.,0,0,others,httpd,0,0
3724,MDRequireHttps,This is a convenience directive to ease http: to https: migration of your Managed Domains. With:,0,0,others,httpd,0,0
3726,MDStapleOthers,"This setting only takes effect when MDStapling is enabled. It controls if mod_md should also provide stapling information for certificates that are not directly controlled by it, e.g. renewed via an ACME CA.",0,0,others,httpd,0,0
3727,MDStapling,"mod_md offers an implementation for providing OCSP stapling information. This is an alternative to the one provided by mod_ssl. For backward compatibility, this is disabled by default.",0,0,others,httpd,0,1
3731,MDWarnWindow,See MDRenewWindow for a description on how you can specify the time.,0,0,others,httpd,0,0
3732,MemcacheConnTTL,Set the time to keep idle connections with the memcache server(s) alive (threaded platforms only).,1,5,workload-specific,httpd,0,0
3733,MergeSlashes,"By default, the server merges (or collapses) multiple consecutive slash ('/') characters in the path component of the request URL.",0,0,others,httpd,0,0
3734,MergeTrailers,"This directive controls whether HTTP trailers are copied into the internal representation of HTTP headers. This merging occurs when the request body has been completely consumed, long after most header processing would have a chance to examine or modify request headers.",0,0,others,httpd,0,0
3739,MinSpareServers,"The MinSpareServers directive sets the desired minimum number of idle child server processes. An idle process is one which is not handling a request. If there are fewer than MinSpareServers idle, then the parent process creates new children: It will spawn one, wait a second, then spawn two, wait a second, then spawn four, and it will continue exponentially until it is spawning 32 children per second. It will stop whenever it satisfies the MinSpareServers setting.",1,1,resource,httpd,0,1
3740,MinSpareThreads,Minimum number of idle threads to handle request spikes. Different MPMs deal with this directive differently.,1,1,resource,httpd,0,0
3741,MMapFile,The MMapFile directive maps one or more files (given as whitespace separated arguments) into memory at server startup time. They are automatically unmapped on a server shutdown. When the files have changed on the filesystem at least a HUP or USR1 signal should be send to the server to re-mmap() them.,0,0,others,httpd,0,0
3742,MultiviewsMatch,"MultiviewsMatch permits three different behaviors for mod_negotiation's Multiviews feature. Multiviews allows a request for a file, e.g. index.html, to match any negotiated extensions following the base request, e.g. index.html.en, index.html.fr, or index.html.gz.",0,0,others,httpd,0,0
3746,NWSSLTrustedCerts,Specifies a list of client certificate files (DER format) that are used when creating a proxied SSL connection. Each client certificate used by a server must be listed separately in its own .der file.,0,0,others,httpd,0,0
3747,NWSSLUpgradeable,Allow a connection that was created on the specified address and/or port to be upgraded to an SSL connection upon request from the client. The address and/or port must have already be defined previously with a Listen directive.,0,0,others,httpd,0,0
3748,Options,The Options directive controls which server features are available in a particular directory.,0,0,others,httpd,0,0
3751,PrivilegesMode,"This directive trades off performance vs security against malicious, privileges-aware code. In SECURE mode, each request runs in a secure subprocess, incurring a substantial performance penalty. In FAST mode, the server is not protected against escalation of privileges as discussed above.",1,2,security-tradeoff,httpd,0,0
3752,Protocol,This directive specifies the protocol used for a specific listening socket. The protocol is used to determine which module should handle a request and to apply protocol specific optimizations with the AcceptFilter directive.,1,4,limited-side-effect,httpd,0,0
3756,Proxy,Directives placed in <Proxy> sections apply only to matching proxied content. Shell-style wildcards are allowed.,0,0,others,httpd,0,0
3760,ProxyBlock,"The ProxyBlock directive specifies a list of words, hosts and/or domains, separated by spaces. HTTP, HTTPS, and FTP document requests to sites whose names contain matched words, hosts or domains are blocked by the proxy server. The proxy module will also attempt to determine IP addresses of list items which may be hostnames during startup, and cache them for match test as well. That may slow down the startup time of the server.",0,0,others,httpd,0,1
3764,ProxyExpressDBMType,The ProxyExpressDBMType directive controls the DBM type expected by the module. The default is the default DBM type created with httxt2dbm.,0,0,others,httpd,0,1
3765,ProxyExpressEnable,The ProxyExpressEnable directive controls whether the module will be active.,0,0,others,httpd,0,0
3767,ProxyFCGISetEnvIf,"Just before passing a request to the configured FastCGI server, the core of the web server sets a number of environment variables based on details of the current request. FastCGI programs often uses these environment variables as inputs that determine what underlying scripts they will process, or what output they directly produce.",0,0,others,httpd,0,0
3771,ProxyHCExpr,The ProxyHCExpr directive allows for creating a named condition expression that checks the response headers of the backend server to determine its health. This named condition can then be assigned to balancer members via the hcexpr parameter.,0,0,others,httpd,0,1
3772,ProxyHCTemplate,The ProxyHCTemplate directive allows for creating a named set (template) of health check parameters that can then be assigned to balancer members via the hctemplate parameter.,0,0,others,httpd,0,0
3773,ProxyHCTPsize,"If Apache httpd and APR are built with thread support, the health check module will offload the work of the actual checking to a threadpool associated with the Watchdog process, allowing for parallel checks. The ProxyHCTPsize directive determines the size of this threadpool. If set to 0, no threadpool is used at all, resulting in serialized health checks.",1,1,resource,httpd,0,0
3774,ProxyHTMLBufSize,"In order to parse non-HTML content (stylesheets and scripts) embedded in HTML documents, mod_proxy_html has to read the entire script or stylesheet into a buffer. This buffer will be expanded as necessary to hold the largest script or stylesheet in a page, in increments of bytes as set by this directive.",1,1,resource,httpd,0,1
3775,ProxyHTMLCharsetOut,"This selects an encoding for mod_proxy_html output. It should not normally be used, as any change from the default UTF-8 (Unicode - as used internally by libxml2) will impose an additional processing overhead. The special token ProxyHTMLCharsetOut * will generate output using the same encoding as the input.",0,0,others,httpd,0,0
3777,ProxyHTMLEnable,A simple switch to enable or disable the proxy_html filter. If mod_xml2enc is loaded it will also automatically set up internationalisation support.,0,0,others,httpd,0,0
3778,ProxyHTMLEvents,Specifies one or more attributes to treat as scripting events and apply ProxyHTMLURLMaps to where enabled. You can specify any number of attributes in one or more ProxyHTMLEvents directives.,0,0,others,httpd,0,0
3779,ProxyHTMLExtended,"Set to Off, HTML links are rewritten according to the ProxyHTMLURLMap directives, but links appearing in Javascript and CSS are ignored.",0,0,others,httpd,0,0
3780,ProxyHTMLFixups,This directive takes one to three arguments as follows:,0,0,others,httpd,0,0
3781,ProxyHTMLInterp,This enables per-request interpolation in ProxyHTMLURLMap to- and from- patterns.,0,0,others,httpd,0,0
3782,ProxyHTMLLinks,"Specifies elements that have URL attributes that should be rewritten using standard ProxyHTMLURLMaps. You will need one ProxyHTMLLinks directive per element, but it can have any number of attributes.",0,0,others,httpd,0,0
3783,ProxyHTMLMeta,This turns on or off pre-parsing of metadata in HTML <head> sections.,1,4,limited-side-effect,httpd,0,1
3784,ProxyHTMLStripComments,"This directive will cause mod_proxy_html to strip HTML comments. Note that this will also kill off any scripts or styles embedded in comments (a bogosity introduced in 1995/6 with Netscape 2 for the benefit of then-older browsers, but still in use today). It may also interfere with comment-based processors such as SSI or ESI: be sure to run any of those before mod_proxy_html in the filter chain if stripping comments!",0,0,others,httpd,0,0
3785,ProxyHTMLURLMap,"This is the key directive for rewriting HTML links. When parsing a document, whenever a link target matches from-pattern, the matching portion will be rewritten to to-pattern, as modified by any flags supplied and by the ProxyHTMLExtended directive. Only the elements specified using the ProxyHTMLLinks directive will be considered as HTML links.",0,0,others,httpd,0,0
3786,ProxyIOBufferSize,The ProxyIOBufferSize directive adjusts the size of the internal buffer which is used as a scratchpad for the data between input and output. The size must be at least 512.,1,1,resource,httpd,0,0
3787,ProxyMatch,"The <ProxyMatch> directive is identical to the <Proxy> directive, except that it matches URLs using regular expressions.",0,0,others,httpd,0,0
3788,ProxyMaxForwards,The ProxyMaxForwards directive specifies the maximum number of proxies through which a request may pass if there's no Max-Forwards header supplied with the request. This may be set to prevent infinite proxy loops or a DoS attack.,0,0,others,httpd,0,0
3790,ProxyPassInherit,"This directive will cause the current server/vhost to ""inherit"" ProxyPass directives defined in the main server. This can cause issues and inconsistent behavior if using the Balancer Manager for dynamic changes and so should be disabled if using that feature.",0,0,others,httpd,0,0
3791,ProxyPassInterpolateEnv,"This directive, together with the interpolate argument to ProxyPass, ProxyPassReverse, ProxyPassReverseCookieDomain, and ProxyPassReverseCookiePath, enables reverse proxies to be dynamically configured using environment variables which may be set by another module such as mod_rewrite. It affects the ProxyPass, ProxyPassReverse, ProxyPassReverseCookieDomain, and ProxyPassReverseCookiePath directives and causes them to substitute the value of an environment variable varname for the string ${varname} in configuration directives if the interpolate option is set.",0,0,others,httpd,0,1
3797,ProxyReceiveBufferSize,"The ProxyReceiveBufferSize directive specifies an explicit (TCP/IP) network buffer size for proxied HTTP and FTP connections, for increased throughput. It has to be greater than 512 or set to 0 to indicate that the system's default buffer size should be used.",1,1,resource,httpd,0,0
3800,ProxyRequests,This allows or prevents Apache httpd from functioning as a forward proxy server. (Setting ProxyRequests to Off does not disable use of the ProxyPass directive.),0,0,others,httpd,0,1
3801,ProxySCGIInternalRedirect,"The ProxySCGIInternalRedirect enables the backend to internally redirect the gateway to a different URL. This feature originates in mod_cgi, which internally redirects the response if the response status is OK (200) and the response contains a Location (or configured alternate header) and its value starts with a slash (/). This value is interpreted as a new local URL that Apache httpd internally redirects to.",0,0,others,httpd,0,0
3802,ProxySCGISendfile,"The ProxySCGISendfile directive enables the SCGI backend to let files be served directly by the gateway. This is useful for performance purposes -- httpd can use sendfile or other optimizations, which are not possible if the file comes over the backend socket. Additionally, the file contents are not transmitted twice.",1,4,limited-side-effect,httpd,0,1
3806,ProxyTimeout,"This directive allows a user to specifiy a timeout on proxy requests. This is useful when you have a slow/buggy appserver which hangs, and you would rather just return a timeout and fail gracefully instead of waiting however long it takes the server to return.",0,0,others,httpd,0,0
3807,ProxyVia,"This directive controls the use of the Via: HTTP header by the proxy. Its intended use is to control the flow of proxy requests along a chain of proxy servers. See RFC 2616 (HTTP/1.1), section 14.45 for an explanation of Via: header lines.",0,0,others,httpd,0,0
3810,ReceiveBufferSize,The server will set the TCP receive buffer size to the number of bytes specified.,1,1,resource,httpd,0,0
3813,RedirectPermanent,This directive makes the client know that the Redirect is permanent (status 301). Exactly equivalent to Redirect permanent.,0,0,others,httpd,0,0
3814,RedirectTemp,This directive makes the client know that the Redirect is only temporary (status 302). Exactly equivalent to Redirect temp.,0,0,others,httpd,0,1
3818,RegexDefaultOptions,This directive adds some default behavior to ANY regular expression used afterwards.,0,0,others,httpd,0,0
3823,RemoteIPProxiesHeader,"The RemoteIPProxiesHeader directive specifies a header into which mod_remoteip will collect a list of all of the intermediate client IP addresses trusted to resolve the useragent IP of the request. Note that intermediate RemoteIPTrustedProxy addresses are recorded in this header, while any intermediate RemoteIPInternalProxy addresses are discarded.",0,0,others,httpd,0,0
3824,RemoteIPProxyProtocol,"The RemoteIPProxyProtocol directive enables or disables the reading and handling of the PROXY protocol connection header. If enabled with the On flag, the upstream client must send the header every time it opens a connection or the connection will be aborted unless it is in the list of disabled hosts provided by the RemoteIPProxyProtocolExceptions directive.",0,0,others,httpd,0,0
3826,RemoteIPTrustedProxy,"The RemoteIPTrustedProxy directive adds one or more addresses (or address blocks) to trust as presenting a valid RemoteIPHeader value of the useragent IP. Unlike the RemoteIPInternalProxy directive, any intranet or private IP address reported by such proxies, including the 10/8, 172.16/12, 192.168/16, 169.254/16 and 127/8 blocks (or outside of the IPv6 public 2000::/3 block) are not trusted as the useragent IP, and are left in the RemoteIPHeader header's value.",0,0,others,httpd,0,0
3828,RequestHeader,"This directive can replace, merge, change or remove HTTP request headers. The header is modified just before the content handler is run, allowing incoming headers to be modified. The action it performs is determined by the first argument. This can be one of the following values:",0,0,others,httpd,0,0
3829,RequestReadTimeout,"This directive can set various timeouts for completing the TLS handshake, receiving the request headers and/or the request body from the client. If the client fails to complete each of these stages within the configured time, a 408 REQUEST TIME OUT error is sent.",0,0,others,httpd,0,0
3831,RewriteEngine,The RewriteEngine directive enables or disables the runtime rewriting engine. If it is set to off this module does no runtime processing at all. It does not even update the SCRIPT_URx environment variables.,1,6,function-tradeoff,httpd,0,0
3832,RewriteMap,The RewriteMap directive defines a Rewriting Map which can be used inside rule substitution strings by the mapping-functions to insert/substitute fields through a key lookup. The source of this lookup can be of various types.,0,0,others,httpd,0,0
3834,RewriteRule,"The RewriteRule directive is the real rewriting workhorse. The directive can occur more than once, with each instance defining a single rewrite rule. The order in which these rules are defined is important - this is the order in which they will be applied at run-time.",0,0,others,httpd,0,0
3835,RLimitCPU,"Takes 1 or 2 parameters. The first parameter sets the soft resource limit for all processes and the second parameter sets the maximum resource limit. Either parameter can be a number, or max to indicate to the server that the limit should be set to the maximum allowed by the operating system configuration. Raising the maximum resource limit requires that the server is running as root or in the initial startup phase.",0,0,others,httpd,0,0
3836,RLimitMEM,"Takes 1 or 2 parameters. The first parameter sets the soft resource limit for all processes and the second parameter sets the maximum resource limit. Either parameter can be a number, or max to indicate to the server that the limit should be set to the maximum allowed by the operating system configuration. Raising the maximum resource limit requires that the server is running as root or in the initial startup phase.",0,0,others,httpd,0,1
3837,RLimitNPROC,"Takes 1 or 2 parameters. The first parameter sets the soft resource limit for all processes, and the second parameter sets the maximum resource limit. Either parameter can be a number, or max to indicate to the server that the limit should be set to the maximum allowed by the operating system configuration. Raising the maximum resource limit requires that the server is running as root or in the initial startup phase.",1,1,resource,httpd,0,0
3838,ScoreBoardFile,"Apache HTTP Server uses a scoreboard to communicate between its parent and child processes. Some architectures require a file to facilitate this communication. If the file is left unspecified, Apache httpd first attempts to create the scoreboard entirely in memory (using anonymous shared memory) and, failing that, will attempt to create the file on disk (using file-based shared memory). Specifying this directive causes Apache httpd to always create the file on the disk.",0,0,others,httpd,0,1
3842,ScriptInterpreterSource,"This directive is used to control how Apache httpd finds the interpreter used to run CGI scripts. The default setting is Script. This causes Apache httpd to use the interpreter pointed to by the shebang line (first line, starting with #!) in the script. On Win32 systems this line usually looks like:",0,0,others,httpd,0,0
3843,ScriptLog,"The ScriptLog directive sets the CGI script error logfile. If no ScriptLog is given, no error log is created. If given, any CGI errors are logged into the filename given as argument. If this is a relative file or path it is taken relative to the ServerRoot.",0,0,others,httpd,0,0
3844,ScriptLogBuffer,"The size of any PUT or POST entity body that is logged to the file is limited, to prevent the log file growing too big too quickly if large bodies are being received. By default, up to 1024 bytes are logged, but this can be changed with this directive.",1,5,workload-specific,httpd,0,1
3845,ScriptLogLength,"ScriptLogLength can be used to limit the size of the CGI script logfile. Since the logfile logs a lot of information per CGI error (all request headers, all script output) it can grow to be a big file. To prevent problems due to unbounded growth, this directive can be used to set an maximum file-size for the CGI logfile. If the file exceeds this size, no more information will be written to it.",1,3,reliability-tradeoff,httpd,0,0
3846,ScriptSock,"This directive sets the filename prefix of the socket to use for communication with the CGI daemon, an extension corresponding to the process ID of the server will be appended. The socket will be opened using the permissions of the user who starts Apache (usually root). To maintain the security of communications with CGI scripts, it is important that no other user has permission to write in the directory where the socket is located.",0,0,others,httpd,0,0
3847,SecureListen,Specifies the port and the eDirectory based certificate name that will be used to enable SSL encryption. An optional third parameter also enables mutual authentication.,0,0,others,httpd,0,0
3848,SeeRequestTail,"mod_status with ExtendedStatus On displays the actual request being handled. For historical purposes, only 63 characters of the request are actually stored for display purposes. This directive controls whether the 1st 63 characters are stored (the previous behavior and the default) or if the last 63 characters are. This is only applicable, of course, if the length of the request is 64 characters or greater.",0,0,others,httpd,0,0
3849,SendBufferSize,"Sets the server's TCP send buffer size to the number of bytes specified. It is often useful to set this past the OS's standard default value on high speed, high latency connections (i.e., 100ms or so, such as transcontinental fast pipes).",1,1,resource,httpd,0,0
3851,ServerLimit,"For the prefork MPM, this directive sets the maximum configured value for MaxRequestWorkers for the lifetime of the Apache httpd process. For the worker and event MPMs, this directive in combination with ThreadLimit sets the maximum configured value for MaxRequestWorkers for the lifetime of the Apache httpd process. For the event MPM, this directive also defines how many old server processes may keep running and finish processing open connections. Any attempts to change this directive during a restart will be ignored, but MaxRequestWorkers can be modified during a restart.",1,1,resource,httpd,0,1
3859,SessionCookieRemove,The SessionCookieRemove flag controls whether the cookies containing the session will be removed from the headers during request processing.,1,6,function-tradeoff,httpd,0,0
3860,SessionCryptoCipher,"The SessionCryptoCipher directive allows the cipher to be used during encryption. If not specified, the cipher defaults to aes256.",0,0,others,httpd,0,0
3861,SessionCryptoDriver,"The SessionCryptoDriver directive specifies the name of the crypto driver to be used for encryption. If not specified, the driver defaults to the recommended driver compiled into APR-util.",0,0,others,httpd,0,0
3862,SessionCryptoPassphrase,"The SessionCryptoPassphrase directive specifies the keys to be used to enable symmetrical encryption on the contents of the session before writing the session, or decrypting the contents of the session after reading the session.",0,0,others,httpd,0,0
3863,SessionCryptoPassphraseFile,"The SessionCryptoPassphraseFile directive specifies the name of a configuration file containing the keys to use for encrypting or decrypting the session, specified one per line. The file is read on server start, and a graceful restart will be necessary for httpd to pick up changes to the keys.",0,0,others,httpd,0,1
3866,SessionDBDCookieRemove,The SessionDBDCookieRemove flag controls whether the cookies containing the session ID will be removed from the headers during request processing.,1,6,function-tradeoff,httpd,0,0
3867,SessionDBDDeleteLabel,The SessionDBDDeleteLabel directive sets the default delete query label to be used to delete an expired or empty session. This label must have been previously defined using the DBDPrepareSQL directive.,0,0,others,httpd,0,0
3868,SessionDBDInsertLabel,The SessionDBDInsertLabel directive sets the default insert query label to be used to load in a session. This label must have been previously defined using the DBDPrepareSQL directive.,0,0,others,httpd,0,0
3869,SessionDBDPerUser,"The SessionDBDPerUser flag enables a per user session keyed against the user's login name. If the user is not logged in, this directive will be ignored.",0,0,others,httpd,0,1
3870,SessionDBDSelectLabel,The SessionDBDSelectLabel directive sets the default select query label to be used to load in a session. This label must have been previously defined using the DBDPrepareSQL directive.,0,0,others,httpd,0,1
3871,SessionDBDUpdateLabel,The SessionDBDUpdateLabel directive sets the default update query label to be used to load in a session. This label must have been previously defined using the DBDPrepareSQL directive.,0,0,others,httpd,0,0
3872,SessionEnv,"If set to On, the SessionEnv directive causes the contents of the session to be written to a CGI environment variable called HTTP_SESSION.",0,0,others,httpd,0,0
3874,SessionExpiryUpdateInterval,The SessionExpiryUpdateInterval directive allows sessions to avoid the cost associated with writing the session each request when only the expiry time has changed. This can be used to make a website more efficient or reduce load on a database when using mod_session_dbd. The session is always written if the data stored in the session has changed or the expiry has changed by more than the configured interval.,0,0,others,httpd,0,0
3877,SessionMaxAge,"The SessionMaxAge directive defines a time limit for which a session will remain valid. When a session is saved, this time limit is reset and an existing session can be continued. If a session becomes older than this limit without a request to the server to refresh the session, the session will time out and be removed. Where a session is used to stored user login details, this has the effect of logging the user out automatically after the given time.",0,0,others,httpd,0,1
3879,SetEnvIf,The SetEnvIf directive defines environment variables based on attributes of the request. The attribute specified in the first argument can be one of four things:,0,0,others,httpd,0,0
3880,SetEnvIfExpr,"The SetEnvIfExpr directive defines environment variables based on an <If> ap_expr. These expressions will be evaluated at runtime, and applied env-variable in the same fashion as SetEnvIf.",0,0,others,httpd,0,0
3881,SetEnvIfNoCase,"The SetEnvIfNoCase is semantically identical to the SetEnvIf directive, and differs only in that the regular expression matching is performed in a case-insensitive manner. For example:",0,0,others,httpd,0,0
3885,SSIEndTag,This directive changes the string that mod_include looks for to mark the end of an include element.,0,0,others,httpd,0,1
3886,SSIErrorMsg,"The SSIErrorMsg directive changes the error message displayed when mod_include encounters an error. For production servers you may consider changing the default error message to ""<!-- Error -->"" so that the message is not presented to the user.",0,0,others,httpd,0,1
3887,SSIStartTag,This directive changes the string that mod_include looks for to mark an include element to process.,0,0,others,httpd,0,0
3888,SSITimeFormat,This directive changes the format in which date strings are displayed when echoing DATE environment variables. The formatstring is as in strftime(3) from the C standard library.,0,0,others,httpd,0,1
3889,SSIUndefinedEcho,"This directive changes the string that mod_include displays when a variable is not set and ""echoed"".",0,0,others,httpd,0,0
3890,SSLCACertificateFile,"This directive sets the all-in-one file where you can assemble the Certificates of Certification Authorities (CA) whose clients you deal with. These are used for Client Authentication. Such a file is simply the concatenation of the various PEM-encoded Certificate files, in order of preference. This can be used alternatively and/or additionally to SSLCACertificatePath.",0,0,others,httpd,0,0
3891,SSLCACertificatePath,This directive sets the directory where you keep the Certificates of Certification Authorities (CAs) whose clients you deal with. These are used to verify the client certificate on Client Authentication.,0,0,others,httpd,0,0
3892,SSLCADNRequestFile,"When a client certificate is requested by mod_ssl, a list of acceptable Certificate Authority names is sent to the client in the SSL handshake. These CA names can be used by the client to select an appropriate client certificate out of those it has available.",0,0,others,httpd,0,0
3893,SSLCADNRequestPath,This optional directive can be used to specify the set of acceptable CA names which will be sent to the client when a client certificate is requested. See the SSLCADNRequestFile directive for more details.,0,0,others,httpd,0,0
3894,SSLCARevocationCheck,"Enables certificate revocation list (CRL) checking. At least one of SSLCARevocationFile or SSLCARevocationPath must be configured. When set to chain (recommended setting), CRL checks are applied to all certificates in the chain, while setting it to leaf limits the checks to the end-entity cert.",0,0,others,httpd,0,0
3895,SSLCARevocationFile,"This directive sets the all-in-one file where you can assemble the Certificate Revocation Lists (CRL) of Certification Authorities (CA) whose clients you deal with. These are used for Client Authentication. Such a file is simply the concatenation of the various PEM-encoded CRL files, in order of preference. This can be used alternatively and/or additionally to SSLCARevocationPath.",0,0,others,httpd,0,0
3896,SSLCARevocationPath,This directive sets the directory where you keep the Certificate Revocation Lists (CRL) of Certification Authorities (CAs) whose clients you deal with. These are used to revoke the client certificate on Client Authentication.,0,0,others,httpd,0,0
3898,SSLCertificateFile,"This directive points to a file with certificate data in PEM format, or the certificate identifier through a configured cryptographic token. If using a PEM file, at minimum, the file must include an end-entity (leaf) certificate. The directive can be used multiple times (referencing different filenames) to support multiple algorithms for server authentication - typically RSA, DSA, and ECC. The number of supported algorithms depends on the OpenSSL version being used for mod_ssl: with version 1.0.0 or later, openssl list-public-key-algorithms will output a list of supported algorithms, see also the note below about limitations of OpenSSL versions prior to 1.0.2 and the ways to work around them.",0,0,others,httpd,0,0
3899,SSLCertificateKeyFile,"This directive points to the PEM-encoded private key file for the server, or the key ID through a configured cryptographic token. If the contained private key is encrypted, the pass phrase dialog is forced at startup time.",0,0,others,httpd,0,0
3900,SSLCipherSuite,"This complex directive uses a colon-separated cipher-spec string consisting of OpenSSL cipher specifications to configure the Cipher Suite the client is permitted to negotiate in the SSL handshake phase. The optional protocol specifier can configure the Cipher Suite for a specific SSL version. Possible values include ""SSL"" for all SSL Protocols up to and including TLSv1.2.",1,2,security-tradeoff,httpd,0,1
3901,SSLCompression,This directive allows to enable compression on the SSL level.,1,4,limited-side-effect,httpd,0,0
3902,SSLCryptoDevice,"This directive enables use of a cryptographic hardware accelerator board to offload some of the SSL processing overhead. This directive can only be used if the SSL toolkit is built with ""engine"" support; OpenSSL 0.9.7 and later releases have ""engine"" support by default, the separate ""-engine"" releases of OpenSSL 0.9.6 must be used.",1,4,limited-side-effect,httpd,0,1
3903,SSLEngine,This directive toggles the usage of the SSL/TLS Protocol Engine. This is should be used inside a <VirtualHost> section to enable SSL/TLS for a that virtual host. By default the SSL/TLS Protocol Engine is disabled for both the main server and all configured virtual hosts.,0,0,others,httpd,0,0
3904,SSLFIPS,This directive toggles the usage of the SSL library FIPS_mode flag. It must be set in the global server context and cannot be configured with conflicting settings (SSLFIPS on followed by SSLFIPS off or similar). The mode applies to all SSL library operations.,0,0,others,httpd,0,1
3905,SSLHonorCipherOrder,"When choosing a cipher during an SSLv3 or TLSv1 handshake, normally the client's preference is used. If this directive is enabled, the server's preference will be used instead.",0,0,others,httpd,0,0
3906,SSLInsecureRenegotiation,"As originally specified, all versions of the SSL and TLS protocols (up to and including TLS/1.2) were vulnerable to a Man-in-the-Middle attack (CVE-2009-3555) during a renegotiation. This vulnerability allowed an attacker to ""prefix"" a chosen plaintext to the HTTP request as seen by the web server. A protocol extension was developed which fixed this vulnerability if supported by both client and server.",1,2,security-tradeoff,httpd,0,0
3907,SSLOCSPDefaultResponder,"This option sets the default OCSP responder to use. If SSLOCSPOverrideResponder is not enabled, the URI given will be used only if no responder URI is specified in the certificate being verified.",0,0,others,httpd,0,1
3908,SSLOCSPEnable,"This option enables OCSP validation of the client certificate chain. If this option is enabled, certificates in the client's certificate chain will be validated against an OCSP responder after normal verification (including CRL checks) have taken place. In mode 'leaf', only the client certificate itself will be validated.",1,2,security-tradeoff,httpd,0,0
3909,SSLOCSPNoverify,"Skip the OCSP responder certificates verification, mostly useful when testing an OCSP server.",1,2,security-tradeoff,httpd,0,0
3910,SSLOCSPOverrideResponder,"This option forces the configured default OCSP responder to be used during OCSP certificate validation, regardless of whether the certificate being validated references an OCSP responder.",0,0,others,httpd,0,0
3913,SSLOCSPResponderTimeout,"This option sets the timeout for queries to OCSP responders, when SSLOCSPEnable is turned on.",0,0,others,httpd,0,0
3915,SSLOCSPResponseTimeSkew,This option sets the maximum allowable time skew for OCSP responses (when checking their thisUpdate and nextUpdate fields).,0,0,others,httpd,0,1
3916,SSLOCSPUseRequestNonce,"This option determines whether queries to OCSP responders should contain a nonce or not. By default, a query nonce is always used and checked against the response's one. When the responder does not use nonces (e.g. Microsoft OCSP Responder), this option should be turned off.",0,0,others,httpd,0,1
3917,SSLOpenSSLConfCmd,"This directive exposes OpenSSL's SSL_CONF API to mod_ssl, allowing a flexible configuration of OpenSSL parameters without the need of implementing additional mod_ssl directives when new features are added to OpenSSL.",0,0,others,httpd,0,0
3918,SSLOptions,"This directive can be used to control various run-time options on a per-directory basis. Normally, if multiple SSLOptions could apply to a directory, then the most specific one is taken completely; the options are not merged. However if all the options on the SSLOptions directive are preceded by a plus (+) or minus (-) symbol, the options are merged. Any options preceded by a + are added to the options currently in force, and any options preceded by a - are removed from the options currently in force.",0,0,others,httpd,0,1
3919,SSLPassPhraseDialog,"When Apache starts up it has to read the various Certificate (see SSLCertificateFile) and Private Key (see SSLCertificateKeyFile) files of the SSL-enabled virtual servers. Because for security reasons the Private Key files are usually encrypted, mod_ssl needs to query the administrator for a Pass Phrase in order to decrypt those files. This query can be done in two ways which can be configured by type:",0,0,others,httpd,0,0
3920,SSLProtocol,This directive can be used to control which versions of the SSL/TLS protocol will be accepted in new connections.,0,0,others,httpd,0,1
3921,SSLProxyCACertificateFile,"This directive sets the all-in-one file where you can assemble the Certificates of Certification Authorities (CA) whose remote servers you deal with. These are used for Remote Server Authentication. Such a file is simply the concatenation of the various PEM-encoded Certificate files, in order of preference. This can be used alternatively and/or additionally to SSLProxyCACertificatePath.",0,0,others,httpd,0,0
3922,SSLProxyCACertificatePath,This directive sets the directory where you keep the Certificates of Certification Authorities (CAs) whose remote servers you deal with. These are used to verify the remote server certificate on Remote Server Authentication.,0,0,others,httpd,0,0
3923,SSLProxyCARevocationCheck,"Enables certificate revocation list (CRL) checking for the remote servers you deal with. At least one of SSLProxyCARevocationFile or SSLProxyCARevocationPath must be configured. When set to chain (recommended setting), CRL checks are applied to all certificates in the chain, while setting it to leaf limits the checks to the end-entity cert.",1,2,security-tradeoff,httpd,0,1
3924,SSLProxyCARevocationFile,"This directive sets the all-in-one file where you can assemble the Certificate Revocation Lists (CRL) of Certification Authorities (CA) whose remote servers you deal with. These are used for Remote Server Authentication. Such a file is simply the concatenation of the various PEM-encoded CRL files, in order of preference. This can be used alternatively and/or additionally to SSLProxyCARevocationPath.",0,0,others,httpd,0,0
3925,SSLProxyCARevocationPath,This directive sets the directory where you keep the Certificate Revocation Lists (CRL) of Certification Authorities (CAs) whose remote servers you deal with. These are used to revoke the remote server certificate on Remote Server Authentication.,0,0,others,httpd,0,0
3927,SSLProxyCheckPeerExpire,This directive sets whether it is checked if the remote server certificate is expired or not. If the check fails a 502 status code (Bad Gateway) is sent.,0,0,others,httpd,0,0
3928,SSLProxyCheckPeerName,"This directive configures host name checking for server certificates when mod_ssl is acting as an SSL client. The check will succeed if the host name from the request URI matches one of the CN attribute(s) of the certificate's subject, or matches the subjectAltName extension. If the check fails, the SSL request is aborted and a 502 status code (Bad Gateway) is returned.",0,0,others,httpd,0,0
3929,SSLProxyCipherSuite,"Equivalent to SSLCipherSuite, but for the proxy connection. Please refer to SSLCipherSuite for additional information.",0,0,others,httpd,0,0
3930,SSLProxyEngine,This directive toggles the usage of the SSL/TLS Protocol Engine for proxy. This is usually used inside a <VirtualHost> section to enable SSL/TLS for proxy usage in a particular virtual host. By default the SSL/TLS Protocol Engine is disabled for proxy both for the main server and all configured virtual hosts.,0,0,others,httpd,0,1
3932,SSLProxyMachineCertificateFile,This directive sets the all-in-one file where you keep the certificates and keys used for authentication of the proxy server to remote servers.,0,0,others,httpd,0,0
3933,SSLProxyMachineCertificatePath,This directive sets the directory where you keep the certificates and keys used for authentication of the proxy server to remote servers.,0,0,others,httpd,0,0
3934,SSLProxyProtocol,This directive can be used to control the SSL protocol flavors mod_ssl should use when establishing its server environment for proxy . It will only connect to servers using one of the provided protocols.,0,0,others,httpd,0,0
3935,SSLProxyVerify,"When a proxy is configured to forward requests to a remote SSL server, this directive can be used to configure certificate verification of the remote server.",1,2,security-tradeoff,httpd,0,0
3936,SSLProxyVerifyDepth,This directive sets how deeply mod_ssl should verify before deciding that the remote server does not have a valid certificate.,1,2,security-tradeoff,httpd,0,0
3937,SSLRandomSeed,This configures one or more sources for seeding the Pseudo Random Number Generator (PRNG) in OpenSSL at startup time (context is startup) and/or just before a new SSL connection is established (context is connect). This directive can only be used in the global server context because the PRNG is a global facility.,0,0,others,httpd,0,0
3938,SSLSessionCache,"This configures the storage type of the global/inter-process SSL Session Cache. This cache is an optional facility which speeds up parallel request processing. For requests to the same server process (via HTTP keep-alive), OpenSSL already caches the SSL session information locally. But because modern clients request inlined images and other data via parallel requests (usually up to four parallel requests are common) those requests are served by different pre-forked server processes. Here an inter-process cache helps to avoid unnecessary session handshakes.",1,6,function-tradeoff,httpd,0,0
3939,SSLSessionCacheTimeout,"This directive sets the timeout in seconds for the information stored in the global/inter-process SSL Session Cache, the OpenSSL internal memory cache and for sessions resumed by TLS session resumption (RFC 5077). It can be set as low as 15 for testing, but should be set to higher values like 300 in real life.",1,2,security-tradeoff,httpd,0,1
3940,SSLSessionTicketKeyFile,"Optionally configures a secret key for encrypting and decrypting TLS session tickets, as defined in RFC 5077. Primarily suitable for clustered environments where TLS sessions information should be shared between multiple nodes. For single-instance httpd setups, it is recommended to not configure a ticket key file, but to rely on (random) keys generated by mod_ssl at startup, instead.",0,0,others,httpd,0,1
3941,SSLSessionTickets,This directive allows to enable or disable the use of TLS session tickets (RFC 5077).,0,0,others,httpd,0,0
3942,SSLSRPUnknownUserSeed,"This directive sets the seed used to fake SRP user parameters for unknown users, to avoid leaking whether a given user exists. Specify a secret string. If this directive is not used, then Apache will return the UNKNOWN_PSK_IDENTITY alert to clients who specify an unknown username.",0,0,others,httpd,0,1
3943,SSLSRPVerifierFile,"This directive enables TLS-SRP and sets the path to the OpenSSL SRP (Secure Remote Password) verifier file containing TLS-SRP usernames, verifiers, salts, and group parameters.",0,0,others,httpd,0,0
3944,SSLStaplingCache,"Configures the cache used to store OCSP responses which get included in the TLS handshake if SSLUseStapling is enabled. Configuration of a cache is mandatory for OCSP stapling. With the exception of none and nonenotnull, the same storage types are supported as with SSLSessionCache.",1,6,function-tradeoff,httpd,0,1
3945,SSLStaplingErrorCacheTimeout,"Sets the timeout in seconds before invalid responses in the OCSP stapling cache (configured through SSLStaplingCache) will expire. To set the cache timeout for valid responses, see SSLStaplingStandardCacheTimeout.",1,2,security-tradeoff,httpd,0,0
3946,SSLStaplingFakeTryLater,"When enabled and a query to an OCSP responder for stapling purposes fails, mod_ssl will synthesize a ""tryLater"" response for the client. Only effective if SSLStaplingReturnResponderErrors is also enabled.",0,0,others,httpd,0,1
3948,SSLStaplingResponderTimeout,This option sets the timeout for queries to OCSP responders when SSLUseStapling is enabled and mod_ssl is querying a responder for OCSP stapling purposes.,0,0,others,httpd,0,0
3950,SSLStaplingResponseTimeSkew,This option sets the maximum allowable time skew when mod_ssl checks the thisUpdate and nextUpdate fields of OCSP responses which get included in the TLS handshake (OCSP stapling). Only applicable if SSLUseStapling is turned on.,0,0,others,httpd,0,0
3951,SSLStaplingReturnResponderErrors,"When enabled, mod_ssl will pass responses from unsuccessful stapling related OCSP queries (such as responses with an overall status other than ""successful"", responses with a certificate status other than ""good"", expired responses etc.) on to the client. If set to off, only responses indicating a certificate status of ""good"" will be included in the TLS handshake.",0,0,others,httpd,0,0
3952,SSLStaplingStandardCacheTimeout,"Sets the timeout in seconds before responses in the OCSP stapling cache (configured through SSLStaplingCache) will expire. This directive applies to valid responses, while SSLStaplingErrorCacheTimeout is used for controlling the timeout for invalid/unavailable responses.",1,5,workload-specific,httpd,0,0
3954,SSLUserName,"This directive sets the ""user"" field in the Apache request object. This is used by lower modules to identify the user with a character string. In particular, this may cause the environment variable REMOTE_USER to be set. The varname can be any of the SSL environment variables.",0,0,others,httpd,0,0
3955,SSLUseStapling,"This option enables OCSP stapling, as defined by the ""Certificate Status Request"" TLS extension specified in RFC 6066. If enabled (and requested by the client), mod_ssl will include an OCSP response for its own certificate in the TLS handshake. Configuring an SSLStaplingCache is a prerequisite for enabling OCSP stapling.",0,0,others,httpd,0,0
3956,SSLVerifyClient,This directive sets the Certificate verification level for the Client Authentication. Notice that this directive can be used both in per-server and per-directory context. In per-server context it applies to the client authentication process used in the standard SSL handshake when a connection is established. In per-directory context it forces a SSL renegotiation with the reconfigured client verification level after the HTTP request was read but before the HTTP response is sent.,1,2,security-tradeoff,httpd,0,0
3957,SSLVerifyDepth,This directive sets how deeply mod_ssl should verify before deciding that the clients don't have a valid certificate. Notice that this directive can be used both in per-server and per-directory context. In per-server context it applies to the client authentication process used in the standard SSL handshake when a connection is established. In per-directory context it forces a SSL renegotiation with the reconfigured client verification depth after the HTTP request was read but before the HTTP response is sent.,1,2,security-tradeoff,httpd,0,1
3958,StartServers,"The StartServers directive sets the number of child server processes created on startup. As the number of processes is dynamically controlled depending on the load, (see MinSpareThreads, MaxSpareThreads, MinSpareServers, MaxSpareServers) there is usually little reason to adjust this parameter.",1,1,resource,httpd,0,0
3959,StartThreads,"Number of threads created on startup. As the number of threads is dynamically controlled depending on the load, (see MinSpareThreads, MaxSpareThreads, MinSpareServers, MaxSpareServers) there is usually little reason to adjust this parameter.",1,1,resource,httpd,0,0
3960,Suexec,"When On, startup will fail if the suexec binary doesn't exist or has an invalid owner or file mode.",0,0,others,httpd,0,0
3961,SuexecUserGroup,The SuexecUserGroup directive allows you to specify a user and group for CGI programs to run as. Non-CGI requests are still processed with the user specified in the User directive.,0,0,others,httpd,0,1
3962,ThreadLimit,"This directive sets the maximum configured value for ThreadsPerChild for the lifetime of the Apache httpd process. Any attempts to change this directive during a restart will be ignored, but ThreadsPerChild can be modified during a restart up to the value of this directive.",1,1,resource,httpd,0,0
3963,ThreadsPerChild,"This directive sets the number of threads created by each child process. The child creates these threads at startup and never creates more. If using an MPM like mpm_winnt, where there is only one child process, this number should be high enough to handle the entire load of the server. If using an MPM like worker, where there are multiple child processes, the total number of threads should be high enough to handle the common load on the server.",1,1,resource,httpd,0,1
3964,ThreadStackSize,"The ThreadStackSize directive sets the size of the stack (for autodata) of threads which handle client connections and call modules to help process those connections. In most cases the operating system default for stack size is reasonable, but there are some conditions where it may need to be adjusted:",1,1,resource,httpd,0,1
3965,TimeOut,The TimeOut directive defines the length of time Apache httpd will wait for I/O in various circumstances:,0,0,others,httpd,0,0
3966,TraceEnable,"This directive overrides the behavior of TRACE for both the core server and mod_proxy. The default TraceEnable on permits TRACE requests per RFC 2616, which disallows any request body to accompany the request. TraceEnable off causes the core server and mod_proxy to return a 405 (Method not allowed) error to the client.",0,0,others,httpd,0,0
3967,TransferLog,"This directive has exactly the same arguments and effect as the CustomLog directive, with the exception that it does not allow the log format to be specified explicitly or for conditional logging of requests. Instead, the log format is determined by the most recently specified LogFormat directive which does not define a nickname. Common Log Format is used if no other format has been specified.",0,0,others,httpd,0,0
3968,TypesConfig,"The TypesConfig directive sets the location of the media types configuration file. File-path is relative to the ServerRoot. This file sets the default list of mappings from filename extensions to content types. Most administrators use the mime.types file provided by their OS, which associates common filename extensions with the official list of IANA registered media types maintained at http://www.iana.org/assignments/media-types/index.html as well as a large number of unofficial types. This simplifies the httpd.conf file by providing the majority of media-type definitions, and may be overridden by AddType directives as needed. You should not edit the mime.types file, because it may be replaced when you upgrade your server.",0,0,others,httpd,0,0
3969,UnDefine,Undoes the effect of a Define or of passing a -D argument to httpd.,0,0,others,httpd,0,1
3970,UndefMacro,The UndefMacro directive undefines a macro which has been defined before hand.,0,0,others,httpd,0,0
3971,UnsetEnv,Removes one or more internal environment variables from those passed on to CGI scripts and SSI pages.,0,0,others,httpd,0,0
3972,Use,The Use directive controls the use of a macro. The specified macro is expanded. It must be given the same number of arguments as in the macro definition. The provided values are associated to their corresponding initial parameters and are substituted before processing.,0,0,others,httpd,0,0
3974,UseCanonicalPhysicalPort,"In many situations Apache httpd must construct a self-referential URL -- that is, a URL that refers back to the same server. With UseCanonicalPhysicalPort On, Apache httpd will, when constructing the canonical port for the server to honor the UseCanonicalName directive, provide the actual physical port number being used by this request as a potential port. With UseCanonicalPhysicalPort Off, Apache httpd will not ever use the actual physical port number, instead relying on all configured information to construct a valid port number.",0,0,others,httpd,0,1
3975,User,"The User directive sets the user ID as which the server will answer requests. In order to use this directive, the server must be run initially as root. If you start the server as a non-root user, it will fail to change to the lesser privileged user, and will instead continue to run as that original user. If you do start the server as root, then it is normal for the parent process to remain running as root. Unix-userid is one of:",0,0,others,httpd,0,0
3979,VirtualHost,"<VirtualHost> and </VirtualHost> are used to enclose a group of directives that will apply only to a particular virtual host. Any directive that is allowed in a virtual host context may be used. When the server receives a request for a document on a particular virtual host, it uses the configuration directives enclosed in the <VirtualHost> section. Addr can be any of the following, optionally followed by a colon and a port number (or *):",0,0,others,httpd,0,1
3983,XBitHack,The XBitHack directive controls the parsing of ordinary html documents. This directive only affects files associated with the MIME-type text/html. XBitHack can take on the following values:,0,0,others,httpd,0,1
3985,xml2EncDefault,"If you are processing data with known encoding but no encoding information, you can set this default to help mod_xml2enc process the data correctly. For example, to work with the default value of Latin1 (iso-8859-1) specified in HTTP/1.0, use:",0,0,others,httpd,0,0
3986,xml2StartParse,Specify that the markup parser should start at the first instance of any of the elements specified. This can be used as a workaround where a broken backend inserts leading junk that messes up the parser (example here).,0,0,others,httpd,0,0
3987,access_token_duration,Number of seconds for the OAuth Access Token to remain valid after being created. This is the amount of time the consumer has to interact with the service provider (which is typically keystone). Setting this option to zero means that access tokens will last forever.,0,0,others,keystone,0,0
3991,admin_port,The port number for the admin service to listen on.,0,0,others,keystone,0,0
3993,admin_project_name,"This is a special project which represents cloud-level administrator privileges across services. Tokens scoped to this project will contain a true is_admin_project attribute to indicate to policy systems that the role assignments on that specific project should apply equally across every project. If left unset, then there is no admin project, and thus no explicit means of cross-project role assignments. [resource] admin_project_domain_name must also be set to use this option.",0,0,others,keystone,0,0
3994,admin_token,"Using this feature is NOT recommended. Instead, use the keystone-manage bootstrap command. The value of this option is treated as a ""shared secret"" that can be used to bootstrap Keystone through the API. This ""token"" does not represent a user (it has no identity), and carries no explicit authorization (it effectively bypasses most authorization checks). If set to None, the value is ignored and the admin_token middleware is effectively disabled.",0,0,others,keystone,0,1
3996,allow_credentials,Indicate that the actual request can include user credentials,0,0,others,keystone,0,0
3997,allow_expired_window,This controls the number of seconds that a token can be retrieved for beyond the built-in expiry time. This allows long running operations to succeed. Defaults to two days.,0,0,others,keystone,0,0
3998,allow_headers,Indicate which header field names may be used during the actual request.,0,0,others,keystone,0,1
3999,allow_methods,Indicate which methods can be used during the actual request.,0,0,others,keystone,0,0
4000,allow_redelegation,"Allows authorization to be redelegated from one user to another, effectively chaining trusts together. When disabled, the remaining_uses attribute of a trust is constrained to be zero.",1,2,security-tradeoff,keystone,0,0
4001,allow_rescope_scoped_token,"This toggles whether scoped tokens may be re-scoped to a new project or domain, thereby preventing users from exchanging a scoped token (including those with a default project scope) for any other token. This forces users to either authenticate for unscoped tokens (and later exchange that unscoped token for tokens with a more specific scope) or to provide their credentials in every request for a scoped token to avoid re-scoping altogether.",1,2,security-tradeoff,keystone,0,1
4003,amqp_auto_delete,Auto-delete queues in AMQP.,1,5,workload-specific,keystone,0,1
4004,amqp_durable_queues,Use durable queues in AMQP.,1,5,workload-specific,keystone,0,0
4006,application_credential,Entry point for the application_credential auth plugin module in the keystone.auth.application_credential namespace. You do not need to set this unless you are overriding keystone's own application_credential authentication plugin.,0,0,others,keystone,0,0
4007,assertion_expiration_time,"Determines the lifetime for any SAML assertions generated by keystone, using NotOnOrAfter attributes.",0,0,others,keystone,0,0
4008,assertion_prefix,Prefix to use when filtering environment variable names for federated assertions. Matched variables are passed into the federated mapping engine.,0,0,others,keystone,0,0
4009,auth_pool_connection_lifetime,"The maximum end user authentication connection lifetime to the LDAP server in seconds. When this lifetime is exceeded, the connection will be unbound and removed from the connection pool. This option has no effect unless [ldap] use_auth_pool is also enabled.",0,0,others,keystone,0,0
4010,auth_pool_size,The size of the connection pool to use for end user authentication. This option has no effect unless [ldap] use_auth_pool is also enabled.,1,1,resource,keystone,0,0
4011,auth_ttl,The length of time in minutes for which a signed EC2 or S3 token request is valid from the timestamp contained in the token request.,0,0,others,keystone,0,1
4012,backend.B,"Cache backend module. For eventlet-based or environments with hundreds of threaded servers, Memcache with pooling (oslo_cache.memcache_pool) is recommended. For environments with less than 100 threaded servers, Memcached (dogpile.cache.memcached) or Redis (dogpile.cache.redis) is recommended. Test environments with a single instance of the server can use the dogpile.cache.memory backend.",0,0,others,keystone,0,0
4013,backend,The back end to use for the database.,0,0,others,keystone,0,1
4014,backend_argument,"Arguments supplied to the backend module. Specify this option once per argument to be passed to the dogpile.cache backend. Example format: ""<argname>:<value>"".",0,0,others,keystone,0,1
4015,backends,Additional backends that can perform health checks and report that information back as part of a request.,0,0,others,keystone,0,0
4016,backward_compatible_ids,"The format of user and group IDs changed in Juno for backends that do not generate UUIDs (for example, LDAP), with keystone providing a hash mapping to the underlying attribute in LDAP. By default this mapping is disabled, which ensures that existing IDs will not change. Even when the mapping is enabled by using domain-specific drivers ([identity] domain_specific_drivers_enabled), any users and groups from the default domain being handled by LDAP will still not be mapped to ensure their IDs remain backward compatible. Setting this value to false will enable the new mapping for all backends, including the default LDAP driver. It is only guaranteed to be safe to enable this option if you do not already have assignments for users and groups from the default LDAP domain, and you consider it to be acceptable for Keystone to provide the different IDs to clients than it did previously (existing IDs in the API will suddenly change). Typically this means that the only time you can set this value to false is when configuring a fresh installation, although that is the recommended value.",0,0,others,keystone,0,0
4020,cache_on_issue.B,Enable storing issued receipt data to receipt validation cache so that first receipt validation doesn't actually cause full validation cycle. This option has no effect unless global caching and receipt caching are enabled.,1,4,limited-side-effect,keystone,0,0
4021,cache_on_issue,Enable storing issued token data to token validation cache so that first token validation doesn't actually cause full validation cycle. This option has no effect unless global caching is enabled and will still cache tokens even if [token] caching = False.,1,4,limited-side-effect,keystone,0,0
4022,cache_time.B,The number of seconds to cache receipt creation and validation data. This has no effect unless both global and [receipt] caching are enabled.,1,5,workload-specific,keystone,0,0
4023,cache_time.C,The number of seconds to cache token creation and validation data. This has no effect unless both global and [token] caching are enabled.,1,5,workload-specific,keystone,0,0
4024,cache_time.D,Time to cache access rule data in seconds. This has no effect unless global caching is enabled.,1,5,workload-specific,keystone,0,0
4025,cache_time.E,Time to cache application credential data in seconds. This has no effect unless global caching is enabled.,1,5,workload-specific,keystone,0,0
4026,cache_time.F,"Time to cache catalog data (in seconds). This has no effect unless global and catalog caching are both enabled. Catalog data (services, endpoints, etc.) typically does not change frequently, and so a longer duration than the global default may be desirable.",1,5,workload-specific,keystone,0,0
4027,cache_time.G,Time to cache identity data (in seconds). This has no effect unless global and identity caching are enabled.,1,5,workload-specific,keystone,0,0
4028,cache_time.H,Time to cache resource data in seconds. This has no effect unless global caching is enabled.,1,5,workload-specific,keystone,0,0
4029,cache_time.I,"Time to cache role data, in seconds. This has no effect unless both global caching and [role] caching are enabled.",1,5,workload-specific,keystone,0,0
4030,cache_time.J,Time to cache the revocation list and the revocation events (in seconds). This has no effect unless global and [revoke] caching are both enabled.,1,5,workload-specific,keystone,0,0
4031,cache_time.K,"Time to cache unified limit data, in seconds. This has no effect unless both global caching and [unified_limit] caching are enabled.",1,5,workload-specific,keystone,0,0
4032,cache_time,"Time-to-live (TTL, in seconds) to cache domain-specific configuration data. This has no effect unless [domain_config] caching is enabled.",1,5,workload-specific,keystone,0,0
4033,caching.B,Toggle for access rules caching. This has no effect unless global caching is enabled.,1,4,limited-side-effect,keystone,0,0
4034,caching.C,Toggle for application credential caching. This has no effect unless global caching is enabled.,1,4,limited-side-effect,keystone,0,0
4035,caching.D,Toggle for caching of the domain-specific configuration backend. This has no effect unless global caching is enabled. There is normally no reason to disable this.,1,4,limited-side-effect,keystone,0,0
4036,caching.E,"Toggle for caching receipt creation and validation data. This has no effect unless global caching is enabled, or if cache_on_issue is disabled as we only cache receipts on issue.",1,4,limited-side-effect,keystone,0,0
4037,caching.F,Toggle for caching token creation and validation data. This has no effect unless global caching is enabled.,1,4,limited-side-effect,keystone,0,0
4038,caching.G,"Toggle for catalog caching. This has no effect unless global caching is enabled. In a typical deployment, there is no reason to disable this.",1,4,limited-side-effect,keystone,0,0
4039,caching.H,Toggle for federation caching. This has no effect unless global caching is enabled. There is typically no reason to disable this.,1,4,limited-side-effect,keystone,0,0
4040,caching.I,Toggle for identity caching. This has no effect unless global caching is enabled. There is typically no reason to disable this.,1,4,limited-side-effect,keystone,0,1
4041,caching.J,Toggle for resource caching. This has no effect unless global caching is enabled.,1,4,limited-side-effect,keystone,0,0
4042,caching.K,Toggle for revocation event caching. This has no effect unless global caching is enabled.,1,4,limited-side-effect,keystone,0,0
4043,caching.L,"Toggle for role caching. This has no effect unless global caching is enabled. In a typical deployment, there is no reason to disable this.",1,4,limited-side-effect,keystone,0,0
4044,caching,"Toggle for unified limit caching. This has no effect unless global caching is enabled. In a typical deployment, there is no reason to disable this.",1,4,limited-side-effect,keystone,0,0
4045,cert_subject,The certificate subject to use when generating a self-signed token signing certificate. There is no reason to set this option unless you are requesting revocation lists in a non-production environment. Use a [signing] certfile issued from a trusted certificate authority instead.,0,0,others,keystone,0,0
4048,change_password_upon_first_use,"Enabling this option requires users to change their password when the user is created, or upon administrative reset. Before accessing any services, affected users will have to change their password. To ignore this requirement for specific users, such as service users, set the options attribute ignore_change_password_upon_first_use to True for the desired user via the update user API. This feature is disabled by default. This feature is only applicable with the sql backend for the [identity] driver.",0,0,others,keystone,0,0
4050,config_prefix,Prefix for building the configuration dictionary for the cache region. This should not need to be changed unless there is another dogpile.cache region with the same configuration name.,0,0,others,keystone,0,0
4051,conn_pool_min_size,The pool size limit for connections expiration policy,0,0,others,keystone,0,1
4052,conn_pool_ttl,The time-to-live in sec of idle connections in the pool,0,0,others,keystone,0,1
4053,connection,The SQLAlchemy connection string to use to connect to the database.,0,0,others,keystone,0,0
4054,connection_debug,"Verbosity of SQL debugging information: 0=None, 100=Everything.",1,6,function-tradeoff,keystone,0,0
4056,connection_recycle_time,Connections which have been present in the connection pool longer than this number of seconds will be replaced with a new one the next time they are checked out from the pool.,0,0,others,keystone,0,0
4057,connection_retry_backoff,Increase the connection_retry_interval by this many seconds after each unsuccessful failover attempt.,0,0,others,keystone,0,0
4058,connection_retry_interval,Seconds to pause before attempting to re-connect.,0,0,others,keystone,0,0
4059,connection_retry_interval_max,Maximum limit for connection_retry_interval + connection_retry_backoff,0,0,others,keystone,0,1
4060,connection_string,Connection string for a notifier backend.,0,0,others,keystone,0,0
4062,connection_trace,Add Python stack traces to SQL as comment strings.,0,0,others,keystone,0,0
4066,db_inc_retry_interval,"If True, increases the interval between retries of a database operation up to db_max_retry_interval.",0,0,others,keystone,0,1
4067,db_max_retries,Maximum retries in case of connection error or deadlock error before error is raised. Set to -1 to specify an infinite retry count.,0,0,others,keystone,0,0
4068,db_max_retry_interval,"If db_inc_retry_interval is set, the maximum seconds between retries of a database operation.",0,0,others,keystone,0,1
4069,db_retry_interval,Seconds between retries of a database transaction.,0,0,others,keystone,0,0
4070,dead_retry,Number of seconds memcached server is considered dead before it is tried again. This is used by the key value store system.,0,0,others,keystone,0,0
4071,debug,"If set to true, the logging level will be set to DEBUG instead of the default INFO level.",1,6,function-tradeoff,keystone,0,0
4072,debug_cache_backend,"Extra debugging from the cache backend (cache keys, get/set/delete/etc calls). This is only really useful if you need to see the specific cache-backend get/set/delete calls with the keys/values. Typically this should be left set to false.",1,6,function-tradeoff,keystone,0,0
4073,debug_level,"Sets the LDAP debugging level for LDAP calls. A value of 0 means that debugging is not enabled. This value is a bitmask, consult your LDAP documentation for possible values.",1,6,function-tradeoff,keystone,0,0
4074,debug_middleware,"If set to true, this enables the oslo debug middleware in Keystone. This Middleware prints a lot of information about the request and the response. It is useful for getting information about the data on the wire (decoded) and passed to the WSGI application pipeline. This middleware has no effect on the ""debug"" setting in the [DEFAULT] section of the config file or setting Keystone's log-level to ""DEBUG""; it is specific to debugging the WSGI data as it enters and leaves Keystone (specific request-related data). This option is used for introspection on the request and response data between the web server (apache, nginx, etc) and Keystone. This middleware is inserted as the first element in the middleware chain and will show the data closest to the wire. WARNING: NOT INTENDED FOR USE IN PRODUCTION. THIS MIDDLEWARE CAN AND WILL EMIT SENSITIVE/PRIVILEGED DATA.",1,6,function-tradeoff,keystone,0,0
4076,default_log_levels,List of package logging levels in logger=LEVEL pairs. This option is ignored if log_config_append is set.,0,0,others,keystone,0,1
4078,default_notify_timeout,The deadline for a sent notification message delivery. Only used when caller does not provide a timeout expiry.,0,0,others,keystone,0,1
4080,default_reply_retry,The maximum number of attempts to re-send a reply message which failed due to a recoverable error.,0,0,others,keystone,0,0
4081,default_reply_timeout,The deadline for an rpc reply message delivery.,0,0,others,keystone,0,1
4083,default_send_timeout,The deadline for an rpc cast or call message delivery. Only used when caller does not provide a timeout expiry.,0,0,others,keystone,0,0
4085,detailed,Show more detailed information as part of the response. Security note: Enabling this option may expose sensitive details about the service being monitored. Be sure to verify that it will not violate your security policies.,1,2,security-tradeoff,keystone,0,0
4086,disable_by_file_path,Check the presence of a file to determine if an application is running on a port. Used by DisableByFileHealthcheck plugin.,0,0,others,keystone,0,0
4087,disable_by_file_paths,"Check the presence of a file based on a port to determine if an application is running on a port. Expects a ""port:path"" list of strings. Used by DisableByFilesPortsHealthcheck plugin.",0,0,others,keystone,0,0
4088,disable_user_account_days_inactive,"The maximum number of days a user can go without authenticating before being considered ""inactive"" and automatically disabled (locked). This feature is disabled by default; set any value to enable it. This feature depends on the sql backend for the [identity] driver. When a user exceeds this threshold and is considered ""inactive"", the user's enabled attribute in the HTTP API may not match the value of the user's enabled column in the user table.",0,0,others,keystone,0,0
4091,domain_name_url_safe,"This controls whether the names of domains are restricted from containing URL-reserved characters. If set to new, attempts to create or update a domain with a URL-unsafe name will fail. If set to strict, attempts to scope a token with a URL-unsafe domain name will fail, thereby forcing all domain names to be updated to be URL-safe.",0,0,others,keystone,0,0
4092,domain_specific_drivers_enabled,"A subset (or all) of domains can have their own identity driver, each with their own partial configuration options, stored in either the resource backend or in a file in a domain configuration directory (depending on the setting of [identity] domain_configurations_from_database). Only values specific to the domain need to be specified in this manner. This feature is disabled by default, but may be enabled by default in a future release; set to true to enable.",0,0,others,keystone,0,0
4113,enable_auto_commit,Enable asynchronous consumer commits,1,4,limited-side-effect,keystone,0,0
4114,enable_proxy_headers_parsing,Whether the application is behind a proxy or not. This determines if the middleware should parse the headers or not.,0,0,others,keystone,0,0
4115,enabled.B,Enable the profiling for all services on this node.,1,6,function-tradeoff,keystone,0,0
4116,enabled,Global toggle for caching.,1,4,limited-side-effect,keystone,0,0
4117,enforce_scope,"This option controls whether or not to enforce scope when evaluating policies. If True, the scope of the token used in the request is compared to the scope_types of the policy being enforced. If the scopes do not match, an InvalidScope exception will be raised. If False, a message will be logged informing operators that policies are being invoked with mismatching scope.",0,0,others,keystone,0,1
4118,enforcement_model,"The enforcement model to use when validating limits associated to projects. Enforcement models will behave differently depending on the existing limits, which may result in backwards incompatible changes if a model is switched in a running deployment.",0,0,others,keystone,0,0
4119,es_doc_type,Document type for notification indexing in elasticsearch.,0,0,others,keystone,0,0
4120,es_scroll_size,Elasticsearch splits large requests in batches. This parameter defines maximum size of each batch (for example: es_scroll_size=10000).,1,5,workload-specific,keystone,0,1
4121,es_scroll_time,"This parameter is a time value parameter (for example: es_scroll_time=2m), indicating for how long the nodes that participate in the search will maintain relevant resources in order to continue and support it.",0,0,others,keystone,0,0
4122,executor_thread_pool_size,Size of executor thread pool when executor is threading or eventlet.,1,1,resource,keystone,0,0
4123,expiration.B,"The amount of time that a receipt should remain valid (in seconds). This value should always be very short, as it represents how long a user has to reattempt auth with the missing auth methods.",0,0,others,keystone,0,0
4124,expiration,"The amount of time that a token should remain valid (in seconds). Drastically reducing this value may break ""long-running"" operations that involve multiple services to coordinate together, and will force users to authenticate with keystone more frequently. Drastically increasing this value will increase the number of tokens that will be simultaneously valid. Keystone tokens are also bearer tokens, so a shorter duration will also reduce the potential security impact of a compromised token.",0,0,others,keystone,0,0
4125,expiration_buffer,The number of seconds after a token has expired before a corresponding revocation event may be purged from the backend.,0,0,others,keystone,0,1
4126,expiration_time,"Default TTL, in seconds, for any cached item in the dogpile.cache region. This applies to any cached method that doesn't have an explicit cache expiration time defined for it.",0,0,others,keystone,0,0
4127,expose_headers,Indicate which headers are safe to expose to the API. Defaults to HTTP Simple Headers.,0,0,others,keystone,0,0
4128,external,"Entry point for the external (REMOTE_USER) auth plugin module in the keystone.auth.external namespace. Supplied drivers are DefaultDomain and Domain. The default driver is DefaultDomain, which assumes that all users identified by the username specified to keystone in the REMOTE_USER variable exist within the context of the default domain. The Domain option expects an additional environment variable be presented to keystone, REMOTE_DOMAIN, containing the domain name of the REMOTE_USER (if REMOTE_DOMAIN is not set, then the default domain will be used instead). You do not need to set this unless you are taking advantage of ""external authentication"", where the application server (such as Apache) is handling authentication instead of keystone.",0,0,others,keystone,0,0
4129,fatal_deprecations,Enables or disables fatal status of deprecations.,0,0,others,keystone,0,0
4130,federated_domain_name,An arbitrary domain name that is reserved to allow federated ephemeral users to have a domain concept. Note that an admin will not be able to create a domain with this name or update an existing domain to this name. You are not advised to change this value unless you really have to.,0,0,others,keystone,0,0
4131,filter_error_trace,Enable filter traces that contain error/exception to a separated place.,0,0,others,keystone,0,0
4135,group_attribute_ignore,List of group attributes to ignore on create and update. or whether a specific group attribute should be filtered for list or show group.,0,0,others,keystone,0,1
4136,group_desc_attribute,The LDAP attribute mapped to group descriptions in keystone.,0,0,others,keystone,0,0
4137,group_filter,The LDAP search filter to use for groups.,0,0,others,keystone,0,0
4138,group_id_attribute,The LDAP attribute mapped to group IDs in keystone. This must NOT be a multivalued attribute. Group IDs are expected to be globally unique across keystone domains and URL-safe.,0,0,others,keystone,0,0
4139,group_member_attribute,The LDAP attribute used to indicate that a user is a member of the group.,0,0,others,keystone,0,0
4141,group_name_attribute,The LDAP attribute mapped to group names in keystone. Group names are expected to be unique only within a keystone domain and are not expected to be URL-safe.,0,0,others,keystone,0,1
4142,group_objectclass,"The LDAP object class to use for groups. If setting this option to posixGroup, you may also be interested in enabling the [ldap] group_members_are_ids option.",0,0,others,keystone,0,1
4144,group_tree_dn,The search base to use for groups. Defaults to the [ldap] suffix value.,0,0,others,keystone,0,0
4145,heartbeat_rate,How often times during the heartbeat_timeout_threshold we check the heartbeat.,0,0,others,keystone,0,0
4146,heartbeat_timeout_threshold,Number of seconds after which the Rabbit broker is considered down if heartbeat's keep-alive fails (0 disable the heartbeat). EXPERIMENTAL,0,0,others,keystone,0,0
4147,hmac_keys,Secret key(s) to use for encrypting context data for performance profiling.,0,0,others,keystone,0,0
4148,idle_timeout,Timeout for inactive connections (in seconds),0,0,others,keystone,0,0
4153,idp_contact_telephone,This is the telephone number of the identity provider's contact person.,0,0,others,keystone,0,0
4154,idp_contact_type,This is the type of contact that best describes the identity provider's contact person.,0,0,others,keystone,0,0
4156,idp_lang,This is the language used by the identity provider's organization.,0,0,others,keystone,0,0
4162,infer_roles,"This controls whether roles should be included with tokens that are not directly assigned to the token's scope, but are instead linked implicitly to other role assignments.",0,0,others,keystone,0,0
4163,insecure_debug,"If set to true, then the server will return information in HTTP responses that may allow an unauthenticated or authenticated user to get more information than normal, such as additional details about why authentication failed. This may be useful for debugging but is insecure.",1,2,security-tradeoff,keystone,0,0
4164,instance_format,The format for an instance that is passed with the log message.,0,0,others,keystone,0,0
4165,instance_uuid_format,The format for an instance UUID that is passed with the log message.,0,0,others,keystone,0,0
4166,issuer_attribute,"The name of the WSGI environment variable used to pass the issuer of the client certificate to keystone. This attribute is used as an identity provider ID for the X.509 tokenless authorization along with the protocol to look up its corresponding mapping. In a typical deployment, there is no reason to change this value.",0,0,others,keystone,0,0
4168,jws_public_key_repository,Directory containing public keys for validating JWS token signatures. This directory must exist in order for keystone's server process to start. It must also be readable by keystone's server process. It must contain at least one public key that corresponds to a private key in keystone.conf [jwt_tokens] jws_private_key_repository. This option is only applicable in deployments issuing JWS tokens and setting keystone.conf [tokens] provider = jws.,0,0,others,keystone,0,0
4169,kafka_consumer_timeout,Default timeout(s) for Kafka consumers,0,0,others,keystone,0,1
4170,kafka_max_fetch_bytes,Max fetch bytes of Kafka consumer,0,0,others,keystone,0,0
4171,key_repository.B,Directory containing Fernet keys used to encrypt and decrypt credentials stored in the credential backend. Fernet keys used to encrypt credentials have no relationship to Fernet keys used to encrypt Fernet tokens. Both sets of keys should be managed separately and require different rotation policies. Do not share this repository with the repository used to manage keys for Fernet tokens.,0,0,others,keystone,0,0
4172,key_repository.C,"Directory containing Fernet receipt keys. This directory must exist before using keystone-manage fernet_setup for the first time, must be writable by the user running keystone-manage fernet_setup or keystone-manage fernet_rotate, and of course must be readable by keystone's server process. The repository may contain keys in one of three states: a single staged key (always index 0) used for receipt validation, a single primary key (always the highest index) used for receipt creation and validation, and any number of secondary keys (all other index values) used for receipt validation. With multiple keystone nodes, each node must share the same key repository contents, with the exception of the staged key (index 0). It is safe to run keystone-manage fernet_rotate once on any one node to promote a staged key (index 0) to be the new primary (incremented from the previous highest index), and produce a new staged key (a new key with index 0); the resulting repository can then be atomically replicated to other nodes without any risk of race conditions (for example, it is safe to run keystone-manage fernet_rotate on host A, wait any amount of time, create a tarball of the directory on host A, unpack it on host B to a temporary location, and atomically move (mv) the directory into place on host B). Running keystone-manage fernet_rotate twice on a key repository without syncing other nodes will result in receipts that can not be validated by all nodes.",0,0,others,keystone,0,0
4173,key_repository,"Directory containing Fernet token keys. This directory must exist before using keystone-manage fernet_setup for the first time, must be writable by the user running keystone-manage fernet_setup or keystone-manage fernet_rotate, and of course must be readable by keystone's server process. The repository may contain keys in one of three states: a single staged key (always index 0) used for token validation, a single primary key (always the highest index) used for token creation and validation, and any number of secondary keys (all other index values) used for token validation. With multiple keystone nodes, each node must share the same key repository contents, with the exception of the staged key (index 0). It is safe to run keystone-manage fernet_rotate once on any one node to promote a staged key (index 0) to be the new primary (incremented from the previous highest index), and produce a new staged key (a new key with index 0); the resulting repository can then be atomically replicated to other nodes without any risk of race conditions (for example, it is safe to run keystone-manage fernet_rotate on host A, wait any amount of time, create a tarball of the directory on host A, unpack it on host B to a temporary location, and atomically move (mv) the directory into place on host B). Running keystone-manage fernet_rotate twice on a key repository without syncing other nodes will result in tokens that can not be validated by all nodes.",0,0,others,keystone,0,0
4174,key_size,Key size (in bits) to use when generating a self-signed token signing certificate. There is no reason to set this option unless you are requesting revocation lists in a non-production environment. Use a [signing] certfile issued from a trusted certificate authority instead.,0,0,others,keystone,0,0
4177,kombu_compression,"EXPERIMENTAL: Possible values are: gzip, bz2. If not set compression will not be used. This option may not be available in future versions.",1,6,function-tradeoff,keystone,0,0
4178,kombu_failover_strategy,Determines how the next RabbitMQ node is chosen in case the one we are currently connected to becomes unavailable. Takes effect only if more than one RabbitMQ node is provided in config.,0,0,others,keystone,0,0
4179,kombu_missing_consumer_retry_timeout,How long to wait a missing client before abandoning to send it its replies. This value should not be longer than rpc_response_timeout.,0,0,others,keystone,0,0
4180,kombu_reconnect_delay,How long to wait before reconnecting in response to an AMQP consumer cancel notification.,0,0,others,keystone,0,0
4181,link_retry_delay,Time to pause between re-connecting an AMQP 1.0 link that failed due to a recoverable error.,0,0,others,keystone,0,0
4182,list_limit.B,"Maximum number of entities that will be returned in a catalog collection. There is typically no reason to set this, as it would be unusual for a deployment to have enough services or endpoints to exceed a reasonable limit.",0,0,others,keystone,0,0
4183,list_limit.C,Maximum number of entities that will be returned in a policy collection.,0,0,others,keystone,0,0
4184,list_limit.D,Maximum number of entities that will be returned in a resource collection.,0,0,others,keystone,0,0
4185,list_limit.E,Maximum number of entities that will be returned in a role collection. This may be useful to tune if you have a large number of discrete roles in your deployment.,0,0,others,keystone,0,0
4186,list_limit.F,Maximum number of entities that will be returned in a role collection. This may be useful to tune if you have a large number of unified limits in your deployment.,0,0,others,keystone,0,0
4187,list_limit.G,Maximum number of entities that will be returned in an identity collection.,0,0,others,keystone,0,0
4188,list_limit,"The maximum number of entities that will be returned in a collection. This global limit may be then overridden for a specific driver, by specifying a list_limit in the appropriate section (for example, [assignment]). No limit is set by default. In larger deployments, it is recommended that you set this to a reasonable number to prevent operations like listing all users and projects from placing an unnecessary load on the system.",0,0,others,keystone,0,0
4189,lockout_duration,The number of seconds a user account will be locked when the maximum number of failed authentication attempts (as specified by [security_compliance] lockout_failure_attempts) is exceeded. Setting this option will have no effect unless you also set [security_compliance] lockout_failure_attempts to a non-zero value. This feature depends on the sql backend for the [identity] driver.,0,0,others,keystone,0,1
4190,lockout_failure_attempts,"The maximum number of times that a user can fail to authenticate before the user account is locked for the number of seconds specified by [security_compliance] lockout_duration. This feature is disabled by default. If this feature is enabled and [security_compliance] lockout_duration is not set, then users may be locked out indefinitely until the user is explicitly enabled via the API. This feature depends on the sql backend for the [identity] driver.",0,0,others,keystone,0,0
4191,log_config_append,"The name of a logging configuration file. This file is appended to any existing logging configuration files. For details about logging configuration files, see the Python logging module documentation. Note that when logging configuration files are used then all logging configuration is set in the configuration file and other logging configuration options are ignored (for example, log-date-format).",0,0,others,keystone,0,0
4192,log_date_format,Defines the format string for %(asctime)s in log records. Default: the value above . This option is ignored if log_config_append is set.,0,0,others,keystone,0,0
4194,log_file,"(Optional) Name of log file to send logging output to. If no default is set, logging will go to stderr as defined by use_stderr. This option is ignored if log_config_append is set.",0,0,others,keystone,0,0
4195,log_rotate_interval,"The amount of time before the log files are rotated. This option is ignored unless log_rotation_type is setto ""interval"".",0,0,others,keystone,0,0
4197,log_rotation_type,Log rotation type.,0,0,others,keystone,0,0
4198,logging_context_format_string,Format string to use for log messages with context. Used by oslo_log.formatters.ContextFormatter,0,0,others,keystone,0,0
4199,logging_debug_format_suffix,Additional data to append to log message when logging level for the message is DEBUG. Used by oslo_log.formatters.ContextFormatter,0,0,others,keystone,0,0
4200,logging_default_format_string,Format string to use for log messages when context is undefined. Used by oslo_log.formatters.ContextFormatter,0,0,others,keystone,0,0
4201,logging_exception_prefix,Prefix each line of exception output with this format. Used by oslo_log.formatters.ContextFormatter,0,0,others,keystone,0,0
4202,logging_user_identity_format,Defines the format string for %(user_identity)s that is used in logging_context_format_string. Used by oslo_log.formatters.ContextFormatter,0,0,others,keystone,0,0
4203,mapped,Entry point for the mapped auth plugin module in the keystone.auth.mapped namespace. You do not need to set this unless you are overriding keystone's own mapped authentication plugin.,0,0,others,keystone,0,0
4204,max_active_keys,"This controls how many keys are held in rotation by keystone-manage fernet_rotate before they are discarded. The default value of 3 means that keystone will maintain one staged key (always index 0), one primary key (the highest numerical index), and one secondary key (every other index). Increasing this value means that additional secondary keys will be kept in the rotation.",0,0,others,keystone,0,0
4205,max_age,Maximum cache age of CORS preflight requests.,0,0,others,keystone,0,0
4206,max_logfile_count,Maximum number of rotated log files.,0,0,others,keystone,0,1
4207,max_logfile_size_mb,"Log file maximum size in MB. This option is ignored if ""log_rotation_type"" is not set to ""size"".",0,0,others,keystone,0,1
4208,max_overflow,"If set, use this value for max_overflow with SQLAlchemy.",0,0,others,keystone,0,1
4209,max_param_size,Limit the sizes of user & project ID/names.,0,0,others,keystone,0,1
4210,max_password_length,Maximum allowed length for user passwords. Decrease this value to improve performance. Changing this value does not effect existing passwords.,1,2,security-tradeoff,keystone,0,0
4211,max_poll_records,The maximum number of records returned in a poll call,0,0,others,keystone,0,0
4212,max_pool_size,Maximum number of SQL connections to keep open in a pool. Setting a value of 0 indicates no limit.,1,1,resource,keystone,0,0
4213,max_project_tree_depth,"Maximum depth of the project hierarchy, excluding the project acting as a domain at the top of the hierarchy. WARNING: Setting it to a large value may adversely impact performance.",1,3,reliability-tradeoff,keystone,0,0
4214,max_redelegation_count,Maximum number of times that authorization can be redelegated from one user to another in a chain of trusts. This number may be reduced further for a specific trust.,0,0,others,keystone,0,0
4215,max_request_body_size,"The maximum body size for each request, in bytes.",0,0,others,keystone,0,0
4216,max_retries,Maximum number of database connection retries during startup. Set to -1 to specify an infinite retry count.,0,0,others,keystone,0,0
4217,max_token_size,"Similar to [DEFAULT] max_param_size, but provides an exception for token values. With Fernet tokens, this can be set as low as 255. With UUID tokens, this should be set to 32).",0,0,others,keystone,0,0
4218,memcache_dead_retry,Number of seconds memcached server is considered dead before it is tried again. (dogpile.cache.memcache and oslo_cache.memcache_pool backends only).,0,0,others,keystone,0,0
4219,memcache_pool_connection_get_timeout,Number of seconds that an operation will wait to get a memcache client connection.,0,0,others,keystone,0,0
4220,memcache_pool_maxsize,Max total number of open connections to every memcached server. (oslo_cache.memcache_pool backend only).,1,1,resource,keystone,0,0
4221,memcache_pool_unused_timeout,Number of seconds a connection to memcached is held unused in the pool before it is closed. (oslo_cache.memcache_pool backend only).,0,0,others,keystone,0,0
4222,memcache_servers,"Memcache servers in the format of ""host:port"". (dogpile.cache.memcache and oslo_cache.memcache_pool backends only).",0,0,others,keystone,0,1
4223,memcache_socket_timeout,Timeout in seconds for every call to a server. (dogpile.cache.memcache and oslo_cache.memcache_pool backends only).,0,0,others,keystone,0,0
4224,methods,"Allowed authentication methods. Note: You should disable the external auth method if you are currently using federation. External auth and federation both use the REMOTE_USER variable. Since both the mapped and external plugin are being invoked to validate attributes in the request environment, it can cause conflicts.",0,0,others,keystone,0,0
4225,min_pool_size,Minimum number of SQL connections to keep open in a pool.,0,0,others,keystone,0,0
4226,minimum_password_age,"The number of days that a password must be used before the user can change it. This prevents users from changing their passwords immediately in order to wipe out their password history and reuse an old password. This feature does not prevent administrators from manually resetting passwords. It is disabled by default and allows for immediate password changes. This feature depends on the sql backend for the [identity] driver. Note: If [security_compliance] password_expires_days is set, then the value for this option should be less than the password_expires_days.",0,0,others,keystone,0,0
4228,mysql_enable_ndb,"If True, transparently enables support for handling MySQL Cluster (NDB).",0,0,others,keystone,0,0
4230,notification_format,"Define the notification format for identity service events. A basic notification only has information about the resource being operated on. A cadf notification has the same information, as well as information about the initiator of the event. The cadf option is entirely backwards compatible with the basic option, but is fully CADF-compliant, and is recommended for auditing use cases.",0,0,others,keystone,0,0
4231,notification_opt_out,"You can reduce the number of notifications keystone emits by explicitly opting out. Keystone will not emit notifications that match the patterns expressed in this list. Values are expected to be in the form of identity.<resource_type>.<operation>. By default, all notifications related to authentication are automatically suppressed. This field can be set multiple times in order to opt-out of multiple notification topics. For example, the following suppresses notifications describing user creation or successful authentication events: notification_opt_out=identity.user.create notification_opt_out=identity.authenticate.success",0,0,others,keystone,0,0
4233,notify_server_credit,Window size for incoming Notification messages,1,1,resource,keystone,0,0
4234,oauth1,Entry point for the OAuth 1.0a auth plugin module in the keystone.auth.oauth1 namespace. You do not need to set this unless you are overriding keystone's own oauth1 authentication plugin.,0,0,others,keystone,0,0
4235,page_size,Defines the maximum number of results per page that keystone should request from the LDAP server when listing objects. A value of zero (0) disables paging.,0,0,others,keystone,0,0
4238,password_expires_days,"The number of days for which a password will be considered valid before requiring it to be changed. This feature is disabled by default. If enabled, new password changes will have an expiration date, however existing passwords would not be impacted. This feature depends on the sql backend for the [identity] driver.",0,0,others,keystone,0,1
4239,password_hash_algorithm,The password hashing algorithm to use for passwords stored within keystone.,1,2,security-tradeoff,keystone,0,1
4240,password_hash_rounds,"This option represents a trade off between security and performance. Higher values lead to slower performance, but higher security. Changing this option will only affect newly created passwords as existing password hashes already have a fixed number of rounds applied, so it is safe to tune this option in a running cluster. The default for bcrypt is 12, must be between 4 and 31, inclusive. The default for scrypt is 16, must be within range(1,32). The default for pbkdf_sha512 is 60000, must be within range(1,1<<32) WARNING: If using scrypt, increasing this value increases BOTH time AND memory requirements to hash a password.",1,2,security-tradeoff,keystone,0,0
4241,password_regex,"The regular expression used to validate password strength requirements. By default, the regular expression will match any password. The following is an example of a pattern which requires at least 1 letter, 1 digit, and have a minimum length of 7 characters: ^(?=.*d)(?=.*[a-zA-Z]).{7,}$ This feature depends on the sql backend for the [identity] driver.",0,0,others,keystone,0,0
4242,password_regex_description,"Describe your password regular expression here in language for humans. If a password fails to match the regular expression, the contents of this configuration variable will be returned to users to explain why their requested password was insufficient.",0,0,others,keystone,0,0
4244,permissive,"Toggles permissive mode for access rules. When enabled, application credentials can be created with any access rules regardless of operator's configuration.",1,2,security-tradeoff,keystone,0,1
4245,policy_default_rule,Default rule. Enforced when a requested rule is not found.,0,0,others,keystone,0,0
4248,pool_connection_get_timeout,Number of seconds that an operation will wait to get a memcache client connection. This is used by the key value store system.,0,0,others,keystone,0,0
4250,pool_connection_timeout,The connection timeout to use when pooling LDAP connections. A value of -1 means that connections will never timeout. This option has no effect unless [ldap] use_pool is also enabled.,0,0,others,keystone,0,1
4251,pool_maxsize,Max total number of open connections to every memcached server. This is used by the key value store system.,1,1,resource,keystone,0,0
4252,pool_retry_delay,The number of seconds to wait before attempting to reconnect to the LDAP server. This option has no effect unless [ldap] use_pool is also enabled.,0,0,others,keystone,0,0
4253,pool_retry_max,The maximum number of times to attempt reconnecting to the LDAP server before aborting. A value of zero prevents retries. This option has no effect unless [ldap] use_pool is also enabled.,0,0,others,keystone,0,0
4254,pool_size.B,Pool Size for Kafka Consumers,1,1,resource,keystone,0,0
4255,pool_size,The size of the LDAP connection pool. This option has no effect unless [ldap] use_pool is also enabled.,1,1,resource,keystone,0,0
4256,pool_timeout,"If set, use this value for pool_timeout with SQLAlchemy.",0,0,others,keystone,0,0
4257,pool_unused_timeout,Number of seconds a connection to memcached is held unused in the pool before it is closed. This is used by the key value store system.,0,0,others,keystone,0,0
4258,pre_settled,Send messages of this type pre-settled. Pre-settled messages will not receive acknowledgement from the peer. Note well: pre-settled messages may be silently discarded if the delivery fails. Permitted values: 'rpc-call' - send RPC Calls pre-settled 'rpc-reply'- send RPC Replies pre-settled 'rpc-cast' - Send RPC Casts pre-settled 'notify' - Send Notifications pre-settled,0,0,others,keystone,0,0
4259,producer_batch_size,Size of batch for the producer async send,0,0,others,keystone,0,0
4260,producer_batch_timeout,Upper bound on the delay for KafkaProducer batching in seconds,0,0,others,keystone,0,0
4261,prohibited_implied_role,A list of role names which are prohibited from being an implied role.,0,0,others,keystone,0,0
4262,project_name_url_safe,"This controls whether the names of projects are restricted from containing URL-reserved characters. If set to new, attempts to create or update a project with a URL-unsafe name will fail. If set to strict, attempts to scope a token with a URL-unsafe project name will fail, thereby forcing all project names to be updated to be URL-safe.",0,0,others,keystone,0,1
4263,protocol,"The federated protocol ID used to represent X.509 tokenless authorization. This is used in combination with the value of [tokenless_auth] issuer_attribute to find a corresponding federated mapping. In a typical deployment, there is no reason to change this value.",0,0,others,keystone,0,0
4264,provider.B,"Entry point for credential encryption and decryption operations in the keystone.credential.provider namespace. Keystone only provides a fernet driver, so there's no reason to change this unless you are providing a custom entry point to encrypt and decrypt credentials.",0,0,others,keystone,0,0
4265,provider.C,"Entry point for the receipt provider in the keystone.receipt.provider namespace. The receipt provider controls the receipt construction and validation operations. Keystone includes just the fernet receipt provider for now. fernet receipts do not need to be persisted at all, but require that you run keystone-manage fernet_setup (also see the keystone-manage fernet_rotate command).",0,0,others,keystone,0,0
4266,provider,"Entry point for the token provider in the keystone.token.provider namespace. The token provider controls the token construction, validation, and revocation operations. Supported upstream providers are fernet and jws. Neither fernet or jws tokens require persistence and both require additional setup. If using fernet, you're required to run keystone-manage fernet_setup, which creates symmetric keys used to encrypt tokens. If using jws, you're required to generate an ECDSA keypair using a SHA-256 hash algorithm for signing and validating token, which can be done with keystone-manage create_jws_keypair. Note that fernet tokens are encrypted and jws tokens are only signed. Please be sure to consider this if your deployment has security requirements regarding payload contents used to generate token IDs.",0,0,others,keystone,0,0
4267,proxies,Proxy classes to import that will affect the way the dogpile.cache backend functions. See the dogpile.cache documentation on changing-backend-behavior.,0,0,others,keystone,0,0
4271,public_port,The port number for the public service to listen on.,0,0,others,keystone,0,1
4272,publish_errors,Enables or disables publication of error events.,0,0,others,keystone,0,0
4273,query_scope,"The search scope which defines how deep to search within the search base. A value of one (representing oneLevel or singleLevel) indicates a search of objects immediately below to the base object, but does not include the base object itself. A value of sub (representing subtree or wholeSubtree) indicates a search of both the base object itself and the entire subtree below it.",0,0,others,keystone,0,0
4274,rabbit_ha_queues,"Try to use HA queues in RabbitMQ (x-ha-policy: all). If you change this option, you must wipe the RabbitMQ database. In RabbitMQ 3.0, queue mirroring is no longer controlled by the x-ha-policy argument when declaring a queue. If you just want to make sure that all queues (except those with auto-generated names) are mirrored across all nodes, run: ""rabbitmqctl set_policy HA '^(?!amq.).*' '{""ha-mode"": ""all""}' """,0,0,others,keystone,0,0
4275,rabbit_interval_max,Maximum interval of RabbitMQ connection retries. Default is 30 seconds.,0,0,others,keystone,0,1
4276,rabbit_login_method,The RabbitMQ login method.,0,0,others,keystone,0,1
4277,rabbit_qos_prefetch_count,Specifies the number of messages to prefetch. Setting to zero allows unlimited messages.,1,1,resource,keystone,0,0
4278,rabbit_retry_backoff,How long to backoff for between retries when connecting to RabbitMQ.,0,0,others,keystone,0,0
4279,rabbit_retry_interval,How frequently to retry connecting with RabbitMQ.,0,0,others,keystone,0,0
4281,rate_limit_burst,Maximum number of logged messages per rate_limit_interval.,1,6,function-tradeoff,keystone,0,1
4282,rate_limit_except_level,"Log level name used by rate limiting: CRITICAL, ERROR, INFO, WARNING, DEBUG or empty string. Logs with level greater or equal to rate_limit_except_level are not filtered. An empty string means that all levels are filtered.",1,6,function-tradeoff,keystone,0,1
4283,rate_limit_interval,"Interval, number of seconds, of log rate limiting.",0,0,others,keystone,0,0
4284,relay_state_prefix,"The prefix of the RelayState SAML attribute to use when generating enhanced client and proxy (ECP) assertions. In a typical deployment, there is no reason to change this value.",0,0,others,keystone,0,0
4285,remote_content_type,Content Type to send and receive data for REST based policy check,0,0,others,keystone,0,1
4287,remote_ssl_ca_crt_file,Absolute path to ca cert file for REST based policy check,0,0,others,keystone,0,0
4288,remote_ssl_client_crt_file,Absolute path to client cert for REST based policy check,0,0,others,keystone,0,1
4289,remote_ssl_client_key_file,Absolute path client key file REST based policy check,0,0,others,keystone,0,0
4290,remote_ssl_verify_server_crt,server identity verification for REST based policy check,0,0,others,keystone,0,0
4291,reply_link_credit,Window size for incoming RPC Reply messages.,1,1,resource,keystone,0,0
4292,request_token_duration,Number of seconds for the OAuth Request Token to remain valid after being created. This is the amount of time the user has to authorize the token. Setting this option to zero means that request tokens will last forever.,0,0,others,keystone,0,1
4293,retry,"The maximum number of attempts to re-send a notification message which failed to be delivered due to a recoverable error. 0 - No retry, -1 - indefinite",0,0,others,keystone,0,0
4294,retry_interval,Interval between retries of opening a SQL connection.,0,0,others,keystone,0,1
4298,rpc_conn_pool_size,Size of RPC connection pool.,1,1,resource,keystone,0,0
4299,rpc_response_timeout,Seconds to wait for a response from a call.,0,0,others,keystone,0,1
4300,rpc_server_credit,Window size for incoming RPC Request messages,1,1,resource,keystone,0,0
4302,salt_bytesize,Number of bytes to use in scrypt and pbkfd2_sha512 hashing salt. Default for scrypt is 16 bytes. Default for pbkfd2_sha512 is 16 bytes. Limited to a maximum of 96 bytes due to the size of the column used to store password hashes.,0,0,others,keystone,0,0
4305,sasl_default_realm,SASL realm to use if no realm present in username,0,0,others,keystone,0,0
4306,sasl_mechanism,Mechanism when security protocol is SASL,1,2,security-tradeoff,keystone,0,0
4307,sasl_mechanisms,Space separated list of acceptable SASL mechanisms,0,0,others,keystone,0,0
4308,scrypt_block_size,Optional block size to pass to scrypt hash function (the r parameter). Useful for tuning scrypt to optimal performance for your CPU architecture. This option is only used when the password_hash_algorithm option is set to scrypt. Defaults to 8.,1,1,resource,keystone,0,1
4309,scrypt_parallelism,Optional parallelism to pass to scrypt hash function (the p parameter). This option is only used when the password_hash_algorithm option is set to scrypt. Defaults to 1.,1,1,resource,keystone,0,1
4310,secure_proxy_ssl_header,"The HTTP Header that will be used to determine what the original request protocol scheme was, even if it was hidden by a SSL termination proxy.",0,0,others,keystone,0,0
4311,security_protocol,Protocol used to communicate with brokers,1,2,security-tradeoff,keystone,0,0
4314,slave_connection,The SQLAlchemy connection string to use to connect to the slave database.,0,0,others,keystone,0,0
4315,socket_timeout.B,Redissentinel provides a timeout option on the connections. This parameter defines that timeout (for example: socket_timeout=0.1).,0,0,others,keystone,0,0
4316,socket_timeout,Timeout in seconds for every call to a server. This is used by the key value store system.,0,0,others,keystone,0,0
4317,sqlite_synchronous,"If True, SQLite uses synchronous mode.",1,3,reliability-tradeoff,keystone,0,1
4318,ssl.B,"Attempt to connect via SSL. If no other ssl-related parameters are given, it will use the system's CA-bundle to verify the server's certificate.",1,2,security-tradeoff,keystone,0,1
4319,ssl,Connect over SSL.,1,2,security-tradeoff,keystone,0,0
4320,ssl_ca_file.B,CA certificate PEM file used to verify the server's certificate,0,0,others,keystone,0,0
4321,ssl_ca_file,SSL certification authority file (valid only if SSL enabled).,0,0,others,keystone,0,0
4322,ssl_cafile,CA certificate PEM file used to verify the server certificate,0,0,others,keystone,0,0
4323,ssl_cert_file.B,Self-identifying certificate PEM file for client authentication,0,0,others,keystone,0,0
4324,ssl_cert_file,SSL cert file (valid only if SSL enabled).,0,0,others,keystone,0,0
4329,ssl_version,"SSL version to use (valid only if SSL enabled). Valid values are TLSv1 and SSLv23. SSLv2, SSLv3, TLSv1_1, and TLSv1_2 may be available on some distributions.",0,0,others,keystone,0,0
4331,strict_password_check,"If set to true, strict password length checking is performed for password manipulation. If a password exceeds the maximum length, the operation will fail with an HTTP 403 Forbidden error. If set to false, passwords are automatically truncated to the maximum length.",0,0,others,keystone,0,0
4333,syslog_log_facility,Syslog facility to receive log lines. This option is ignored if log_config_append is set.,0,0,others,keystone,0,0
4337,tls_req_cert,"Specifies which checks to perform against client certificates on incoming TLS sessions. If set to demand, then a certificate will always be requested and required from the LDAP server. If set to allow, then a certificate will always be requested but not required from the LDAP server. If set to never, then a certificate will never be requested.",1,2,security-tradeoff,keystone,0,1
4338,token,Entry point for the token auth plugin module in the keystone.auth.token namespace. You do not need to set this unless you are overriding keystone's own token authentication plugin.,0,0,others,keystone,0,1
4339,topics,AMQP topic used for OpenStack notifications.,0,0,others,keystone,0,0
4340,trace,Debug: dump AMQP frames to stdout,1,6,function-tradeoff,keystone,0,1
4341,trace_sqlalchemy,Enable SQL requests profiling in services.,1,6,function-tradeoff,keystone,0,0
4344,trusted_dashboard,"A list of trusted dashboard hosts. Before accepting a Single Sign-On request to return a token, the origin host must be a member of this list. This configuration option may be repeated for multiple values. You must set this in order to use web-based SSO flows. For example: trusted_dashboard=https://acme.example.com/auth/websso trusted_dashboard=https://beta.example.com/auth/websso",0,0,others,keystone,0,0
4345,trusted_issuer,"The list of distinguished names which identify trusted issuers of client certificates allowed to use X.509 tokenless authorization. If the option is absent then no certificates will be allowed. The format for the values of a distinguished name (DN) must be separated by a comma and contain no spaces. Furthermore, because an individual DN may contain commas, this configuration option may be repeated multiple times to represent multiple values. For example, keystone.conf would include two consecutive lines in order to trust two different DNs, such as trusted_issuer = CN=john,OU=keystone,O=openstack and trusted_issuer = CN=mary,OU=eng,O=abc.",0,0,others,keystone,0,0
4347,unique_last_password_count,"This controls the number of previous user password iterations to keep in history, in order to enforce that newly created passwords are unique. The total number which includes the new password should not be greater or equal to this value. Setting the value to zero (the default) disables this feature. Thus, to enable this feature, values must be greater than 0. This feature depends on the sql backend for the [identity] driver.",0,0,others,keystone,0,0
4349,use_auth_pool,Enable LDAP connection pooling for end user authentication. There is typically no reason to disable this.,1,4,limited-side-effect,keystone,0,1
4350,use_db_reconnect,Enable the experimental use of database reconnect on connection lost.,0,0,others,keystone,0,0
4351,use_eventlog,Log output to Windows Event Log.,0,0,others,keystone,0,0
4352,use_journal,Enable journald for logging. If running in a systemd environment you may wish to enable journal support. Doing so will use the journal native protocol which includes structured metadata in addition to log messages.This option is ignored if log_config_append is set.,0,0,others,keystone,0,0
4353,use_json,Use JSON formatting for logging. This option is ignored if log_config_append is set.,0,0,others,keystone,0,0
4354,use_pool,Enable LDAP connection pooling for queries to the LDAP server. There is typically no reason to disable this.,1,4,limited-side-effect,keystone,0,0
4355,use_stderr,Log output to standard error. This option is ignored if log_config_append is set.,0,0,others,keystone,0,0
4356,use_syslog,Use syslog for logging. Existing syslog format is DEPRECATED and will be changed later to honor RFC5424. This option is ignored if log_config_append is set.,0,0,others,keystone,0,0
4357,use_tls,Enable TLS when communicating with LDAP servers. You should also set the [ldap] tls_cacertfile and [ldap] tls_cacertdir options when using this option. Do not set this option if you are using LDAP over SSL (LDAPS) instead of TLS.,1,2,security-tradeoff,keystone,0,0
4360,user_attribute_ignore,"List of user attributes to ignore on create and update, or whether a specific user attribute should be filtered for list or show user.",0,0,others,keystone,0,0
4361,user_default_project_id_attribute,The LDAP attribute mapped to a user's default_project_id in keystone. This is most commonly used when keystone has write access to LDAP.,0,0,others,keystone,0,1
4362,user_description_attribute,The LDAP attribute mapped to user descriptions in keystone.,0,0,others,keystone,0,0
4363,user_enabled_attribute,"The LDAP attribute mapped to the user enabled attribute in keystone. If setting this option to userAccountControl, then you may be interested in setting [ldap] user_enabled_mask and [ldap] user_enabled_default as well.",0,0,others,keystone,0,1
4365,user_enabled_emulation,"If enabled, keystone uses an alternative method to determine if a user is enabled or not by checking if they are a member of the group defined by the [ldap] user_enabled_emulation_dn option. Enabling this option causes keystone to ignore the value of [ldap] user_enabled_invert.",0,0,others,keystone,0,0
4366,user_enabled_emulation_dn,DN of the group entry to hold enabled users when using enabled emulation. Setting this option has no effect unless [ldap] user_enabled_emulation is also enabled.,0,0,others,keystone,0,0
4367,user_enabled_emulation_use_group_config,Use the [ldap] group_member_attribute and [ldap] group_objectclass settings to determine membership in the emulated enabled group. Enabling this option has no effect unless [ldap] user_enabled_emulation is also enabled.,0,0,others,keystone,0,0
4370,user_filter,The LDAP search filter to use for users.,0,0,others,keystone,0,1
4371,user_id_attribute,The LDAP attribute mapped to user IDs in keystone. This must NOT be a multivalued attribute. User IDs are expected to be globally unique across keystone domains and URL-safe.,0,0,others,keystone,0,0
4372,user_limit,"Maximum number of application credentials a user is permitted to create. A value of -1 means unlimited. If a limit is not set, users are permitted to create application credentials at will, which could lead to bloat in the keystone database or open keystone to a DoS attack.",1,3,reliability-tradeoff,keystone,0,0
4373,user_mail_attribute,The LDAP attribute mapped to user emails in keystone.,0,0,others,keystone,0,0
4374,user_name_attribute,The LDAP attribute mapped to user names in keystone. User names are expected to be unique only within a keystone domain and are not expected to be URL-safe.,0,0,others,keystone,0,1
4375,user_objectclass,The LDAP object class to use for users.,0,0,others,keystone,0,0
4376,user_pass_attribute,The LDAP attribute mapped to user passwords in keystone.,0,0,others,keystone,0,0
4377,user_tree_dn,The search base to use for users. Defaults to the [ldap] suffix value.,0,0,others,keystone,0,0
4378,valid_days,The validity period (in days) to use when generating a self-signed token signing certificate. There is no reason to set this option unless you are requesting revocation lists in a non-production environment. Use a [signing] certfile issued from a trusted certificate authority instead.,0,0,others,keystone,0,0
4379,watch_log_file,Uses logging handler designed to watch file system. When log file is moved or removed this handler will open a new log file with specified path instantaneously. It makes sense only if log_file option is specified and Linux platform is used. This option is ignored if log_config_append is set.,0,0,others,keystone,0,0
4380,xmlsec1_binary,"Name of, or absolute path to, the binary to be used for XML signing. Although only the XML Security Library (xmlsec1) is supported, it may have a non-standard name or path on your system. If keystone cannot find the binary itself, you may need to install the appropriate package, use this option to specify an absolute path, or adjust keystone's PATH environment variable.",0,0,others,keystone,0,1
4381,map.sort.class,The default sort class for sorting keys.,0,0,others,mapreduce,0,0
4382,mapred.child.env,User added environment variables for the task processes. Example : 1) A=foo This will set the env variable A to foo 2) B=$B:c This is inherit nodemanager's B env variable on Unix. 3) B=%B%;c This is inherit nodemanager's B env variable on Windows.,0,0,others,mapreduce,0,1
4383,mapred.child.java.opts,"Java opts for the task processes. The following symbol, if present, will be interpolated: @taskid@ is replaced by current TaskID. Any other occurrences of '@' will go unchanged. For example, to enable verbose gc logging to a file named for the taskid in /tmp and to set the heap maximum to be a gigabyte, pass a 'value' of: -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc Usage of -Djava.library.path can cause programs to no longer function if hadoop native libraries are used. These values should instead be set as part of LD_LIBRARY_PATH in the map / reduce JVM env using the mapreduce.map.env and mapreduce.reduce.env config settings.",0,0,others,mapreduce,0,1
4384,mapreduce.admin.user.env,"Expert: Additional execution environment entries for map and reduce task processes. This is not an additive property. You must preserve the original value if you want your map and reduce tasks to have access to native libraries (compression, etc). When this value is empty, the command to set execution envrionment will be OS dependent: For linux, use LD_LIBRARY_PATH=$HADOOP_COMMON_HOME/lib/native. For windows, use PATH = %PATH%;%HADOOP_COMMON_HOME%\\bin.",0,0,others,mapreduce,0,0
4385,mapreduce.am.max-attempts,"The maximum number of application attempts. It is a application-specific setting. It should not be larger than the global number set by resourcemanager. Otherwise, it will be override. The default number is set to 2, to allow at least one retry for AM.",0,0,others,mapreduce,0,0
4387,mapreduce.application.framework.path,"Path to the MapReduce framework archive. If set, the framework archive will automatically be distributed along with the job, and this path would normally reside in a public location in an HDFS filesystem. As with distributed cache files, this can be a URL with a fragment specifying the alias to use for the archive name. For example, hdfs:/mapred/framework/hadoop-mapreduce-2.1.1.tar.gz#mrframework would alias the localized archive as ""mrframework"". Note that mapreduce.application.classpath must include the appropriate classpath for the specified framework. The base name of the archive, or alias of the archive if an alias is used, must appear in the specified classpath.",0,0,others,mapreduce,0,1
4389,mapreduce.client.completion.pollinterval,The interval (in milliseconds) between which the JobClient polls the MapReduce ApplicationMaster for updates about job status. You may want to set this to a lower value to make tests run faster on a single node system. Adjusting this value in production may lead to unwanted client-server traffic.,0,0,others,mapreduce,0,0
4390,mapreduce.client.libjars.wildcard,"Whether the libjars cache files should be localized using a wildcarded directory instead of naming each archive independently. Using wildcards reduces the space needed for storing the job information in the case of a highly available resource manager configuration. This propery should only be set to false for specific jobs which are highly sensitive to the details of the archive localization. Having this property set to true will cause the archives to all be localized to the same local cache location. If false, each archive will be localized to its own local cache location. In both cases a symbolic link will be created to every archive from the job's working directory.",1,4,limited-side-effect,mapreduce,0,0
4391,mapreduce.client.output.filter,"The filter for controlling the output of the task's userlogs sent to the console of the JobClient. The permissible options are: NONE, KILLED, FAILED, SUCCEEDED and ALL.",1,6,function-tradeoff,mapreduce,0,0
4392,mapreduce.client.progressmonitor.pollinterval,The interval (in milliseconds) between which the JobClient reports status to the console and checks for job completion. You may want to set this to a lower value to make tests run faster on a single node system. Adjusting this value in production may lead to unwanted client-server traffic.,0,0,others,mapreduce,0,0
4393,mapreduce.client.submit.file.replication,The replication level for submitted job files. This should be around the square root of the number of nodes.,1,3,reliability-tradeoff,mapreduce,0,1
4394,mapreduce.cluster.acls.enabled,"Specifies whether ACLs should be checked for authorization of users for doing various queue and job level operations. ACLs are disabled by default. If enabled, access control checks are made by MapReduce ApplicationMaster when requests are made by users for queue operations like submit job to a queue and kill a job in the queue and job operations like viewing the job-details (See mapreduce.job.acl-view-job) or for modifying the job (See mapreduce.job.acl-modify-job) using Map/Reduce APIs, RPCs or via the console and web user interfaces. For enabling this flag, set to true in mapred-site.xml file of all MapReduce clients (MR job submitting nodes).",0,0,others,mapreduce,0,0
4397,mapreduce.fileoutputcommitter.algorithm.version,"The file output committer algorithm version valid algorithm version number: 1 or 2 default to 1, which is the original algorithm In algorithm version 1, 1. commitTask will rename directory $joboutput/_temporary/$appAttemptID/_temporary/$taskAttemptID/ to $joboutput/_temporary/$appAttemptID/$taskID/ 2. recoverTask will also do a rename $joboutput/_temporary/$appAttemptID/$taskID/ to $joboutput/_temporary/($appAttemptID + 1)/$taskID/ 3. commitJob will merge every task output file in $joboutput/_temporary/$appAttemptID/$taskID/ to $joboutput/, then it will delete $joboutput/_temporary/ and write $joboutput/_SUCCESS It has a performance regression, which is discussed in MAPREDUCE-4815. If a job generates many files to commit then the commitJob method call at the end of the job can take minutes. the commit is single-threaded and waits until all tasks have completed before commencing. algorithm version 2 will change the behavior of commitTask, recoverTask, and commitJob. 1. commitTask will rename all files in $joboutput/_temporary/$appAttemptID/_temporary/$taskAttemptID/ to $joboutput/ 2. recoverTask actually doesn't require to do anything, but for upgrade from version 1 to version 2 case, it will check if there are any files in $joboutput/_temporary/($appAttemptID - 1)/$taskID/ and rename them to $joboutput/ 3. commitJob can simply delete $joboutput/_temporary and write $joboutput/_SUCCESS This algorithm will reduce the output commit time for large jobs by having the tasks commit directly to the final output directory as they were completing and commitJob had very little to do.",0,0,others,mapreduce,0,0
4398,mapreduce.framework.name,"The runtime framework for executing MapReduce jobs. Can be one of local, classic or yarn.",0,0,others,mapreduce,0,0
4399,mapreduce.ifile.readahead,Configuration key to enable/disable IFile readahead.,1,4,limited-side-effect,mapreduce,0,1
4400,mapreduce.ifile.readahead.bytes,Configuration key to set the IFile readahead length in bytes.,1,1,resource,mapreduce,0,0
4401,mapreduce.input.fileinputformat.list-status.num-threads,The number of threads to use to list and fetch block locations for the specified input paths. Note: multiple threads should not be used if a custom non thread-safe path filter is used.,1,1,resource,mapreduce,0,1
4402,mapreduce.input.fileinputformat.split.minsize,The minimum size chunk that map input should be split into. Note that some file formats may have minimum split sizes that take priority over this setting.,1,1,resource,mapreduce,0,1
4403,mapreduce.input.lineinputformat.linespermap,"When using NLineInputFormat, the number of lines of input data to include in each split.",0,0,others,mapreduce,0,0
4404,mapreduce.job.acl-modify-job,"Job specific access-control list for 'modifying' the job. It is only used if authorization is enabled in Map/Reduce by setting the configuration property mapreduce.cluster.acls.enabled to true. This specifies the list of users and/or groups who can do modification operations on the job. For specifying a list of users and groups the format to use is ""user1,user2 group1,group"". If set to '*', it allows all users/groups to modify this job. If set to ' '(i.e. space), it allows none. This configuration is used to guard all the modifications with respect to this job and takes care of all the following operations: o killing this job o killing a task of this job, failing a task of this job o setting the priority of this job Each of these operations are also protected by the per-queue level ACL ""acl-administer-jobs"" configured via mapred-queues.xml. So a caller should have the authorization to satisfy either the queue-level ACL or the job-level ACL. Irrespective of this ACL configuration, (a) job-owner, (b) the user who started the cluster, (c) members of an admin configured supergroup configured via mapreduce.cluster.permissions.supergroup and (d) queue administrators of the queue to which this job was submitted to configured via acl-administer-jobs for the specific queue in mapred-queues.xml can do all the modification operations on a job. By default, nobody else besides job-owner, the user who started the cluster, members of supergroup and queue administrators can perform modification operations on a job.",0,0,others,mapreduce,0,0
4405,mapreduce.job.acl-view-job,"Job specific access-control list for 'viewing' the job. It is only used if authorization is enabled in Map/Reduce by setting the configuration property mapreduce.cluster.acls.enabled to true. This specifies the list of users and/or groups who can view private details about the job. For specifying a list of users and groups the format to use is ""user1,user2 group1,group"". If set to '*', it allows all users/groups to modify this job. If set to ' '(i.e. space), it allows none. This configuration is used to guard some of the job-views and at present only protects APIs that can return possibly sensitive information of the job-owner like o job-level counters o task-level counters o tasks' diagnostic information o task-logs displayed on the HistoryServer's web-UI and o job.xml showed by the HistoryServer's web-UI Every other piece of information of jobs is still accessible by any other user, for e.g., JobStatus, JobProfile, list of jobs in the queue, etc. Irrespective of this ACL configuration, (a) job-owner, (b) the user who started the cluster, (c) members of an admin configured supergroup configured via mapreduce.cluster.permissions.supergroup and (d) queue administrators of the queue to which this job was submitted to configured via acl-administer-jobs for the specific queue in mapred-queues.xml can do all the view operations on a job. By default, nobody else besides job-owner, the user who started the cluster, memebers of supergroup and queue administrators can perform view operations on a job.",0,0,others,mapreduce,0,0
4406,mapreduce.job.am.node-label-expression,This is node-label configuration for Map Reduce Application Master container. If not configured it will make use of mapreduce.job.node-label-expression and if job's node-label expression is not configured then it will use queue's default-node-label-expression.,0,0,others,mapreduce,0,0
4407,mapreduce.job.cache.limit.max-resources,"The maximum number of resources a map reduce job is allowed to submit for localization via files, libjars, archives, and jobjar command line arguments and through the distributed cache. If set to 0 the limit is ignored.",1,1,resource,mapreduce,0,1
4408,mapreduce.job.cache.limit.max-resources-mb,"The maximum size (in MB) a map reduce job is allowed to submit for localization via files, libjars, archives, and jobjar command line arguments and through the distributed cache. If set to 0 the limit is ignored.",1,1,resource,mapreduce,0,0
4409,mapreduce.job.cache.limit.max-single-resource-mb,"The maximum size (in MB) of a single resource a map reduce job is allow to submit for localization via files, libjars, archives, and jobjar command line arguments and through the distributed cache. If set to 0 the limit is ignored.",1,1,resource,mapreduce,0,1
4410,mapreduce.job.classloader,Whether to use a separate (isolated) classloader for user classes in the task JVM.,0,0,others,mapreduce,0,0
4412,mapreduce.job.committer.setup.cleanup.needed,"true, if job needs job-setup and job-cleanup. false, otherwise",1,6,function-tradeoff,mapreduce,0,1
4413,mapreduce.job.complete.cancel.delegation.tokens,"if false - do not unregister/cancel delegation tokens from renewal, because same tokens may be used by spawned jobs",0,0,others,mapreduce,0,0
4414,mapreduce.job.counters.limit,Limit on the number of user counters allowed per job.,1,3,reliability-tradeoff,mapreduce,0,1
4415,mapreduce.job.emit-timeline-data,Specifies if the Application Master should emit timeline data to the timeline server. Individual jobs can override this value.,0,0,others,mapreduce,0,0
4416,mapreduce.job.end-notification.max.attempts,"The maximum number of times a URL will be read for providing job end notification. Cluster administrators can set this to limit how long after end of a job, the Application Master waits before exiting. Must be marked as final to prevent users from overriding this.",0,0,others,mapreduce,0,0
4417,mapreduce.job.end-notification.max.retry.interval,The maximum amount of time (in milliseconds) to wait before retrying job end notification. Cluster administrators can set this to limit how long the Application Master waits before exiting. Must be marked as final to prevent users from overriding this.,0,0,others,mapreduce,0,0
4418,mapreduce.job.end-notification.retry.attempts,The number of times the submitter of the job wants to retry job end notification if it fails. This is capped by mapreduce.job.end-notification.max.attempts,0,0,others,mapreduce,0,1
4419,mapreduce.job.end-notification.retry.interval,The number of milliseconds the submitter of the job wants to wait before job end notification is retried if it fails. This is capped by mapreduce.job.end-notification.max.retry.interval,0,0,others,mapreduce,0,0
4421,mapreduce.job.finish-when-all-reducers-done,"Specifies whether the job should complete once all reducers have finished, regardless of whether there are still running mappers.",1,3,reliability-tradeoff,mapreduce,0,0
4422,mapreduce.job.log4j-properties-file,"Used to override the default settings of log4j in container-log4j.properties for NodeManager. Like container-log4j.properties, it requires certain framework appenders properly defined in this overriden file. The file on the path will be added to distributed cache and classpath. If no-scheme is given in the path, it defaults to point to a log4j file on the local FS.",0,0,others,mapreduce,0,0
4423,mapreduce.job.map.output.collector.class,"The MapOutputCollector implementation(s) to use. This may be a comma-separated list of class names, in which case the map task will try to initialize each of the collectors in turn. The first to successfully initialize will be used.",0,0,others,mapreduce,0,0
4424,mapreduce.job.maps,"The default number of map tasks per job. Ignored when mapreduce.framework.name is ""local"".",1,1,resource,mapreduce,0,0
4425,mapreduce.job.max.map,Limit on the number of map tasks allowed per job. There is no limit if this value is negative.,1,1,resource,mapreduce,0,0
4426,mapreduce.job.max.split.locations,The max number of block locations to store for each split for locality calculation.,0,0,others,mapreduce,0,0
4427,mapreduce.job.maxtaskfailures.per.tracker,The number of task-failures on a node manager of a given job after which new tasks of that job aren't assigned to it. It MUST be less than mapreduce.map.maxattempts and mapreduce.reduce.maxattempts otherwise the failed task will never be tried on a different node.,1,3,reliability-tradeoff,mapreduce,0,0
4428,mapreduce.job.node-label-expression,"All the containers of the Map Reduce job will be run with this node label expression. If the node-label-expression for job is not set, then it will use queue's default-node-label-expression for all job's containers.",0,0,others,mapreduce,0,0
4429,mapreduce.job.queuename,"Queue to which a job is submitted. This must match one of the queues defined in mapred-queues.xml for the system. Also, the ACL setup for the queue must allow the current user to submit a job to the queue. Before specifying a queue, ensure that the system is configured with the queue, and access is allowed for submitting jobs to the queue.",0,0,others,mapreduce,0,0
4430,mapreduce.job.redacted-properties,The list of job configuration properties whose value will be redacted.,0,0,others,mapreduce,0,0
4432,mapreduce.job.reduce.slowstart.completedmaps,Fraction of the number of maps in the job which should be complete before reduces are scheduled for the job.,1,1,resource,mapreduce,0,1
4433,mapreduce.job.reducer.preempt.delay.sec,"The threshold (in seconds) after which an unsatisfied mapper request triggers reducer preemption when there is no anticipated headroom. If set to 0 or a negative value, the reducer is preempted as soon as lack of headroom is detected. Default is 0.",1,5,workload-specific,mapreduce,0,0
4434,mapreduce.job.reducer.unconditional-preempt.delay.sec,"The threshold (in seconds) after which an unsatisfied mapper request triggers a forced reducer preemption irrespective of the anticipated headroom. By default, it is set to 5 mins. Setting it to 0 leads to immediate reducer preemption. Setting to -1 disables this preemption altogether.",0,0,others,mapreduce,0,0
4435,mapreduce.job.reduces,"The default number of reduce tasks per job. Typically set to 99% of the cluster's reduce capacity, so that if a node fails the reduces can still be executed in a single wave. Ignored when mapreduce.framework.name is ""local"".",1,1,resource,mapreduce,0,0
4436,mapreduce.job.running.map.limit,The maximum number of simultaneous map tasks per job. There is no limit if this value is 0 or negative.,1,1,resource,mapreduce,0,0
4437,mapreduce.job.running.reduce.limit,The maximum number of simultaneous reduce tasks per job. There is no limit if this value is 0 or negative.,1,1,resource,mapreduce,0,0
4439,mapreduce.job.sharedcache.mode,"A comma delimited list of resource categories to submit to the shared cache. The valid categories are: jobjar, libjars, files, archives. If ""disabled"" is specified then the job submission code will not use the shared cache.",1,5,workload-specific,mapreduce,0,1
4441,mapreduce.job.speculative.minimum-allowed-tasks,The minimum allowed tasks that can be speculatively re-executed at any time.,1,4,limited-side-effect,mapreduce,0,1
4442,mapreduce.job.speculative.retry-after-no-speculate,The waiting time(ms) to do next round of speculation if there is no task speculated in this round.,0,0,others,mapreduce,0,1
4443,mapreduce.job.speculative.retry-after-speculate,The waiting time(ms) to do next round of speculation if there are tasks speculated in this round.,0,0,others,mapreduce,0,0
4444,mapreduce.job.speculative.slowtaskthreshold,The number of standard deviations by which a task's ave progress-rates must be lower than the average of all running tasks' for the task to be considered too slow.,1,5,workload-specific,mapreduce,0,0
4445,mapreduce.job.speculative.speculative-cap-running-tasks,The max percent (0-1) of running tasks that can be speculatively re-executed at any time.,1,5,workload-specific,mapreduce,0,0
4446,mapreduce.job.speculative.speculative-cap-total-tasks,The max percent (0-1) of all tasks that can be speculatively re-executed at any time.,1,5,workload-specific,mapreduce,0,1
4447,mapreduce.job.split.metainfo.maxsize,The maximum permissible size of the split metainfo file. The MapReduce ApplicationMaster won't attempt to read submitted split metainfo files bigger than this configured value. No limits if set to -1.,1,1,resource,mapreduce,0,0
4448,mapreduce.job.tags,Tags for the job that will be passed to YARN at submission time. Queries to YARN for applications can filter on these tags.,0,0,others,mapreduce,0,0
4449,mapreduce.job.token.tracking.ids,"When mapreduce.job.token.tracking.ids.enabled is set to true, this is set by the framework to the token-tracking-ids used by the job.",0,0,others,mapreduce,0,0
4450,mapreduce.job.token.tracking.ids.enabled,"Whether to write tracking ids of tokens to job-conf. When true, the configuration property ""mapreduce.job.token.tracking.ids"" is set to the token-tracking-ids of the job",0,0,others,mapreduce,0,0
4451,mapreduce.job.ubertask.enable,"Whether to enable the small-jobs ""ubertask"" optimization, which runs ""sufficiently small"" jobs sequentially within a single JVM. ""Small"" is defined by the following maxmaps, maxreduces, and maxbytes settings. Note that configurations for application masters also affect the ""Small"" definition - yarn.app.mapreduce.am.resource.mb must be larger than both mapreduce.map.memory.mb and mapreduce.reduce.memory.mb, and yarn.app.mapreduce.am.resource.cpu-vcores must be larger than both mapreduce.map.cpu.vcores and mapreduce.reduce.cpu.vcores to enable ubertask. Users may override this value.",1,4,limited-side-effect,mapreduce,0,0
4452,mapreduce.job.ubertask.maxbytes,"Threshold for number of input bytes, beyond which job is considered too big for the ubertasking optimization. If no value is specified, dfs.block.size is used as a default. Be sure to specify a default value in mapred-site.xml if the underlying filesystem is not HDFS. Users may override this value, but only downward.",1,5,workload-specific,mapreduce,0,0
4453,mapreduce.job.ubertask.maxmaps,"Threshold for number of maps, beyond which job is considered too big for the ubertasking optimization. Users may override this value, but only downward.",1,5,workload-specific,mapreduce,0,0
4454,mapreduce.job.ubertask.maxreduces,"Threshold for number of reduces, beyond which job is considered too big for the ubertasking optimization. CURRENTLY THE CODE CANNOT SUPPORT MORE THAN ONE REDUCE and will ignore larger values. (Zero is a valid max, however.) Users may override this value, but only downward.",1,5,workload-specific,mapreduce,0,0
4458,mapreduce.jobhistory.cleaner.interval-ms,"How often the job history cleaner checks for files to delete, in milliseconds. Defaults to 86400000 (one day). Files are only deleted if they are older than mapreduce.jobhistory.max-age-ms.",0,0,others,mapreduce,0,0
4459,mapreduce.jobhistory.client.thread-count,The number of threads to handle client API requests,1,1,resource,mapreduce,0,1
4460,mapreduce.jobhistory.datestring.cache.size,Size of the date string cache. Effects the number of directories which will be scanned to find a job.,1,1,resource,mapreduce,0,0
4462,mapreduce.jobhistory.jhist.format,"File format the AM will use when generating the .jhist file. Valid values are ""json"" for text output and ""binary"" for faster parsing.",0,0,others,mapreduce,0,0
4463,mapreduce.jobhistory.joblist.cache.size,Size of the job list cache,1,1,resource,mapreduce,0,0
4464,mapreduce.jobhistory.jobname.limit,Number of characters allowed for job name in Job History Server web page.,0,0,others,mapreduce,0,0
4466,mapreduce.jobhistory.loadedjob.tasks.max,The maximum number of tasks that a job can have so that the Job History Server will fully parse its associated job history file and load it into memory. A value of -1 (default) will allow all jobs to be loaded.,1,1,resource,mapreduce,0,1
4467,mapreduce.jobhistory.loadedjobs.cache.size,Size of the loaded job cache. This property is ignored if the property mapreduce.jobhistory.loadedtasks.cache.size is set to a positive value.,1,1,resource,mapreduce,0,1
4468,mapreduce.jobhistory.loadedtasks.cache.size,"Change the job history cache limit to be set in terms of total task count. If the total number of tasks loaded exceeds this value, then the job cache will be shrunk down until it is under this limit (minimum 1 job in cache). If this value is empty or nonpositive then the cache reverts to using the property mapreduce.jobhistory.loadedjobs.cache.size as a job cache size. Two recommendations for the mapreduce.jobhistory.loadedtasks.cache.size property: 1) For every 100k of cache size, set the heap size of the Job History Server to 1.2GB. For example, mapreduce.jobhistory.loadedtasks.cache.size=500000, heap size=6GB. 2) Make sure that the cache size is larger than the number of tasks required for the largest job run on the cluster. It might be a good idea to set the value slightly higher (say, 20%) in order to allow for job size growth.",1,1,resource,mapreduce,0,1
4470,mapreduce.jobhistory.minicluster.fixed.ports,Whether to use fixed ports with the minicluster,0,0,others,mapreduce,0,0
4471,mapreduce.jobhistory.move.interval-ms,Scan for history files to move from intermediate done dir to done dir at this frequency.,0,0,others,mapreduce,0,0
4472,mapreduce.jobhistory.move.thread-count,The number of threads used to move files.,1,1,resource,mapreduce,0,1
4474,mapreduce.jobhistory.recovery.enable,Enable the history server to store server state and recover server state upon startup. If enabled then mapreduce.jobhistory.recovery.store.class must be specified.,1,3,reliability-tradeoff,mapreduce,0,0
4475,mapreduce.jobhistory.recovery.store.class,The HistoryServerStateStoreService class to store history server state for recovery.,1,3,reliability-tradeoff,mapreduce,0,0
4478,mapreduce.jobhistory.store.class,The HistoryStorage class to use to cache history data.,0,0,others,mapreduce,0,0
4481,mapreduce.jobhistory.webapp.rest-csrf.custom-header,Optional parameter that indicates the custom header name to use for CSRF protection.,0,0,others,mapreduce,0,0
4482,mapreduce.jobhistory.webapp.rest-csrf.enabled,Enable the CSRF filter for the job history web app,1,2,security-tradeoff,mapreduce,0,0
4483,mapreduce.jobhistory.webapp.rest-csrf.methods-to-ignore,Optional parameter that indicates the list of HTTP methods that do not require CSRF protection,0,0,others,mapreduce,0,0
4484,mapreduce.jobhistory.webapp.xfs-filter.xframe-options,Value of the xframe-options,0,0,others,mapreduce,0,0
4488,mapreduce.jvm.system-properties-to-log,Comma-delimited list of system properties to log on mapreduce JVM start,0,0,others,mapreduce,0,0
4489,mapreduce.local.clientfactory.class.name,This the client factory that is responsible for creating local job runner client,0,0,others,mapreduce,0,0
4490,mapreduce.map.cpu.vcores,The number of virtual cores to request from the scheduler for each map task.,1,1,resource,mapreduce,0,0
4491,mapreduce.map.log.level,"The logging level for the map task. The allowed levels are: OFF, FATAL, ERROR, WARN, INFO, DEBUG, TRACE and ALL. The setting here could be overridden if ""mapreduce.job.log4j-properties-file"" is set.",1,6,function-tradeoff,mapreduce,0,1
4492,mapreduce.map.maxattempts,"Expert: The maximum number of attempts per map task. In other words, framework will try to execute a map task these many number of times before giving up on it.",0,0,others,mapreduce,0,0
4493,mapreduce.map.memory.mb,The amount of memory to request from the scheduler for each map task.,1,1,resource,mapreduce,0,1
4494,mapreduce.map.node-label-expression,This is node-label configuration for Map task containers. If not configured it will use mapreduce.job.node-label-expression and if job's node-label expression is not configured then it will use queue's default-node-label-expression.,0,0,others,mapreduce,0,0
4495,mapreduce.map.output.compress,Should the outputs of the maps be compressed before being sent across the network. Uses SequenceFile compression.,1,4,limited-side-effect,mapreduce,0,0
4496,mapreduce.map.output.compress.codec,"If the map outputs are compressed, how should they be compressed?",1,6,function-tradeoff,mapreduce,0,0
4497,mapreduce.map.skip.maxrecords,"The number of acceptable skip records surrounding the bad record PER bad record in mapper. The number includes the bad record as well. To turn the feature of detection/skipping of bad records off, set the value to 0. The framework tries to narrow down the skipped range by retrying until this threshold is met OR all attempts get exhausted for this task. Set the value to Long.MAX_VALUE to indicate that framework need not try to narrow down. Whatever records(depends on application) get skipped are acceptable.",0,0,others,mapreduce,0,0
4498,mapreduce.map.skip.proc-count.auto-incr,"The flag which if set to true, SkipBadRecords.COUNTER_MAP_PROCESSED_RECORDS is incremented by MapRunner after invoking the map function. This value must be set to false for applications which process the records asynchronously or buffer the input records. For example streaming. In such cases applications should increment this counter on their own.",0,0,others,mapreduce,0,0
4499,mapreduce.map.sort.spill.percent,"The soft limit in the serialization buffer. Once reached, a thread will begin to spill the contents to disk in the background. Note that collection will not block if this threshold is exceeded while a spill is already in progress, so spills may be larger than this threshold when it is set to less than .5",1,1,resource,mapreduce,0,0
4500,mapreduce.map.speculative,"If true, then multiple instances of some map tasks may be executed in parallel.",1,4,limited-side-effect,mapreduce,0,1
4501,mapreduce.output.fileoutputformat.compress,Should the job outputs be compressed?,1,4,limited-side-effect,mapreduce,0,0
4502,mapreduce.output.fileoutputformat.compress.codec,"If the job outputs are compressed, how should they be compressed?",1,6,function-tradeoff,mapreduce,0,0
4503,mapreduce.output.fileoutputformat.compress.type,"If the job outputs are to compressed as SequenceFiles, how should they be compressed? Should be one of NONE, RECORD or BLOCK.",1,6,function-tradeoff,mapreduce,0,0
4504,mapreduce.reduce.cpu.vcores,The number of virtual cores to request from the scheduler for each reduce task.,1,1,resource,mapreduce,0,0
4505,mapreduce.reduce.input.buffer.percent,"The percentage of memory- relative to the maximum heap size- to retain map outputs during the reduce. When the shuffle is concluded, any remaining map outputs in memory must consume less than this threshold before the reduce can begin.",1,1,resource,mapreduce,0,1
4506,mapreduce.reduce.log.level,"The logging level for the reduce task. The allowed levels are: OFF, FATAL, ERROR, WARN, INFO, DEBUG, TRACE and ALL. The setting here could be overridden if ""mapreduce.job.log4j-properties-file"" is set.",1,6,function-tradeoff,mapreduce,0,0
4507,mapreduce.reduce.markreset.buffer.percent,The percentage of memory -relative to the maximum heap size- to be used for caching values when using the mark-reset functionality.,1,1,resource,mapreduce,0,0
4508,mapreduce.reduce.maxattempts,"Expert: The maximum number of attempts per reduce task. In other words, framework will try to execute a reduce task these many number of times before giving up on it.",1,5,workload-specific,mapreduce,0,1
4509,mapreduce.reduce.memory.mb,The amount of memory to request from the scheduler for each reduce task.,1,1,resource,mapreduce,0,0
4510,mapreduce.reduce.merge.inmem.threshold,"The threshold, in terms of the number of files for the in-memory merge process. When we accumulate threshold number of files we initiate the in-memory merge and spill to disk. A value of 0 or less than 0 indicates we want to DON'T have any threshold and instead depend only on the ramfs's memory consumption to trigger the merge.",1,5,workload-specific,mapreduce,0,0
4511,mapreduce.reduce.node-label-expression,This is node-label configuration for Reduce task containers. If not configured it will use mapreduce.job.node-label-expression and if job's node-label expression is not configured then it will use queue's default-node-label-expression.,0,0,others,mapreduce,0,1
4512,mapreduce.reduce.shuffle.connect.timeout,Expert: The maximum amount of time (in milli seconds) reduce task spends in trying to connect to a remote node for getting map output.,1,5,workload-specific,mapreduce,0,0
4514,mapreduce.reduce.shuffle.fetch.retry.interval-ms,Time of interval that fetcher retry to fetch again when some non-fatal failure happens because of some events like NM restart.,0,0,others,mapreduce,0,0
4515,mapreduce.reduce.shuffle.fetch.retry.timeout-ms,Timeout value for fetcher to retry to fetch again when some non-fatal failure happens because of some events like NM restart.,0,0,others,mapreduce,0,0
4516,mapreduce.reduce.shuffle.input.buffer.percent,The percentage of memory to be allocated from the maximum heap size to storing map outputs during the shuffle.,1,1,resource,mapreduce,0,0
4517,mapreduce.reduce.shuffle.memory.limit.percent,"Expert: Maximum percentage of the in-memory limit that a single shuffle can consume. Range of valid values is [0.0, 1.0]. If the value is 0.0 map outputs are shuffled directly to disk.",1,1,resource,mapreduce,0,0
4518,mapreduce.reduce.shuffle.merge.percent,"The usage threshold at which an in-memory merge will be initiated, expressed as a percentage of the total memory allocated to storing in-memory map outputs, as defined by mapreduce.reduce.shuffle.input.buffer.percent.",1,1,resource,mapreduce,0,0
4519,mapreduce.reduce.shuffle.parallelcopies,The default number of parallel transfers run by reduce during the copy(shuffle) phase.,1,1,resource,mapreduce,0,0
4520,mapreduce.reduce.shuffle.read.timeout,Expert: The maximum amount of time (in milli seconds) reduce task waits for map output data to be available for reading after obtaining connection.,0,0,others,mapreduce,0,0
4521,mapreduce.reduce.shuffle.retry-delay.max.ms,The maximum number of ms the reducer will delay before retrying to download map data.,0,0,others,mapreduce,0,0
4522,mapreduce.reduce.skip.maxgroups,"The number of acceptable skip groups surrounding the bad group PER bad group in reducer. The number includes the bad group as well. To turn the feature of detection/skipping of bad groups off, set the value to 0. The framework tries to narrow down the skipped range by retrying until this threshold is met OR all attempts get exhausted for this task. Set the value to Long.MAX_VALUE to indicate that framework need not try to narrow down. Whatever groups(depends on application) get skipped are acceptable.",0,0,others,mapreduce,0,0
4523,mapreduce.reduce.skip.proc-count.auto-incr,"The flag which if set to true, SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS is incremented by framework after invoking the reduce function. This value must be set to false for applications which process the records asynchronously or buffer the input records. For example streaming. In such cases applications should increment this counter on their own.",0,0,others,mapreduce,0,0
4524,mapreduce.reduce.speculative,"If true, then multiple instances of some reduce tasks may be executed in parallel.",1,4,limited-side-effect,mapreduce,0,0
4525,mapreduce.shuffle.connection-keep-alive.enable,set to true to support keep-alive connections.,0,0,others,mapreduce,0,1
4526,mapreduce.shuffle.connection-keep-alive.timeout,"The number of seconds a shuffle client attempts to retain http connection. Refer ""Keep-Alive: timeout="" header in Http specification",0,0,others,mapreduce,0,0
4527,mapreduce.shuffle.listen.queue.size,The length of the shuffle server listen queue.,1,1,resource,mapreduce,0,0
4528,mapreduce.shuffle.max.connections,Max allowed connections for the shuffle. Set to 0 (zero) to indicate no limit on the number of connections.,1,1,resource,mapreduce,0,0
4529,mapreduce.shuffle.max.threads,"Max allowed threads for serving shuffle connections. Set to zero to indicate the default of 2 times the number of available processors (as reported by Runtime.availableProcessors()). Netty is used to serve requests, so a thread is not needed for each connection.",1,1,resource,mapreduce,0,0
4531,mapreduce.shuffle.ssl.enabled,Whether to use SSL for for the Shuffle HTTP endpoints.,1,2,security-tradeoff,mapreduce,0,0
4532,mapreduce.shuffle.ssl.file.buffer.size,Buffer size for reading spills from file when using SSL.,1,1,resource,mapreduce,0,0
4533,mapreduce.shuffle.transfer.buffer.size,"This property is used only if mapreduce.shuffle.transferTo.allowed is set to false. In that case, this property defines the size of the buffer used in the buffer copy code for the shuffle phase. The size of this buffer determines the size of the IO requests.",1,1,resource,mapreduce,0,0
4534,mapreduce.shuffle.transferTo.allowed,"This option can enable/disable using nio transferTo method in the shuffle phase. NIO transferTo does not perform well on windows in the shuffle phase. Thus, with this configuration property it is possible to disable it, in which case custom transfer method will be used. Recommended value is false when running Hadoop on Windows. For Linux, it is recommended to set it to true. If nothing is set then the default value is false for Windows, and true for Linux.",1,1,resource,mapreduce,0,0
4535,mapreduce.task.combine.progress.records,The number of records to process during combine output collection before sending a progress notification.,1,1,resource,mapreduce,0,0
4536,mapreduce.task.exit.timeout,"The number of milliseconds before a task will be terminated if it stays in finishing state for too long. After a task attempt completes from TaskUmbilicalProtocol's point of view, it will be transitioned to finishing state. That will give a chance for the task to exit by itself.",0,0,others,mapreduce,0,1
4537,mapreduce.task.exit.timeout.check-interval-ms,The interval in milliseconds between which the MR framework checks if task attempts stay in finishing state for too long.,0,0,others,mapreduce,0,0
4538,mapreduce.task.files.preserve.failedtasks,"Should the files for failed tasks be kept. This should only be used on jobs that are failing, because the storage is never reclaimed. It also prevents the map outputs from being erased from the reduce directory as they are consumed.",0,0,others,mapreduce,0,0
4539,mapreduce.task.io.sort.factor,The number of streams to merge at once while sorting files. This determines the number of open file handles.,1,1,resource,mapreduce,0,0
4540,mapreduce.task.io.sort.mb,"The total amount of buffer memory to use while sorting files, in megabytes. By default, gives each merge stream 1MB, which should minimize seeks.",1,1,resource,mapreduce,0,0
4541,mapreduce.task.local-fs.write-limit.bytes,"Limit on the byte written to the local file system by each task. This limit only applies to writes that go through the Hadoop filesystem APIs within the task process (i.e.: writes that will update the local filesystem's BYTES_WRITTEN counter). It does not cover other writes such as logging, sideband writes from subprocesses (e.g.: streaming jobs), etc. Negative values disable the limit. default is -1",1,3,reliability-tradeoff,mapreduce,0,0
4542,mapreduce.task.merge.progress.records,The number of records to process during merge before sending a progress notification to the MR ApplicationMaster.,1,1,resource,mapreduce,0,0
4543,mapreduce.task.profile,"To set whether the system should collect profiler information for some of the tasks in this job? The information is stored in the user log directory. The value is ""true"" if task profiling is enabled.",1,6,function-tradeoff,mapreduce,0,0
4544,mapreduce.task.profile.map.params,Map-task-specific JVM profiler parameters. See mapreduce.task.profile.params,0,0,others,mapreduce,0,0
4545,mapreduce.task.profile.maps,To set the ranges of map tasks to profile. mapreduce.task.profile has to be set to true for the value to be accounted.,0,0,others,mapreduce,0,0
4546,mapreduce.task.profile.params,"JVM profiler parameters used to profile map and reduce task attempts. This string may contain a single format specifier %s that will be replaced by the path to profile.out in the task attempt log directory. To specify different profiling options for map tasks and reduce tasks, more specific parameters mapreduce.task.profile.map.params and mapreduce.task.profile.reduce.params should be used.",0,0,others,mapreduce,0,1
4547,mapreduce.task.profile.reduce.params,Reduce-task-specific JVM profiler parameters. See mapreduce.task.profile.params,0,0,others,mapreduce,0,1
4548,mapreduce.task.profile.reduces,To set the ranges of reduce tasks to profile. mapreduce.task.profile has to be set to true for the value to be accounted.,0,0,others,mapreduce,0,0
4549,mapreduce.task.skip.start.attempts,"The number of Task attempts AFTER which skip mode will be kicked off. When skip mode is kicked off, the tasks reports the range of records which it will process next, to the MR ApplicationMaster. So that on failures, the MR AM knows which ones are possibly the bad records. On further executions, those are skipped.",0,0,others,mapreduce,0,1
4550,mapreduce.task.timeout,"The number of milliseconds before a task will be terminated if it neither reads an input, writes an output, nor updates its status string. A value of 0 disables the timeout.",0,0,others,mapreduce,0,0
4551,mapreduce.task.userlog.limit.kb,The maximum size of user-logs of each task in KB. 0 disables the cap.,0,0,others,mapreduce,0,0
4552,yarn.app.mapreduce.am.admin.user.env,Environment variables for the MR App Master processes for admin purposes. These values are set first and can be overridden by the user env (yarn.app.mapreduce.am.env) Example : 1) A=foo This will set the env variable A to foo 2) B=$B:c This is inherit app master's B env variable.,0,0,others,mapreduce,0,0
4553,yarn.app.mapreduce.am.admin-command-opts,Java opts for the MR App Master processes for admin purposes. It will appears before the opts set by yarn.app.mapreduce.am.command-opts and thus its options can be overridden user. Usage of -Djava.library.path can cause programs to no longer function if hadoop native libraries are used. These values should instead be set as part of LD_LIBRARY_PATH in the map / reduce JVM env using the mapreduce.map.env and mapreduce.reduce.env config settings.,0,0,others,mapreduce,0,0
4554,yarn.app.mapreduce.am.command-opts,"Java opts for the MR App Master processes. The following symbol, if present, will be interpolated: @taskid@ is replaced by current TaskID. Any other occurrences of '@' will go unchanged. For example, to enable verbose gc logging to a file named for the taskid in /tmp and to set the heap maximum to be a gigabyte, pass a 'value' of: -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc Usage of -Djava.library.path can cause programs to no longer function if hadoop native libraries are used. These values should instead be set as part of LD_LIBRARY_PATH in the map / reduce JVM env using the mapreduce.map.env and mapreduce.reduce.env config settings.",0,0,others,mapreduce,0,0
4555,yarn.app.mapreduce.am.container.log.backups,"Number of backup files for the ApplicationMaster logs when using ContainerRollingLogAppender (CRLA). See org.apache.log4j.RollingFileAppender.maxBackupIndex. By default, ContainerLogAppender (CLA) is used, and container logs are not rolled. CRLA is enabled for the ApplicationMaster when both yarn.app.mapreduce.am.container.log.limit.kb and yarn.app.mapreduce.am.container.log.backups are greater than zero.",1,3,reliability-tradeoff,mapreduce,0,0
4556,yarn.app.mapreduce.am.container.log.limit.kb,The maximum size of the MRAppMaster attempt container logs in KB. 0 disables the cap.,0,0,others,mapreduce,0,0
4557,yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size,The initial size of thread pool to launch containers in the app master.,1,1,resource,mapreduce,0,1
4558,yarn.app.mapreduce.am.env,User added environment variables for the MR App Master processes. Example : 1) A=foo This will set the env variable A to foo 2) B=$B:c This is inherit tasktracker's B env variable.,0,0,others,mapreduce,0,0
4559,yarn.app.mapreduce.am.hard-kill-timeout-ms,Number of milliseconds to wait before the job client kills the application.,0,0,others,mapreduce,0,0
4560,yarn.app.mapreduce.am.job.client.port-range,"Range of ports that the MapReduce AM can use when binding. Leave blank if you want all possible ports. For example 50000-50050,50100-50200",0,0,others,mapreduce,0,0
4561,yarn.app.mapreduce.am.job.committer.cancel-timeout,The amount of time in milliseconds to wait for the output committer to cancel an operation if the job is killed,0,0,others,mapreduce,0,0
4562,yarn.app.mapreduce.am.job.committer.commit-window,"Defines a time window in milliseconds for output commit operations. If contact with the RM has occurred within this window then commits are allowed, otherwise the AM will not allow output commits until contact with the RM has been re-established.",0,0,others,mapreduce,0,0
4563,yarn.app.mapreduce.am.job.task.listener.thread-count,The number of threads used to handle RPC calls in the MR AppMaster from remote tasks,1,1,resource,mapreduce,0,1
4564,yarn.app.mapreduce.am.log.level,"The logging level for the MR ApplicationMaster. The allowed levels are: OFF, FATAL, ERROR, WARN, INFO, DEBUG, TRACE and ALL. The setting here could be overriden if ""mapreduce.job.log4j-properties-file"" is set.",1,6,function-tradeoff,mapreduce,0,0
4565,yarn.app.mapreduce.am.resource.cpu-vcores,The number of virtual CPU cores the MR AppMaster needs.,1,1,resource,mapreduce,0,0
4566,yarn.app.mapreduce.am.resource.mb,The amount of memory the MR AppMaster needs.,1,1,resource,mapreduce,0,0
4567,yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms,The interval in ms at which the MR AppMaster should send heartbeats to the ResourceManager,0,0,others,mapreduce,0,0
4568,yarn.app.mapreduce.am.staging-dir,The staging dir used while submitting jobs.,0,0,others,mapreduce,0,0
4569,yarn.app.mapreduce.am.webapp.port-range,"Range of ports that the MapReduce AM can use for its webapp when binding. Leave blank if you want all possible ports. For example 50000-50050,50100-50200",0,0,others,mapreduce,0,1
4570,yarn.app.mapreduce.client.job.max-retries,"The number of retries the client will make for getJob and dependent calls. This is needed for non-HDFS DFS where additional, high level retries are required to avoid spurious failures during the getJob call. 30 is a good value for WASB",0,0,others,mapreduce,0,0
4571,yarn.app.mapreduce.client.job.retry-interval,The delay between getJob retries in ms for retries configured with yarn.app.mapreduce.client.job.max-retries.,0,0,others,mapreduce,0,0
4572,yarn.app.mapreduce.client.max-retries,The number of client retries to the RM/HS before throwing exception. This is a layer above the ipc.,0,0,others,mapreduce,0,1
4573,yarn.app.mapreduce.client-am.ipc.max-retries,The number of client retries to the AM - before reconnecting to the RM to fetch Application Status.,0,0,others,mapreduce,0,0
4574,yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts,The number of client retries on socket timeouts to the AM - before reconnecting to the RM to fetch Application Status.,0,0,others,mapreduce,0,0
4575,yarn.app.mapreduce.shuffle.log.backups,If yarn.app.mapreduce.shuffle.log.limit.kb and yarn.app.mapreduce.shuffle.log.backups are greater than zero then a ContainerRollngLogAppender is used instead of ContainerLogAppender for syslog.shuffle. See org.apache.log4j.RollingFileAppender.maxBackupIndex,1,3,reliability-tradeoff,mapreduce,0,1
4576,yarn.app.mapreduce.shuffle.log.limit.kb,Maximum size of the syslog.shuffle file in kilobytes (0 for no limit).,0,0,others,mapreduce,0,0
4577,yarn.app.mapreduce.shuffle.log.separate,If enabled ('true') logging generated by the client-side shuffle classes in a reducer will be written in a dedicated log file 'syslog.shuffle' instead of 'syslog'.,0,0,others,mapreduce,0,0
4578,yarn.app.mapreduce.task.container.log.backups,"Number of backup files for task logs when using ContainerRollingLogAppender (CRLA). See org.apache.log4j.RollingFileAppender.maxBackupIndex. By default, ContainerLogAppender (CLA) is used, and container logs are not rolled. CRLA is enabled for tasks when both mapreduce.task.userlog.limit.kb and yarn.app.mapreduce.task.container.log.backups are greater than zero.",1,3,reliability-tradeoff,mapreduce,0,1
4580,analyze_sample_percentage,Percentage of rows from the table ANALYZE TABLE will sample to collect table statistics. Set to 0 to let MariaDB decide what percentage of rows to sample.,0,0,others,mariadb,0,0
4581,autocommit,"If set to 1, the default, all queries are committed immediately. The LOCK IN SHARE MODE and FOR UPDATE clauses therefore have no effect. If set to 0, they are only committed upon a COMMIT statement, or rolled back with a ROLLBACK statement. If autocommit is set to 0, and then changed to 1, all open transactions are immediately committed.",1,3,reliability-tradeoff,mariadb,0,1
4582,automatic_sp_privileges,"When set to 1, the default, when a stored routine is created, the creator is automatically granted permission to ALTER (which includes dropping) and to EXECUTE the routine. If set to 0, the creator is not automatically granted these privileges.",0,0,others,mariadb,0,1
4583,back_log,"Connections take a small amount of time to start, and this setting determines the number of outstanding connection requests MariaDB can have, or the size of the listen queue for incoming TCP/IP requests. Requests beyond this will be refused. Increase if you expect short bursts of connections. Cannot be set higher than the operating system limit (see the Unix listen() man page). If not set, set to 0, or the --autoset-back-log option is used, will be autoset to the lower of 900 and (50 + max_connections/5) (>= MariaDB 10.1.7).",1,5,workload-specific,mariadb,0,0
4585,big_tables,"If this system variable is set to 1, then temporary tables will be saved to disk intead of memory.
This system variable's original intention was to allow result sets that were too big for memory-based temporary tables and to avoid the resulting 'table full' errors. 
This system variable is no longer needed, because the server can automatically convert large memory-based temporary tables into disk-based temporary tables when they exceed the value of the mp_memory_table_size system variable.
To prevent memory-based temporary tables from being used at all, set the tmp_memory_table_size system variable to 0.
In MariaDB 5.5 and earlier, sql_big_tables is a synonym.
In MariaDB 10.5, this system variable is deprecated.",0,0,others,mariadb,0,0
4587,bulk_insert_buffer_size,Size in bytes of the per-thread cache tree used to speed up bulk inserts into MyISAM and Aria tables. A value of 0 disables the cache tree.,1,1,resource,mariadb,0,0
4589,character_set_connection,"Character set used for number to string conversion, as well as for literals that don't have a character set introducer.",0,0,others,mariadb,0,0
4592,character_set_results,Character set used for results and error messages returned to the client.,0,0,others,mariadb,0,0
4596,check_constraint_checks,"If set to 0, will disable constraint checks, for example when loading a table that violates some constraints that you plan to fix later.",1,2,security-tradeoff,mariadb,0,0
4597,collation_connection,Collation used for the connection character set.,0,0,others,mariadb,0,0
4600,completion_type,"The transaction completion type. If set to NO_CHAIN or 0 (the default), there is no effect on commits and rollbacks. If set to CHAIN or 1, a COMMIT statement is equivalent to COMMIT AND CHAIN, while a ROLLBACK is equivalent to ROLLBACK AND CHAIN, so a new transaction starts straight away with the same isolation level as transaction that's just finished. If set to RELEASE or 2, a COMMIT statement is equivalent to COMMIT RELEASE, while a ROLLBACK is equivalent to ROLLBACK RELEASE, so the server will disconnect after the transaction completes. Note that the transaction completion type only applies to explicit commits, not implicit commits.",1,3,reliability-tradeoff,mariadb,0,0
4601,concurrent_insert,"If set to AUTO or 1, the default, MariaDB allows concurrent INSERTs and SELECTs for MyISAM tables with no free blocks in the data (deleted rows in the middle). If set to NEVER or 0, concurrent inserts are disabled. If set to ALWAYS or 2, concurrent inserts are permitted for all MyISAM tables, even those with holes, in which case new rows are added at the end of a table if the table is being used by another thread. If the --skip-new option is used when starting the server, concurrent_insert is set to NEVER. Changing the variable only affects new opened tables. Use FLUSH TABLES If you want it to also affect cached tables. See Concurrent Inserts for more.",1,4,limited-side-effect,mariadb,0,0
4603,core_file,"Write a core-file on crashes. The file name and location are system dependent. On Linux it is usually called core.${PID}, and it is usually written to the data directory. However, this can be changed.
See Enabling Core Dumps for more information.
Previously this system variable existed only as an option, but it was also made into a read-only system variable starting with MariaDB 10.3.9, MariaDB 10.2.17 and MariaDB 10.1.35.
On Windows >= MariaDB 10.4.3, this option is set by default. 
Note that the option accepts no arguments; specifying --core-file sets the value to ON. It cannot be disabled in the case of Windows >= MariaDB 10.4.3.",0,0,others,mariadb,0,0
4605,date_format,Unused.,0,0,others,mariadb,0,0
4606,datetime_format,Unused.,0,0,others,mariadb,0,1
4607,debug,Available in debug builds only (built with -DWITH_DEBUG=1). Used in debugging through the DBUG library to write to a trace file. Just using --debug will write a trace of what mysqld is doing to the default trace file.,0,0,others,mariadb,0,0
4608,debug_no_thread_alarm,"Disable system thread alarm calls. Disabling it may be useful in debugging or testing, never do it in production.",1,6,function-tradeoff,mariadb,0,0
4609,debug_sync,Used in debugging to show the interface to the Debug Sync facility. MariaDB needs to be configured with -DENABLE_DEBUG_SYNC=1 for this variable to be available.,1,3,reliability-tradeoff,mariadb,0,0
4610,default_password_lifetime,"This defines the global password expiration policy. 0 means automatic password expiration is disabled. If the value is a positive integer N, the passwords must be changed every N days. This behavior can be overridden using the password expiration options in ALTER USER.",0,0,others,mariadb,0,0
4613,default_table_type,A synonym for default_storage_engine. Removed in MariaDB 5.5.,0,0,others,mariadb,0,0
4615,default_week_format,Default mode for the WEEK() function. See that page for details on the different modes,0,0,others,mariadb,0,1
4616,delay_key_write,"Specifies how MyISAM tables handles CREATE TABLE DELAY_KEY_WRITE. If set to ON, the default, any DELAY KEY WRITEs are honored. The key buffer is then flushed only when the table closes, speeding up writes. MyISAM tables should be automatically checked upon startup in this case, and --external locking should not be used, as it can lead to index corruption. If set to OFF, DELAY KEY WRITEs are ignored, while if set to ALL, all new opened tables are treated as if created with DELAY KEY WRITEs enabled.",1,3,reliability-tradeoff,mariadb,0,0
4617,delayed_insert_limit,"After this many rows have been inserted with INSERT DELAYED, the handler will check for and execute any waiting SELECT statements.",1,5,workload-specific,mariadb,0,1
4618,delayed_insert_timeout,Time in seconds that the INSERT DELAYED handler will wait for INSERTs before terminating.,0,0,others,mariadb,0,0
4619,delayed_queue_size,"Number of rows, per table, that can be queued when performing INSERT DELAYED statements. If the queue becomes full, clients attempting to perform INSERT DELAYED's will wait until the queue has room available again.",1,5,workload-specific,mariadb,0,1
4620,disconnect_on_expired_password,"When a user password has expired (see User Password Expiry), this variable controls how the server handles clients that are not aware of the sandbox mode. If enabled, the client is not permitted to connect, otherwise the server puts the client in a sandbox mode.",1,2,security-tradeoff,mariadb,0,0
4621,div_precision_increment,"The precision of the result of the decimal division will be the larger than the precision of the dividend by that number. By default it's 4, so SELECT 2/15 would return 0.1333 and SELECT 2.0/15 would return 0.13333. After setting div_precision_increment to 6, for example, the same operation would return 0.133333 and 0.1333333 respectively.",0,0,others,mariadb,0,0
4622,encrypt_tmp_disk_tables,Enables automatic encryption of all internal on-disk temporary tables that are created during query execution if aria_used_for_temp_tables=ON is set. See Data at Rest Encryption and Enabling Encryption for Internal On-disk Temporary Tables.,1,2,security-tradeoff,mariadb,0,0
4623,encrypt_tmp_files,"Enables automatic encryption of temporary files, such as those created for filesort operations, binary log file caches, etc. See Data at Rest Encryption.",1,2,security-tradeoff,mariadb,0,1
4624,encryption_algorithm,Which encryption algorithm to use for table encryption. aes_cbc is the recommended one. See Table and Tablespace Encryption.,1,2,security-tradeoff,mariadb,0,0
4625,enforce_storage_engine,"Force the use of a particular storage engine for new tables. Used to avoid unwanted creation of tables using another engine. For example, setting to InnoDB will prevent any MyISAM tables from being created. If another engine is specified in a CREATE TABLE statement, the outcome depends on whether the NO_ENGINE_SUBSTITUTION SQL_MODE has been set or not. If set (the default from MariaDB 10.1.7), the query will fail, while if not set, a warning will be returned and the table created according to the engine specified by this variable. The variable has a session scope, but is only modifiable by a user with the SUPER privilege.",0,0,others,mariadb,0,0
4626,engine_condition_pushdown,"Deprecated in MariaDB 5.5 and removed and replaced by the optimizer_switch engine_condition_pushdown={on|off} flag in MariaDB 10.0.. Specifies whether the engine condition pushdown optimization is enabled. Since MariaDB 10.1.1, engine condition pushdown is enabled for all engines that support it.",0,0,others,mariadb,0,1
4627,eq_range_index_dive_limit,"Limit used for speeding up queries listed by long nested INs. The optimizer will use existing index statistics instead of doing index dives for equality ranges if the number of equality ranges for the index is larger than or equal to this number. If set to 0 (unlimited, the default), index dives are always used.",1,5,workload-specific,mariadb,0,0
4628,error_count,Read-only variable denoting the number of errors from the most recent statement in the current session that generated errors. See SHOW_ERRORS().,0,0,others,mariadb,0,0
4629,event_scheduler,"Status of the Event Scheduler. Can be set to ON or OFF, while DISABLED means it cannot be set at runtime. Setting the variable will cause a load of events if they were not loaded at startup.",1,6,function-tradeoff,mariadb,0,1
4630,expensive_subquery_limit,"Number of rows to be examined for a query to be considered expensive, that is, maximum number of rows a subquery may examine in order to be executed during optimization and used for constant optimization.",0,0,others,mariadb,0,0
4631,explicit_defaults_for_timestamp,"This option causes CREATE TABLE to create all TIMESTAMP columns as NULL with the DEFAULT NULL attribute, Without this option, TIMESTAMP columns are NOT NULL and have implicit DEFAULT clauses.",0,0,others,mariadb,0,0
4632,external_user,External user name set by the plugin used to authenticate the client. NULL if native MariaDB authentication is used.,0,0,others,mariadb,0,0
4633,flush,"Usually, MariaDB writes changes to disk after each SQL statement, and the operating system handles synchronizing (flushing) it to disk. If set to ON, the server will synchronize all changes to disk after each statement.",1,3,reliability-tradeoff,mariadb,0,1
4634,flush_time,"Interval in seconds that tables are closed to synchronize (flush) data to disk and free up resources. If set to 0, the default, there is no automatic synchronizing tables and closing of tables. This option should not be necessary on systems with sufficient resources.",0,0,others,mariadb,0,0
4635,foreign_key_checks,"If set to 1 (the default) foreign key constraints (including ON UPDATE and ON DELETE behavior) InnoDB tables are checked, while if set to 0, they are not checked. 0 is not recommended for normal use, though it can be useful in situations where you know the data is consistent, but want to reload data in a different order from that that specified by parent/child relationships. Setting this variable to 1 does not retrospectively check for inconsistencies introduced while set to 0.",1,3,reliability-tradeoff,mariadb,0,0
4637,ft_max_word_len,"Maximum length for a word to be included in the MyISAM full-text index. If this variable is changed, the full-text index must be rebuilt. The quickest way to do this is by issuing a REPAIR TABLE table_name QUICK statement. See innodb_ft_max_token_size for the InnoDB equivalent.",1,1,resource,mariadb,0,0
4638,ft_min_word_len,"Minimum length for a word to be included in the MyISAM full-text index. If this variable is changed, the full-text index must be rebuilt. The quickest way to do this is by issuing a REPAIR TABLE table_name QUICK statement. See innodb_ft_min_token_size for the InnoDB equivalent.",1,1,resource,mariadb,0,0
4639,ft_query_expansion_limit,"For full-text searches, denotes the numer of top matches when using WITH QUERY EXPANSION.",0,0,others,mariadb,0,0
4640,ft_stopword_file,"File containing a list of stopwords for use in MyISAM full-text searches. Unless an absolute path is specified the file will be looked for in the data directory. The file is not parsed for comments, so all words found become stopwords. By default, a built-in list of words (built from storage/myisam/ft_static.c file) is used. Stopwords can be disabled by setting this variable to '' (an empty string). If this variable is changed, the full-text index must be rebuilt. The quickest way to do this is by issuing a REPAIR TABLE table_name QUICK statement. See innodb_ft_server_stopword_table for the InnoDB equivalent.",0,0,others,mariadb,0,0
4641,general_log,"If set to 0, the default unless the --general-log option is used, the general query log is disabled, while if set to 1, the general query log is enabled. See log_output for how log files are written. If that variable is set to NONE, no logs will be written even if general_query_log is set to 1.",0,0,others,mariadb,0,0
4642,general_log_file,"Name of the general query log file. If this is not specified, the name is taken from the log-basename setting or from your system hostname with .log as a suffix.",0,0,others,mariadb,0,0
4643,group_concat_max_len,Maximum length in bytes of the returned result for a GROUP_CONCAT() function.,0,0,others,mariadb,0,0
4644,have_compress,"If the zlib compression library is accessible to the server, this will be set to YES, otherwise it will be NO. The COMPRESS() and UNCOMPRESS() functions will only be available if set to YES.",1,4,limited-side-effect,mariadb,0,0
4645,have_crypt,"If the crypt() system call is available this variable will be set to YES, otherwise it will be set to NO. If set to NO, the ENCRYPT() function cannot be used.",1,2,security-tradeoff,mariadb,0,1
4651,have_profiling,"If statement profiling is available, will be set to YES, otherwise will be set to NO. See SHOW PROFILES() and SHOW PROFILE().",0,0,others,mariadb,0,1
4652,have_query_cache,"If the server supports the query cache, will be set to YES, otherwise will be set to NO.",1,4,limited-side-effect,mariadb,0,0
4653,have_rtree_keys,"If RTREE indexes (used for spatial indexes) are available, will be set to YES, otherwise will be set to NO.",0,0,others,mariadb,0,0
4655,histogram_size,"Number of bytes used for a histogram. If set to 0, no histograms are created by ANALYZE.",1,1,resource,mariadb,0,0
4656,histogram_type,"Specifies the type of histograms created by ANALYZE. 
SINGLE_PREC_HB - single precision height-balanced.
DOUBLE_PREC_HB - double precision height-balanced.",0,0,others,mariadb,0,0
4657,host_cache_size,"Number of host names that will be cached to avoid resolving. Setting to 0 disables the cache. Changing the value while the server is running causes an implicit FLUSH HOSTS, clearing the host cache and truncating the performance_schema.host_cache table. If you are connecting from a lot of different machines you should consider increasing.",1,1,resource,mariadb,0,0
4659,identity,A synonym for last_insert_id variable.,0,0,others,mariadb,0,1
4664,in_predicate_conversion_threshold,The minimum number of scalar elements in the value list of an IN predicate that triggers its conversion to an IN subquery. Set to 0 to disable the conversion. See Conversion of Big IN Predicates Into Subqueries.,1,5,workload-specific,mariadb,0,0
4665,in_transaction,"Session-only and read-only variable that is set to 1 if a transaction is in progress, 0 if not.",0,0,others,mariadb,0,0
4666,init_connect,"String containing one or more SQL statements, separated by semicolons, that will be executed by the server for each client connecting. If there's a syntax error in the one of the statements, the client will fail to connect. For this reason, the statements are not executed for users with the SUPER privilege or, from MariaDB 10.5.2, the CONNECTION ADMIN privilege, who can then still connect and correct the error. See also init_file.",0,0,others,mariadb,0,0
4668,insert_id,Value to be used for the next statement inserting a new AUTO_INCREMENT value.,0,0,others,mariadb,0,1
4670,join_buffer_size,"Minimum size in bytes of the buffer used for queries that cannot use an index, and instead perform a full table scan. Increase to get faster full joins when adding indexes is not possible, although be aware of memory issues, since joins will always allocate the minimum size. Best left low globally and set high in sessions that require large full joins. In 64-bit platforms, Windows truncates values above 4GB to 4GB with a warning. See also Block-Based Join Algorithms - Size of Join Buffers.",1,1,resource,mariadb,0,0
4671,join_buffer_space_limit,"Maximum size in bytes of the query buffer, By default 1024*128*10. See Block-based join algorithms.",1,1,resource,mariadb,0,1
4672,join_cache_level,Controls which of the eight block-based algorithms can be used for join operations. See Block-based join algorithms for more information.,1,6,function-tradeoff,mariadb,0,1
4675,large_page_size,Indicates the size of memory page if large page support (Linux only) is enabled. The page size is determined from the Hugepagesize setting in /proc/meminfo. See large_pages. Deprecated and unused in MariaDB 10.5.3 since multiple page size support was added.,1,5,workload-specific,mariadb,0,0
4676,large_pages,"Indicates whether large page support (Linux only - called huge pages) is used. This is set with --large-pages or disabled with --skip-large-pages. Large pages are used for the innodb buffer pool and for online DDL (of size 3* innodb_sort_buffer_size (or 6 when encryption is used)). To use large pages, the Linux sysctl variable kernel.shmmax must be large than the llocation. Also the sysctl variable vm.nr_hugepages multipled by large-page) must be larger than the usage. The ulimit for locked memory must be sufficient to cover the amount used (ulimit -l and equalivent in /etc/security/limits.conf / or in systemd LimitMEMLOCK). If these operating system controls or insufficient free huge pages are available, the allocation of large pages will fall back to conventional memory allocation and a warning will appear in the logs. Only allocations of the default Hugepagesize currently occur (see /proc/meminfo).",1,4,limited-side-effect,mariadb,0,0
4677,last_insert_id,Contains the same value as that returned by LAST_INSERT_ID(). Note that setting this variable don't update the value returned by the underlying function.,0,0,others,mariadb,0,1
4680,lc_time_names,"The locale that determines the language used for the date and time functions DAYNAME(), MONTHNAME() and DATE_FORMAT(). Locale names are language and region subtags, for example 'en_ZA' (English - South Africa) or 'es_US: Spanish - United States'. The default is always 'en-US' regardless of the system's locale setting. See server locale for a full list of supported locales.",0,0,others,mariadb,0,0
4682,local_infile,"If set to 1, LOCAL is supported for LOAD DATA INFILE statements. If set to 0, usually for security reasons, attempts to perform a LOAD DATA LOCAL will fail with an error message.",1,2,security-tradeoff,mariadb,0,0
4683,lock_wait_timeout,"Timeout in seconds for attempts to acquire metadata locks. Statements using metadata locks include FLUSH TABLES WITH READ LOCK, LOCK TABLES, HANDLER and DML and DDL operations on tables, stored procedures and functions, and views. The timeout is separate for each attempt, of which there may be multiple in a single statement. 0 (from MariaDB 10.3.0) means no wait. See WAIT and NOWAIT.",0,0,others,mariadb,0,0
4684,locked_in_memory,Indicates whether --memlock was used to lock mysqld in memory.,0,0,others,mariadb,0,0
4686,log_disabled_statements,"If set, the specified type of statements (slave or stored procedure statements) will not be logged to the general log.",0,0,others,mariadb,0,0
4687,log_error,"Specifies the name of the error log. If --console is specified later in the configuration (Windows only) or this option isn't specified, errors will be logged to stderr. If no name is provided, errors will still be logged to hostname.err in the datadir directory by default. If a configuration file sets --log-error, one can reset it with --skip-log-error (useful to override a system wide configuration file). MariaDB always writes its error log, but the destination is configurable. See error log for details.",0,0,others,mariadb,0,0
4688,log_output,"How the output for the general query log and the slow query log is stored. By default written to file (FILE), it can also be stored in the general_log and slow_log tables in the mysql database (TABLE), or not stored at all (NONE). More than one option can be chosen at the same time, with NONE taking precedence if present. Logs will not be written if logging is not enabled. See Writing logs into tables, and the slow_query_log and general_log server system variables.",0,0,others,mariadb,0,0
4689,log_queries_not_using_indexes,"Queries that don't use an index, or that perform a full index scan where the index doesn't limit the number of rows, will be logged to the slow query log (regardless of time taken). The slow query log needs to be enabled for this to have an effect.",0,0,others,mariadb,0,0
4690,log_slow_admin_statements,"Log slow OPTIMIZE, ANALYZE, ALTER and other administrative statements to the slow log if it is open. Before MariaDB 10.1.13, this was only available as a mysqld option, not a server variable. See also log_slow_disabled_statements and log_slow_filter.",1,6,function-tradeoff,mariadb,0,1
4691,log_slow_disabled_statements,"If set, the specified type of statements will not be logged to the slow query log. See also log_slow_admin_statements and log_slow_filter.",0,0,others,mariadb,0,0
4692,log_slow_filter,"Comma-delimited string containing one or more settings for filtering what is logged to the slow query log. If a query matches one of the types listed in the filter, and takes longer than long_query_time, it will be logged. Sets log-slow-admin-statements to ON. See also log_slow_disabled_statements.
admin log administrative queries (create, optimize, drop etc...) 
filesort logs queries that use a filesort. 
filesort_on_disk logs queries that perform a a filesort on disk.
filesort_priority_queue (from MariaDB 10.3.2)
full_join logs queries that perform a join without indexes.
full_scan logs queries that perform full table scans.
query_cache log queries that are resolved by the query cache .
query_cache_miss logs queries that are not found in the query cache.
tmp_table logs queries that create an implicit temporary table. 
tmp_table_on_disk logs queries that create a temporary table on disk.",0,0,others,mariadb,0,1
4694,log_slow_rate_limit,"The slow query log will log every this many queries. The default is 1, or every query, while setting it to 20 would log every 20 queries, or five percent. Aims to reduce I/O usage and excessively large slow query logs. See also Slow Query Log Extended Statistics.",1,6,function-tradeoff,mariadb,0,0
4695,log_slow_verbosity,"Controls information to be added to the slow query log. Options are added in a comma-delimited string. See also Slow Query Log Extended Statistics. log_slow_verbosity is not supported when log_output='TABLE'.
query_plan logs query execution plan information
innodb an unused Percona XtraDB option for logging XtraDB/InnoDB statistics.
explain prints EXPLAIN output in the slow query log. See EXPLAIN in the Slow Query Log.",0,0,others,mariadb,0,0
4696,log_tc_size,"Defines the size in bytes of the memory-mapped file-based transaction coordinator log, which is only used if the binary log is disabled. If you have two or more XA-capable storage engines enabled, then a transaction coordinator log must be available. This size is defined in multiples of 4096. This size could always be set as a commandline option, but it was made into a system variable in MariaDB 10.1.3. See Transaction Coordinator Log for more information. Also see the --log-tc server option and the --tc-heuristic-recover option.",1,5,workload-specific,mariadb,0,0
4697,log_warnings,"Determines which additional warnings are logged. Setting to 0 disables additional warning logging. Note that this does not prevent all warnings, there is a core set of warnings that will always be written to the error log.",1,6,function-tradeoff,mariadb,0,0
4698,long_query_time,"If a query takes longer than this many seconds to execute (microseconds can be specified too), the Slow_queries status variable is incremented and, if enabled, the query is logged to the slow query log.",0,0,others,mariadb,0,0
4699,low_priority_updates,"If set to 1 (0 is the default), for storage engines that use only table-level locking (Aria, MyISAM, MEMORY and MERGE), all INSERTs, UPDATEs, DELETEs and LOCK TABLE WRITEs will wait until there are no more SELECTs or LOCK TABLE READs pending on the relevant tables. Set this to 1 if reads are prioritized over writes. 
In MariaDB 5.5 and earlier, sql_low_priority_updates is a synonym.",0,0,others,mariadb,0,0
4701,lower_case_table_names,"If set to 0 (the default on Unix-based systems), table names and aliases and database names are compared in a case-sensitive manner. If set to 1 (the default on Windows), names are stored in lowercase and not compared in a case-sensitive manner. If set to 2 (the default on Mac OS X), names are stored as declared, but compared in lowercase.",0,0,others,mariadb,0,0
4702,max_allowed_packet,"Maximum size in bytes of a packet or a generated/intermediate string. The packet message buffer is initialized with the value from net_buffer_length, but can grow up to max_allowed_packet bytes. Set as large as the largest BLOB, in multiples of 1024. If this value is changed, it should be changed on the client side as well. See slave_max_allowed_packet for a specific limit for replication purposes.",1,1,resource,mariadb,0,0
4703,max_connect_errors,"Limit to the number of successive failed connects from a host before the host is blocked from making further connections. The count for a host is reset to zero if they successfully connect. To unblock, flush the host cache with a FLUSH HOSTS statement or mysqladmin flush-hosts.",1,3,reliability-tradeoff,mariadb,0,0
4704,max_connections,The maximum number of simultaneous client connections. See also Handling Too Many Connections. Note that this value affects the number of file descriptors required on the operating system. Minimum was changed from 1 to 10 to avoid possible unexpected results for the user (MDEV-18252).,1,1,resource,mariadb,0,0
4705,max_delayed_threads,"Limits to the number of INSERT DELAYED threads. Once this limit is reached, the insert is handled as if there was no DELAYED attribute. If set to 0, DELAYED is ignored entirely. The session value can only be set to 0 or to the same as the global value.",1,1,resource,mariadb,0,0
4706,max_digest_length,"Maximum length considered for computing a statement digest, such as used by the Performance Schema and query rewrite plugins. Statements that differ after this many bytes produce the same digest, and are aggregated for statistics purposes. The variable is allocated per session. Increasing will allow longer statements to be distinguished from each other, but increase memory use, while decreasing will reduce memory use, but more statements may become indistinguishable.",1,5,workload-specific,mariadb,0,0
4707,max_error_count,Specifies the maximum number of messages stored for display by SHOW ERRORS and SHOW WARNINGS statements.,0,0,others,mariadb,0,0
4708,max_heap_table_size,"Maximum size in bytes for user-created MEMORY tables. Setting the variable while the server is active has no effect on existing tables unless they are recreated or altered. The smaller of max_heap_table_size and tmp_table_size also limits internal in-memory tables. When the maximum size is reached, any further attempts to insert data will receive a ""table ... is full"" error. Temporary tables created with CREATE TEMPORARY will not be converted to Aria, as occurs with internal temporary tables, but will also receive a table full error.",1,1,resource,mariadb,0,0
4709,max_insert_delayed_threads,Synonym for max_delayed_threads.,0,0,others,mariadb,0,0
4710,max_join_size,"Statements will not be performed if they are likely to need to examine more than this number of rows, row combinations or do more disk seeks. Can prevent poorly-formatted queries from taking server resources. Changing this value to anything other the default will reset sql_big_selects to 0. If sql_big_selects is set again, max_join_size will be ignored. This limit is also ignored if the query result is sitting in the query cache. Previously named sql_max_join_size, which is still a synonym.",1,1,resource,mariadb,0,0
4711,max_length_for_sort_data,"Used to decide which algorithm to choose when sorting rows. If the total size of the column data, not including columns that are part of the sort, is less than max_length_for_sort_data, then we add these to the sort key. This can speed up the sort as we don't have to re-read the same row again later. Setting the value too high can slow things down as there will be a higher disk activity for doing the sort.",1,5,workload-specific,mariadb,0,0
4712,max_long_data_size,"Maximum size for parameter values sent with mysql_stmt_send_long_data(). If not set, will default to the value of max_allowed_packet. Deprecated in MariaDB 5.5 and removed in MariaDB 10.5.0; use max_allowed_packet instead.",0,0,others,mariadb,0,0
4713,max_password_errors,"The maximum permitted number of failed connection attempts due to an invalid password before a user is blocked from further connections. FLUSH_PRIVILEGES will permit the user to connect again. This limit is ignored for users with the SUPER privilege or, from MariaDB 10.5.2, the CONNECTION ADMIN privilege.",1,3,reliability-tradeoff,mariadb,0,0
4714,max_prepared_stmt_count,"Maximum number of prepared statements on the server. Can help prevent certain forms of denial-of-service attacks. If set to 0, no prepared statements are permitted on the server.",1,3,reliability-tradeoff,mariadb,0,0
4715,max_recursive_iterations,"Maximum number of iterations when executing recursive queries, used to prevent infinite loops in recursive CTEs.",1,3,reliability-tradeoff,mariadb,0,0
4716,max_rowid_filter_size,The maximum size of the container of a rowid filter.,1,5,workload-specific,mariadb,0,0
4717,max_seeks_for_key,"The optimizer assumes that the number specified here is the most key seeks required when searching with an index, regardless of the actual index cardinality. If this value is set lower than its default and maximum, indexes will tend to be preferred over table scans.",1,5,workload-specific,mariadb,0,1
4718,max_session_mem_used,Amount of memory a single user session is allowed to allocate. This limits the value of the session variable Memory_used.,1,1,resource,mariadb,0,0
4719,max_sort_length,Maximum size in bytes used for sorting data values - anything exceeding this is ignored. The server uses only the first max_sort_length bytes of each value and ignores the rest.,1,1,resource,mariadb,0,0
4720,max_sp_recursion_depth,"Permitted number of recursive calls for a stored procedure. 0, the default, no recursion is permitted. Increasing this value increases the thread stack requirements, so you may need to increase thread_stack as well. This limit doesn't apply to stored functions.",1,3,reliability-tradeoff,mariadb,0,0
4721,max_statement_time,"Maximum time in seconds that a query can execute before being aborted. This includes all queries, not just SELECT statements, but excludes statements in stored procedures. If set to 0, no limit is applied. See Aborting statements that take longer than a certain time to execute for details and limitations. Useful when combined with SET STATEMENT for limiting the execution times of individual queries.",0,0,others,mariadb,0,0
4722,max_tmp_tables,Unused.,0,0,others,mariadb,0,1
4723,max_user_connections,"Maximum simultaneous connections permitted for each user account. When set to 0, there is no per user limit. Setting it to -1 stops users without the SUPER privilege or, from MariaDB 10.5.2, the CONNECTION ADMIN privilege, from connecting to the server. The session variable is always read-only and only privileged users can modify user limits. The session variable defaults to the global max_user_connections variable, unless the user's specific MAX_USER_CONNECTIONS resource option is non-zero. When both global variable and the user resource option are set, the user's MAX_USER_CONNECTIONS is used. Note: This variable does not affect users with the SUPER privilege or, from MariaDB 10.5.2, the CONNECTION ADMIN privilege.",1,3,reliability-tradeoff,mariadb,0,0
4724,max_write_lock_count,"Read lock requests will be permitted for processing after this many write locks. Applies only to storage engines that use table level locks (thr_lock), so no effect with InnoDB or Archive.",0,0,others,mariadb,0,0
4725,metadata_locks_cache_size,"Size of the metadata locks cache, used for reducing the need to create and destroy synchronization objects. It is particularly helpful on systems where this process is inefficient, such as Windows XP.",1,1,resource,mariadb,0,0
4726,metadata_locks_hash_instances,Number of hashes used by the set of metadata locks. The metadata locks are partitioned into separate hashes in order to reduce contention.,0,0,others,mariadb,0,0
4727,min_examined_row_limit,"If a query examines more than this number of rows, it is logged to the slow query log. If set to 0, the default, no row limit is used.",1,5,workload-specific,mariadb,0,0
4728,mrr_buffer_size,Size of buffer to use when using multi-range read with range access. See Multi Range Read optimization for more information.,1,1,resource,mariadb,0,0
4729,multi_range_count,Ignored. Use mrr_buffer_size instead.,0,0,others,mariadb,0,0
4730,mysql56_temporal_format,"If set (the default), MariaDB uses the MySQL 5.6 low level formats for TIME, DATETIME and TIMESTAMP instead of the MariaDB 5.3 version. The version MySQL introduced in 5.6 requires more storage, but potentially allows negative dates and has some advantages in replication. There should be no reason to revert to the old MariaDB 5.3 microsecond format. See also MDEV-10723.",0,0,others,mariadb,0,0
4731,named_pipe,"On Windows systems, determines whether connections over named pipes are permitted.",0,0,others,mariadb,0,0
4732,net_buffer_length,"The starting size, in bytes, for the connection and thread buffers for each client thread. The size can grow to max_allowed_packet. This variable's session value is read-only. Can be set to the expected length of client statements if memory is a limitation.",1,1,resource,mariadb,0,1
4734,net_retry_count,Permit this many retries before aborting when attempting to read or write on a communication port. On FreeBSD systems should be set higher as threads are sent internal interrupts..,0,0,others,mariadb,0,1
4735,net_write_timeout,Time in seconds to wait on writing a block to a connection before aborting the write. See also net_read_timeout and slave_net_timeout.,0,0,others,mariadb,0,0
4736,old,"Disabled by default, enabling it reverts index hints to those used before MySQL 5.1.17. Enabling may lead to replication errors. Being replaced by old_mode.",0,0,others,mariadb,0,1
4737,old_alter_table,"From MariaDB 10.3.7, an alias for alter_algorithm. Prior to that, if set to 1 (0 is default), MariaDB reverts to the non-optimized, pre-MySQL 5.1, method of processing ALTER TABLE statements. A temporary table is created, the data is copied over, and then the temporary table is renamed to the original.",0,0,others,mariadb,0,0
4739,old_passwords,"If set to 1 (0 is default), MariaDB reverts to using the mysql_old_password authentication plugin by default for newly created users and passwords, instead of the mysql_native_password authentication plugin.",1,2,security-tradeoff,mariadb,0,0
4740,open_files_limit,"The number of file descriptors available to MariaDB. If you are getting the Too many open files error, then you should increase this limit. If set to 0, then MariaDB will calculate a limit based on the following: 

MAX(max_connections*5, max_connections +table_open_cache*2) 

MariaDB sets the limit with setrlimit. MariaDB cannot set this to exceed the hard limit imposed by the operating system. Therefore, you may also need to change the hard limit. There are a few ways to do so. 
If you are using mysqld_safe to start mysqld, then see the instructions at mysqld_safe: Configuring the Open Files Limit. 
If you are using systemd to start mysqld, then see the instructions at systemd: Configuring the Open Files Limit. 
Otherwise, you can change the hard limit for the mysql user account by modifying /etc/security/limits.conf. See Configuring Linux for MariaDB: Configuring the Open Files Limit for more details.",0,0,others,mariadb,0,0
4741,optimizer_prune_level,"If set to 1, the default, the optimizer will use heuristics to prune less-promising partial plans from the optimizer search space. If set to 0, heuristics are disabled and an exhaustive search is performed.",1,4,limited-side-effect,mariadb,0,1
4742,optimizer_search_depth,"Maximum search depth by the query optimizer. Smaller values lead to less time spent on execution plans, but potentially less optimal results. If set to 0, MariaDB will automatically choose a reasonable value. Since the better results from more optimal planning usually offset the longer time spent on planning, this is set as high as possible by default. 63 is a valid value, but its effects (switching to the original find_best search) are deprecated.",1,3,reliability-tradeoff,mariadb,0,0
4743,optimizer_selectivity_sampling_limit,Controls number of record samples to check condition selectivity,0,0,others,mariadb,0,0
4744,optimizer_switch,"A series of flags for controlling the query optimizer. See Optimizer Switch for defaults, and a comparison to MySQL.",0,0,others,mariadb,0,0
4745,optimizer_trace,"Controls tracing of the optimizer: optimizer_trace=option=val[,option=val...], where option is one of {enabled} and val is one of {on, off, default}",1,4,limited-side-effect,mariadb,0,0
4746,optimizer_trace_max_mem_size,"Limits the memory used while tracing a query by specifying the maximum allowed cumulated size, in bytes, of stored optimizer traces.",1,1,resource,mariadb,0,0
4747,optimizer_use_condition_selectivity,"Controls which statistics can be used by the optimizer when looking for
the best query execution plan.
1 Use selectivity of predicates as in MariaDB 5.5.
2 Use selectivity of all range predicates supported by indexes.
3 Use selectivity of all range predicates estimated without histogram.
4 Use selectivity of all range predicates estimated with histogram.
5 Additionally use selectivity of certain non-range predicates calculated on record sample.",0,0,others,mariadb,0,1
4749,plugin_dir,"Path to the plugin directory. For security reasons, either make sure this directory can only be read by the server, or set secure_file_priv.",0,0,others,mariadb,0,0
4750,plugin_maturity,The lowest acceptable plugin maturity. MariaDB will not load plugins less mature than the specified level.,0,0,others,mariadb,0,0
4752,preload_buffer_size,Size in bytes of the buffer allocated when indexes are preloaded.,1,1,resource,mariadb,0,0
4753,profiling,"If set to 1 (0 is default), statement profiling will be enabled. See SHOW PROFILES() and SHOW PROFILE().",0,0,others,mariadb,0,0
4754,profiling_history_size,"Number of statements about which profiling information is maintained. If set to 0, no profiles are stored. See SHOW PROFILES.",1,1,resource,mariadb,0,1
4755,progress_report_time,"Time in seconds between sending progress reports to the client for time-consuming statements. If set to 0, progress reporting will be disabled.",0,0,others,mariadb,0,0
4757,proxy_protocol_networks,"Enable proxy protocol for these source networks. The syntax is a comma separated list of IPv4 and IPv6 networks. If the network doesn't contain a mask, it is considered to be a single host. ""*"" represents all networks and must be the only directive on the line. String ""localhost"" represents non-TCP local connections (Unix domain socket, Windows named pipe or shared memory). See Proxy Protocol Support.",0,0,others,mariadb,0,0
4760,pseudo_thread_id,For internal use only.,0,0,others,mariadb,0,0
4761,query_alloc_block_size,Size in bytes of the extra blocks allocated during query parsing and execution (after query_prealloc_size is used up).,1,1,resource,mariadb,0,0
4762,query_cache_limit,Size in bytes for which results larger than this are not stored in the query cache.,1,1,resource,mariadb,0,1
4763,query_cache_min_res_unit,Minimum size in bytes of the blocks allocated for query cache results.,1,1,resource,mariadb,0,0
4764,query_cache_size,"Size in bytes available to the query cache. About 40KB is needed for query cache structures, so setting a size lower than this will result in a warning. 0, the default before MariaDB 10.1.7, effectively disables the query cache.",1,1,resource,mariadb,0,0
4765,query_cache_strip_comments,"If set to 1 (0 is default), the server will strip any comments from the query before searching to see if it exists in the query cache. Multiple space, line feeds, tab and other white space characters will also be removed.",1,6,function-tradeoff,mariadb,0,1
4766,query_cache_type,"If set to 0, the query cache is disabled (although a buffer of query_cache_size bytes is still allocated). If set to 1 all SELECT queries will be cached unless SQL_NO_CACHE is specified. If set to 2 (or DEMAND), only queries with the SQL CACHE clause will be cached. Note that if the server is started with the query cache disabled, it cannot be enabled at runtime.",1,4,limited-side-effect,mariadb,0,0
4767,query_cache_wlock_invalidate,"If set to 0, the default, results present in the query cache will be returned even if there's a write lock on the table. If set to 1, the client will first have to wait for the lock to be released.",1,4,limited-side-effect,mariadb,0,0
4768,query_prealloc_size,"Size in bytes of the persistent buffer for query parsing and execution, allocated on connect and freed on disconnect. Increasing may be useful if complex queries are being run, as this will reduce the need for more memory allocations during query operation. See also query_alloc_block_size.",1,1,resource,mariadb,0,0
4769,rand_seed1,"rand_seed1 and rand_seed2 facilitate replication of the RAND() function. The master passes the value of these to the slaves so that the random number generator is seeded in the same way, and generates the same value, on the slave as on the master. Until MariaDB 10.1.4, the variable value could not be viewed, with the SHOW VARIABLES output always displaying zero.",0,0,others,mariadb,0,0
4770,rand_seed2,See rand_seed1.,0,0,others,mariadb,0,1
4771,range_alloc_block_size,Size in bytes of blocks allocated during range optimization. The unit size in 1024.,1,1,resource,mariadb,0,0
4772,read_buffer_size,"Each thread performing a sequential scan (for MyISAM, Aria and MERGE tables) allocates a buffer of this size in bytes for each table scanned. Increase if you perform many sequential scans. If not in a multiple of 4KB, will be rounded down to the nearest multiple. Also used in ORDER BY's for caching indexes in a temporary file (not temporary table), for caching results of nested queries, for bulk inserts into partitions, and to determine the memory block size of MEMORY tables.",1,1,resource,mariadb,0,1
4773,read_only,"When set to 1 (0 is default), no updates are permitted except from users with the SUPER privilege or, from MariaDB 10.5.2, the READ ONLY ADMIN privilege, or replica servers updating from a primary. The read_only variable is useful for replica servers to ensure no updates are accidentally made outside of what are performed on the primary. Inserting rows to log tables, updates to temporary tables and OPTIMIZE TABLE or ANALYZE TABLE statements are excluded from this limitation. If read_only is set to 1, then the SET PASSWORD statement is limited only to users with the SUPER privilege (<= MariaDB 10.5.1) or READ ONLY ADMIN privilege (>= MariaDB 10.5.2). Attempting to set this variable to 1 will fail if the current session has table locks or transactions pending, while if other sessions hold table locks, the statement will wait until these locks are released before completing. While the attempt to set read_only is waiting, other requests for table locks or transactions will also wait until read_only has been set. See Read-Only Replicas for more.",1,3,reliability-tradeoff,mariadb,0,1
4774,read_rnd_buffer_size,"Size in bytes of the buffer used when reading rows from a MyISAM table in sorted order after a key sort. Larger values improve ORDER BY performance, although rather increase the size by SESSION where the need arises to avoid excessive memory use.",1,1,resource,mariadb,0,0
4775,require_secure_transport,"When this option is enabled, connections attempted using insecure transport will be rejected. Secure transports are SSL/TLS, Unix sockets or named pipes. Note that per-account requirements take precedence.",1,2,security-tradeoff,mariadb,0,0
4776,rowid_merge_buff_size,The maximum size in bytes of the memory available to the Rowid-merge strategy. See Non-semi-join subquery optimizations for more information.,1,1,resource,mariadb,0,0
4777,rpl_recovery_rank,Unused.,0,0,others,mariadb,0,1
4778,safe_show_database,"This variable was removed in MariaDB 5.5, and has been replaced by the more flexible SHOW DATABASES privilege.",0,0,others,mariadb,0,0
4779,secure_auth,"Connections will be blocked if they use the the mysql_old_password authentication plugin. The server will also fail to start if the privilege tables are in the old, pre-MySQL 4.1 format.",1,2,security-tradeoff,mariadb,0,0
4781,secure_timestamp,"Restricts direct setting of a session timestamp. Possible levels are: 
YES - timestamp cannot deviate from the system clock
REPLICATION - replication thread can adjust timestamp to match the master's
SUPER - a user with this privilege and a replication thread can adjust timestamp
NO - historical behavior, anyone can modify session timestamp",1,3,reliability-tradeoff,mariadb,0,1
4782,session_track_schema,Whether to track changes to the default schema within the current session.,0,0,others,mariadb,0,0
4783,session_track_state_change,Whether to track changes to the session state.,0,0,others,mariadb,0,0
4784,session_track_system_variables,"Comma-separated list of session system variables for which to track changes. In MariaDB 10.2, by default no variables are tracked. For compatibility with MySQL defaults, this variable should be set to ""autocommit, character_set_client, character_set_connection, character_set_results, time_zone"" (the default from MariaDB 10.3.1). The * character tracks all session variables.",0,0,others,mariadb,0,0
4785,session_track_transaction_info,"Track changes to the transaction attributes. OFF to disable; STATE to track just transaction state (Is there an active transaction? Does it have any data? etc.); CHARACTERISTICS to track transaction state and report all statements needed to start a transaction with the same characteristics (isolation level, read only/read write,snapshot - but not any work done / data modified within the transaction).",1,6,function-tradeoff,mariadb,0,0
4786,shared_memory,"Windows only, determines whether the server permits shared memory connections. See also shared_memory_base_name.",0,0,others,mariadb,0,0
4787,shared_memory_base_name,"Windows only, specifies the name of the shared memory to use for shared memory connection. Mainly used when running more than one instance on the same physical machine. By default the name is MYSQL and is case sensitive. See also shared_memory.",0,0,others,mariadb,0,1
4788,skip_external_locking,"If this system variable is set, then some kinds of external table locks will be disabled for some storage engines.
If this system variable is set, then the MyISAM storage engine will not use file-based locks. Otherwise, it will use the fcntl() function with the F_SETLK option to get file-based locks on Unix, and it will use the LockFileEx() function to get file-based locks on Windows.
If this system variable is set, then the Aria storage engine will not lock a table when it decrements the table's in-file counter that keeps track of how many connections currently have the table open. See MDEV-19393 for more information.",0,0,others,mariadb,0,0
4790,skip_networking,"If set to 1, (0 is the default), the server does not listen for TCP/IP connections. All interaction with the server by be through socket files (Unix) or named pipes or shared memory (Windows). It's recommended to use this option if only local clients are permitted to connect to the server. Enabling this option also prevents a server from functioning as a replication client.",1,6,function-tradeoff,mariadb,0,0
4791,skip_show_database,"If set to 1, (0 is the default), only users with the SHOW DATABASES privilege can use the SHOW DATABASES statement to see all database names.",0,0,others,mariadb,0,0
4792,slow_launch_time,"Time in seconds. If a thread takes longer than this to launch, the slow_launch_threads server status variable is incremented.",0,0,others,mariadb,0,1
4793,slow_query_log,"If set to 0, the default unless the --slow-query-log option is used, the slow query log is disabled, while if set to 1 (both global and session variables), the slow query log is enabled. MariaDB 10.1 added support for session variables.",0,0,others,mariadb,0,0
4794,slow_query_log_file,Name of the slow query log file.,0,0,others,mariadb,0,0
4796,sort_buffer_size,"Each session performing a sort allocates a buffer with this amount of memory. Not specific to any storage engine. If the status variable sort_merge_passes is too high, you may need to look at improving your query indexes, or increasing this. Consider reducing where there are many small sorts, such as OLTP, and increasing where needed by session. 16k is a suggested minimum.",1,1,resource,mariadb,0,0
4797,sql_auto_is_null,"If set to 1, the query SELECT * FROM table_name WHERE auto_increment_column IS NULL will return an auto-increment that has just been successfully inserted, the same as the LAST_INSERT_ID() function. Some ODBC programs make use of this IS NULL comparison.",0,0,others,mariadb,0,0
4798,sql_big_selects,"If set to 0, MariaDB will not perform large SELECTs. See max_join_size for details. If max_join_size is set to anything but DEFAULT, sql_big_selects is automatically set to 0. If sql_big_selects is again set, max_join_size will be ignored.",1,4,limited-side-effect,mariadb,0,1
4801,sql_if_exists,"If set to 1, adds an implicit IF EXISTS to ALTER, RENAME and DROP of TABLES, VIEWS, FUNCTIONS and PACKAGES. This variable is mainly used in replication to tag DDLs that can be ignored on the slave if the target table doesn't exist.",0,0,others,mariadb,0,0
4802,sql_log_off,"If set to 1 (0 is the default), no logging to the general query log is done for the client. Only clients with the SUPER privilege can update this variable.",1,6,function-tradeoff,mariadb,0,0
4803,sql_log_update,Removed. Use sql_log_bin instead.,0,0,others,mariadb,0,1
4804,sql_low_priority_updates,"If set to 1 (0 is the default), for storage engines that use only table-level locking (Aria, MyISAM, MEMORY and MERGE), all INSERTs, UPDATEs, DELETEs and LOCK TABLE WRITEs will wait until there are no more SELECTs or LOCK TABLE READs pending on the relevant tables. Set this to 1 if reads are prioritized over writes.
This is a synonym for low_priority_updates.",0,0,others,mariadb,0,1
4806,sql_mode,"Sets the SQL Mode. Multiple modes can be set, separated by a comma.",0,0,others,mariadb,0,0
4807,sql_notes,"If set to 1, the default, warning_count is incremented each time a Note warning is encountered. If set to 0, Note warnings are not recorded. mysqldump has outputs to set this variable to 0 so that no unnecessary increments occur when data is reloaded.",0,0,others,mariadb,0,0
4808,sql_quote_show_create,"If set to 1, the default, the server will quote identifiers for SHOW CREATE DATABASE, SHOW CREATE TABLE and SHOW CREATE VIEW statements. Quoting is disabled if set to 0. Enable to ensure replications works when identifiers require quoting.",0,0,others,mariadb,0,0
4809,sql_safe_updates,"If set to 1, UPDATEs and DELETEs need either a key in the WHERE clause, or a LIMIT clause, or else they will aborted. Prevents the common mistake of accidentally deleting or updating every row in a table. Until MariaDB 10.3.11, could not be set as a command-line option or in my.cnf.",0,0,others,mariadb,0,0
4810,sql_select_limit,"Maximum number of rows that can be returned from a SELECT query. Default is the maximum number of rows permitted per table by the server, usually 232-1 or 264-1. Can be restored to the default value after being changed by assigning it a value of DEFAULT.",0,0,others,mariadb,0,0
4811,sql_warnings,"If set to 1, single-row INSERTs will produce a string containing warning information if a warning occurs.",0,0,others,mariadb,0,0
4812,standard_compliant_cte,"Allow only standard-compliant common table expressions. Prior to MariaDB 10.2.4, this variable was named standards_compliant_cte.",0,0,others,mariadb,0,0
4813,storage_engine,See default_storage_engine.,0,0,others,mariadb,0,0
4814,stored_program_cache,"Limit to the number of stored routines held in the stored procedures and stored functions caches. Each time a stored routine is executed, this limit is first checked, and if the number held in the cache exceeds this, that cache is flushed and memory freed.",1,1,resource,mariadb,0,1
4815,strict_password_validation,"When password validation plugins are enabled, reject passwords that cannot be validated (passwords specified as a hash). This excludes direct updates to the privilege tables.",1,2,security-tradeoff,mariadb,0,0
4816,sync_frm,"If set to 1, the default, each time a non-temporary table is created, its .frm definition file is synced to disk. Fractionally slower, but safer in case of a crash.",1,3,reliability-tradeoff,mariadb,0,0
4818,table_definition_cache,"Number of table definitions that can be cached. Table definitions are taken from the .frm files, and if there are a large number of tables increasing the cache size can speed up table opening. Unlike the table_open_cache, as the table_definition_cache doesn't use file descriptors, and is much smaller.",1,1,resource,mariadb,0,1
4819,table_lock_wait_timeout,"Unused, and removed.",0,0,others,mariadb,0,1
4820,table_open_cache,Maximum number of open tables cached in one table cache instance. See Optimizing table_open_cache for suggestions on optimizing. Increasing table_open_cache increases the number of file descriptors required.,1,1,resource,mariadb,0,0
4821,table_open_cache_instances,"This system variable specifies the maximum number of table cache instances. MariaDB Server initially creates just a single instance. However, whenever it detects contention on the existing instances, it will automatically create a new instance. When the number of instances has been increased due to contention, it does not decrease again. The default value of this system variable is 8, which is expected to handle up to 100 CPU cores. If your system is larger than this, then you may benefit from increasing the value of this system variable.
Depending on the ratio of actual available file handles, and table_open_cache size, the max. instance count may be auto adjusted to a lower value on server startup.
The implementation and behavior of this feature is different than the same feature in MySQL 5.6.
See Optimizing table_open_cache: Automatic Creation of New Table Open Cache Instances for more information.",1,1,resource,mariadb,0,0
4822,table_type,Removed and replaced by storage_engine. Use default_storage_engine instead.,0,0,others,mariadb,0,0
4823,tcp_keepalive_interval,"The interval, in seconds, between when successive keep-alive packets are sent if no acknowledgement is received. If set to 0, the system dependent default is used.",0,0,others,mariadb,0,0
4824,tcp_keepalive_probes,"The number of unacknowledged probes to send before considering the connection dead and notifying the application layer. If set to 0, a system dependent default is used.",0,0,others,mariadb,0,0
4825,tcp_keepalive_time,"Timeout, in seconds, with no activity until the first TCP keep-alive packet is sent. If set to 0, a system dependent default is used.",0,0,others,mariadb,0,0
4826,tcp_nodelay,Set the TCP_NODELAY option (disable Nagle's algorithm) on socket.,0,0,others,mariadb,0,0
4827,thread_cache_size,"Number of threads server caches for re-use. If this limit hasn't been reached, when a client disconnects, its threads are put into the cache, and re-used where possible. In MariaDB 10.2.0 and newer the threads are freed after 5 minutes of idle time. Normally this setting has little effect, as the other aspects of the thread implementation are more important, but increasing it can help servers with high volumes of connections per second so that most can use a cached, rather than a new, thread. The cache miss rate can be calculated as the server status variables threads_created/connections. If the thread pool is active, thread_cache_size is ignored. If thread_cache_size is set to greater than the value of max_connections, thread_cache_size will be set to the max_connections value.",1,1,resource,mariadb,0,1
4828,thread_concurrency,"Allows applications to give the system a hint about the desired number of threads. Specific to Solaris only, invokes thr_setconcurrency(). Deprecated and has no effect from MariaDB 5.5.",1,1,resource,mariadb,0,0
4829,thread_stack,"Stack size for each thread. If set too small, limits recursion depth of stored procedures and complexity of SQL statements the server can handle in memory. Also affects limits in the crash-me test.",1,1,resource,mariadb,0,1
4830,time_format,Unused.,0,0,others,mariadb,0,0
4831,time_zone,"The global value determines the default time zone for sessions that connect. The session value determines the session's active time zone. When it is set to SYSTEM, the session's time zone is determined by the system_time_zone system variable.",0,0,others,mariadb,0,0
4833,timestamp,"Sets the time for the client. This will affect the result returned by the NOW() function, not the SYSDATE() function, unless the server is started with the --sysdate-is-now option, in which case SYSDATE becomes an alias of NOW, and will also be affected. Also used to get the original timestamp when restoring rows from the binary log.",0,0,others,mariadb,0,0
4834,tmp_disk_table_size,Max size for data for an internal temporary on-disk MyISAM or Aria table. These tables are created as part of complex queries when the result doesn't fit into the memory engine. You can set this variable if you want to limit the size of temporary tables created in your temporary directory tmpdir.,1,1,resource,mariadb,0,0
4835,tmp_memory_table_size,An alias for tmp_table_size.,1,1,resource,mariadb,0,0
4836,tmp_table_size,"The largest size for temporary tables in memory (not MEMORY tables) although if max_heap_table_size is smaller the lower limit will apply. If a table exceeds the limit, MariaDB converts it to a MyISAM or Aria table. You can see if it's necessary to increase by comparing the status variables Created_tmp_disk_tables and Created_tmp_tables to see how many temporary tables out of the total created needed to be converted to disk. Often complex GROUP BY queries are responsible for exceeding the limit. Defaults may be different on some systems, see for example Differences in MariaDB in Debian. From MariaDB 10.2.7, tmp_memory_table_size is an alias.",1,1,resource,mariadb,0,0
4837,tmpdir,"Directory for storing temporary tables and files. Can specify a list (separated by semicolons in Windows, and colons in Unix) that will then be used in round-robin fashion. This can be used for load balancing across several disks. Note that if the server is a replication replica, and slave_load_tmpdir, which overrides tmpdir for replicas, is not set, you should not set tmpdir to a directory that is cleared when the machine restarts, or else replication may fail.",0,0,others,mariadb,0,0
4838,transaction_alloc_block_size,Size in bytes to increase the memory pool available to each transaction when the available pool is not large enough. See transaction_prealloc_size.,1,1,resource,mariadb,0,0
4839,transaction_prealloc_size,"Initial size of a memory pool available to each transaction for various memory allocations. If the memory pool is not large enough for an allocation, it is increased by transaction_alloc_block_size bytes, and truncated back to transaction_prealloc_size bytes when the transaction is completed. If set large enough to contain all statements in a transaction, extra malloc() calls are avoided.",1,1,resource,mariadb,0,0
4840,tx_isolation,The transaction isolation level. See also SET TRANSACTION ISOLATION LEVEL.,1,3,reliability-tradeoff,mariadb,0,0
4841,tx_read_only,"Default transaction access mode. If set to OFF, the default, access is read/write. If set to ON, access is read-only. The SET TRANSACTION statement can also change the value of this variable. See SET TRANSACTION and START TRANSACTION.",0,0,others,mariadb,0,1
4842,unique_checks,"If set to 1, the default, secondary indexes in InnoDB tables are performed. If set to 0, storage engines can (but are not required to) assume that duplicate keys are not present in input data. Set to 0 to speed up imports of large tables to InnoDB. The storage engine will still issue a duplicate key error if it detects one, even if set to 0.",0,0,others,mariadb,0,0
4843,updatable_views_with_limit,"Determines whether view updates can be made with an UPDATE or DELETE statement with a LIMIT clause if the view does not contain all primary or not null unique key columns from the underlying table. 0 prohibits this, while 1 permits it while issuing a warning (the default).",0,0,others,mariadb,0,0
4844,use_stat_tables,"Controls the use of engine-independent table statistics. 
never: The optimizer will not use data from statistics tables. 
complementary: The optimizer uses data from statistics tables if the same kind of data is not provided by the storage engine.
preferably: Prefer the data from statistics tables, if it's not available there, use the data from the storage engine.
complementary_for_queries: Same as complementary, but for queries only (to avoid needlessly collecting for ANALYZE TABLE). From MariaDB 10.4.1.
preferably_for_queries: Same as preferably, but for queries only (to avoid needlessly collecting for ANALYZE TABLE). From MariaDB 10.4.1.",0,0,others,mariadb,0,0
4845,version,"Server version number. It may also include a suffix with configuration or build information. -debug indicates debugging support was enabled on the server, and -log indicates at least one of the binary log, general log or slow query log are enabled, for example 10.0.1-MariaDB-mariadb1precise-log. From MariaDB 10.2.1, this variable can be set at startup in order to fake the server version.",0,0,others,mariadb,0,0
4846,version_comment,"Value of the COMPILATION_COMMENT option specified by CMake when building MariaDB, for example mariadb.org binary distribution.",0,0,others,mariadb,0,0
4847,version_compile_machine,"The machine type or architecture MariaDB was built on, for example i686.",0,0,others,mariadb,0,1
4848,version_compile_os,"Operating system that MariaDB was built on, for example debian-linux-gnu.",0,0,others,mariadb,0,0
4851,wait_timeout,"Time in seconds that the server waits for a connection to become active before closing it. The session value is initialized when a thread starts up from either the global value, if the connection is non-interactive, or from the interactive_timeout value, if the connection is interactive.",0,0,others,mariadb,0,0
4852,warning_count,"Read-only variable indicating the number of warnings, errors and notes resulting from the most recent statement that generated messages. See SHOW WARNINGS for more. Note warnings will only be recorded if sql_notes is true (the default).",0,0,others,mariadb,0,1
4853,abort-slave-event-count,"When this option is set to some positive integer value other than 0 (the default) it affects replication behavior as follows: After the replication SQL thread has started, value log events are permitted to be executed; after that, the replication SQL thread does not receive any more events, just as if the network connection from the source were cut. The replication SQL thread continues to run, and the output from SHOW REPLICA STATUS displays Yes in both the Replica_IO_Running and the Replica_SQL_Running columns, but no further events are read from the relay log.",1,5,workload-specific,mysql,0,0
4854,admin-ssl,"The --admin-ssl option is like the --ssl option, except that it applies to the administrative connection interface rather than the main connection interface.",1,2,security-tradeoff,mysql,0,0
4855,allow-suspicious-udfs,"This option controls whether loadable functions that have only an xxx symbol for the main function can be loaded. By default, the option is off and only loadable functions that have at least one auxiliary symbol can be loaded; this prevents attempts at loading functions from shared object files other than those containing legitimate functions. See Loadable Function Security Precautions.",1,2,security-tradeoff,mysql,0,0
4856,ansi,"Use standard (ANSI) SQL syntax instead of MySQL syntax. For more precise control over the server SQL mode, use the --sql-mode option instead.",0,0,others,mysql,0,0
4857,audit_log_buffer_size,"When the audit log plugin writes events to the log asynchronously, it uses a buffer to store event contents prior to writing them. This variable controls the size of that buffer, in bytes. The server adjusts the value to a multiple of 4096. The plugin uses a single buffer, which it allocates when it initializes and removes when it terminates. The plugin allocates this buffer only if logging is asynchronous.",1,1,resource,mysql,0,0
4858,audit_log_compression,"The type of compression for the audit log file. Permitted values are NONE (no compression; the default) and GZIP (GNU Zip compression). For more information, see Compressing Audit Log Files.",1,5,workload-specific,mysql,0,0
4859,audit_log_connection_policy,The policy controlling how the audit log plugin writes connection events to its log file. The following table shows the permitted values.,1,6,function-tradeoff,mysql,0,0
4860,audit_log_current_session,"Whether audit logging is enabled for the current session. The session value of this variable is read only. It is set when the session begins based on the values of the audit_log_include_accounts and audit_log_exclude_accounts system variables. The audit log plugin uses the session value to determine whether to audit events for the session. (There is a global value, but the plugin does not use it.)",1,6,function-tradeoff,mysql,0,0
4861,Audit_log_current_size,The size of the current audit log file. The value increases when an event is written to the log and is reset to 0 when the log is rotated.,0,0,others,mysql,0,0
4862,audit_log_encryption,"The type of encryption for the audit log file. Permitted values are NONE (no encryption; the default) and AES (AES-256-CBC cipher encryption). For more information, see Encrypting Audit Log Files.",1,2,security-tradeoff,mysql,0,0
4863,Audit_log_event_max_drop_size,The size of the largest dropped event in performance logging mode.,0,0,others,mysql,0,0
4864,Audit_log_events,"The number of events handled by the audit log plugin, whether or not they were written to the log based on filtering policy.",1,1,resource,mysql,0,0
4865,Audit_log_events_filtered,The number of events handled by the audit log plugin that were filtered (not written to the log) based on filtering policy .,1,1,resource,mysql,0,0
4866,Audit_log_events_lost,The number of events lost in performance logging mode because an event was larger than the available audit log buffer space. This value may be useful for assessing how to set audit_log_buffer_size to size the buffer for performance mode.,0,0,others,mysql,0,1
4867,Audit_log_events_written,The number of events written to the audit log.,0,0,others,mysql,0,0
4868,audit_log_exclude_accounts,The accounts for which events should not be logged. The value should be NULL or a string containing a list of one or more comma-separated account names.,0,0,others,mysql,0,0
4869,audit_log_file,"The base name and suffix of the file to which the audit log plugin writes events. The default value is audit.log, regardless of logging format. To have the name suffix correspond to the format, set the name explicitly, choosing a different suffix (for example, audit.xml for XML format, audit.json for JSON format).",0,0,others,mysql,0,0
4870,audit_log_filter_id,The session value of this variable indicates the internally maintained ID of the audit filter for the current session. A value of 0 means that the session has no filter assigned.,0,0,others,mysql,0,0
4871,audit_log_flush,"If audit_log_rotate_on_size is 0, automatic audit log file rotation is disabled and rotation occurs only when performed manually. In that case, enabling audit_log_flush by setting it to 1 or ON causes the audit log plugin to close and reopen its log file to flush it. (The variable value remains OFF so that you need not disable it explicitly before enabling it again to perform another flush.) For more information, see Section6.4.5.5, “Configuring Audit Logging Characteristics”.",1,3,reliability-tradeoff,mysql,0,0
4872,audit_log_format,"The audit log file format. Permitted values are OLD (old-style XML), NEW (new-style XML; the default), and JSON. For details about each format, see Section6.4.5.4, “Audit Log File Formats”.",0,0,others,mysql,0,0
4873,audit_log_format_unix_timestamp,"This variable applies only for JSON-format audit log output. When that is true, enabling this variable causes each log file record to include a time field. The field value is an integer that represents the UNIX timestamp value indicating the date and time when the audit event was generated.",0,0,others,mysql,0,1
4874,audit_log_include_accounts,"The accounts for which events should be logged. The value should be NULL or a string containing a list of one or more comma-separated account names. For more information, see Section6.4.5.7, “Audit Log Filtering”.",0,0,others,mysql,0,0
4875,audit_log_max_size,"audit_log_max_size pertains to audit log file pruning, which is supported for JSON-format log files only. It controls pruning based on combined log file size. A value of 0 (the default) disables size-based pruning. No size limit is enforced. A value greater than 0 enables size-based pruning. The value is the combined size above which audit log files become subject to pruning.",0,0,others,mysql,0,0
4876,audit_log_password_history_keep_days,"When the audit log plugin creates a new encryption password, it archives the previous password, if one exists, for later use. The audit_log_password_history_keep_days variable controls automatic removal of expired archived passwords. Its value indicates the number of days after which archived audit log encryption passwords are removed.",0,0,others,mysql,0,0
4877,audit_log_policy,The policy controlling how the audit log plugin writes events to its log file. The following table shows the permitted values.,1,6,function-tradeoff,mysql,0,0
4878,audit_log_prune_seconds,"audit_log_prune_seconds pertains to audit log file pruning, which is supported for JSON-format log files only. It controls pruning based on log file age. A value of 0 (the default) disables age-based pruning. No age limit is enforced. A value greater than 0 enables age-based pruning. The value is the number of seconds after which audit log files become subject to pruning.",1,5,workload-specific,mysql,0,0
4879,audit_log_read_buffer_size,"The buffer size for reading from the audit log file, in bytes. The audit_log_read() function reads no more than this many bytes. Log file reading is supported only for JSON log format. For more information, see Section6.4.5.6, “Reading Audit Log Files”.",1,1,resource,mysql,0,1
4880,audit_log_rotate_on_size,"If audit_log_rotate_on_size is 0, the audit log plugin does not perform automatic size-based log file rotation. If rotation is to occur, you must perform it manually; see Manual Audit Log File Rotation.",0,0,others,mysql,0,0
4881,audit_log_statement_policy,The policy controlling how the audit log plugin writes statement events to its log file. The following table shows the permitted values.,1,6,function-tradeoff,mysql,0,1
4882,audit_log_strategy,The logging method used by the audit log plugin. These strategy values are permitted: ASYNCHRONOUS: Log asynchronously. Wait for space in the output buffer. PERFORMANCE: Log asynchronously. Drop requests for which there is insufficient space in the output buffer. SEMISYNCHRONOUS: Log synchronously. Permit caching by the operating system. SYNCHRONOUS: Log synchronously. Call sync() after each request.,1,3,reliability-tradeoff,mysql,0,0
4883,Audit_log_total_size,"The total size of events written to all audit log files. Unlike Audit_log_current_size, the value of Audit_log_total_size increases even when the log is rotated.",0,0,others,mysql,0,0
4884,Audit_log_write_waits,The number of times an event had to wait for space in the audit log buffer in asynchronous logging mode.,0,0,others,mysql,0,1
4885,audit-log,"This option controls how the server loads the audit_log plugin at startup. It is available only if the plugin has been previously registered with INSTALL PLUGIN or is loaded with --plugin-load or --plugin-load-add. See Section6.4.5.2, “Installing or Uninstalling MySQL Enterprise Audit”.",0,0,others,mysql,0,1
4886,authentication_kerberos_service_key_tab,"The name of the server-side key-table (“keytab”) file containing Kerberos service keys to authenticate MySQL service tickets received from clients. The file name should be given as an absolute path name. If this variable is not set, the default is mysql.keytab in the data directory.",0,0,others,mysql,0,1
4887,authentication_kerberos_service_principal,The Kerberos service principal name (SPN) that the MySQL server sends to clients.,0,0,others,mysql,0,0
4888,authentication_ldap_sasl_auth_method_name,"For SASL LDAP authentication, the authentication method name. Communication between the authentication plugin and the LDAP server occurs according to this authentication method to ensure password security.",0,0,others,mysql,0,1
4889,authentication_ldap_sasl_bind_base_dn,"For SASL LDAP authentication, the base distinguished name (DN). This variable can be used to limit the scope of searches by anchoring them at a certain location (the “base”) within the search tree.",0,0,others,mysql,0,1
4890,authentication_ldap_sasl_bind_root_dn,"For SASL LDAP authentication, the root distinguished name (DN). This variable is used in conjunction with authentication_ldap_sasl_bind_root_pwd as the credentials for authenticating to the LDAP server for the purpose of performing searches. Authentication uses either one or two LDAP bind operations, depending on whether the MySQL account names an LDAP user DN:",0,0,others,mysql,0,0
4891,authentication_ldap_sasl_bind_root_pwd,"For SASL LDAP authentication, the password for the root distinguished name. This variable is used in conjunction with authentication_ldap_sasl_bind_root_dn. See the description of that variable.",0,0,others,mysql,0,0
4892,authentication_ldap_sasl_ca_path,"For SASL LDAP authentication, the absolute path of the certificate authority file. Specify this file if it is desired that the authentication plugin perform verification of the LDAP server certificate.",0,0,others,mysql,0,0
4893,authentication_ldap_sasl_group_search_attr,"For SASL LDAP authentication, the name of the attribute that specifies group names in LDAP directory entries. If authentication_ldap_sasl_group_search_attr has its default value of cn, searches return the cn value as the group name. For example, if an LDAP entry with a uid value of user1 has a cn attribute of mygroup, searches for user1 return mygroup as the group name.",0,0,others,mysql,0,1
4894,authentication_ldap_sasl_group_search_filter,"For SASL LDAP authentication, the custom group search filter.",0,0,others,mysql,0,0
4895,authentication_ldap_sasl_init_pool_size,"For SASL LDAP authentication, the initial size of the pool of connections to the LDAP server. Choose the value for this variable based on the average number of concurrent authentication requests to the LDAP server.",0,0,others,mysql,0,0
4896,authentication_ldap_sasl_log_status,"For SASL LDAP authentication, the logging level for messages written to the error log. The following table shows the permitted level values and their meanings.",1,6,function-tradeoff,mysql,0,0
4897,authentication_ldap_sasl_max_pool_size,"For SASL LDAP authentication, the maximum size of the pool of connections to the LDAP server. To disable connection pooling, set this variable to 0.",1,1,resource,mysql,0,0
4898,authentication_ldap_sasl_referral,"For SASL LDAP authentication, whether to enable LDAP search referral. See LDAP Search Referral.",0,0,others,mysql,0,1
4899,authentication_ldap_sasl_server_host,"For SASL LDAP authentication, the LDAP server host. The permitted values for this variable depend on the authentication method:",0,0,others,mysql,0,0
4900,authentication_ldap_sasl_server_port,"For SASL LDAP authentication, the LDAP server TCP/IP port number.",0,0,others,mysql,0,0
4901,authentication_ldap_sasl_tls,"For SASL LDAP authentication, whether connections by the plugin to the LDAP server are secure. If this variable is enabled, the plugin uses TLS to connect securely to the LDAP server. This variable can be set to override the default OpenLDAP TLS configuration; see LDAP Pluggable Authentication and ldap.conf If you enable this variable, you may also wish to set the authentication_ldap_sasl_ca_path variable.",1,2,security-tradeoff,mysql,0,0
4902,authentication_ldap_sasl_user_search_attr,"For SASL LDAP authentication, the name of the attribute that specifies user names in LDAP directory entries. If a user distinguished name is not provided, the authentication plugin searches for the name using this attribute. For example, if the authentication_ldap_sasl_user_search_attr value is uid, a search for the user name user1 finds entries with a uid value of user1.",0,0,others,mysql,0,0
4903,authentication_ldap_simple_auth_method_name,"For simple LDAP authentication, the authentication method name. Communication between the authentication plugin and the LDAP server occurs according to this authentication method.",0,0,others,mysql,0,0
4904,authentication_ldap_simple_bind_base_dn,"For simple LDAP authentication, the base distinguished name (DN). This variable can be used to limit the scope of searches by anchoring them at a certain location (the “base”) within the search tree.",0,0,others,mysql,0,0
4905,authentication_ldap_simple_bind_root_dn,"For simple LDAP authentication, the root distinguished name (DN). This variable is used in conjunction with authentication_ldap_simple_bind_root_pwd as the credentials for authenticating to the LDAP server for the purpose of performing searches. Authentication uses either one or two LDAP bind operations, depending on whether the MySQL account names an LDAP user DN:",0,0,others,mysql,0,0
4906,authentication_ldap_simple_bind_root_pwd,"For simple LDAP authentication, the password for the root distinguished name. This variable is used in conjunction with authentication_ldap_simple_bind_root_dn. See the description of that variable.",0,0,others,mysql,0,0
4907,authentication_ldap_simple_ca_path,"For simple LDAP authentication, the absolute path of the certificate authority file. Specify this file if it is desired that the authentication plugin perform verification of the LDAP server certificate.",0,0,others,mysql,0,1
4908,authentication_ldap_simple_group_search_attr,"For simple LDAP authentication, the name of the attribute that specifies group names in LDAP directory entries. If authentication_ldap_simple_group_search_attr has its default value of cn, searches return the cn value as the group name. For example, if an LDAP entry with a uid value of user1 has a cn attribute of mygroup, searches for user1 return mygroup as the group name.",0,0,others,mysql,0,0
4909,authentication_ldap_simple_group_search_filter,"For simple LDAP authentication, the custom group search filter.",0,0,others,mysql,0,0
4910,authentication_ldap_simple_init_pool_size,"For simple LDAP authentication, the initial size of the pool of connections to the LDAP server. Choose the value for this variable based on the average number of concurrent authentication requests to the LDAP server.",1,1,resource,mysql,0,0
4911,authentication_ldap_simple_log_status,"For simple LDAP authentication, the logging level for messages written to the error log. The following table shows the permitted level values and their meanings.",1,6,function-tradeoff,mysql,0,0
4912,authentication_ldap_simple_max_pool_size,"For simple LDAP authentication, the maximum size of the pool of connections to the LDAP server. To disable connection pooling, set this variable to 0.",1,1,resource,mysql,0,1
4913,authentication_ldap_simple_referral,"For simple LDAP authentication, whether to enable LDAP search referral. See LDAP Search Referral.",0,0,others,mysql,0,0
4914,authentication_ldap_simple_server_host,"For simple LDAP authentication, the LDAP server host. The permitted values for this variable depend on the authentication method:",0,0,others,mysql,0,1
4915,authentication_ldap_simple_server_port,"For simple LDAP authentication, the LDAP server TCP/IP port number.",0,0,others,mysql,0,0
4916,authentication_ldap_simple_tls,"For simple LDAP authentication, whether connections by the plugin to the LDAP server are secure. If this variable is enabled, the plugin uses TLS to connect securely to the LDAP server. This variable can be set to override the default OpenLDAP TLS configuration; see LDAP Pluggable Authentication and ldap.conf If you enable this variable, you may also wish to set the authentication_ldap_simple_ca_path variable.",1,2,security-tradeoff,mysql,0,0
4917,authentication_ldap_simple_user_search_attr,"For simple LDAP authentication, the name of the attribute that specifies user names in LDAP directory entries. If a user distinguished name is not provided, the authentication plugin searches for the name using this attribute. For example, if the authentication_ldap_simple_user_search_attr value is uid, a search for the user name user1 finds entries with a uid value of user1.",0,0,others,mysql,0,0
4918,auto_increment_increment,"auto_increment_increment and auto_increment_offset are intended for use with circular (source-to-source) replication, and can be used to control the operation of AUTO_INCREMENT columns. Both variables have global and session values, and each can assume an integer value between 1 and 65,535 inclusive. Setting the value of either of these two variables to 0 causes its value to be set to 1 instead. Attempting to set the value of either of these two variables to an integer greater than 65,535 or less than 0 causes its value to be set to 65,535 instead. Attempting to set the value of auto_increment_increment or auto_increment_offset to a noninteger value produces an error, and the actual value of the variable remains unchanged.",0,0,others,mysql,0,0
4919,auto_increment_offset,"This variable has a default value of 1. If it is left with its default value, and Group Replication is started on the server in multi-primary mode, it is changed to the server ID. For more information, see the description for auto_increment_increment.",0,0,others,mysql,0,1
4920,binlog_cache_size,The size of the memory buffer to hold changes to the binary log during a transaction. The value must be a multiple of 4096.,1,1,resource,mysql,0,0
4921,binlog_checksum,"When enabled, this variable causes the source to write a checksum for each event in the binary log. binlog_checksum supports the values NONE (which disables checksums) and CRC32. The default is CRC32. When binlog_checksum is disabled (value NONE), the server verifies that it is writing only complete events to the binary log by writing and checking the event length (rather than a checksum) for each event.",1,2,security-tradeoff,mysql,0,0
4922,binlog_direct_non_transactional_updates,"Due to concurrency issues, a replica can become inconsistent when a transaction contains updates to both transactional and nontransactional tables. MySQL tries to preserve causality among these statements by writing nontransactional statements to the transaction cache, which is flushed upon commit. However, problems arise when modifications done to nontransactional tables on behalf of a transaction become immediately visible to other connections because these changes may not be written immediately into the binary log.",0,0,others,mysql,0,0
4923,binlog_encryption,"Enables encryption for binary log files and relay log files on this server. OFF is the default. ON sets encryption on for binary log files and relay log files. Binary logging does not need to be enabled on the server to enable encryption, so you can encrypt the relay log files on a replica that has no binary log. To use encryption, a keyring plugin must be installed and configured to supply MySQL Server's keyring service. For instructions to do this, see Section6.4.4, “The MySQL Keyring”. Any supported keyring plugin can be used to store binary log encryption keys.",1,2,security-tradeoff,mysql,0,1
4924,binlog_error_action,"Controls what happens when the server encounters an error such as not being able to write to, flush or synchronize the binary log, which can cause the source's binary log to become inconsistent and replicas to lose synchronization.",1,6,function-tradeoff,mysql,0,1
4925,binlog_expire_logs_seconds,"Sets the binary log expiration period in seconds. After their expiration period ends, binary log files can be automatically removed. Possible removals happen at startup and when the binary log is flushed. Log flushing occurs as indicated in Section5.4, “MySQL Server Logs”.",0,0,others,mysql,0,0
4926,binlog_format,"This system variable sets the binary logging format, and can be any one of STATEMENT, ROW, or MIXED. See Section17.2.1, “Replication Formats”. The setting takes effect when binary logging is enabled on the server, which is the case when the log_bin system variable is set to ON. From MySQL 8.0, binary logging is enabled by default.",0,0,others,mysql,0,0
4927,binlog_group_commit_sync_delay,"Controls how many microseconds the binary log commit waits before synchronizing the binary log file to disk. By default binlog_group_commit_sync_delay is set to 0, meaning that there is no delay. Setting binlog_group_commit_sync_delay to a microsecond delay enables more transactions to be synchronized together to disk at once, reducing the overall time to commit a group of transactions because the larger groups require fewer time units per group.",0,0,others,mysql,0,0
4928,binlog_group_commit_sync_no_delay_count,"The maximum number of transactions to wait for before aborting the current delay as specified by binlog_group_commit_sync_delay. If binlog_group_commit_sync_delay is set to 0, then this option has no effect.",0,0,others,mysql,0,0
4929,binlog_gtid_simple_recovery,This variable controls how binary log files are iterated during the search for GTIDs when MySQL starts or restarts.,1,6,function-tradeoff,mysql,0,0
4930,binlog_max_flush_queue_time,"binlog_max_flush_queue_time is deprecated, and is marked for eventual removal in a future MySQL release. Formerly, this system variable controlled the time in microseconds to continue reading transactions from the flush queue before proceeding with group commit. It no longer has any effect.",0,0,others,mysql,0,0
4931,binlog_order_commits,"When this variable is enabled on a replication source server (which is the default), transaction commit instructions issued to storage engines are serialized on a single thread, so that transactions are always committed in the same order as they are written to the binary log. Disabling this variable permits transaction commit instructions to be issued using multiple threads. Used in combination with binary log group commit, this prevents the commit rate of a single transaction being a bottleneck to throughput, and might therefore produce a performance improvement.",1,3,reliability-tradeoff,mysql,0,1
4932,binlog_rotate_encryption_master_key_at_startup,"Specifies whether or not the binary log master key is rotated at server startup. The binary log master key is the binary log encryption key that is used to encrypt file passwords for the binary log files and relay log files on the server. When a server is started for the first time with binary log encryption enabled (binlog_encryption=ON), a new binary log encryption key is generated and used as the binary log master key. If the binlog_rotate_encryption_master_key_at_startup system variable is also set to ON, whenever the server is restarted, a further binary log encryption key is generated and used as the binary log master key for all subsequent binary log files and relay log files. If the binlog_rotate_encryption_master_key_at_startup system variable is set to OFF, which is the default, the existing binary log master key is used again after the server restarts. For more information on binary log encryption keys and the binary log master key, see Section17.3.2, “Encrypting Binary Log Files and Relay Log Files”.",1,2,security-tradeoff,mysql,0,0
4933,binlog_row_event_max_size,"When row-based binary logging is used, this setting is a soft limit on the maximum size of a row-based binary log event, in bytes. Where possible, rows stored in the binary log are grouped into events with a size not exceeding the value of this setting. If an event cannot be split, the maximum size can be exceeded. The value must be (or else gets rounded down to) a multiple of 256. The default is 8192 bytes.",1,1,resource,mysql,0,1
4934,binlog_row_image,"For MySQL row-based replication, this variable determines how row images are written to the binary log.",1,6,function-tradeoff,mysql,0,0
4935,binlog_row_metadata,"Configures the amount of table metadata added to the binary log when using row-based logging. When set to MINIMAL, the default, only metadata related to SIGNED flags, column character set and geometry types are logged. When set to FULL complete metadata for tables is logged, such as column name, ENUM or SET string values, PRIMARY KEY information, and so on.",1,6,function-tradeoff,mysql,0,0
4936,binlog_row_value_options,"When set to PARTIAL_JSON, this enables use of a space-efficient binary log format for updates that modify only a small portion of a JSON document, which causes row-based replication to write only the modified parts of the JSON document to the after-image for the update in the binary log, rather than writing the full document (see Partial Updates of JSON Values). This works for an UPDATE statement which modifies a JSON column using any sequence of JSON_SET(), JSON_REPLACE(), and JSON_REMOVE(). If the server is unable to generate a partial update, the full document is used instead.",1,4,limited-side-effect,mysql,0,0
4937,binlog_rows_query_log_events,"This system variable affects row-based logging only. When enabled, it causes the server to write informational log events such as row query log events into its binary log. This information can be used for debugging and related purposes, such as obtaining the original query issued on the source when it cannot be reconstructed from the row updates.",1,6,function-tradeoff,mysql,0,0
4938,binlog_stmt_cache_size,The size of the memory buffer for the binary log to hold nontransactional statements issued during a transaction. The value must be a multiple of 4096.,1,1,resource,mysql,0,0
4939,binlog_transaction_compression,Enables compression for transactions that are written to binary log files on this server. OFF is the default. Use the binlog_transaction_compression_level_zstd system variable to set the level for the zstd algorithm that is used for compression.,1,4,limited-side-effect,mysql,0,0
4940,binlog_transaction_compression_level_zstd,"Sets the compression level for binary log transaction compression on this server, which is enabled by the binlog_transaction_compression system variable. The value is an integer that determines the compression effort, from 1 (the lowest effort) to 22 (the highest effort). If you do not specify this system variable, the compression level is set to 3.",1,5,workload-specific,mysql,0,0
4941,binlog_transaction_dependency_history_size,"Sets an upper limit on the number of row hashes which are kept in memory and used for looking up the transaction that last modified a given row. Once this number of hashes has been reached, the history is purged.",1,5,workload-specific,mysql,0,1
4942,binlog_transaction_dependency_tracking,"For a replication source server that has multithreaded replicas (replicas on which replica_parallel_workers or slave_parallel_workers is set to a value greater than 0), binlog_transaction_dependency_tracking specifies the source of dependency information that the source records in the binary log to help replicas determine which transactions can be executed in parallel. The possible values are:",1,6,function-tradeoff,mysql,0,0
4943,binlog-checksum,"Enabling this option causes the source to write checksums for events written to the binary log. Set to NONE to disable, or the name of the algorithm to be used for generating checksums; currently, only CRC32 checksums are supported, and CRC32 is the default. You cannot change the setting for this option within a transaction.",1,2,security-tradeoff,mysql,0,0
4944,binlog-do-db,This option affects binary logging in a manner similar to the way that --replicate-do-db affects replication.,0,0,others,mysql,0,0
4945,binlog-ignore-db,This option affects binary logging in a manner similar to the way that --replicate-ignore-db affects replication.,0,0,others,mysql,0,0
4946,character-set-client-handshake,"Do not ignore character set information sent by the client. To ignore client information and use the default server character set, use --skip-character-set-client-handshake; this makes MySQL behave like MySQL 4.0.",0,0,others,mysql,0,0
4947,chroot,Put the mysqld server in a closed environment during startup by using the chroot() system call. This is a recommended security measure. Use of this option somewhat limits LOAD DATA and SELECT ... INTO OUTFILE.,0,0,others,mysql,0,0
4948,clone_autotune_concurrency,"When clone_autotune_concurrency is enabled (the default), additional threads for remote cloning operations are spawned dynamically to optimize data transfer speed. The setting is applicable to recipient MySQL server instance only.",1,4,limited-side-effect,mysql,0,1
4949,clone_buffer_size,"Defines the size of the intermediate buffer used when transferring data during a local cloning operation. The default value is 4 mebibytes (MiB). A larger buffer size may permit I/O device drivers to fetch data in parallel, which can improve cloning performance.",1,1,resource,mysql,0,0
4950,clone_ddl_timeout,"The time in seconds to wait for a backup lock when executing a cloning operation. This setting is applied on both the donor and recipient MySQL server instances. A cloning operation cannot run concurrently with DDL operations. A backup lock is required on the donor and recipient MySQL server instances. The cloning operation waits for current DDL operations to finish. Once backup locks are acquired, DDL operations must wait for the cloning operation to finish. A value of 0 means that no backup lock is to be taken for the cloning operation. In this case, the cloning operation fails with an error if a DDL operation is attempted concurrently.",0,0,others,mysql,0,0
4951,clone_donor_timeout_after_network_failure,"Defines the amount of time in minutes the donor allows for the recipient to reconnect and restart a cloning operation after a network failure. For more information, see Section5.6.7.8, “Remote Cloning Operation Failure Handling”.",0,0,others,mysql,0,0
4952,clone_enable_compression,Enables compression of data at the network layer during a remote cloning operation. Compression saves network bandwidth at the cost of CPU. Enabling compression may improve the data transfer rate. This setting is only applied on the recipient MySQL server instance.,1,4,limited-side-effect,mysql,0,1
4953,clone_max_concurrency,"Defines the maximum number of concurrent threads for a remote cloning operation. The default value is 16. A greater number of threads can improve cloning performance but also reduces the number of permitted simultaneous client connections, which can affect the performance of existing client connections. This setting is only applied on the recipient MySQL server instance.",1,1,resource,mysql,0,0
4954,clone_max_data_bandwidth,"Defines the maximum data transfer rate in mebibytes (MiB) per second for a remote cloning operation. This variable helps manage the performance impact of a cloning operation. A limit should be set only when donor disk I/O bandwidth is saturated, affecting performance. A value of 0 means “unlimited”, which permits cloning operations to run at the highest possible data transfer rate. This setting is only applicable to the recipient MySQL server instance.",1,1,resource,mysql,0,0
4955,clone_max_network_bandwidth,"Specifies the maximum approximate network transfer rate in mebibytes (MiB) per second for a remote cloning operation. This variable can be used to manage the performance impact of a cloning operation on network bandwidth. It should be set only when network bandwidth is saturated, affecting performance on the donor instance. A value of 0 means “unlimited”, which permits cloning at the highest possible data transfer rate over the network, providing the best performance. This setting is only applicable to the recipient MySQL server instance.",1,1,resource,mysql,0,0
4956,clone_ssl_ca,Specifies the path to the certificate authority (CA) file. Used to configure an encrypted connection for a remote cloning operation. This setting configured on the recipient and used when connecting to the donor.,0,0,others,mysql,0,1
4957,clone_ssl_cert,Specifies the path to the public key certificate. Used to configure an encrypted connection for a remote cloning operation. This setting configured on the recipient and used when connecting to the donor.,0,0,others,mysql,0,0
4958,clone_ssl_key,Specifies the path to the private key file. Used to configure an encrypted connection for a remote cloning operation. This setting configured on the recipient and used when connecting to the donor.,0,0,others,mysql,0,0
4959,clone_valid_donor_list,"Defines valid donor host addresses for remote cloning operations. This setting is applied on the recipient MySQL server instance. A comma-separated list of values is permitted in the following format: “HOST1:PORT1,HOST2:PORT2,HOST3:PORT3”. Spaces are not permitted.",0,0,others,mysql,0,0
4960,Connection_control_delay_generated,The number of times the server added a delay to its response to a failed connection attempt. This does not count attempts that occur before reaching the threshold defined by the connection_control_failed_connections_threshold system variable.,0,0,others,mysql,0,0
4961,connection_control_failed_connections_threshold,The number of consecutive failed connection attempts permitted to accounts before the server adds a delay for subsequent connection attempts:,0,0,others,mysql,0,0
4962,connection_control_max_connection_delay,"The maximum delay in milliseconds for server response to failed connection attempts, if connection_control_failed_connections_threshold is greater than zero.",0,0,others,mysql,0,0
4963,connection_control_min_connection_delay,"The minimum delay in milliseconds for server response to failed connection attempts, if connection_control_failed_connections_threshold is greater than zero.",0,0,others,mysql,0,0
4964,console,"(Windows only.) Cause the default error log destination to be the console. This affects log sinks that base their own output destination on the default destination. See Section5.4.2, “The Error Log”. mysqld does not close the console window if this option is used.",0,0,others,mysql,0,1
4965,core-file,"Write a core file if mysqld dies. The name and location of the core file is system dependent. On Linux, a core file named core.pid is written to the current working directory of the process, which for mysqld is the data directory. pid represents the process ID of the server process. On macOS, a core file named core.pid is written to the /cores directory. On Solaris, use the coreadm command to specify where to write the core file and how to name it.",0,0,others,mysql,0,0
4966,daemon_memcached_enable_binlog,Enable this option on the source server to use the InnoDB memcached plugin (daemon_memcached) with the MySQL binary log. This option can only be set at server startup. You must also enable the MySQL binary log on the source server using the --log-bin option.,0,0,others,mysql,0,1
4967,daemon_memcached_engine_lib_name,Specifies the shared library that implements the InnoDB memcached plugin.,0,0,others,mysql,0,0
4968,daemon_memcached_engine_lib_path,"The path of the directory containing the shared library that implements the InnoDB memcached plugin. The default value is NULL, representing the MySQL plugin directory. You should not need to modify this parameter unless specifying a memcached plugin for a different storage engine that is located outside of the MySQL plugin directory.",0,0,others,mysql,0,0
4969,daemon_memcached_option,"Used to pass space-separated memcached options to the underlying memcached memory object caching daemon on startup. For example, you might change the port that memcached listens on, reduce the maximum number of simultaneous connections, change the maximum memory size for a key-value pair, or enable debugging messages for the error log.",1,5,workload-specific,mysql,0,0
4970,daemon_memcached_r_batch_size,Specifies how many memcached read operations (get operations) to perform before doing a COMMIT to start a new transaction. Counterpart of daemon_memcached_w_batch_size.,1,1,resource,mysql,0,0
4971,daemon_memcached_w_batch_size,"Specifies how many memcached write operations, such as add, set, and incr, to perform before doing a COMMIT to start a new transaction. Counterpart of daemon_memcached_r_batch_size.",1,1,resource,mysql,0,0
4972,daemonize,"This option causes the server to run as a traditional, forking daemon, permitting it to work with operating systems that use systemd for process control.",0,0,others,mysql,0,0
4973,ddl-rewriter,"This option controls how the server loads the ddl_rewriter plugin at startup. It is available only if the plugin has been previously registered with INSTALL PLUGIN or is loaded with --plugin-load or --plugin-load-add. See Section5.6.5.1, “Installing or Uninstalling ddl_rewriter”.",0,0,others,mysql,0,0
4974,debug,"If MySQL is configured with the -DWITH_DEBUG=1 CMake option, you can use this option to get a trace file of what mysqld is doing. A typical debug_options string is d:t:o,file_name. The default is d:t:i:o,/tmp/mysqld.trace on Unix and d:t:i:O,\mysqld.trace on Windows.",0,0,others,mysql,0,0
4975,debug-sync-timeout,"Controls whether the Debug Sync facility for testing and debugging is enabled. Use of Debug Sync requires that MySQL be configured with the -DENABLE_DEBUG_SYNC=1 CMake option (see Section2.9.7, “MySQL Source-Configuration Options”). If Debug Sync is not compiled in, this option is not available. The option value is a timeout in seconds. The default value is 0, which disables Debug Sync. To enable it, specify a value greater than 0; this value also becomes the default timeout for individual synchronization points. If the option is given without a value, the timeout is set to 300 seconds.",0,0,others,mysql,0,1
4976,defaults-extra-file,"Read this option file after the global option file but (on Unix) before the user option file. If the file does not exist or is otherwise inaccessible, an error occurs. If file_name is not an absolute path name, it is interpreted relative to the current directory. This must be the first option on the command line if it is used.",0,0,others,mysql,0,0
4977,defaults-file,"Read only the given option file. If the file does not exist or is otherwise inaccessible, an error occurs. If file_name is not an absolute path name, it is interpreted relative to the current directory.",0,0,others,mysql,0,0
4978,defaults-group-suffix,"Read not only the usual option groups, but also groups with the usual names and a suffix of str. For example, mysqld normally reads the [mysqld] group. If this option is given as --defaults-group-suffix=_other, mysqld also reads the [mysqld_other] group.",0,0,others,mysql,0,0
4979,default-time-zone,"Set the default server time zone. This option sets the global time_zone system variable. If this option is not given, the default time zone is the same as the system time zone (given by the value of the system_time_zone system variable.",0,0,others,mysql,0,0
4980,early-plugin-load,"This option tells the server which plugins to load before loading mandatory built-in plugins and before storage engine initialization. If multiple --early-plugin-load options are given, only the last one applies.",0,0,others,mysql,0,1
4981,enforce_gtid_consistency,"Depending on the value of this variable, the server enforces GTID consistency by allowing execution of only statements that can be safely logged using a GTID. You must set this variable to ON before enabling GTID based replication.",1,3,reliability-tradeoff,mysql,0,0
4982,exit-info,This is a bitmask of different flags that you can use for debugging the mysqld server. Do not use this option unless you know exactly what it does!,0,0,others,mysql,0,0
4983,expire_logs_days,"Specifies the number of days before automatic removal of binary log files. expire_logs_days is deprecated, and you should expect it to be removed in a future release. Instead, use binlog_expire_logs_seconds, which sets the binary log expiration period in seconds. If you do not set a value for either system variable, the default expiration period is 30 days. Possible removals happen at startup and when the binary log is flushed. Log flushing occurs as indicated in Section5.4, “MySQL Server Logs”.",0,0,others,mysql,0,1
4984,external-locking,"Enable external locking (system locking), which is disabled by default. If you use this option on a system on which lockd does not fully work (such as Linux), it is easy for mysqld to deadlock.",0,0,others,mysql,0,0
4985,Firewall_access_denied,The number of statements rejected by MySQL Enterprise Firewall.,0,0,others,mysql,0,0
4986,Firewall_access_granted,The number of statements accepted by MySQL Enterprise Firewall.,0,0,others,mysql,0,0
4987,Firewall_cached_entries,"The number of statements recorded by MySQL Enterprise Firewall, including duplicates.",0,0,others,mysql,0,0
4988,gdb,"Install an interrupt handler for SIGINT (needed to stop mysqld with ^C to set breakpoints) and disable stack tracing and core file handling. See Section5.9.1.4, “Debugging mysqld under gdb”.",0,0,others,mysql,0,0
4989,group_replication_advertise_recovery_endpoints,"The value of this system variable can be changed while Group Replication is running. The change takes effect immediately on the member. However, a joining member that already received the previous value of the system variable continues to use that value. Only members that join after the value change receive the new value.",0,0,others,mysql,0,0
4990,group_replication_allow_local_lower_version_join,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,1
4991,group_replication_auto_increment_increment,"This system variable should have the same value on all group members. You cannot change the value of this system variable while Group Replication is running. You must stop Group Replication, change the value of the system variable, then restart Group Replication, on each of the group members. During this process, the value of the system variable is permitted to differ between group members, but some transactions on group members might be rolled back.",0,0,others,mysql,0,0
4992,group_replication_autorejoin_tries,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately. The system variable's current value is read when an issue occurs that means the behavior is needed.",0,0,others,mysql,0,0
4993,group_replication_bootstrap_group,"group_replication_bootstrap_group configures this server to bootstrap the group. This system variable must only be set on one server, and only when starting the group for the first time or restarting the entire group. After the group has been bootstrapped, set this option to OFF. It should be set to OFF both dynamically and in the configuration files. Starting two servers or restarting one server with this option set while the group is running may lead to an artificial split brain situation, where two independent groups with the same name are bootstrapped.",0,0,others,mysql,0,0
4994,group_replication_clone_threshold,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
4995,group_replication_communication_debug_options,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
4996,group_replication_communication_max_message_size,"This system variable should have the same value on all group members. You cannot change the value of this system variable while Group Replication is running. You must stop Group Replication, change the value of the system variable, then restart Group Replication, on each of the group members. During this process, the value of the system variable is permitted to differ between group members, but some transactions on group members might be rolled back.",0,0,others,mysql,0,0
4997,group_replication_components_stop_timeout,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
4998,group_replication_compression_threshold,"This system variable should have the same value on all group members. The value of this system variable can be changed while Group Replication is running. The change takes effect on each group member after you stop and restart Group Replication on the member. During this process, the value of the system variable is permitted to differ between group members, but message delivery does not have the same efficiency on all members.",0,0,others,mysql,0,1
4999,group_replication_consistency,"The value of this system variable can be changed while Group Replication is running. group_replication_consistency is a server system variable rather than a Group Replication plugin-specific variable, so a restart of Group Replication is not required for the change to take effect. Changing the session value of the system variable takes effect immediately, and changing the global value takes effect for new sessions that start after the change. The GROUP_REPLICATION_ADMIN privilege is required to change the global setting for this system variable.",0,0,others,mysql,0,0
5000,group_replication_enforce_update_everywhere_checks,"This system variable is a group-wide configuration setting. It must have the same value on all group members, cannot be changed while Group Replication is running, and requires a full reboot of the group (a bootstrap by a server with group_replication_bootstrap_group=ON) in order for the value change to take effect. If the group has a value set for this system variable, and a joining member has a different value set for the system variable, the joining member cannot join the group until the value is changed to match. If the group members have a value set for this system variable, and the joining member does not support the system variable, it cannot join the group.",0,0,others,mysql,0,0
5001,group_replication_exit_state_action,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately. The system variable's current value is read when an issue occurs that means the behavior is needed.",0,0,others,mysql,0,0
5002,group_replication_flow_control_applier_threshold,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately.",0,0,others,mysql,0,1
5003,group_replication_flow_control_certifier_threshold,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately.",0,0,others,mysql,0,0
5004,group_replication_flow_control_hold_percent,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately.",0,0,others,mysql,0,0
5005,group_replication_flow_control_max_commit_quota,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately.",0,0,others,mysql,0,0
5006,group_replication_flow_control_member_quota_percent,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately.",0,0,others,mysql,0,0
5007,group_replication_flow_control_min_quota,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately.",0,0,others,mysql,0,0
5008,group_replication_flow_control_min_recovery_quota,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately.",0,0,others,mysql,0,0
5009,group_replication_flow_control_mode,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately.",0,0,others,mysql,0,0
5010,group_replication_flow_control_period,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately.",0,0,others,mysql,0,0
5011,group_replication_flow_control_release_percent,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately.",0,0,others,mysql,0,0
5012,group_replication_force_members,"This system variable is used to force a new group membership. The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately. You only need to set the value of the system variable on one of the group members that is to remain in the group. For details of the situation in which you might need to force a new group membership, and a procedure to follow when using this system variable, see Section18.5.4, “Network Partitioning”.",0,0,others,mysql,0,0
5013,group_replication_group_name,The value of this system variable cannot be changed while Group Replication is running.,0,0,others,mysql,0,0
5014,group_replication_group_seeds,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
5015,group_replication_gtid_assignment_block_size,"This system variable is a group-wide configuration setting. It must have the same value on all group members, cannot be changed while Group Replication is running, and requires a full reboot of the group (a bootstrap by a server with group_replication_bootstrap_group=ON) in order for the value change to take effect. If the group has a value set for this system variable, and a joining member has a different value set for the system variable, the joining member cannot join the group until the value is changed to match. If the group members have a value set for this system variable, and the joining member does not support the system variable, it cannot join the group.",0,0,others,mysql,0,0
5016,group_replication_ip_allowlist,"group_replication_ip_allowlist is available from MySQL 8.0.22 to replace group_replication_ip_whitelist. From MySQL 8.0.24, the value of this system variable can be changed while Group Replication is running, and the change takes effect immediately on the member.",0,0,others,mysql,0,0
5017,group_replication_ip_whitelist,"At Group Replication startup, if either one of the system variables has been set to a user-defined value and the other has not, the changed value is used. If both of the system variables have been set to a user-defined value, the value of group_replication_ip_allowlist is used.",0,0,others,mysql,0,0
5018,group_replication_local_address,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
5019,group_replication_member_expel_timeout,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately. The current value of the system variable is read whenever Group Replication checks the timeout. It is not mandatory for all members of a group to have the same setting, but it is recommended in order to avoid unexpected expulsions.",1,2,security-tradeoff,mysql,0,0
5020,group_replication_member_weight,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately. The system variable's current value is read when a failover situation occurs.",0,0,others,mysql,0,0
5021,group_replication_message_cache_size,"This system variable should have the same value on all group members. The value of this system variable can be changed while Group Replication is running. The change takes effect on each group member after you stop and restart Group Replication on the member. During this process, the value of the system variable is permitted to differ between group members, but members might be unable to reconnect in the event of a disconnection.",1,1,resource,mysql,0,1
5022,group_replication_poll_spin_loops,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
5023,group_replication_recovery_complete_at,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
5024,group_replication_recovery_compression_algorithms,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",1,4,limited-side-effect,mysql,0,0
5025,group_replication_recovery_get_public_key,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
5026,group_replication_recovery_public_key_path,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
5027,group_replication_recovery_reconnect_interval,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,1
5028,group_replication_recovery_retry_count,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,1
5029,group_replication_recovery_ssl_ca,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
5030,group_replication_recovery_ssl_capath,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
5031,group_replication_recovery_ssl_cert,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
5032,group_replication_recovery_ssl_cipher,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
5033,group_replication_recovery_ssl_crl,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
5034,group_replication_recovery_ssl_crlpath,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
5035,group_replication_recovery_ssl_key,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
5036,group_replication_recovery_ssl_verify_server_cert,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
5037,group_replication_recovery_tls_ciphersuites,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
5038,group_replication_recovery_tls_version,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
5039,group_replication_recovery_use_ssl,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
5040,group_replication_recovery_zstd_compression_level,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
5041,group_replication_single_primary_mode,"This system variable is a group-wide configuration setting. It must have the same value on all group members, cannot be changed while Group Replication is running, and requires a full reboot of the group (a bootstrap by a server with group_replication_bootstrap_group=ON) in order for the value change to take effect. If the group has a value set for this system variable, and a joining member has a different value set for the system variable, the joining member cannot join the group until the value is changed to match. If the group members have a value set for this system variable, and the joining member does not support the system variable, it cannot join the group.",0,0,others,mysql,0,0
5042,group_replication_ssl_mode,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
5043,group_replication_start_on_boot,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,0
5044,group_replication_tls_source,"The value of this system variable can be changed while Group Replication is running, but the change only takes effect after you stop and restart Group Replication on the group member.",0,0,others,mysql,0,1
5045,group_replication_transaction_size_limit,"This system variable should have the same value on all group members. The value of this system variable can be changed while Group Replication is running. The change takes effect immediately on the group member, and applies from the next transaction started on that member. During this process, the value of the system variable is permitted to differ between group members, but some transactions might be rejected.",0,0,others,mysql,0,0
5046,group_replication_unreachable_majority_timeout,"The value of this system variable can be changed while Group Replication is running, and the change takes effect immediately. The current value of the system variable is read when an issue occurs that means the behavior is needed.",0,0,others,mysql,0,0
5047,group_replication_view_change_uuid,"This system variable is a group-wide configuration setting. It must have the same value on all group members, cannot be changed while Group Replication is running, and requires a full reboot of the group (a bootstrap by a server with group_replication_bootstrap_group=ON) in order for the value change to take effect. If the group has a value set for this system variable, and a joining member has a different value set for the system variable, the joining member cannot join the group until the value is changed to match. If the group members have a value set for this system variable, and the joining member does not support the system variable, it cannot join the group.",0,0,others,mysql,0,1
5048,gtid_executed,"When used with global scope, this variable contains a representation of the set of all transactions executed on the server and GTIDs that have been set by a SET gtid_purged statement. This is the same as the value of the Executed_Gtid_Set column in the output of SHOW MASTER STATUS and SHOW REPLICA STATUS. The value of this variable is a GTID set, see GTID Sets for more information.",0,0,others,mysql,0,0
5049,gtid_executed_compression_period,"Compress the mysql.gtid_executed table each time this many transactions have been processed. When binary logging is enabled on the server, this compression method is not used, and instead the mysql.gtid_executed table is compressed on each binary log rotation. When binary logging is disabled on the server, the compression thread sleeps until the specified number of transactions have been executed, then wakes up to perform compression of the mysql.gtid_executed table. Setting the value of this system variable to 0 means that the thread never wakes up, so this explicit compression method is not used. Instead, compression occurs implicitly as required.",0,0,others,mysql,0,0
5050,gtid_mode,"Controls whether GTID based logging is enabled and what type of transactions the logs can contain. You must have privileges sufficient to set global system variables. See Section5.1.9.1, “System Variable Privileges”. enforce_gtid_consistency must be set to ON before you can set gtid_mode=ON. Before modifying this variable, see Section17.1.4, “Changing GTID Mode on Online Servers”.",0,0,others,mysql,0,0
5051,gtid_next,This variable is used to specify whether and how the next GTID is obtained.,0,0,others,mysql,0,0
5052,gtid_owned,This read-only variable is primarily for internal use. Its contents depend on its scope.,0,0,others,mysql,0,0
5053,gtid_purged,"The global value of the gtid_purged system variable (@@GLOBAL.gtid_purged) is a GTID set consisting of the GTIDs of all the transactions that have been committed on the server, but do not exist in any binary log file on the server. gtid_purged is a subset of gtid_executed. The following categories of GTIDs are in gtid_purged:",0,0,others,mysql,0,1
5054,Handler_discover,The MySQL server can ask the NDBCLUSTER storage engine if it knows about a table with a given name. This is called discovery. Handler_discover indicates the number of times that tables have been discovered using this mechanism.,0,0,others,mysql,0,0
5055,help,Display a short help message and exit. Use both the --verbose and --help options to see the full message.,0,0,others,mysql,0,0
5056,immediate_server_version,"For internal use by replication. This session system variable holds the MySQL Server release number of the server that is the immediate source in a replication topology (for example, 80014 for a MySQL 8.0.14 server instance). If this immediate server is at a release that does not support the session system variable, the value of the variable is set to 0 (UNKNOWN_SERVER_VERSION).",0,0,others,mysql,0,0
5057,init_replica,"init_replica is similar to init_connect, but is a string to be executed by a replica server each time the replication SQL thread starts. The format of the string is the same as for the init_connect variable. The setting of this variable takes effect for subsequent START REPLICA statements.",0,0,others,mysql,0,0
5058,init_slave,"init_slave is similar to init_connect, but is a string to be executed by a replica server each time the replication SQL thread starts. The format of the string is the same as for the init_connect variable. The setting of this variable takes effect for subsequent START REPLICA statements.",0,0,others,mysql,0,0
5059,initialize,"This option is used to initialize a MySQL installation by creating the data directory and populating the tables in the mysql system schema. For more information, see Section2.10.1, “Initializing the Data Directory”.",0,0,others,mysql,0,0
5060,initialize-insecure,"This option is used to initialize a MySQL installation by creating the data directory and populating the tables in the mysql system schema. This option implies --initialize. For more information, see the description of that option, and Section2.10.1, “Initializing the Data Directory”.",0,0,others,mysql,0,0
5061,innodb,"Controls loading of the InnoDB storage engine, if the server was compiled with InnoDB support. This option has a tristate format, with possible values of OFF, ON, or FORCE. See Section5.6.1, “Installing and Uninstalling Plugins”.",0,0,others,mysql,0,0
5062,innodb_adaptive_flushing,"Specifies whether to dynamically adjust the rate of flushing dirty pages in the InnoDB buffer pool based on the workload. Adjusting the flush rate dynamically is intended to avoid bursts of I/O activity. This setting is enabled by default. See Section15.8.3.5, “Configuring Buffer Pool Flushing” for more information. For general I/O tuning advice, see Section8.5.8, “Optimizing InnoDB Disk I/O”.",1,2,security-tradeoff,mysql,0,1
5063,innodb_adaptive_flushing_lwm,"Defines the low water mark representing percentage of redo log capacity at which adaptive flushing is enabled. For more information, see Section15.8.3.5, “Configuring Buffer Pool Flushing”.",0,0,others,mysql,0,0
5064,innodb_adaptive_hash_index,"Whether the InnoDB adaptive hash index is enabled or disabled. It may be desirable, depending on your workload, to dynamically enable or disable adaptive hash indexing to improve query performance. Because the adaptive hash index may not be useful for all workloads, conduct benchmarks with it both enabled and disabled, using realistic workloads. See Section15.5.3, “Adaptive Hash Index” for details.",1,5,workload-specific,mysql,0,0
5065,innodb_adaptive_hash_index_parts,"Partitions the adaptive hash index search system. Each index is bound to a specific partition, with each partition protected by a separate latch.",0,0,others,mysql,0,1
5066,innodb_adaptive_max_sleep_delay,"Permits InnoDB to automatically adjust the value of innodb_thread_sleep_delay up or down according to the current workload. Any nonzero value enables automated, dynamic adjustment of the innodb_thread_sleep_delay value, up to the maximum value specified in the innodb_adaptive_max_sleep_delay option. The value represents the number of microseconds. This option can be useful in busy systems, with greater than 16 InnoDB threads. (In practice, it is most valuable for MySQL systems with hundreds or thousands of simultaneous connections.)",0,0,others,mysql,0,0
5067,innodb_api_bk_commit_interval,"How often to auto-commit idle connections that use the InnoDB memcached interface, in seconds. For more information, see Section15.20.6.4, “Controlling Transactional Behavior of the InnoDB memcached Plugin”.",1,3,reliability-tradeoff,mysql,0,0
5068,innodb_api_disable_rowlock,"Use this option to disable row locks when InnoDB memcached performs DML operations. By default, innodb_api_disable_rowlock is disabled, which means that memcached requests row locks for get and set operations. When innodb_api_disable_rowlock is enabled, memcached requests a table lock instead of row locks.",0,0,others,mysql,0,0
5069,innodb_api_enable_binlog,"Lets you use the InnoDB memcached plugin with the MySQL binary log. For more information, see Enabling the InnoDB memcached Binary Log.",0,0,others,mysql,0,0
5070,innodb_api_enable_mdl,"Locks the table used by the InnoDB memcached plugin, so that it cannot be dropped or altered by DDL through the SQL interface. For more information, see Section15.20.6.4, “Controlling Transactional Behavior of the InnoDB memcached Plugin”.",0,0,others,mysql,0,0
5071,innodb_api_trx_level,Controls the transaction isolation level on queries processed by the memcached interface. The constants corresponding to the familiar names are:,0,0,others,mysql,0,0
5072,innodb_autoextend_increment,"The increment size (in megabytes) for extending the size of an auto-extending InnoDB system tablespace file when it becomes full. The default value is 64. For related information, see System Tablespace Data File Configuration, and Resizing the System Tablespace.",0,0,others,mysql,0,0
5073,innodb_autoinc_lock_mode,"The lock mode to use for generating auto-increment values. Permissible values are 0, 1, or 2, for traditional, consecutive, or interleaved, respectively.",0,0,others,mysql,0,0
5074,innodb_background_drop_list_empty,"Enabling the innodb_background_drop_list_empty debug option helps avoid test case failures by delaying table creation until the background drop list is empty. For example, if test case A places table t1 on the background drop list, test case B waits until the background drop list is empty before creating table t1.",0,0,others,mysql,0,0
5075,innodb_buffer_pool_chunk_size,innodb_buffer_pool_chunk_size defines the chunk size for InnoDB buffer pool resizing operations.,1,1,resource,mysql,0,0
5076,innodb_buffer_pool_debug,"Enabling this option permits multiple buffer pool instances when the buffer pool is less than 1GB in size, ignoring the 1GB minimum buffer pool size constraint imposed on innodb_buffer_pool_instances. The innodb_buffer_pool_debug option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.",0,0,others,mysql,0,0
5077,innodb_buffer_pool_dump_at_shutdown,"Specifies whether to record the pages cached in the InnoDB buffer pool when the MySQL server is shut down, to shorten the warmup process at the next restart. Typically used in combination with innodb_buffer_pool_load_at_startup. The innodb_buffer_pool_dump_pct option defines the percentage of most recently used buffer pool pages to dump.",1,6,function-tradeoff,mysql,0,1
5078,innodb_buffer_pool_dump_now,Immediately records the pages cached in the InnoDB buffer pool. Typically used in combination with innodb_buffer_pool_load_now.,0,0,others,mysql,0,1
5079,innodb_buffer_pool_dump_pct,"Specifies the percentage of the most recently used pages for each buffer pool to read out and dump. The range is 1 to 100. The default value is 25. For example, if there are 4 buffer pools with 100 pages each, and innodb_buffer_pool_dump_pct is set to 25, the 25 most recently used pages from each buffer pool are dumped.",0,0,others,mysql,0,0
5080,innodb_buffer_pool_filename,"Specifies the name of the file that holds the list of tablespace IDs and page IDs produced by innodb_buffer_pool_dump_at_shutdown or innodb_buffer_pool_dump_now. Tablespace IDs and page IDs are saved in the following format: space, page_id. By default, the file is named ib_buffer_pool and is located in the InnoDB data directory. A non-default location must be specified relative to the data directory.",0,0,others,mysql,0,1
5081,innodb_buffer_pool_in_core_file,"Disabling the innodb_buffer_pool_in_core_file variable reduces the size of core files by excluding InnoDB buffer pool pages. To use this variable, the core_file variable must be enabled and the operating system must support the MADV_DONTDUMP non-POSIX extension to madvise(), which is supported in Linux 3.4 and later. For more information, see Section15.8.3.7, “Excluding Buffer Pool Pages from Core Files”.",1,4,limited-side-effect,mysql,0,0
5082,innodb_buffer_pool_instances,"The number of regions that the InnoDB buffer pool is divided into. For systems with buffer pools in the multi-gigabyte range, dividing the buffer pool into separate instances can improve concurrency, by reducing contention as different threads read and write to cached pages. Each page that is stored in or read from the buffer pool is assigned to one of the buffer pool instances randomly, using a hashing function. Each buffer pool manages its own free lists, flush lists, LRUs, and all other data structures connected to a buffer pool, and is protected by its own buffer pool mutex.",0,0,others,mysql,0,0
5083,innodb_buffer_pool_load_abort,Interrupts the process of restoring InnoDB buffer pool contents triggered by innodb_buffer_pool_load_at_startup or innodb_buffer_pool_load_now.,0,0,others,mysql,0,0
5084,innodb_buffer_pool_load_at_startup,"Specifies that, on MySQL server startup, the InnoDB buffer pool is automatically warmed up by loading the same pages it held at an earlier time. Typically used in combination with innodb_buffer_pool_dump_at_shutdown.",0,0,others,mysql,0,0
5085,innodb_buffer_pool_load_now,"Immediately warms up the InnoDB buffer pool by loading a set of data pages, without waiting for a server restart. Can be useful to bring cache memory back to a known state during benchmarking, or to ready the MySQL server to resume its normal workload after running queries for reports or maintenance.",1,3,reliability-tradeoff,mysql,0,0
5086,innodb_buffer_pool_size,"The size in bytes of the buffer pool, the memory area where InnoDB caches table and index data. The default value is 134217728 bytes (128MB). The maximum value depends on the CPU architecture; the maximum is 4294967295 (232-1) on 32-bit systems and 18446744073709551615 (264-1) on 64-bit systems. On 32-bit systems, the CPU architecture and operating system may impose a lower practical maximum size than the stated maximum. When the size of the buffer pool is greater than 1GB, setting innodb_buffer_pool_instances to a value greater than 1 can improve the scalability on a busy server.",1,1,resource,mysql,0,0
5087,innodb_change_buffer_max_size,"Maximum size for the InnoDB change buffer, as a percentage of the total size of the buffer pool. You might increase this value for a MySQL server with heavy insert, update, and delete activity, or decrease it for a MySQL server with unchanging data used for reporting. For more information, see Section15.5.2, “Change Buffer”. For general I/O tuning advice, see Section8.5.8, “Optimizing InnoDB Disk I/O”.",1,1,resource,mysql,0,0
5088,innodb_change_buffering,"Whether InnoDB performs change buffering, an optimization that delays write operations to secondary indexes so that the I/O operations can be performed sequentially. Permitted values are described in the following table. Values may also be specified numerically.",1,6,function-tradeoff,mysql,0,0
5089,innodb_change_buffering_debug,Sets a debug flag for InnoDB change buffering. A value of 1 forces all changes to the change buffer. A value of 2 causes an unexpected exit at merge. A default value of 0 indicates that the change buffering debug flag is not set. This option is only available when debugging support is compiled in using the WITH_DEBUG CMake option.,0,0,others,mysql,0,0
5090,innodb_checkpoint_disabled,"This is a debug option that is only intended for expert debugging use. It disables checkpoints so that a deliberate server exit always initiates InnoDB recovery. It should only be enabled for a short interval, typically before running DML operations that write redo log entries that would require recovery following a server exit. This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.",0,0,others,mysql,0,0
5091,innodb_checksum_algorithm,Specifies how to generate and verify the checksum stored in the disk blocks of InnoDB tablespaces. The default value for innodb_checksum_algorithm is crc32.,0,0,others,mysql,0,1
5092,innodb_cmp_per_index_enabled,"Enables per-index compression-related statistics in the INFORMATION_SCHEMA.INNODB_CMP_PER_INDEX table. Because these statistics can be expensive to gather, only enable this option on development, test, or replica instances during performance tuning related to InnoDB compressed tables.",1,4,limited-side-effect,mysql,0,1
5093,innodb_commit_concurrency,The number of threads that can commit at the same time. A value of 0 (the default) permits any number of transactions to commit simultaneously.,1,1,resource,mysql,0,0
5094,innodb_compress_debug,Compresses all tables using a specified compression algorithm without having to define a COMPRESSION attribute for each table. This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.,0,0,others,mysql,0,0
5095,innodb_compression_failure_threshold_pct,"Defines the compression failure rate threshold for a table, as a percentage, at which point MySQL begins adding padding within compressed pages to avoid expensive compression failures. When this threshold is passed, MySQL begins to leave additional free space within each new compressed page, dynamically adjusting the amount of free space up to the percentage of page size specified by innodb_compression_pad_pct_max. A value of zero disables the mechanism that monitors compression efficiency and dynamically adjusts the padding amount.",1,5,workload-specific,mysql,0,1
5096,innodb_compression_level,"Specifies the level of zlib compression to use for InnoDB compressed tables and indexes. A higher value lets you fit more data onto a storage device, at the expense of more CPU overhead during compression. A lower value lets you reduce CPU overhead when storage space is not critical, or you expect the data is not especially compressible.",1,1,resource,mysql,0,1
5097,innodb_compression_pad_pct_max,"Specifies the maximum percentage that can be reserved as free space within each compressed page, allowing room to reorganize the data and modification log within the page when a compressed table or index is updated and the data might be recompressed. Only applies when innodb_compression_failure_threshold_pct is set to a nonzero value, and the rate of compression failures passes the cutoff point.",0,0,others,mysql,0,0
5098,innodb_concurrency_tickets,"Determines the number of threads that can enter InnoDB concurrently. A thread is placed in a queue when it tries to enter InnoDB if the number of threads has already reached the concurrency limit. When a thread is permitted to enter InnoDB, it is given a number of “ tickets” equal to the value of innodb_concurrency_tickets, and the thread can enter and leave InnoDB freely until it has used up its tickets. After that point, the thread again becomes subject to the concurrency check (and possible queuing) the next time it tries to enter InnoDB. The default value is 5000.",1,1,resource,mysql,0,0
5099,innodb_data_file_path,"Defines the name, size, and attributes of InnoDB system tablespace data files. If you do not specify a value for innodb_data_file_path, the default behavior is to create a single auto-extending data file, slightly larger than 12MB, named ibdata1.",0,0,others,mysql,0,0
5100,innodb_data_home_dir,"The common part of the directory path for InnoDB system tablespace data files. The default value is the MySQL data directory. The setting is concatenated with the innodb_data_file_path setting, unless that setting is defined with an absolute path.",0,0,others,mysql,0,0
5101,innodb_ddl_log_crash_reset_debug,Enable this debug option to reset DDL log crash injection counters to 1. This option is only available when debugging support is compiled in using the WITH_DEBUG CMake option.,0,0,others,mysql,0,1
5102,innodb_deadlock_detect,"This option is used to disable deadlock detection. On high concurrency systems, deadlock detection can cause a slowdown when numerous threads wait for the same lock. At times, it may be more efficient to disable deadlock detection and rely on the innodb_lock_wait_timeout setting for transaction rollback when a deadlock occurs.",1,6,function-tradeoff,mysql,0,0
5103,innodb_dedicated_server,"When innodb_dedicated_server is enabled, InnoDB automatically configures the following variables:",0,0,others,mysql,0,1
5104,innodb_default_row_format,"The innodb_default_row_format option defines the default row format for InnoDB tables and user-created temporary tables. The default setting is DYNAMIC. Other permitted values are COMPACT and REDUNDANT. The COMPRESSED row format, which is not supported for use in the system tablespace, cannot be defined as the default.",0,0,others,mysql,0,0
5105,innodb_directories,Defines directories to scan at startup for tablespace files. This option is used when moving or restoring tablespace files to a new location while the server is offline. It is also used to specify directories of tablespace files created using an absolute path or that reside outside of the data directory.,0,0,others,mysql,0,0
5106,innodb_disable_sort_file_cache,Disables the operating system file system cache for merge-sort temporary files. The effect is to open such files with the equivalent of O_DIRECT.,0,0,others,mysql,0,0
5107,innodb_doublewrite,"The innodb_doublewrite variable controls whether the doublwrite buffer is enabled. It is enabled by default in most cases. To disable the doublewrite buffer, set innodb_doublewrite to 0 or start the server with --skip-innodb-doublewrite. You might consider disabling the doublewrite buffer if you are more concerned with performance than data integrity, as may be the case when performing benchmarks, for example.",1,3,reliability-tradeoff,mysql,0,1
5108,innodb_doublewrite_batch_size,Defines the number of doublewrite pages to write in a batch.,1,1,resource,mysql,0,0
5109,innodb_doublewrite_dir,"Defines the directory for doublewrite files. If no directory is specified, doublewrite files are created in the innodb_data_home_dir directory, which defaults to the data directory if unspecified.",0,0,others,mysql,0,0
5110,innodb_doublewrite_files,"Defines the number of doublewrite files. By default, two doublewrite files are created for each buffer pool instance.",1,1,resource,mysql,0,1
5111,innodb_doublewrite_pages,"Defines the maximum number of doublewrite pages per thread for a batch write. If no value is specified, innodb_doublewrite_pages is set to the innodb_write_io_threads value.",1,1,resource,mysql,0,0
5112,innodb_extend_and_initialize,Controls how space is allocated to file-per-table and general tablespaces on Linux systems.,1,4,limited-side-effect,mysql,0,0
5113,innodb_fast_shutdown,"The InnoDB shutdown mode. If the value is 0, InnoDB does a slow shutdown, a full purge and a change buffer merge before shutting down. If the value is 1 (the default), InnoDB skips these operations at shutdown, a process known as a fast shutdown. If the value is 2, InnoDB flushes its logs and shuts down cold, as if MySQL had crashed; no committed transactions are lost, but the crash recovery operation makes the next startup take longer.",0,0,others,mysql,0,0
5114,innodb_fil_make_page_dirty_debug,"By default, setting innodb_fil_make_page_dirty_debug to the ID of a tablespace immediately dirties the first page of the tablespace. If innodb_saved_page_number_debug is set to a non-default value, setting innodb_fil_make_page_dirty_debug dirties the specified page. The innodb_fil_make_page_dirty_debug option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.",0,0,others,mysql,0,0
5115,innodb_file_per_table,"When innodb_file_per_table is enabled, tables are created in file-per-table tablespaces by default. When disabled, tables are created in the system tablespace by default. For information about file-per-table tablespaces, see Section15.6.3.2, “File-Per-Table Tablespaces”. For information about the InnoDB system tablespace, see Section15.6.3.1, “The System Tablespace”.",1,5,workload-specific,mysql,0,0
5116,innodb_fill_factor,InnoDB performs a bulk load when creating or rebuilding indexes. This method of index creation is known as a “sorted index build”.,0,0,others,mysql,0,0
5117,innodb_flush_log_at_timeout,Write and flush the logs every N seconds. innodb_flush_log_at_timeout allows the timeout period between flushes to be increased in order to reduce flushing and avoid impacting performance of binary log group commit. The default setting for innodb_flush_log_at_timeout is once per second.,0,0,others,mysql,0,0
5118,innodb_flush_log_at_trx_commit,Controls the balance between strict ACID compliance for commit operations and higher performance that is possible when commit-related I/O operations are rearranged and done in batches. You can achieve better performance by changing the default value but then you can lose transactions in a crash.,1,3,reliability-tradeoff,mysql,0,1
5119,innodb_flush_method,"Defines the method used to flush data to InnoDB data files and log files, which can affect I/O throughput.",1,5,workload-specific,mysql,0,1
5120,innodb_flush_neighbors,Specifies whether flushing a page from the InnoDB buffer pool also flushes other dirty pages in the same extent.,1,4,limited-side-effect,mysql,0,1
5121,innodb_flush_sync,"The innodb_flush_sync variable, which is enabled by default, causes the innodb_io_capacity setting to be ignored during bursts of I/O activity that occur at checkpoints. To adhere to the I/O rate defined by the innodb_io_capacity setting, disable innodb_flush_sync.",1,6,function-tradeoff,mysql,0,1
5122,innodb_flushing_avg_loops,"Number of iterations for which InnoDB keeps the previously calculated snapshot of the flushing state, controlling how quickly adaptive flushing responds to changing workloads. Increasing the value makes the rate of flush operations change smoothly and gradually as the workload changes. Decreasing the value makes adaptive flushing adjust quickly to workload changes, which can cause spikes in flushing activity if the workload increases and decreases suddenly.",1,5,workload-specific,mysql,0,0
5123,innodb_force_load_corrupted,"Permits InnoDB to load tables at startup that are marked as corrupted. Use only during troubleshooting, to recover data that is otherwise inaccessible. When troubleshooting is complete, disable this setting and restart the server.",0,0,others,mysql,0,0
5124,innodb_force_recovery,"The crash recovery mode, typically only changed in serious troubleshooting situations. Possible values are from 0 to 6. For the meanings of these values and important information about innodb_force_recovery, see Section15.21.3, “Forcing InnoDB Recovery”.",0,0,others,mysql,0,0
5125,innodb_fsync_threshold,"By default, when InnoDB creates a new data file, such as a new log file or tablespace file, the file is fully written to the operating system cache before it is flushed to disk, which can cause a large amount of disk write activity to occur at once. To force smaller, periodic flushes of data from the operating system cache, you can use the innodb_fsync_threshold variable to define a threshold value, in bytes. When the byte threshold is reached, the contents of the operating system cache are flushed to disk. The default value of 0 forces the default behavior, which is to flush data to disk only after a file is fully written to the cache.",1,5,workload-specific,mysql,0,0
5126,innodb_ft_aux_table,Specifies the qualified name of an InnoDB table containing a FULLTEXT index. This variable is intended for diagnostic purposes and can only be set at runtime. For example:,0,0,others,mysql,0,1
5127,innodb_ft_cache_size,"The memory allocated, in bytes, for the InnoDB FULLTEXT search index cache, which holds a parsed document in memory while creating an InnoDB FULLTEXT index. Index inserts and updates are only committed to disk when the innodb_ft_cache_size size limit is reached. innodb_ft_cache_size defines the cache size on a per table basis. To set a global limit for all tables, see innodb_ft_total_cache_size.",1,1,resource,mysql,0,0
5128,innodb_ft_enable_diag_print,Whether to enable additional full-text search (FTS) diagnostic output. This option is primarily intended for advanced FTS debugging and is not of interest to most users. Output is printed to the error log and includes information such as:,0,0,others,mysql,0,0
5129,innodb_ft_enable_stopword,"Specifies that a set of stopwords is associated with an InnoDB FULLTEXT index at the time the index is created. If the innodb_ft_user_stopword_table option is set, the stopwords are taken from that table. Else, if the innodb_ft_server_stopword_table option is set, the stopwords are taken from that table. Otherwise, a built-in set of default stopwords is used.",0,0,others,mysql,0,0
5130,innodb_ft_max_token_size,"Maximum character length of words that are stored in an InnoDB FULLTEXT index. Setting a limit on this value reduces the size of the index, thus speeding up queries, by omitting long keywords or arbitrary collections of letters that are not real words and are not likely to be search terms.",1,1,resource,mysql,0,0
5131,innodb_ft_min_token_size,"Minimum length of words that are stored in an InnoDB FULLTEXT index. Increasing this value reduces the size of the index, thus speeding up queries, by omitting common words that are unlikely to be significant in a search context, such as the English words “a” and “to”. For content using a CJK (Chinese, Japanese, Korean) character set, specify a value of 1.",1,1,resource,mysql,0,0
5132,innodb_ft_num_word_optimize,"Number of words to process during each OPTIMIZE TABLE operation on an InnoDB FULLTEXT index. Because a bulk insert or update operation to a table containing a full-text search index could require substantial index maintenance to incorporate all changes, you might do a series of OPTIMIZE TABLE statements, each picking up where the last left off.",1,5,workload-specific,mysql,0,0
5133,innodb_ft_result_cache_limit,"The InnoDB full-text search query result cache limit (defined in bytes) per full-text search query or per thread. Intermediate and final InnoDB full-text search query results are handled in memory. Use innodb_ft_result_cache_limit to place a size limit on the full-text search query result cache to avoid excessive memory consumption in case of very large InnoDB full-text search query results (millions or hundreds of millions of rows, for example). Memory is allocated as required when a full-text search query is processed. If the result cache size limit is reached, an error is returned indicating that the query exceeds the maximum allowed memory.",1,5,workload-specific,mysql,0,0
5134,innodb_ft_server_stopword_table,"This option is used to specify your own InnoDB FULLTEXT index stopword list for all InnoDB tables. To configure your own stopword list for a specific InnoDB table, use innodb_ft_user_stopword_table.",0,0,others,mysql,0,0
5135,innodb_ft_sort_pll_degree,Number of threads used in parallel to index and tokenize text in an InnoDB FULLTEXT index when building a search index.,0,0,others,mysql,0,0
5136,innodb_ft_total_cache_size,"The total memory allocated, in bytes, for the InnoDB full-text search index cache for all tables. Creating numerous tables, each with a FULLTEXT search index, could consume a significant portion of available memory. innodb_ft_total_cache_size defines a global memory limit for all full-text search indexes to help avoid excessive memory consumption. If the global limit is reached by an index operation, a forced sync is triggered.",1,1,resource,mysql,0,0
5137,innodb_ft_user_stopword_table,"This option is used to specify your own InnoDB FULLTEXT index stopword list on a specific table. To configure your own stopword list for all InnoDB tables, use innodb_ft_server_stopword_table.",0,0,others,mysql,0,0
5138,innodb_idle_flush_pct,"Limits page flushing when InnoDB is idle. The innodb_idle_flush_pct value is a percentage of the innodb_io_capacity setting, which defines the number of I/O operations per second available to InnoDB. For more information, see Limiting Buffer Flushing During Idle Periods.",0,0,others,mysql,0,0
5139,innodb_io_capacity,"The innodb_io_capacity variable defines the number of I/O operations per second (IOPS) available to InnoDB background tasks, such as flushing pages from the buffer pool and merging data from the change buffer.",1,1,resource,mysql,0,0
5140,innodb_io_capacity_max,"If flushing activity falls behind, InnoDB can flush more aggressively, at a higher rate of I/O operations per second (IOPS) than defined by the innodb_io_capacity variable. The innodb_io_capacity_max variable defines a maximum number of IOPS performed by InnoDB background tasks in such situations.",1,3,reliability-tradeoff,mysql,0,0
5141,innodb_limit_optimistic_insert_debug,Limits the number of records per B-tree page. A default value of 0 means that no limit is imposed. This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.,1,1,resource,mysql,0,0
5142,innodb_lock_wait_timeout,The length of time in seconds an InnoDB transaction waits for a row lock before giving up. The default value is 50 seconds. A transaction that tries to access a row that is locked by another InnoDB transaction waits at most this many seconds for write access to the row before issuing the following error:,0,0,others,mysql,0,0
5143,innodb_log_buffer_size,"The size in bytes of the buffer that InnoDB uses to write to the log files on disk. The default is 16MB. A large log buffer enables large transactions to run without the need to write the log to disk before the transactions commit. Thus, if you have transactions that update, insert, or delete many rows, making the log buffer larger saves disk I/O. For related information, see Memory Configuration, and Section8.5.4, “Optimizing InnoDB Redo Logging”. For general I/O tuning advice, see Section8.5.8, “Optimizing InnoDB Disk I/O”.",1,1,resource,mysql,0,0
5144,innodb_log_checkpoint_fuzzy_now,Enable this debug option to force InnoDB to write a fuzzy checkpoint. This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.,1,6,function-tradeoff,mysql,0,0
5145,innodb_log_checkpoint_now,Enable this debug option to force InnoDB to write a checkpoint. This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.,1,6,function-tradeoff,mysql,0,0
5146,innodb_log_checksums,Enables or disables checksums for redo log pages.,1,6,function-tradeoff,mysql,0,0
5147,innodb_log_compressed_pages,Specifies whether images of re-compressed pages are written to the redo log. Re-compression may occur when changes are made to compressed data.,1,5,workload-specific,mysql,0,0
5148,innodb_log_file_size,"The size in bytes of each log file in a log group. The combined size of log files (innodb_log_file_size * innodb_log_files_in_group) cannot exceed a maximum value that is slightly less than 512GB. A pair of 255 GB log files, for example, approaches the limit but does not exceed it. The default value is 48MB.",1,1,resource,mysql,0,1
5149,innodb_log_files_in_group,The number of log files in the log group. InnoDB writes to the files in a circular fashion. The default (and recommended) value is 2. The location of the files is specified by innodb_log_group_home_dir. The combined size of log files (innodb_log_file_size * innodb_log_files_in_group) can be up to 512GB.,0,0,others,mysql,0,0
5150,innodb_log_group_home_dir,"The directory path to the InnoDB redo log files, whose number is specified by innodb_log_files_in_group. If you do not specify any InnoDB log variables, the default is to create two files named ib_logfile0 and ib_logfile1 in the MySQL data directory. Log file size is given by the innodb_log_file_size system variable.",0,0,others,mysql,0,0
5151,innodb_log_spin_cpu_abs_lwm,"Defines the minimum amount of CPU usage below which user threads no longer spin while waiting for flushed redo. The value is expressed as a sum of CPU core usage. For example, The default value of 80 is 80% of a single CPU core. On a system with a multi-core processor, a value of 150 represents 100% usage of one CPU core plus 50% usage of a second CPU core.",1,1,resource,mysql,0,0
5152,innodb_log_spin_cpu_pct_hwm,"Defines the maximum amount of CPU usage above which user threads no longer spin while waiting for flushed redo. The value is expressed as a percentage of the combined total processing power of all CPU cores. The default value is 50%. For example, 100% usage of two CPU cores is 50% of the combined CPU processing power on a server with four CPU cores.",1,1,resource,mysql,0,0
5153,innodb_log_wait_for_flush_spin_hwm,Defines the maximum average log flush time beyond which user threads no longer spin while waiting for flushed redo. The default value is 400 microseconds.,0,0,others,mysql,0,0
5154,innodb_log_write_ahead_size,"Defines the write-ahead block size for the redo log, in bytes. To avoid “read-on-write”, set innodb_log_write_ahead_size to match the operating system or file system cache block size. The default setting is 8192 bytes. Read-on-write occurs when redo log blocks are not entirely cached to the operating system or file system due to a mismatch between write-ahead block size for the redo log and operating system or file system cache block size.",1,1,resource,mysql,0,1
5155,innodb_log_writer_threads,"Enables dedicated log writer threads for writing redo log records from the log buffer to the system buffers and flushing the system buffers to the redo log files. Dedicated log writer threads can improve performance on high-concurrency systems, but for low-concurrency systems, disabling dedicated log writer threads provides better performance.",0,0,others,mysql,0,0
5156,innodb_lru_scan_depth,"A parameter that influences the algorithms and heuristics for the flush operation for the InnoDB buffer pool. Primarily of interest to performance experts tuning I/O-intensive workloads. It specifies, per buffer pool instance, how far down the buffer pool LRU page list the page cleaner thread scans looking for dirty pages to flush. This is a background operation performed once per second.",1,5,workload-specific,mysql,0,1
5157,innodb_max_dirty_pages_pct,InnoDB tries to flush data from the buffer pool so that the percentage of dirty pages does not exceed this value.,0,0,others,mysql,0,1
5158,innodb_max_dirty_pages_pct_lwm,"Defines a low water mark representing the percentage of dirty pages at which preflushing is enabled to control the dirty page ratio. A value of 0 disables the pre-flushing behavior entirely. The configured value should always be lower than the innodb_max_dirty_pages_pct value. For more information, see Section15.8.3.5, “Configuring Buffer Pool Flushing”.",1,4,limited-side-effect,mysql,0,0
5159,innodb_max_purge_lag,"Defines the desired maximum purge lag. If this value is exceeded, a delay is imposed on INSERT, UPDATE, and DELETE operations to allow time for purge to catch up. The default value is 0, which means there is no maximum purge lag and no delay.",1,3,reliability-tradeoff,mysql,0,0
5160,innodb_max_purge_lag_delay,Specifies the maximum delay in microseconds for the delay imposed when the innodb_max_purge_lag threshold is exceeded. The specified innodb_max_purge_lag_delay value is an upper limit on the delay period calculated by the innodb_max_purge_lag formula.,0,0,others,mysql,0,0
5161,innodb_max_undo_log_size,"Defines a threshold size for undo tablespaces. If an undo tablespace exceeds the threshold, it can be marked for truncation when innodb_undo_log_truncate is enabled. The default value is 1073741824 bytes (1024 MiB).",1,5,workload-specific,mysql,0,0
5162,innodb_merge_threshold_set_all_debug,"Defines a page-full percentage value for index pages that overrides the current MERGE_THRESHOLD setting for all indexes that are currently in the dictionary cache. This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option. For related information, see Section15.8.11, “Configuring the Merge Threshold for Index Pages”.",0,0,others,mysql,0,0
5163,innodb_monitor_disable,"Disables InnoDB metrics counters. Counter data may be queried using the INFORMATION_SCHEMA.INNODB_METRICS table. For usage information, see Section15.15.6, “InnoDB INFORMATION_SCHEMA Metrics Table”.",0,0,others,mysql,0,0
5164,innodb_monitor_enable,"Enables InnoDB metrics counters. Counter data may be queried using the INFORMATION_SCHEMA.INNODB_METRICS table. For usage information, see Section15.15.6, “InnoDB INFORMATION_SCHEMA Metrics Table”.",0,0,others,mysql,0,1
5165,innodb_monitor_reset,"Resets the count value for InnoDB metrics counters to zero. Counter data may be queried using the INFORMATION_SCHEMA.INNODB_METRICS table. For usage information, see Section15.15.6, “InnoDB INFORMATION_SCHEMA Metrics Table”.",0,0,others,mysql,0,0
5166,innodb_monitor_reset_all,"Resets all values (minimum, maximum, and so on) for InnoDB metrics counters. Counter data may be queried using the INFORMATION_SCHEMA.INNODB_METRICS table. For usage information, see Section15.15.6, “InnoDB INFORMATION_SCHEMA Metrics Table”.",0,0,others,mysql,0,0
5167,innodb_numa_interleave,"Enables the NUMA interleave memory policy for allocation of the InnoDB buffer pool. When innodb_numa_interleave is enabled, the NUMA memory policy is set to MPOL_INTERLEAVE for the mysqld process. After the InnoDB buffer pool is allocated, the NUMA memory policy is set back to MPOL_DEFAULT. For the innodb_numa_interleave option to be available, MySQL must be compiled on a NUMA-enabled Linux system.",0,0,others,mysql,0,0
5168,innodb_old_blocks_pct,"Specifies the approximate percentage of the InnoDB buffer pool used for the old block sublist. The range of values is 5 to 95. The default value is 37 (that is, 3/8 of the pool). Often used in combination with innodb_old_blocks_time.",0,0,others,mysql,0,0
5169,innodb_old_blocks_time,"Non-zero values protect against the buffer pool being filled by data that is referenced only for a brief period, such as during a full table scan. Increasing this value offers more protection against full table scans interfering with data cached in the buffer pool.",1,5,workload-specific,mysql,0,0
5170,innodb_online_alter_log_max_size,"Specifies an upper limit in bytes on the size of the temporary log files used during online DDL operations for InnoDB tables. There is one such log file for each index being created or table being altered. This log file stores data inserted, updated, or deleted in the table during the DDL operation. The temporary log file is extended when needed by the value of innodb_sort_buffer_size, up to the maximum specified by innodb_online_alter_log_max_size. If a temporary log file exceeds the upper size limit, the ALTER TABLE operation fails and all uncommitted concurrent DML operations are rolled back. Thus, a large value for this option allows more DML to happen during an online DDL operation, but also extends the period of time at the end of the DDL operation when the table is locked to apply the data from the log.",0,0,others,mysql,0,0
5171,innodb_open_files,"This variable is only relevant if you have numerous InnoDB tablespaces. It specifies the maximum number of .ibd files that MySQL can keep open at one time. The minimum value is 10. The default value is 300 if innodb_file_per_table is not enabled, and the higher of 300 and table_open_cache otherwise.",0,0,others,mysql,0,0
5172,innodb_optimize_fulltext_only,"Changes the way OPTIMIZE TABLE operates on InnoDB tables. Intended to be enabled temporarily, during maintenance operations for InnoDB tables with FULLTEXT indexes.",0,0,others,mysql,0,0
5173,innodb_page_cleaners,"The number of page cleaner threads that flush dirty pages from buffer pool instances. Page cleaner threads perform flush list and LRU flushing. When there are multiple page cleaner threads, buffer pool flushing tasks for each buffer pool instance are dispatched to idle page cleaner threads. The innodb_page_cleaners default value is 4. If the number of page cleaner threads exceeds the number of buffer pool instances, innodb_page_cleaners is automatically set to the same value as innodb_buffer_pool_instances.",1,1,resource,mysql,0,0
5174,innodb_page_size,"Specifies the page size for InnoDB tablespaces. Values can be specified in bytes or kilobytes. For example, a 16 kilobyte page size value can be specified as 16384, 16KB, or 16k.",0,0,others,mysql,0,0
5175,innodb_parallel_read_threads,"Defines the number of threads that can be used for parallel clustered index reads. Parallel scanning of partitions is supported as of MySQL 8.0.17. Parallel read threads can improve CHECK TABLE performance. InnoDB reads the clustered index twice during a CHECK TABLE operation. The second read can be performed in parallel. This feature does not apply to secondary index scans. The innodb_parallel_read_threads session variable must be set to a value greater than 1 for parallel clustered index reads to occur. The actual number of threads used to perform a parallel clustered index read is determined by the innodb_parallel_read_threads setting or the number of index subtrees to scan, whichever is smaller. The pages read into the buffer pool during the scan are kept at the tail of the buffer pool LRU list so that they can be discarded quickly when free buffer pool pages are required.",1,1,resource,mysql,0,0
5176,innodb_print_all_deadlocks,"When this option is enabled, information about all deadlocks in InnoDB user transactions is recorded in the mysqld error log. Otherwise, you see information about only the last deadlock, using the SHOW ENGINE INNODB STATUS command. An occasional InnoDB deadlock is not necessarily an issue, because InnoDB detects the condition immediately and rolls back one of the transactions automatically. You might use this option to troubleshoot why deadlocks are occurring if an application does not have appropriate error-handling logic to detect the rollback and retry its operation. A large number of deadlocks might indicate the need to restructure transactions that issue DML or SELECT ... FOR UPDATE statements for multiple tables, so that each transaction accesses the tables in the same order, thus avoiding the deadlock condition.",0,0,others,mysql,0,0
5177,innodb_print_ddl_logs,"Enabling this option causes MySQL to write DDL logs to stderr. For more information, see Viewing DDL Logs.",0,0,others,mysql,0,0
5178,innodb_purge_batch_size,"Defines the number of undo log pages that purge parses and processes in one batch from the history list. In a multithreaded purge configuration, the coordinator purge thread divides innodb_purge_batch_size by innodb_purge_threads and assigns that number of pages to each purge thread. The innodb_purge_batch_size variable also defines the number of undo log pages that purge frees after every 128 iterations through the undo logs.",1,5,workload-specific,mysql,0,0
5179,innodb_purge_rseg_truncate_frequency,"Defines the frequency with which the purge system frees rollback segments in terms of the number of times that purge is invoked. An undo tablespace cannot be truncated until its rollback segments are freed. Normally, the purge system frees rollback segments once every 128 times that purge is invoked. The default value is 128. Reducing this value increases the frequency with which the purge thread frees rollback segments.",1,5,workload-specific,mysql,0,0
5180,innodb_purge_threads,"The number of background threads devoted to the InnoDB purge operation. Increasing the value creates additional purge threads, which can improve efficiency on systems where DML operations are performed on multiple tables.",1,1,resource,mysql,0,0
5181,innodb_random_read_ahead,Enables the random read-ahead technique for optimizing InnoDB I/O.,1,4,limited-side-effect,mysql,0,1
5182,innodb_read_ahead_threshold,"Controls the sensitivity of linear read-ahead that InnoDB uses to prefetch pages into the buffer pool. If InnoDB reads at least innodb_read_ahead_threshold pages sequentially from an extent (64 pages), it initiates an asynchronous read for the entire following extent. The permissible range of values is 0 to 64. A value of 0 disables read-ahead. For the default of 56, InnoDB must read at least 56 pages sequentially from an extent to initiate an asynchronous read for the following extent.",0,0,others,mysql,0,0
5183,innodb_read_io_threads,"The number of I/O threads for read operations in InnoDB. Its counterpart for write threads is innodb_write_io_threads. For more information, see Section15.8.5, “Configuring the Number of Background InnoDB I/O Threads”. For general I/O tuning advice, see Section8.5.8, “Optimizing InnoDB Disk I/O”.",1,1,resource,mysql,0,0
5184,innodb_read_only,"Starts InnoDB in read-only mode. For distributing database applications or data sets on read-only media. Can also be used in data warehouses to share the same data directory between multiple instances. For more information, see Section15.8.2, “Configuring InnoDB for Read-Only Operation”.",0,0,others,mysql,0,0
5185,innodb_redo_log_archive_dirs,Defines labeled directories where redo log archive files can be created. You can define multiple labeled directories in a semicolon-separated list. For example:,0,0,others,mysql,0,0
5186,innodb_redo_log_encrypt,"Controls encryption of redo log data for tables encrypted using the InnoDB data-at-rest encryption feature. Encryption of redo log data is disabled by default. For more information, see Redo Log Encryption.",1,2,security-tradeoff,mysql,0,0
5187,innodb_replication_delay,The replication thread delay in milliseconds on a replica server if innodb_thread_concurrency is reached.,0,0,others,mysql,0,1
5188,innodb_rollback_on_timeout,"InnoDB rolls back only the last statement on a transaction timeout by default. If --innodb-rollback-on-timeout is specified, a transaction timeout causes InnoDB to abort and roll back the entire transaction.",0,0,others,mysql,0,1
5189,innodb_rollback_segments,"innodb_rollback_segments defines the number of rollback segments allocated to each undo tablespace and the global temporary tablespace for transactions that generate undo records. The number of transactions that each rollback segment supports depends on the InnoDB page size and the number of undo logs assigned to each transaction. For more information, see Section15.6.6, “Undo Logs”.",0,0,others,mysql,0,0
5190,innodb_saved_page_number_debug,Saves a page number. Setting the innodb_fil_make_page_dirty_debug option dirties the page defined by innodb_saved_page_number_debug. The innodb_saved_page_number_debug option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.,0,0,others,mysql,0,0
5191,innodb_segment_reserve_factor,"Defines the percentage of tablespace file segment pages reserved as empty pages. The setting is applicable to file-per-table and general tablespaces. The innodb_segment_reserve_factor default setting is 12.5 percent, which is the same percentage of pages reserved in previous MySQL releases.",0,0,others,mysql,0,0
5192,innodb_sort_buffer_size,"Specifies the size of sort buffers used to sort data during creation of an InnoDB index. The specified size defines the amount of data that is read into memory for internal sorting and then written out to disk. This process is referred to as a “run”. During the merge phase, pairs of buffers of the specified size are read and merged. The larger the setting, the fewer runs and merges there are.",1,1,resource,mysql,0,0
5193,innodb_spin_wait_delay,"The maximum delay between polls for a spin lock. The low-level implementation of this mechanism varies depending on the combination of hardware and operating system, so the delay does not correspond to a fixed time interval.",0,0,others,mysql,0,0
5194,innodb_spin_wait_pause_multiplier,Defines a multiplier value used to determine the number of PAUSE instructions in spin-wait loops that occur when a thread waits to acquire a mutex or rw-lock.,0,0,others,mysql,0,0
5195,innodb_stats_auto_recalc,Causes InnoDB to automatically recalculate persistent statistics after the data in a table is changed substantially. The threshold value is 10% of the rows in the table. This setting applies to tables created when the innodb_stats_persistent option is enabled. Automatic statistics recalculation may also be configured by specifying STATS_PERSISTENT=1 in a CREATE TABLE or ALTER TABLE statement. The amount of data sampled to produce the statistics is controlled by the innodb_stats_persistent_sample_pages variable.,1,3,reliability-tradeoff,mysql,0,0
5196,innodb_stats_include_delete_marked,"By default, InnoDB reads uncommitted data when calculating statistics. In the case of an uncommitted transaction that deletes rows from a table, InnoDB excludes records that are delete-marked when calculating row estimates and index statistics, which can lead to non-optimal execution plans for other transactions that are operating on the table concurrently using a transaction isolation level other than READ UNCOMMITTED. To avoid this scenario, innodb_stats_include_delete_marked can be enabled to ensure that InnoDB includes delete-marked records when calculating persistent optimizer statistics.",1,3,reliability-tradeoff,mysql,0,0
5197,innodb_stats_method,"How the server treats NULL values when collecting statistics about the distribution of index values for InnoDB tables. Permitted values are nulls_equal, nulls_unequal, and nulls_ignored. For nulls_equal, all NULL index values are considered equal and form a single value group with a size equal to the number of NULL values. For nulls_unequal, NULL values are considered unequal, and each NULL forms a distinct value group of size 1. For nulls_ignored, NULL values are ignored.",1,5,workload-specific,mysql,0,0
5198,innodb_stats_on_metadata,"This option only applies when optimizer statistics are configured to be non-persistent. Optimizer statistics are not persisted to disk when innodb_stats_persistent is disabled or when individual tables are created or altered with STATS_PERSISTENT=0. For more information, see Section15.8.10.2, “Configuring Non-Persistent Optimizer Statistics Parameters”.",0,0,others,mysql,0,0
5199,innodb_stats_persistent,"Specifies whether InnoDB index statistics are persisted to disk. Otherwise, statistics may be recalculated frequently which can lead to variations in query execution plans. This setting is stored with each table when the table is created. You can set innodb_stats_persistent at the global level before creating a table, or use the STATS_PERSISTENT clause of the CREATE TABLE and ALTER TABLE statements to override the system-wide setting and configure persistent statistics for individual tables.",0,0,others,mysql,0,0
5200,innodb_stats_persistent_sample_pages,"The number of index pages to sample when estimating cardinality and other statistics for an indexed column, such as those calculated by ANALYZE TABLE. Increasing the value improves the accuracy of index statistics, which can improve the query execution plan, at the expense of increased I/O during the execution of ANALYZE TABLE for an InnoDB table. For more information, see Section15.8.10.1, “Configuring Persistent Optimizer Statistics Parameters”.",1,5,workload-specific,mysql,0,0
5201,innodb_stats_transient_sample_pages,"The number of index pages to sample when estimating cardinality and other statistics for an indexed column, such as those calculated by ANALYZE TABLE. The default value is 8. Increasing the value improves the accuracy of index statistics, which can improve the query execution plan, at the expense of increased I/O when opening an InnoDB table or recalculating statistics. For more information, see Section15.8.10.2, “Configuring Non-Persistent Optimizer Statistics Parameters”.",1,5,workload-specific,mysql,0,0
5202,innodb_status_output,"Enables or disables periodic output for the standard InnoDB Monitor. Also used in combination with innodb_status_output_locks to enable or disable periodic output for the InnoDB Lock Monitor. For more information, see Section15.17.2, “Enabling InnoDB Monitors”.",0,0,others,mysql,0,0
5203,innodb_status_output_locks,"Enables or disables the InnoDB Lock Monitor. When enabled, the InnoDB Lock Monitor prints additional information about locks in SHOW ENGINE INNODB STATUS output and in periodic output printed to the MySQL error log. Periodic output for the InnoDB Lock Monitor is printed as part of the standard InnoDB Monitor output. The standard InnoDB Monitor must therefore be enabled for the InnoDB Lock Monitor to print data to the MySQL error log periodically. For more information, see Section15.17.2, “Enabling InnoDB Monitors”.",1,6,function-tradeoff,mysql,0,0
5204,innodb_strict_mode,"When innodb_strict_mode is enabled, InnoDB returns errors rather than warnings for certain conditions.",0,0,others,mysql,0,0
5205,innodb_sync_array_size,"Defines the size of the mutex/lock wait array. Increasing the value splits the internal data structure used to coordinate threads, for higher concurrency in workloads with large numbers of waiting threads. This setting must be configured when the MySQL instance is starting up, and cannot be changed afterward. Increasing the value is recommended for workloads that frequently produce a large number of waiting threads, typically greater than 768.",1,5,workload-specific,mysql,0,0
5206,innodb_sync_debug,Enables sync debug checking for the InnoDB storage engine. This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.,1,4,limited-side-effect,mysql,0,1
5207,innodb_sync_spin_loops,The number of times a thread waits for an InnoDB mutex to be freed before the thread is suspended.,0,0,others,mysql,0,1
5208,innodb_table_locks,"If autocommit = 0, InnoDB honors LOCK TABLES; MySQL does not return from LOCK TABLES ... WRITE until all other threads have released all their locks to the table. The default value of innodb_table_locks is 1, which means that LOCK TABLES causes InnoDB to lock a table internally if autocommit = 0.",0,0,others,mysql,0,0
5209,innodb_temp_data_file_path,"Defines the relative path, name, size, and attributes of global temporary tablespace data files. The global temporary tablespace stores rollback segments for changes made to user-created temporary tables.",0,0,others,mysql,0,0
5210,innodb_temp_tablespaces_dir,Defines the location where InnoDB creates a pool of session temporary tablespaces at startup. The default location is the #innodb_temp directory in the data directory. A fully qualified path or path relative to the data directory is permitted.,0,0,others,mysql,0,0
5211,innodb_thread_concurrency,Defines the maximum number of threads permitted inside of InnoDB. A value of 0 (the default) is interpreted as infinite concurrency (no limit). This variable is intended for performance tuning on high concurrency systems.,1,1,resource,mysql,0,0
5212,innodb_thread_sleep_delay,"How long InnoDB threads sleep before joining the InnoDB queue, in microseconds. The default value is 10000. A value of 0 disables sleep. You can set innodb_adaptive_max_sleep_delay to the highest value you would allow for innodb_thread_sleep_delay, and InnoDB automatically adjusts innodb_thread_sleep_delay up or down depending on current thread-scheduling activity. This dynamic adjustment helps the thread scheduling mechanism to work smoothly during times when the system is lightly loaded or when it is operating near full capacity.",0,0,others,mysql,0,0
5213,innodb_tmpdir,Used to define an alternate directory for temporary sort files created during online ALTER TABLE operations that rebuild the table.,0,0,others,mysql,0,0
5214,innodb_trx_purge_view_update_only_debug,Pauses purging of delete-marked records while allowing the purge view to be updated. This option artificially creates a situation in which the purge view is updated but purges have not yet been performed. This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.,1,2,security-tradeoff,mysql,0,0
5215,innodb_trx_rseg_n_slots_debug,Sets a debug flag that limits TRX_RSEG_N_SLOTS to a given value for the trx_rsegf_undo_find_free function that looks for free slots for undo log segments. This option is only available if debugging support is compiled in using the WITH_DEBUG CMake option.,0,0,others,mysql,0,1
5216,innodb_undo_directory,The path where InnoDB creates undo tablespaces. Typically used to place undo tablespaces on a different storage device.,0,0,others,mysql,0,0
5217,innodb_undo_log_encrypt,"Controls encryption of undo log data for tables encrypted using the InnoDB data-at-rest encryption feature. Only applies to undo logs that reside in separate undo tablespaces. See Section15.6.3.4, “Undo Tablespaces”. Encryption is not supported for undo log data that resides in the system tablespace. For more information, see Undo Log Encryption.",1,2,security-tradeoff,mysql,0,0
5218,innodb_undo_log_truncate,"When enabled, undo tablespaces that exceed the threshold value defined by innodb_max_undo_log_size are marked for truncation. Only undo tablespaces can be truncated. Truncating undo logs that reside in the system tablespace is not supported. For truncation to occur, there must be at least two undo tablespaces.",0,0,others,mysql,0,0
5219,innodb_undo_tablespaces,Defines the number of undo tablespaces used by InnoDB. The default and minimum value is 2.,1,1,resource,mysql,0,0
5220,innodb_use_fdatasync,"On platforms that support fdatasync() system calls, enabling the innodb_use_fdatasync variable permits using fdatasync() instead of fsync() system calls for operating system flushes. An fdatasync() call does not flush changes to file metadata unless required for subsequent data retrieval, providing a potential performance benefit.",0,0,others,mysql,0,0
5221,innodb_use_native_aio,"Specifies whether to use the Linux asynchronous I/O subsystem. This variable applies to Linux systems only, and cannot be changed while the server is running. Normally, you do not need to configure this option, because it is enabled by default.",0,0,others,mysql,0,0
5222,innodb_validate_tablespace_paths,"Controls tablespace file path validation. At startup, InnoDB validates the paths of known tablespace files against tablespace file paths stored in the data dictionary in case tablespace files have been moved to a different location. The innodb_validate_tablespace_paths variable permits disabling tablespace path validation. This feature is intended for environments where tablespaces files are not moved. Disabling path validation improves startup time on systems with a large number of tablespace files.",0,0,others,mysql,0,0
5223,innodb_version,"The InnoDB version number. In MySQL 8.0, separate version numbering for InnoDB does not apply and this value is the same the version number of the server.",0,0,others,mysql,0,0
5224,innodb_write_io_threads,"The number of I/O threads for write operations in InnoDB. The default value is 4. Its counterpart for read threads is innodb_read_io_threads. For more information, see Section15.8.5, “Configuring the Number of Background InnoDB I/O Threads”. For general I/O tuning advice, see Section8.5.8, “Optimizing InnoDB Disk I/O”.",1,1,resource,mysql,0,1
5225,innodb-status-file,"The --innodb-status-file startup option controls whether InnoDB creates a file named innodb_status.pid in the data directory and writes SHOW ENGINE INNODB STATUS output to it every 15 seconds, approximately.",0,0,others,mysql,0,0
5226,install,"(Windows only) Install the server as a Windows service that starts automatically during Windows startup. The default service name is MySQL if no service_name value is given. For more information, see Section2.3.4.8, “Starting MySQL as a Windows Service”.",0,0,others,mysql,0,0
5227,install-manual,"(Windows only) Install the server as a Windows service that must be started manually. It does not start automatically during Windows startup. The default service name is MySQL if no service_name value is given. For more information, see Section2.3.4.8, “Starting MySQL as a Windows Service”.",0,0,others,mysql,0,1
5228,keyring_aws_cmk_id,The customer master key (CMK) ID obtained from the AWS KMS server and used by the keyring_aws plugin. This variable is unavailable unless that plugin is installed.,0,0,others,mysql,0,1
5229,keyring_aws_conf_file,The location of the configuration file for the keyring_aws plugin. This variable is unavailable unless that plugin is installed.,0,0,others,mysql,0,0
5230,keyring_aws_data_file,The location of the storage file for the keyring_aws plugin. This variable is unavailable unless that plugin is installed.,0,0,others,mysql,0,0
5231,keyring_aws_region,The AWS region for the keyring_aws plugin. This variable is unavailable unless that plugin is installed.,0,0,others,mysql,0,0
5232,keyring_encrypted_file_data,"The path name of the data file used for secure data storage by the keyring_encrypted_file plugin. This variable is unavailable unless that plugin is installed. The file location should be in a directory considered for use only by keyring plugins. For example, do not locate the file under the data directory.",0,0,others,mysql,0,1
5233,keyring_encrypted_file_password,The password used by the keyring_encrypted_file plugin. This variable is unavailable unless that plugin is installed.,0,0,others,mysql,0,0
5234,keyring_file_data,"The path name of the data file used for secure data storage by the keyring_file plugin. This variable is unavailable unless that plugin is installed. The file location should be in a directory considered for use only by keyring plugins. For example, do not locate the file under the data directory.",0,0,others,mysql,0,0
5235,keyring_hashicorp_auth_path,"The authentication path where AppRole authentication is enabled within the HashiCorp Vault server, for use by the keyring_hashicorp plugin. This variable is unavailable unless that plugin is installed.",0,0,others,mysql,0,0
5236,keyring_hashicorp_ca_path,The absolute path name of a local file accessible to the MySQL server that contains a properly formatted TLS certificate authority for use by the keyring_hashicorp plugin. This variable is unavailable unless that plugin is installed.,0,0,others,mysql,0,1
5237,keyring_hashicorp_caching,"Whether to enable the optional in-memory key cache used by the keyring_hashicorp plugin to cache keys from the HashiCorp Vault server. This variable is unavailable unless that plugin is installed. If the cache is enabled, the plugin populates it during initialization. Otherwise, the plugin populates only the key list during initialization.",1,4,limited-side-effect,mysql,0,1
5238,keyring_hashicorp_commit_auth_path,"This variable is associated with keyring_hashicorp_auth_path, from which it takes its value during keyring_hashicorp plugin initialization. This variable is unavailable unless that plugin is installed. It reflects the “committed” value actually used for plugin operation if initialization succeeds. For additional information, see keyring_hashicorp Configuration.",0,0,others,mysql,0,0
5239,keyring_hashicorp_commit_ca_path,"This variable is associated with keyring_hashicorp_ca_path, from which it takes its value during keyring_hashicorp plugin initialization. This variable is unavailable unless that plugin is installed. It reflects the “committed” value actually used for plugin operation if initialization succeeds. For additional information, see keyring_hashicorp Configuration.",0,0,others,mysql,0,1
5240,keyring_hashicorp_commit_caching,"This variable is associated with keyring_hashicorp_caching, from which it takes its value during keyring_hashicorp plugin initialization. This variable is unavailable unless that plugin is installed. It reflects the “committed” value actually used for plugin operation if initialization succeeds. For additional information, see keyring_hashicorp Configuration.",0,0,others,mysql,0,1
5241,keyring_hashicorp_commit_role_id,"This variable is associated with keyring_hashicorp_role_id, from which it takes its value during keyring_hashicorp plugin initialization. This variable is unavailable unless that plugin is installed. It reflects the “committed” value actually used for plugin operation if initialization succeeds. For additional information, see keyring_hashicorp Configuration.",0,0,others,mysql,0,1
5242,keyring_hashicorp_commit_server_url,"This variable is associated with keyring_hashicorp_server_url, from which it takes its value during keyring_hashicorp plugin initialization. This variable is unavailable unless that plugin is installed. It reflects the “committed” value actually used for plugin operation if initialization succeeds. For additional information, see keyring_hashicorp Configuration.",0,0,others,mysql,0,0
5243,keyring_hashicorp_commit_store_path,"This variable is associated with keyring_hashicorp_store_path, from which it takes its value during keyring_hashicorp plugin initialization. This variable is unavailable unless that plugin is installed. It reflects the “committed” value actually used for plugin operation if initialization succeeds. For additional information, see keyring_hashicorp Configuration.",0,0,others,mysql,0,0
5244,keyring_hashicorp_role_id,"The HashiCorp Vault AppRole authentication role ID, for use by the keyring_hashicorp plugin. This variable is unavailable unless that plugin is installed. The value must be in UUID format.",0,0,others,mysql,0,0
5245,keyring_hashicorp_secret_id,"The HashiCorp Vault AppRole authentication secret ID, for use by the keyring_hashicorp plugin. This variable is unavailable unless that plugin is installed. The value must be in UUID format.",0,0,others,mysql,0,0
5246,keyring_hashicorp_server_url,"The HashiCorp Vault server URL, for use by the keyring_hashicorp plugin. This variable is unavailable unless that plugin is installed. The value must begin with https://.",0,0,others,mysql,0,0
5247,keyring_hashicorp_store_path,"A store path within the HashiCorp Vault server that is writeable when appropiate AppRole credentials are provided by the keyring_hashicorp plugin. This variable is unavailable unless that plugin is installed. To specify the credentials, set the keyring_hashicorp_role_id and keyring_hashicorp_secret_id system variables (for example, as shown in keyring_hashicorp Configuration).",0,0,others,mysql,0,0
5248,keyring_oci_ca_certificate,The path name of the CA certificate bundle file that the keyring_oci plugin uses for Oracle Cloud Infrastructure certificate verification. This variable is unavailable unless that plugin is installed.,0,0,others,mysql,0,1
5249,keyring_oci_compartment,The OCID of the tenancy compartment that the keyring_oci plugin uses as the location of the MySQL keys. This variable is unavailable unless that plugin is installed.,0,0,others,mysql,0,1
5250,keyring_oci_encryption_endpoint,The endpoint of the Oracle Cloud Infrastructure encryption server that the keyring_oci plugin uses for generating ciphertext for new keys. This variable is unavailable unless that plugin is installed.,0,0,others,mysql,0,0
5251,keyring_oci_key_file,The path name of the file containing the RSA private key that the keyring_oci plugin uses for Oracle Cloud Infrastructure authentication. This variable is unavailable unless that plugin is installed.,0,0,others,mysql,0,0
5252,keyring_oci_key_fingerprint,The fingerprint of the RSA private key that the keyring_oci plugin uses for Oracle Cloud Infrastructure authentication. This variable is unavailable unless that plugin is installed.,0,0,others,mysql,0,0
5253,keyring_oci_management_endpoint,The endpoint of the Oracle Cloud Infrastructure key management server that the keyring_oci plugin uses for listing existing keys. This variable is unavailable unless that plugin is installed.,0,0,others,mysql,0,0
5254,keyring_oci_master_key,The OCID of the Oracle Cloud Infrastructure master encryption key that the keyring_oci plugin uses for encryption of secrets. This variable is unavailable unless that plugin is installed.,0,0,others,mysql,0,0
5255,keyring_oci_secrets_endpoint,"The endpoint of the Oracle Cloud Infrastructure secrets server that the keyring_oci plugin uses for listing, creating, and retiring secrets. This variable is unavailable unless that plugin is installed.",0,0,others,mysql,0,1
5256,keyring_oci_tenancy,The OCID of the Oracle Cloud Infrastructure tenancy that the keyring_oci plugin uses as the location of the MySQL compartment. This variable is unavailable unless that plugin is installed.,0,0,others,mysql,0,0
5257,keyring_oci_user,The OCID of the Oracle Cloud Infrastructure user that the keyring_oci plugin uses for cloud connections. This variable is unavailable unless that plugin is installed.,0,0,others,mysql,0,0
5258,keyring_oci_vaults_endpoint,The endpoint of the Oracle Cloud Infrastructure vaults server that the keyring_oci plugin uses for obtaining the value of secrets. This variable is unavailable unless that plugin is installed.,0,0,others,mysql,0,1
5259,keyring_oci_virtual_vault,The OCID of the Oracle Cloud Infrastructure Vault that the keyring_oci plugin uses for encryption operations. This variable is unavailable unless that plugin is installed.,0,0,others,mysql,0,0
5260,keyring_okv_conf_dir,"The path name of the directory that stores configuration information used by the keyring_okv plugin. This variable is unavailable unless that plugin is installed. The location should be a directory considered for use only by the keyring_okv plugin. For example, do not locate the directory under the data directory.",0,0,others,mysql,0,0
5261,keyring_operations,"Whether keyring operations are enabled. This variable is used during key migration operations. See Section6.4.4.13, “Migrating Keys Between Keyring Keystores”. The privileges required to modify this variable are ENCRYPTION_KEY_ADMIN in addition to either SYSTEM_VARIABLES_ADMIN or the deprecated SUPER privilege.",0,0,others,mysql,0,1
5262,keyring-migration-destination,"The destination keyring plugin for key migration. See Section6.4.4.13, “Migrating Keys Between Keyring Keystores”. The option value interpretation depends on whether --keyring-migration-to-component is specified:",0,0,others,mysql,0,0
5263,keyring-migration-host,"The host location of the running server that is currently using one of the key migration keystores. See Section6.4.4.13, “Migrating Keys Between Keyring Keystores”. Migration always occurs on the local host, so the option always specifies a value for connecting to a local server, such as localhost, 127.0.0.1, ::1, or the local host IP address or host name.",0,0,others,mysql,0,0
5264,keyring-migration-password,"The password of the MySQL account used for connecting to the running server that is currently using one of the key migration keystores. See Section6.4.4.13, “Migrating Keys Between Keyring Keystores”.",0,0,others,mysql,0,1
5265,keyring-migration-port,"For TCP/IP connections, the port number for connecting to the running server that is currently using one of the key migration keystores. See Section6.4.4.13, “Migrating Keys Between Keyring Keystores”.",0,0,others,mysql,0,1
5266,keyring-migration-socket,"For Unix socket file or Windows named pipe connections, the socket file or named pipe for connecting to the running server that is currently using one of the key migration keystores. See Section6.4.4.13, “Migrating Keys Between Keyring Keystores”.",0,0,others,mysql,0,0
5267,keyring-migration-source,"The source keyring plugin for key migration. See Section6.4.4.13, “Migrating Keys Between Keyring Keystores”.",0,0,others,mysql,0,0
5268,keyring-migration-to-component,"Indicates that a key migration is from a keyring plugin to a keyring component. This option makes it possible to migrate keys from any keyring plugin to any keyring component, which facilitates transitioning a MySQL installation from keyring plugins to keyring components.",0,0,others,mysql,0,0
5269,keyring-migration-user,"The user name of the MySQL account used for connecting to the running server that is currently using one of the key migration keystores. See Section6.4.4.13, “Migrating Keys Between Keyring Keystores”.",0,0,others,mysql,0,1
5270,language,"The language to use for error messages. lang_name can be given as the language name or as the full path name to the directory where the language files are installed. See Section10.12, “Setting the Error Message Language”.",0,0,others,mysql,0,1
5271,large_pages,Some hardware/operating system architectures support memory pages greater than the default (usually 4KB). The actual implementation of this support depends on the underlying hardware and operating system. Applications that perform a lot of memory accesses may obtain performance improvements by using large pages due to reduced Translation Lookaside Buffer (TLB) misses.,1,4,limited-side-effect,mysql,0,1
5272,lc_messages,"The locale to use for error messages. The default is en_US. The server converts the argument to a language name and combines it with the value of --lc-messages-dir to produce the location for the error message file. See Section10.12, “Setting the Error Message Language”.",0,0,others,mysql,0,0
5273,lc_messages_dir,"The directory where error messages are located. The server uses the value together with the value of --lc-messages to produce the location for the error message file. See Section10.12, “Setting the Error Message Language”.",0,0,others,mysql,0,0
5274,local-service,"(Windows only) A --local-service option following the service name causes the server to run using the LocalService Windows account that has limited system privileges. If both --defaults-file and --local-service are given following the service name, they can be in any order. See Section2.3.4.8, “Starting MySQL as a Windows Service”.",0,0,others,mysql,0,0
5275,lock_order,"Whether to enable the LOCK_ORDER tool at runtime. If lock_order is disabled (the default), no other LOCK_ORDER system variables have any effect. If lock_order is enabled, the other system variables configure which LOCK_ORDER features to enable.",0,0,others,mysql,0,1
5276,lock_order_debug_loop,Whether the LOCK_ORDER tool causes a debug assertion failure when it encounters a dependency that is flagged as a loop in the lock-order graph.,0,0,others,mysql,0,1
5277,lock_order_debug_missing_arc,Whether the LOCK_ORDER tool causes a debug assertion failure when it encounters a dependency that is not declared in the lock-order graph.,0,0,others,mysql,0,0
5278,lock_order_debug_missing_key,Whether the LOCK_ORDER tool causes a debug assertion failure when it encounters an object that is not properly instrumented with the Performance Schema.,0,0,others,mysql,0,0
5279,lock_order_debug_missing_unlock,Whether the LOCK_ORDER tool causes a debug assertion failure when it encounters a lock that is destroyed while still held.,0,0,others,mysql,0,0
5280,lock_order_dependencies,The path to the lock_order_dependencies.txt file that defines the server lock-order dependency graph.,0,0,others,mysql,0,0
5281,lock_order_extra_dependencies,"The path to a file containing additional dependencies for the lock-order dependency graph. This is useful to amend the primary server dependency graph, defined in the lock_order_dependencies.txt file, with additional dependencies describing the behavior of third party code. (The alternative is to modify lock_order_dependencies.txt itself, which is not encouraged.)",0,0,others,mysql,0,0
5282,lock_order_output_directory,"The directory where the LOCK_ORDER tool writes its logs. If this variable is not set, the default is the current directory.",0,0,others,mysql,0,0
5283,lock_order_print_txt,Whether the LOCK_ORDER tool performs a lock-order graph analysis and prints a textual report. The report includes any lock-acquisition cycles detected.,0,0,others,mysql,0,0
5284,lock_order_trace_loop,Whether the LOCK_ORDER tool prints a trace in the log file when it encounters a dependency that is flagged as a loop in the lock-order graph.,0,0,others,mysql,0,1
5285,lock_order_trace_missing_arc,Whether the LOCK_ORDER tool prints a trace in the log file when it encounters a dependency that is not declared in the lock-order graph.,0,0,others,mysql,0,0
5286,lock_order_trace_missing_key,Whether the LOCK_ORDER tool prints a trace in the log file when it encounters an object that is not properly instrumented with the Performance Schema.,0,0,others,mysql,0,1
5287,lock_order_trace_missing_unlock,Whether the LOCK_ORDER tool prints a trace in the log file when it encounters a lock that is destroyed while still held.,0,0,others,mysql,0,0
5288,log_bin,"Shows the status of binary logging on the server, either enabled (ON) or disabled (OFF). With binary logging enabled, the server logs all statements that change data to the binary log, which is used for backup and replication. ON means that the binary log is available, OFF means that it is not in use. The --log-bin option can be used to specify a base name and location for the binary log.",1,6,function-tradeoff,mysql,0,0
5289,log_bin_basename,"Holds the base name and path for the binary log files, which can be set with the --log-bin server option. The maximum variable length is 256. In MySQL 8.0, if the --log-bin option is not supplied, the default base name is binlog. For compatibility with MySQL 5.7, if the --log-bin option is supplied with no string or with an empty string, the default base name is host_name-bin, using the name of the host machine. The default location is the data directory.",0,0,others,mysql,0,0
5290,log_bin_index,"The name for the binary log index file, which contains the names of the binary log files. By default, it has the same location and base name as the value specified for the binary log files using the --log-bin option, plus the extension .index. If you do not specify --log-bin, the default binary log index file name is binlog.index. If you specify --log-bin option with no string or an empty string, the default binary log index file name is host_name-bin.index, using the name of the host machine.",0,0,others,mysql,0,0
5291,log_bin_trust_function_creators,"This variable applies when binary logging is enabled. It controls whether stored function creators can be trusted not to create stored functions that may cause unsafe events to be written to the binary log. If set to 0 (the default), users are not permitted to create or alter stored functions unless they have the SUPER privilege in addition to the CREATE ROUTINE or ALTER ROUTINE privilege. A setting of 0 also enforces the restriction that a function must be declared with the DETERMINISTIC characteristic, or with the READS SQL DATA or NO SQL characteristic. If the variable is set to 1, MySQL does not enforce these restrictions on stored function creation. This variable also applies to trigger creation. See Section25.7, “Stored Program Binary Logging”.",0,0,others,mysql,0,1
5292,log_bin_use_v1_row_events,"This read-only system variable is deprecated. Setting the system variable to ON at server startup enabled row-based replication with replicas running MySQL Server 5.5 and earlier by writing the binary log using Version 1 binary log row events, instead of Version 2 binary log row events which are the default as of MySQL 5.6.",0,0,others,mysql,0,0
5293,log_error,"Set the default error log destination to the named file. This affects log sinks that base their own output destination on the default destination. See Section5.4.2, “The Error Log”.",0,0,others,mysql,0,0
5294,log_raw,"Passwords in certain statements written to the general query log, slow query log, and binary log are rewritten by the server not to occur literally in plain text. Password rewriting can be suppressed for the general query log by starting the server with the --log-raw option. This option may be useful for diagnostic purposes, to see the exact text of statements as received by the server, but for security reasons is not recommended for production use.",0,0,others,mysql,0,0
5295,log_replica_updates,log_replica_updates specifies whether updates received by a replica server from a replication source server should be logged to the replica's own binary log.,0,0,others,mysql,0,0
5296,log_slave_updates,log_slave_updates specifies whether updates received by a replica server from a replication source server should be logged to the replica's own binary log.,0,0,others,mysql,0,0
5297,log_slow_replica_statements,"When the slow query log is enabled, log_slow_replica_statements enables logging for queries that have taken more than long_query_time seconds to execute on the replica. Note that if row-based replication is in use (binlog_format=ROW), log_slow_replica_statements has no effect. Queries are only added to the replica's slow query log when they are logged in statement format in the binary log, that is, when binlog_format=STATEMENT is set, or when binlog_format=MIXED is set and the statement is logged in statement format. Slow queries that are logged in row format when binlog_format=MIXED is set, or that are logged when binlog_format=ROW is set, are not added to the replica's slow query log, even if log_slow_replica_statements is enabled.",1,6,function-tradeoff,mysql,0,0
5298,log_slow_slave_statements,"When the slow query log is enabled, log_slow_slave_statements enables logging for queries that have taken more than long_query_time seconds to execute on the replica. Note that if row-based replication is in use (binlog_format=ROW), log_slow_slave_statements has no effect. Queries are only added to the replica's slow query log when they are logged in statement format in the binary log, that is, when binlog_format=STATEMENT is set, or when binlog_format=MIXED is set and the statement is logged in statement format. Slow queries that are logged in row format when binlog_format=MIXED is set, or that are logged when binlog_format=ROW is set, are not added to the replica's slow query log, even if log_slow_slave_statements is enabled.",1,6,function-tradeoff,mysql,0,0
5299,log_statements_unsafe_for_binlog,"If error 1592 is encountered, controls whether the generated warnings are added to the error log or not.",0,0,others,mysql,0,0
5300,log-bin,"Specifies the base name to use for binary log files. With binary logging enabled, the server logs all statements that change data to the binary log, which is used for backup and replication. The binary log is a sequence of files with a base name and numeric extension. The --log-bin option value is the base name for the log sequence. The server creates binary log files in sequence by adding a numeric suffix to the base name.",0,0,others,mysql,0,0
5301,log-isam,Log all MyISAM changes to this file (used only when debugging MyISAM).,0,0,others,mysql,0,0
5302,log-short-format,"Log less information to the slow query log, if it has been activated.",0,0,others,mysql,0,0
5303,log-tc,The name of the memory-mapped transaction coordinator log file (for XA transactions that affect multiple storage engines when the binary log is disabled). The default name is tc.log. The file is created under the data directory if not given as a full path name. This option is unused.,0,0,others,mysql,0,1
5304,log-tc-size,"The size in bytes of the memory-mapped transaction coordinator log. The default and minimum values are 6 times the page size, and the value must be a multiple of the page size.",0,0,others,mysql,0,1
5305,master_info_repository,"The use of this system variable is now deprecated. The setting TABLE is the default, and is required when multiple replication channels are configured. The alternative setting FILE was previously deprecated.",0,0,others,mysql,0,0
5306,master_verify_checksum,"Enabling master_verify_checksum causes the source to verify events read from the binary log by examining checksums, and to stop with an error in the event of a mismatch. master_verify_checksum is disabled by default; in this case, the source uses the event length from the binary log to verify events, so that only complete events are read from the binary log.",1,2,security-tradeoff,mysql,0,0
5307,master-info-file,"The use of this option is now deprecated. It was used to set the file name for the replica's connection metadata repository if master_info_repository=FILE was set. --master-info-file and the use of the master_info_repository system variable are deprecated because the use of a file for the connection metadata repository has been superseded by crash-safe tables. For information about the connection metadata repository, see Section17.2.4.2, “Replication Metadata Repositories”.",0,0,others,mysql,0,1
5308,master-retry-count,"The number of times that the replica tries to reconnect to the source before giving up. The default value is 86400 times. A value of 0 means “infinite”, and the replica attempts to connect forever. Reconnection attempts are triggered when the replica reaches its connection timeout (specified by the replica_net_timeout or slave_net_timeout system variable) without receiving data or a heartbeat signal from the source. Reconnection is attempted at intervals set by the SOURCE_CONNECT_RETRY | MASTER_CONNECT_RETRY option of the CHANGE REPLICATION SOURCE TO | CHANGE MASTER TO statement (which defaults to every 60 seconds).",0,0,others,mysql,0,0
5309,max_binlog_cache_size,"If a transaction requires more than this many bytes of memory, the server generates a Multi-statement transaction required more than 'max_binlog_cache_size' bytes of storage error. The minimum value is 4096. The maximum possible value is 16EiB (exbibytes). The maximum recommended value is 4GB; this is due to the fact that MySQL currently cannot work with binary log positions greater than 4GB. The value must be a multiple of 4096.",1,5,workload-specific,mysql,0,0
5310,max_binlog_size,"If a write to the binary log causes the current log file size to exceed the value of this variable, the server rotates the binary logs (closes the current file and opens the next one). The minimum value is 4096 bytes. The maximum and default value is 1GB. Encrypted binary log files have an additional 512-byte header, which is included in max_binlog_size.",1,5,workload-specific,mysql,0,0
5311,max_binlog_stmt_cache_size,"If nontransactional statements within a transaction require more than this many bytes of memory, the server generates an error. The minimum value is 4096. The maximum and default values are 4GB on 32-bit platforms and 16EB (exabytes) on 64-bit platforms. The value must be a multiple of 4096.",1,5,workload-specific,mysql,0,1
5312,max_relay_log_size,"If a write by a replica to its relay log causes the current log file size to exceed the value of this variable, the replica rotates the relay logs (closes the current file and opens the next one). If max_relay_log_size is 0, the server uses max_binlog_size for both the binary log and the relay log. If max_relay_log_size is greater than 0, it constrains the size of the relay log, which enables you to have different sizes for the two logs. You must set max_relay_log_size to between 4096 bytes and 1GB (inclusive), or to 0. The default value is 0. See Section17.2.3, “Replication Threads”.",1,5,workload-specific,mysql,0,0
5313,max-binlog-dump-events,This option is used internally by the MySQL test suite for replication testing and debugging.,0,0,others,mysql,0,0
5314,memlock,Lock the mysqld process in memory. This option might help if you have a problem where the operating system is causing mysqld to swap to disk.,1,6,function-tradeoff,mysql,0,0
5315,myisam-block-size,The block size to be used for MyISAM index pages.,1,5,workload-specific,mysql,0,1
5316,mysql_firewall_mode,Whether MySQL Enterprise Firewall is enabled (the default) or disabled.,0,0,others,mysql,0,0
5317,mysql_firewall_trace,"Whether the MySQL Enterprise Firewall trace is enabled or disabled (the default). When mysql_firewall_trace is enabled, for PROTECTING mode, the firewall writes rejected statements to the error log.",0,0,others,mysql,0,0
5318,mysqlx,"This option controls how the server loads X Plugin at startup. In MySQL 8.0, X Plugin is enabled by default, but this option may be used to control its activation state.",0,0,others,mysql,0,1
5319,mysqlx_bind_address,The network address on which X Plugin listens for TCP/IP connections. This variable is not dynamic and can be configured only at startup. This is the X Plugin equivalent of the bind_address system variable; see that variable description for more information.,0,0,others,mysql,0,0
5320,mysqlx_compression_algorithms,"The compression algorithms that are permitted for use on X Protocol connections. By default, the Deflate, LZ4, and zstd algorithms are all permitted. To disallow any of the algorithms, set mysqlx_compression_algorithms to include only the ones you permit. The algorithm names deflate_stream, lz4_message, and zstd_stream can be specified in any combination, and the order and case are not important. If you set the system variable to the empty string, no compression algorithms are permitted and only uncompressed connections are used. Use the algorithm-specific system variables to adjust the default and maximum compression level for each permitted algorithm.",1,5,workload-specific,mysql,0,0
5321,mysqlx_connect_timeout,The number of seconds X Plugin waits for the first packet to be received from newly connected clients. This is the X Plugin equivalent of connect_timeout; see that variable description for more information.,0,0,others,mysql,0,1
5322,mysqlx_deflate_default_compression_level,"The default compression level that the server uses for the Deflate algorithm on X Protocol connections. Specify the level as an integer from 1 (the lowest compression effort) to 9 (the highest effort). This level is used if the client does not request a compression level during capability negotiation. If you do not specify this system variable, the server uses level 3 as the default.",1,6,function-tradeoff,mysql,0,0
5323,mysqlx_deflate_max_client_compression_level,"The maximum compression level that the server permits for the Deflate algorithm on X Protocol connections. The range is the same as for the default compression level for this algorithm. If the client requests a higher compression level than this, the server uses the level you set here. If you do not specify this system variable, the server sets a maximum compression level of 5.",1,6,function-tradeoff,mysql,0,0
5324,mysqlx_document_id_unique_prefix,"Sets the first 4 bytes of document IDs generated by the server when documents are added to a collection. By setting this variable to a unique value per instance, you can ensure document IDs are unique across instances.",0,0,others,mysql,0,0
5325,mysqlx_enable_hello_notice,"Controls messages sent to classic MySQL protocol clients that try to connect over X Protocol. When enabled, clients which do not support X Protocol that attempt to connect to the server X Protocol port receive an error explaining they are using the wrong protocol.",0,0,others,mysql,0,0
5326,mysqlx_idle_worker_thread_timeout,The number of seconds after which idle worker threads are terminated.,0,0,others,mysql,0,0
5327,mysqlx_interactive_timeout,The default value of the mysqlx_wait_timeout session variable for interactive clients. (The number of seconds to wait for interactive clients to timeout.),0,0,others,mysql,0,0
5328,mysqlx_lz4_default_compression_level,"The default compression level that the server uses for the LZ4 algorithm on X Protocol connections. Specify the level as an integer from 0 (the lowest compression effort) to 16 (the highest effort). This level is used if the client does not request a compression level during capability negotiation. If you do not specify this system variable, the server uses level 2 as the default. For more information, see Section20.5.5, “Connection Compression with X Plugin”.",1,6,function-tradeoff,mysql,0,0
5329,mysqlx_lz4_max_client_compression_level,"The maximum compression level that the server permits for the LZ4 algorithm on X Protocol connections. The range is the same as for the default compression level for this algorithm. If the client requests a higher compression level than this, the server uses the level you set here. If you do not specify this system variable, the server sets a maximum compression level of 8.",1,6,function-tradeoff,mysql,0,0
5330,mysqlx_max_allowed_packet,"The maximum size of network packets that can be received by X Plugin. This limit also applies when compression is used for the connection, so the network packet must be smaller than this size after the message has been decompressed.",1,5,workload-specific,mysql,0,0
5331,mysqlx_max_connections,The maximum number of concurrent client connections X Plugin can accept. This is the X Plugin equivalent of max_connections; see that variable description for more information.,1,1,resource,mysql,0,0
5332,mysqlx_min_worker_threads,The minimum number of worker threads used by X Plugin for handling client requests.,1,1,resource,mysql,0,0
5333,mysqlx_port,The network port on which X Plugin listens for TCP/IP connections. This is the X Plugin equivalent of port; see that variable description for more information.,0,0,others,mysql,0,1
5334,mysqlx_port_open_timeout,The number of seconds X Plugin waits for a TCP/IP port to become free.,0,0,others,mysql,0,0
5335,mysqlx_read_timeout,"The number of seconds that X Plugin waits for blocking read operations to complete. After this time, if the read operation is not successful, X Plugin closes the connection and returns a warning notice with the error code ER_IO_READ_ERROR to the client application.",0,0,others,mysql,0,0
5336,mysqlx_socket,The path to a Unix socket file which X Plugin uses for connections. This setting is only used by MySQL Server when running on Unix operating systems. Clients can use this socket to connect to MySQL Server using X Plugin.,0,0,others,mysql,0,1
5337,mysqlx_ssl_ca,"The mysqlx_ssl_ca system variable is like ssl_ca, except that it applies to X Plugin rather than the MySQL Server main connection interface.",0,0,others,mysql,0,0
5338,mysqlx_ssl_capath,"The mysqlx_ssl_capath system variable is like ssl_capath, except that it applies to X Plugin rather than the MySQL Server main connection interface.",0,0,others,mysql,0,0
5339,mysqlx_ssl_cert,"The mysqlx_ssl_cert system variable is like ssl_cert, except that it applies to X Plugin rather than the MySQL Server main connection interface. For information about configuring encryption support for X Plugin, see Section20.5.3, “Using Encrypted Connections with X Plugin”.",0,0,others,mysql,0,0
5340,mysqlx_ssl_cipher,"The mysqlx_ssl_cipher system variable is like ssl_cipher, except that it applies to X Plugin rather than the MySQL Server main connection interface.",0,0,others,mysql,0,0
5341,mysqlx_ssl_crl,"The mysqlx_ssl_crl system variable is like ssl_crl, except that it applies to X Plugin rather than the MySQL Server main connection interface. For information about configuring encryption support for X Plugin, see Section20.5.3, “Using Encrypted Connections with X Plugin”.",0,0,others,mysql,0,1
5342,mysqlx_ssl_crlpath,"The mysqlx_ssl_crlpath system variable is like ssl_crlpath, except that it applies to X Plugin rather than the MySQL Server main connection interface. For information about configuring encryption support for X Plugin, see Section20.5.3, “Using Encrypted Connections with X Plugin”.",0,0,others,mysql,0,0
5343,mysqlx_ssl_key,"The mysqlx_ssl_key system variable is like ssl_key, except that it applies to X Plugin rather than the MySQL Server main connection interface. For information about configuring encryption support for X Plugin, see Section20.5.3, “Using Encrypted Connections with X Plugin”.",0,0,others,mysql,0,0
5344,mysqlx_wait_timeout,"The number of seconds that X Plugin waits for activity on a connection. After this time, if the read operation is not successful, X Plugin closes the connection. If the client is noninteractive, the initial value of the session variable is copied from the global mysqlx_wait_timeout variable. For interactive clients, the initial value is copied from the session mysqlx_interactive_timeout.",0,0,others,mysql,0,0
5345,mysqlx_write_timeout,"The number of seconds that X Plugin waits for blocking write operations to complete. After this time, if the write operation is not successful, X Plugin closes the connection.",0,0,others,mysql,0,0
5346,mysqlx_zstd_default_compression_level,"The default compression level that the server uses for the zstd algorithm on X Protocol connections. For versions of the zstd library from 1.4.0, you can set positive values from 1 to 22 (the highest compression effort), or negative values which represent progressively lower effort. A value of 0 is converted to a value of 1. For earlier versions of the zstd library, you can only specify the value 3. This level is used if the client does not request a compression level during capability negotiation. If you do not specify this system variable, the server uses level 3 as the default. For more information, see Section20.5.5, “Connection Compression with X Plugin”.",1,6,function-tradeoff,mysql,0,0
5347,mysqlx_zstd_max_client_compression_level,"The maximum compression level that the server permits for the zstd algorithm on X Protocol connections. The range is the same as for the default compression level for this algorithm. If the client requests a higher compression level than this, the server uses the level you set here. If you do not specify this system variable, the server sets a maximum compression level of 11.",1,6,function-tradeoff,mysql,0,0
5348,ndb_allow_copying_alter_table,Let ALTER TABLE and other DDL statements use copying operations on NDB tables. Set to OFF to keep this from happening; doing so may improve performance of critical applications.,1,6,function-tradeoff,mysql,0,0
5349,Ndb_api_adaptive_send_deferred_count,Number of adaptive send calls that were not actually sent.,0,0,others,mysql,0,1
5350,Ndb_api_adaptive_send_deferred_count_replica,Number of adaptive send calls that were not actually sent by this replica.,0,0,others,mysql,0,1
5351,Ndb_api_adaptive_send_deferred_count_session,Number of adaptive send calls that were not actually sent.,0,0,others,mysql,0,0
5352,Ndb_api_adaptive_send_deferred_count_slave,Number of adaptive send calls that were not actually sent by this replica.,0,0,others,mysql,0,0
5353,Ndb_api_adaptive_send_forced_count,Number of adaptive send calls using forced-send sent by this MySQL Server (SQL node).,0,0,others,mysql,0,0
5354,Ndb_api_adaptive_send_forced_count_replica,Number of adaptive send calls using forced-send sent by this replica.,0,0,others,mysql,0,0
5355,Ndb_api_adaptive_send_forced_count_session,Number of adaptive send calls using forced-send sent in this client session.,0,0,others,mysql,0,1
5356,Ndb_api_adaptive_send_forced_count_slave,Number of adaptive send calls using forced-send sent by this replica.,0,0,others,mysql,0,0
5357,Ndb_api_adaptive_send_unforced_count,Number of adaptive send calls without forced-send sent by this MySQL server (SQL node).,0,0,others,mysql,0,1
5358,Ndb_api_adaptive_send_unforced_count_replica,Number of adaptive send calls without forced-send sent by this replica.,0,0,others,mysql,0,0
5359,Ndb_api_adaptive_send_unforced_count_session,Number of adaptive send calls without forced-send sent in this client session.,0,0,others,mysql,0,0
5360,Ndb_api_adaptive_send_unforced_count_slave,Number of adaptive send calls without forced-send sent by this replica.,0,0,others,mysql,0,0
5361,Ndb_api_bytes_received_count,Amount of data (in bytes) received from the data nodes by this MySQL Server (SQL node).,0,0,others,mysql,0,0
5362,Ndb_api_bytes_received_count_replica,Amount of data (in bytes) received from the data nodes by this replica.,0,0,others,mysql,0,0
5363,Ndb_api_bytes_received_count_session,Amount of data (in bytes) received from the data nodes in this client session.,0,0,others,mysql,0,0
5364,Ndb_api_bytes_received_count_slave,Amount of data (in bytes) received from the data nodes by this replica.,0,0,others,mysql,0,0
5365,Ndb_api_bytes_sent_count,Amount of data (in bytes) sent to the data nodes by this MySQL Server (SQL node).,0,0,others,mysql,0,0
5366,Ndb_api_bytes_sent_count_replica,Amount of data (in bytes) sent to the data nodes by this replica.,0,0,others,mysql,0,0
5367,Ndb_api_bytes_sent_count_session,Amount of data (in bytes) sent to the data nodes in this client session.,0,0,others,mysql,0,0
5368,Ndb_api_bytes_sent_count_slave,Amount of data (in bytes) sent to the data nodes by this replica.,0,0,others,mysql,0,0
5369,Ndb_api_event_bytes_count,The number of bytes of events received by this MySQL Server (SQL node).,0,0,others,mysql,0,0
5370,Ndb_api_event_bytes_count_injector,The number of bytes of events received by the NDB binlog injector thread.,0,0,others,mysql,0,0
5371,Ndb_api_event_data_count,The number of row change events received by this MySQL Server (SQL node).,0,0,others,mysql,0,1
5372,Ndb_api_event_data_count_injector,The number of row change events received by the NDB binlog injector thread.,0,0,others,mysql,0,1
5373,Ndb_api_event_nondata_count,"The number of events received, other than row change events, by this MySQL Server (SQL node).",0,0,others,mysql,0,0
5374,Ndb_api_event_nondata_count_injector,"The number of events received, other than row change events, by the NDB binary log injector thread.",0,0,others,mysql,0,1
5375,Ndb_api_pk_op_count,"The number of operations by this MySQL Server (SQL node) based on or using primary keys. This includes operations on blob tables, implicit unlock operations, and auto-increment operations, as well as user-visible primary key operations.",0,0,others,mysql,0,1
5376,Ndb_api_pk_op_count_replica,"The number of operations by this replica based on or using primary keys. This includes operations on blob tables, implicit unlock operations, and auto-increment operations, as well as user-visible primary key operations.",0,0,others,mysql,0,0
5377,Ndb_api_pk_op_count_session,"The number of operations in this client session based on or using primary keys. This includes operations on blob tables, implicit unlock operations, and auto-increment operations, as well as user-visible primary key operations.",0,0,others,mysql,0,0
5378,Ndb_api_pk_op_count_slave,"The number of operations by this replica based on or using primary keys. This includes operations on blob tables, implicit unlock operations, and auto-increment operations, as well as user-visible primary key operations.",0,0,others,mysql,0,0
5379,Ndb_api_pruned_scan_count,The number of scans by this MySQL Server (SQL node) that have been pruned to a single partition.,0,0,others,mysql,0,0
5380,Ndb_api_pruned_scan_count_replica,The number of scans by this replica that have been pruned to a single partition.,0,0,others,mysql,0,0
5381,Ndb_api_pruned_scan_count_session,The number of scans in this client session that have been pruned to a single partition.,0,0,others,mysql,0,0
5382,Ndb_api_pruned_scan_count_slave,The number of scans by this replica that have been pruned to a single partition.,0,0,others,mysql,0,0
5383,Ndb_api_range_scan_count,The number of range scans that have been started by this MySQL Server (SQL node).,0,0,others,mysql,0,1
5384,Ndb_api_range_scan_count_replica,The number of range scans that have been started by this replica.,0,0,others,mysql,0,0
5385,Ndb_api_range_scan_count_session,The number of range scans that have been started in this client session.,0,0,others,mysql,0,1
5386,Ndb_api_range_scan_count_slave,The number of range scans that have been started by this replica.,0,0,others,mysql,0,0
5387,Ndb_api_read_row_count,"The total number of rows that have been read by this MySQL Server (SQL node). This includes all rows read by any primary key, unique key, or scan operation made by this MySQL Server (SQL node).",0,0,others,mysql,0,1
5388,Ndb_api_read_row_count_replica,"The total number of rows that have been read by this replica. This includes all rows read by any primary key, unique key, or scan operation made by this replica.",0,0,others,mysql,0,0
5389,Ndb_api_read_row_count_session,"The total number of rows that have been read in this client session. This includes all rows read by any primary key, unique key, or scan operation made in this client session.",0,0,others,mysql,0,0
5390,Ndb_api_read_row_count_slave,"The total number of rows that have been read by this replica. This includes all rows read by any primary key, unique key, or scan operation made by this replica.",0,0,others,mysql,0,1
5391,Ndb_api_scan_batch_count,The number of batches of rows received by this MySQL Server (SQL node). 1 batch is defined as 1 set of scan results from a single fragment.,0,0,others,mysql,0,0
5392,Ndb_api_scan_batch_count_replica,The number of batches of rows received by this replica. 1 batch is defined as 1 set of scan results from a single fragment.,0,0,others,mysql,0,0
5393,Ndb_api_scan_batch_count_session,The number of batches of rows received in this client session. 1 batch is defined as 1 set of scan results from a single fragment.,0,0,others,mysql,0,0
5394,Ndb_api_scan_batch_count_slave,The number of batches of rows received by this replica. 1 batch is defined as 1 set of scan results from a single fragment.,0,0,others,mysql,0,0
5395,Ndb_api_table_scan_count,"The number of table scans that have been started by this MySQL Server (SQL node), including scans of internal tables,.",0,0,others,mysql,0,0
5396,Ndb_api_table_scan_count_replica,"The number of table scans that have been started by this replica, including scans of internal tables.",0,0,others,mysql,0,0
5397,Ndb_api_table_scan_count_session,"The number of table scans that have been started in this client session, including scans of internal tables.",0,0,others,mysql,0,0
5398,Ndb_api_table_scan_count_slave,"The number of table scans that have been started by this replica, including scans of internal tables.",0,0,others,mysql,0,0
5399,Ndb_api_trans_abort_count,The number of transactions aborted by this MySQL Server (SQL node).,0,0,others,mysql,0,0
5400,Ndb_api_trans_abort_count_replica,The number of transactions aborted by this replica.,0,0,others,mysql,0,0
5401,Ndb_api_trans_abort_count_session,The number of transactions aborted in this client session.,0,0,others,mysql,0,0
5402,Ndb_api_trans_abort_count_slave,The number of transactions aborted by this replica.,0,0,others,mysql,0,0
5403,Ndb_api_trans_close_count,"The number of transactions closed by this MySQL Server (SQL node). This value may be greater than the sum of Ndb_api_trans_commit_count and Ndb_api_trans_abort_count, since some transactions may have been rolled back.",0,0,others,mysql,0,0
5404,Ndb_api_trans_close_count_replica,"The number of transactions closed by this replica. This value may be greater than the sum of Ndb_api_trans_commit_count_replica and Ndb_api_trans_abort_count_replica, since some transactions may have been rolled back.",0,0,others,mysql,0,0
5405,Ndb_api_trans_close_count_session,"The number of transactions closed in this client session. This value may be greater than the sum of Ndb_api_trans_commit_count_session and Ndb_api_trans_abort_count_session, since some transactions may have been rolled back.",0,0,others,mysql,0,0
5406,Ndb_api_trans_close_count_slave,"The number of transactions closed by this replica. This value may be greater than the sum of Ndb_api_trans_commit_count_replica and Ndb_api_trans_abort_count_replica, since some transactions may have been rolled back.",0,0,others,mysql,0,0
5407,Ndb_api_trans_commit_count,The number of transactions committed by this MySQL Server (SQL node).,0,0,others,mysql,0,0
5408,Ndb_api_trans_commit_count_replica,The number of transactions committed by this replica.,0,0,others,mysql,0,1
5409,Ndb_api_trans_commit_count_session,The number of transactions committed in this client session.,0,0,others,mysql,0,1
5410,Ndb_api_trans_commit_count_slave,The number of transactions committed by this replica.,0,0,others,mysql,0,1
5411,Ndb_api_trans_local_read_row_count,"The total number of rows that have been read by this MySQL Server (SQL node). This includes all rows read by any primary key, unique key, or scan operation made by this MySQL Server (SQL node).",0,0,others,mysql,0,0
5412,Ndb_api_trans_local_read_row_count_replica,"The total number of rows that have been read by this replica. This includes all rows read by any primary key, unique key, or scan operation made by this replica.",0,0,others,mysql,0,0
5413,Ndb_api_trans_local_read_row_count_session,"The total number of rows that have been read in this client session. This includes all rows read by any primary key, unique key, or scan operation made in this client session.",0,0,others,mysql,0,0
5414,Ndb_api_trans_local_read_row_count_slave,"The total number of rows that have been read by this replica. This includes all rows read by any primary key, unique key, or scan operation made by this replica.",0,0,others,mysql,0,0
5415,Ndb_api_trans_start_count,The number of transactions started by this MySQL Server (SQL node).,0,0,others,mysql,0,0
5416,Ndb_api_trans_start_count_replica,The number of transactions started by this replica.,0,0,others,mysql,0,0
5417,Ndb_api_trans_start_count_session,The number of transactions started in this client session.,0,0,others,mysql,0,0
5418,Ndb_api_trans_start_count_slave,The number of transactions started by this replica.,0,0,others,mysql,0,0
5419,Ndb_api_uk_op_count,The number of operations by this MySQL Server (SQL node) based on or using unique keys.,0,0,others,mysql,0,0
5420,Ndb_api_uk_op_count_replica,The number of operations by this replica based on or using unique keys.,0,0,others,mysql,0,0
5421,Ndb_api_uk_op_count_session,The number of operations in this client session based on or using unique keys.,0,0,others,mysql,0,0
5422,Ndb_api_uk_op_count_slave,The number of operations by this replica based on or using unique keys.,0,0,others,mysql,0,0
5423,Ndb_api_wait_exec_complete_count,The number of times a thread has been blocked by this MySQL Server (SQL node) while waiting for execution of an operation to complete. This includes all execute() calls as well as implicit executes for blob and auto-increment operations not visible to clients.,0,0,others,mysql,0,0
5424,Ndb_api_wait_exec_complete_count_replica,The number of times a thread has been blocked by this replica while waiting for execution of an operation to complete. This includes all execute() calls as well as implicit executes for blob and auto-increment operations not visible to clients.,0,0,others,mysql,0,1
5425,Ndb_api_wait_exec_complete_count_session,The number of times a thread has been blocked in this client session while waiting for execution of an operation to complete. This includes all execute() calls as well as implicit executes for blob and auto-increment operations not visible to clients.,0,0,others,mysql,0,0
5426,Ndb_api_wait_exec_complete_count_slave,The number of times a thread has been blocked by this replica while waiting for execution of an operation to complete. This includes all execute() calls as well as implicit executes for blob and auto-increment operations not visible to clients.,0,0,others,mysql,0,0
5427,Ndb_api_wait_meta_request_count,"The number of times a thread has been blocked by this MySQL Server (SQL node) waiting for a metadata-based signal, such as is expected for DDL requests, new epochs, and seizure of transaction records.",0,0,others,mysql,0,0
5428,Ndb_api_wait_meta_request_count_replica,"The number of times a thread has been blocked by this replica waiting for a metadata-based signal, such as is expected for DDL requests, new epochs, and seizure of transaction records.",0,0,others,mysql,0,0
5429,Ndb_api_wait_meta_request_count_session,"The number of times a thread has been blocked in this client session waiting for a metadata-based signal, such as is expected for DDL requests, new epochs, and seizure of transaction records.",0,0,others,mysql,0,0
5430,Ndb_api_wait_meta_request_count_slave,"The number of times a thread has been blocked by this replica waiting for a metadata-based signal, such as is expected for DDL requests, new epochs, and seizure of transaction records.",0,0,others,mysql,0,0
5431,Ndb_api_wait_nanos_count,Total time (in nanoseconds) spent by this MySQL Server (SQL node) waiting for any type of signal from the data nodes.,0,0,others,mysql,0,0
5432,Ndb_api_wait_nanos_count_replica,Total time (in nanoseconds) spent by this replica waiting for any type of signal from the data nodes.,0,0,others,mysql,0,0
5433,Ndb_api_wait_nanos_count_session,Total time (in nanoseconds) spent in this client session waiting for any type of signal from the data nodes.,0,0,others,mysql,0,0
5434,Ndb_api_wait_nanos_count_slave,Total time (in nanoseconds) spent by this replica waiting for any type of signal from the data nodes.,0,0,others,mysql,0,1
5435,Ndb_api_wait_scan_result_count,"The number of times a thread has been blocked by this MySQL Server (SQL node) while waiting for a scan-based signal, such as when waiting for more results from a scan, or when waiting for a scan to close.",0,0,others,mysql,0,1
5436,Ndb_api_wait_scan_result_count_replica,"The number of times a thread has been blocked by this replica while waiting for a scan-based signal, such as when waiting for more results from a scan, or when waiting for a scan to close.",0,0,others,mysql,0,1
5437,Ndb_api_wait_scan_result_count_session,"The number of times a thread has been blocked in this client session while waiting for a scan-based signal, such as when waiting for more results from a scan, or when waiting for a scan to close.",0,0,others,mysql,0,0
5438,Ndb_api_wait_scan_result_count_slave,"The number of times a thread has been blocked by this replica while waiting for a scan-based signal, such as when waiting for more results from a scan, or when waiting for a scan to close.",0,0,others,mysql,0,0
5439,ndb_autoincrement_prefetch_sz,"Determines the probability of gaps in an autoincremented column. Set it to 1 to minimize this. Setting it to a high value for optimization makes inserts faster, but decreases the likelihood of consecutive autoincrement numbers being used in a batch of inserts.",1,5,workload-specific,mysql,0,0
5440,ndb_batch_size,This sets the size in bytes that is used for NDB transaction batches.,1,1,resource,mysql,0,0
5441,ndb_blob_read_batch_bytes,"This option can be used to set the size (in bytes) for batching of BLOB data reads in NDB Cluster applications. When this batch size is exceeded by the amount of BLOB data to be read within the current transaction, any pending BLOB read operations are immediately executed.",1,1,resource,mysql,0,0
5442,ndb_blob_write_batch_bytes,"This option can be used to set the size (in bytes) for batching of BLOB data writes in NDB Cluster applications. When this batch size is exceeded by the amount of BLOB data to be written within the current transaction, any pending BLOB write operations are immediately executed.",1,1,resource,mysql,0,0
5443,ndb_cache_check_time,The number of milliseconds that elapse between checks of NDB Cluster SQL nodes by the MySQL query cache. Setting this to 0 (the default and minimum value) means that the query cache checks for validation on every query.,1,6,function-tradeoff,mysql,0,0
5444,ndb_clear_apply_status,"By the default, executing RESET SLAVE causes an NDB Cluster replica to purge all rows from its ndb_apply_status table. You can disable this by setting ndb_clear_apply_status=OFF.",0,0,others,mysql,0,1
5445,ndb_cluster_connection_pool,"By setting this option to a value greater than 1 (the default), a mysqld process can use multiple connections to the cluster, effectively mimicking several SQL nodes. Each connection requires its own [api] or [mysqld] section in the cluster configuration (config.ini) file, and counts against the maximum number of API connections supported by the cluster.",1,5,workload-specific,mysql,0,0
5446,ndb_cluster_connection_pool_nodeids,Specifies a comma-separated list of node IDs for connections to the cluster used by an SQL node. The number of nodes in this list must be the same as the value set for the --ndb-cluster-connection-pool option.,0,0,others,mysql,0,0
5447,Ndb_cluster_node_id,"If the server is acting as an NDB Cluster node, then the value of this variable its node ID in the cluster.",0,0,others,mysql,0,1
5448,Ndb_config_from_host,"If the server is part of an NDB Cluster, the value of this variable is the host name or IP address of the Cluster management server from which it gets its configuration data.",0,0,others,mysql,0,1
5449,Ndb_config_from_port,"If the server is part of an NDB Cluster, the value of this variable is the number of the port through which it is connected to the Cluster management server from which it gets its configuration data.",0,0,others,mysql,0,0
5450,Ndb_config_generation,Shows the generation number of the cluster's current configuration. This can be used as an indicator to determine whether the configuration of the cluster has changed since this SQL node last connected to the cluster.,0,0,others,mysql,0,0
5451,Ndb_conflict_fn_epoch,"Used in NDB Cluster Replication conflict resolution, this variable shows the number of rows found to be in conflict using NDB$EPOCH() conflict resolution on a given mysqld since the last time it was restarted.",0,0,others,mysql,0,0
5452,Ndb_conflict_fn_epoch_trans,"Used in NDB Cluster Replication conflict resolution, this variable shows the number of rows found to be in conflict using NDB$EPOCH_TRANS() conflict resolution on a given mysqld since the last time it was restarted.",0,0,others,mysql,0,0
5453,Ndb_conflict_fn_epoch2,"Shows the number of rows found to be in conflict in NDB Cluster Replication conflict resolution, when using NDB$EPOCH2(), on the source designated as the primary since the last time it was restarted.",0,0,others,mysql,0,0
5454,Ndb_conflict_fn_epoch2_trans,"Used in NDB Cluster Replication conflict resolution, this variable shows the number of rows found to be in conflict using NDB$EPOCH_TRANS2() conflict resolution on a given mysqld since the last time it was restarted.",0,0,others,mysql,0,0
5455,Ndb_conflict_fn_max,"Used in NDB Cluster Replication conflict resolution, this variable shows the number of times that a row was not applied on the current SQL node due to “greatest timestamp wins” conflict resolution since the last time that this mysqld was started.",0,0,others,mysql,0,0
5456,Ndb_conflict_fn_old,"Used in NDB Cluster Replication conflict resolution, this variable shows the number of times that a row was not applied as the result of “same timestamp wins” conflict resolution on a given mysqld since the last time it was restarted.",0,0,others,mysql,0,0
5457,Ndb_conflict_last_conflict_epoch,"The most recent epoch in which a conflict was detected on this replica. You can compare this value with Ndb_replica_max_replicated_epoch; if Ndb_replica_max_replicated_epoch is greater than Ndb_conflict_last_conflict_epoch, no conflicts have yet been detected.",0,0,others,mysql,0,0
5458,Ndb_conflict_last_stable_epoch,Number of rows found to be in conflict by a transactional conflict function,0,0,others,mysql,0,0
5459,Ndb_conflict_reflected_op_discard_count,"When using NDB Cluster Replication conflict resolution, this is the number of reflected operations that were not applied on the secondary, due to encountering an error during execution.",1,5,workload-specific,mysql,0,0
5460,Ndb_conflict_reflected_op_prepare_count,"When using conflict resolution with NDB Cluster Replication, this status variable contains the number of reflected operations that have been defined (that is, prepared for execution on the secondary).",1,1,resource,mysql,0,0
5461,Ndb_conflict_refresh_op_count,"When using conflict resolution with NDB Cluster Replication, this gives the number of refresh operations that have been prepared for execution on the secondary.",0,0,others,mysql,0,0
5462,ndb_conflict_role,"Determines the role of this SQL node (and NDB Cluster) in a circular (“active-active”) replication setup. ndb_slave_conflict_role can take any one of the values PRIMARY, SECONDARY, PASS, or NULL (the default). The replica SQL thread must be stopped before you can change ndb_slave_conflict_role. In addition, it is not possible to change directly between PASS and either of PRIMARY or SECONDARY directly; in such cases, you must ensure that the SQL thread is stopped, then execute SET @@GLOBAL.ndb_slave_conflict_role = 'NONE' first.",1,6,function-tradeoff,mysql,0,0
5463,Ndb_conflict_trans_conflict_commit_count,"Used in NDB Cluster Replication conflict resolution, this shows the number of epoch transactions committed after they required transactional conflict handling.",0,0,others,mysql,0,0
5464,Ndb_conflict_trans_detect_iter_count,"Used in NDB Cluster Replication conflict resolution, this shows the number of internal iterations required to commit an epoch transaction. Should be (slightly) greater than or equal to Ndb_conflict_trans_conflict_commit_count.",0,0,others,mysql,0,0
5465,Ndb_conflict_trans_reject_count,"Used in NDB Cluster Replication conflict resolution, this status variable shows the number of transactions found to be in conflict by a transactional conflict detection function.",0,0,others,mysql,0,0
5466,Ndb_conflict_trans_row_conflict_count,"Used in NDB Cluster Replication conflict resolution, this status variable shows the number of rows found to be directly in-conflict by a transactional conflict function on a given mysqld since the last time it was restarted.",0,0,others,mysql,0,1
5467,Ndb_conflict_trans_row_reject_count,"Used in NDB Cluster Replication conflict resolution, this status variable shows the total number of rows realigned due to being determined as conflicting by a transactional conflict detection function. This includes not only Ndb_conflict_trans_row_conflict_count, but any rows in or dependent on conflicting transactions.",0,0,others,mysql,0,0
5468,ndb_data_node_neighbour,"Sets the ID of a “nearest” data node—that is, a preferred nonlocal data node is chosen to execute the transaction, rather than one running on the same host as the SQL or API node. This used to ensure that when a fully replicated table is accessed, we access it on this data node, to ensure that the local copy of the table is always used whenever possible. This can also be used for providing hints for transactions.",0,0,others,mysql,0,0
5469,ndb_dbg_check_shares,"When set to 1, check that no shares are lingering. Available in debug builds only.",0,0,others,mysql,0,1
5470,ndb_default_column_format,"Sets the default COLUMN_FORMAT and ROW_FORMAT for new tables (see Section13.1.20, “CREATE TABLE Statement”). The default is FIXED.",0,0,others,mysql,0,1
5471,ndb_deferred_constraints,"Controls whether or not constraint checks are deferred, where these are supported. 0 is the default.",1,2,security-tradeoff,mysql,0,1
5472,ndb_distribution,Controls the default distribution method for NDB tables. Can be set to either of KEYHASH (key hashing) or LINHASH (linear hashing). KEYHASH is the default.,0,0,others,mysql,0,0
5473,Ndb_epoch_delete_delete_count,"When using delete-delete conflict detection, this is the number of delete-delete conflicts detected, where a delete operation is applied, but the indicated row does not exist.",0,0,others,mysql,0,0
5474,ndb_eventbuffer_free_percent,"Sets the percentage of the maximum memory allocated to the event buffer (ndb_eventbuffer_max_alloc) that should be available in event buffer after reaching the maximum, before starting to buffer again.",1,1,resource,mysql,0,0
5475,ndb_eventbuffer_max_alloc,"Sets the maximum amount memory (in bytes) that can be allocated for buffering events by the NDB API. 0 means that no limit is imposed, and is the default.",1,1,resource,mysql,0,0
5476,Ndb_execute_count,Provides the number of round trips to the NDB kernel made by operations.,0,0,others,mysql,0,0
5477,ndb_extra_logging,This variable enables recording in the MySQL error log of information specific to the NDB storage engine.,0,0,others,mysql,0,0
5478,ndb_force_send,"Forces sending of buffers to NDB immediately, without waiting for other threads. Defaults to ON.",0,0,others,mysql,0,0
5479,ndb_fully_replicated,"Determines whether new NDB tables are fully replicated. This setting can be overridden for an individual table using COMMENT=""NDB_TABLE=FULLY_REPLICATED=..."" in a CREATE TABLE or ALTER TABLE statement; see Section13.1.20.11, “Setting NDB_TABLE Options”, for syntax and other information.",1,4,limited-side-effect,mysql,0,0
5480,ndb_index_stat_enable,Use NDB index statistics in query optimization. The default is ON.,1,6,function-tradeoff,mysql,0,0
5481,ndb_index_stat_option,"This variable is used for providing tuning options for NDB index statistics generation. The list consist of comma-separated name-value pairs of option names and values, and this list must not contain any space characters.",0,0,others,mysql,0,0
5482,ndb_join_pushdown,"This variable controls whether joins on NDB tables are pushed down to the NDB kernel (data nodes). Previously, a join was handled using multiple accesses of NDB by the SQL node; however, when ndb_join_pushdown is enabled, a pushable join is sent in its entirety to the data nodes, where it can be distributed among the data nodes and executed in parallel on multiple copies of the data, with a single, merged result being returned to mysqld. This can reduce greatly the number of round trips between an SQL node and the data nodes required to handle such a join.",0,0,others,mysql,0,0
5483,Ndb_last_commit_epoch_server,The epoch most recently committed by NDB.,0,0,others,mysql,0,0
5484,Ndb_last_commit_epoch_session,The epoch most recently committed by this NDB client.,0,0,others,mysql,0,0
5485,ndb_log_apply_status,A read-only variable which shows whether the server was started with the --ndb-log-apply-status option.,0,0,others,mysql,0,0
5486,ndb_log_bin,"Causes updates to NDB tables to be written to the binary log. The setting for this variable has no effect if binary logging is not already enabled on the server using log_bin. In NDB 8.0, ndb_log_bin defaults to 0 (FALSE).",0,0,others,mysql,0,0
5487,ndb_log_binlog_index,"Causes a mapping of epochs to positions in the binary log to be inserted into the ndb_binlog_index table. Setting this variable has no effect if binary logging is not already enabled for the server using log_bin. (In addition, ndb_log_bin must not be disabled.) ndb_log_binlog_index defaults to 1 (ON); normally, there is never any need to change this value in a production environment.",0,0,others,mysql,0,0
5488,ndb_log_empty_epochs,"When this variable is set to 0, epoch transactions with no changes are not written to the binary log, although a row is still written even for an empty epoch in ndb_binlog_index.",0,0,others,mysql,0,1
5489,ndb_log_empty_update,"When this variable is set to ON (1), update transactions with no changes are written to the binary log, even when log_replica_updates or log_slave_updates is enabled.",0,0,others,mysql,0,0
5490,ndb_log_exclusive_reads,"This variable determines whether primary key reads are logged with exclusive locks, which allows for NDB Cluster Replication conflict detection and resolution based on read conflicts. To enable these locks, set the value of ndb_log_exclusive_reads to 1. 0, which disables such locking, is the default.",0,0,others,mysql,0,0
5491,ndb_log_fail_terminate,"When this option is specified, and complete logging of all found row events is not possible, the mysqld process is terminated.",1,3,reliability-tradeoff,mysql,0,0
5492,ndb_log_orig,Shows whether the originating server ID and epoch are logged in the ndb_binlog_index table. Set using the --ndb-log-orig server option.,0,0,others,mysql,0,0
5493,ndb_log_transaction_id,"This read-only, Boolean system variable shows whether a replica mysqld writes NDB transaction IDs in the binary log (required to use “active-active” NDB Cluster Replication with NDB$EPOCH_TRANS() conflict detection). To change the setting, use the --ndb-log-transaction-id option.",1,4,limited-side-effect,mysql,0,0
5494,ndb_log_update_as_write,"Whether updates on the source are written to the binary log as updates (OFF) or writes (ON). Used in NDB Replication conflict resolution; for more information, see Logging Changed Data as Updates.",1,6,function-tradeoff,mysql,0,0
5495,ndb_log_update_minimal,"Log updates in a minimal fashion, by writing only the primary key values in the before image, and only the changed columns in the after image. This may cause compatibility problems if replicating to storage engines other than NDB.",1,6,function-tradeoff,mysql,0,0
5496,ndb_log_updated_only,"Whether mysqld writes complete rows (ON) or updates only (OFF) to the binary log. Used in NDB Replication conflict resolution; see Logging Full or Partial Rows, for more information.",1,6,function-tradeoff,mysql,0,1
5497,Ndb_metadata_blacklist_size,The number of metadata objects that the NDB binlog thread has been unable to synchronize on this SQL node since it was last restarted.,0,0,others,mysql,0,0
5498,ndb_metadata_check,NDB uses a background thread to check for metadata changes each ndb_metadata_check_interval seconds as compared with the MySQL data dictionary. This metadata change detection thread can be disabled by setting ndb_metadata_check to OFF. The thread is enabled by default.,0,0,others,mysql,0,0
5499,ndb_metadata_check_interval,"NDB runs a metadata change detection thread in the background to determine when the NDB dictionary has changed with respect to the MySQL data dictionary. By default,the interval between such checks is 60 seconds; this can be adjusted by setting the value of ndb_metadata_check_interval. To enable or disable the thread, use ndb_metadata_check.",0,0,others,mysql,0,0
5500,Ndb_metadata_detected_count,The number of times since this server was last started that the NDB metadata change detection thread has discovered changes with respect to the MySQL data dictionary.,0,0,others,mysql,0,0
5501,Ndb_metadata_excluded_count,The number of metadata objects that the NDB binlog thread has been unable to synchronize on this SQL node since it was last restarted.,0,0,others,mysql,0,0
5502,ndb_metadata_sync,"Setting this variable causes the change monitor thread to override any values set for ndb_metadata_check or ndb_metadata_check_interval, and to enter a period of continuous change detection. When the thread ascertains that there are no more changes to be detected, it stalls until the binary logging thread has finished synchronization of all detected objects. ndb_metadata_sync is then set to false, and the change monitor thread reverts to the behavior determined by the settings for ndb_metadata_check and ndb_metadata_check_interval.",1,3,reliability-tradeoff,mysql,0,1
5503,Ndb_metadata_synced_count,The number of NDB metadata objects which have been synchronized on this SQL node since it was last restarted.,0,0,others,mysql,0,0
5504,ndb_nodeid,Set this MySQL server's node ID in an NDB Cluster.,0,0,others,mysql,0,1
5505,Ndb_number_of_data_nodes,"If the server is part of an NDB Cluster, the value of this variable is the number of data nodes in the cluster.",0,0,others,mysql,0,0
5506,ndb_optimization_delay,Set the number of milliseconds to wait between sets of rows by OPTIMIZE TABLE statements on NDB tables. The default is 10.,0,0,others,mysql,0,0
5507,ndb_optimized_node_selection,"There are two forms of optimized node selection, described here:",0,0,others,mysql,0,0
5508,Ndb_pruned_scan_count,This variable holds a count of the number of scans executed by NDBCLUSTER since the NDB Cluster was last started where NDBCLUSTER was able to use partition pruning.,0,0,others,mysql,0,0
5509,Ndb_pushed_queries_defined,The total number of joins pushed down to the NDB kernel for distributed handling on the data nodes.,0,0,others,mysql,0,0
5510,Ndb_pushed_queries_dropped,The number of joins that were pushed down to the NDB kernel but that could not be handled there.,0,0,others,mysql,0,0
5511,Ndb_pushed_queries_executed,The number of joins successfully pushed down to NDB and executed there.,0,0,others,mysql,0,0
5512,Ndb_pushed_reads,The number of rows returned to mysqld from the NDB kernel by joins that were pushed down.,0,0,others,mysql,0,0
5513,ndb_read_backup,Enable read from any fragment replica for any NDB table subsequently created; doing so greatly improves the table read performance at a relatively small cost to writes.,1,4,limited-side-effect,mysql,0,0
5514,ndb_recv_thread_activation_threshold,"When this number of concurrently active threads is reached, the receive thread takes over polling of the cluster connection.",0,0,others,mysql,0,1
5515,ndb_recv_thread_cpu_mask,"CPU mask for locking receiver threads to specific CPUs. This is specified as a hexadecimal bitmask. For example, 0x33 means that one CPU is used per receiver thread. An empty string is the default; setting ndb_recv_thread_cpu_mask to this value removes any receiver thread locks previously set.",1,1,resource,mysql,0,0
5516,Ndb_replica_max_replicated_epoch,"The most recently committed epoch on this replica. You can compare this value with Ndb_conflict_last_conflict_epoch; if Ndb_replica_max_replicated_epoch is the greater of the two, no conflicts have yet been detected.",0,0,others,mysql,0,0
5517,ndb_report_thresh_binlog_epoch_slip,"This represents the threshold for the number of epochs completely buffered in the event buffer, but not yet consumed by the binlog injector thread. When this degree of slippage (lag) is exceeded, an event buffer status message is reported, with BUFFERED_EPOCHS_OVER_THRESHOLD supplied as the reason (see Section23.6.2.3, “Event Buffer Reporting in the Cluster Log”). Slip is increased when an epoch is received from data nodes and buffered completely in the event buffer; it is decreased when an epoch is consumed by the binlog injector thread, it is reduced. Empty epochs are buffered and queued, and so included in this calculation only when this is enabled using the Ndb::setEventBufferQueueEmptyEpoch() method from the NDB API.",1,5,workload-specific,mysql,0,0
5518,ndb_report_thresh_binlog_mem_usage,"This is a threshold on the percentage of free memory remaining before reporting binary log status. For example, a value of 10 (the default) means that if the amount of available memory for receiving binary log data from the data nodes falls below 10%, a status message is sent to the cluster log.",1,1,resource,mysql,0,0
5519,ndb_row_checksum,"Traditionally, NDB has created tables with row checksums, which checks for hardware issues at the expense of performance. Setting ndb_row_checksum to 0 means that row checksums are not used for new or altered tables, which has a significant impact on performance for all types of queries. This variable is set to 1 by default, to provide backward-compatible behavior.",1,2,security-tradeoff,mysql,0,0
5520,Ndb_scan_count,This variable holds a count of the total number of scans executed by NDBCLUSTER since the NDB Cluster was last started.,0,0,others,mysql,0,0
5521,ndb_schema_dist_lock_wait_timeout,"Number of seconds to wait during schema distribution for the metadata lock taken on each SQL node in order to change its local data dictionary to reflect the DDL statement change. After this time has elapsed, a warning is returned to the effect that a given SQL node's data dictionary was not updated with the change. This avoids having the binary logging thread wait an excessive length of time while handling schema operations.",0,0,others,mysql,0,0
5522,ndb_schema_dist_timeout,"Number of seconds to wait before detecting a timeout during schema distribution. This can indicate that other SQL nodes are experiencing excessive activity, or that they are somehow being prevented from acquiring necessary resources at this time.",0,0,others,mysql,0,0
5523,ndb_schema_dist_upgrade_allowed,"Allow upgrading of the schema distribution table when connecting to NDB. When true (the default), this change is deferred until all SQL nodes have been upgraded to the same version of the NDB Cluster software.",0,0,others,mysql,0,0
5524,ndb_show_foreign_key_mock_tables,"Show the mock tables used by NDB to support foreign_key_checks=0. When this is enabled, extra warnings are shown when creating and dropping the tables. The real (internal) name of the table can be seen in the output of SHOW CREATE TABLE.",0,0,others,mysql,0,0
5525,ndb_slave_conflict_role,"Deprecated in NDB 8.0.23, and subject to removal in a future release. Use ndb_conflict_role instead.",0,0,others,mysql,0,1
5526,Ndb_slave_max_replicated_epoch,"The most recently committed epoch on this replica. You can compare this value with Ndb_conflict_last_conflict_epoch; if Ndb_slave_max_replicated_epoch is the greater of the two, no conflicts have yet been detected.",0,0,others,mysql,0,1
5527,Ndb_system_name,"If this MySQL Server is connected to an NDB cluster, this read-only variable shows the cluster system name. Otherwise, the value is an empty string.",0,0,others,mysql,0,0
5528,ndb_table_no_logging,"When this variable is set to ON or 1, it causes NDB tables not to be checkpointed to disk. More specifically, this setting applies to tables which are created or altered using ENGINE NDB when ndb_table_no_logging is enabled, and continues to apply for the lifetime of the table, even if ndb_table_no_logging is later changed. Suppose that A, B, C, and D are tables that we create (and perhaps also alter), and that we also change the setting for ndb_table_no_logging as shown here:",0,0,others,mysql,0,0
5529,ndb_table_temporary,"When set to ON or 1, this variable causes NDB tables not to be written to disk: This means that no table schema files are created, and that the tables are not logged.",0,0,others,mysql,0,0
5530,Ndb_trans_hint_count_session,The number of transactions using hints that have been started in the current session. Compare with Ndb_api_trans_start_count_session to obtain the proportion of all NDB transactions able to use hints.,1,1,resource,mysql,0,0
5531,ndb_use_copying_alter_table,Forces NDB to use copying of tables in the event of problems with online ALTER TABLE operations. The default value is OFF.,0,0,others,mysql,0,0
5532,ndb_use_exact_count,"Forces NDB to use a count of records during SELECT COUNT(*) query planning to speed up this type of query. The default value is OFF, which allows for faster queries overall.",0,0,others,mysql,0,0
5533,ndb_use_transactions,You can disable NDB transaction support by setting this variable's values to OFF (not recommended). The default is ON.,1,6,function-tradeoff,mysql,0,0
5534,ndb_version,"NDB engine version, as a composite integer.",0,0,others,mysql,0,0
5535,ndb_version_string,NDB engine version in ndb-x.y.z format.,0,0,others,mysql,0,0
5536,ndb_wait_connected,This option sets the period of time that the MySQL server waits for connections to NDB Cluster management and data nodes to be established before accepting MySQL client connections. The time is specified in seconds. The default value is 30.,0,0,others,mysql,0,0
5537,ndb_wait_setup,This variable shows the period of time that the MySQL server waits for the NDB storage engine to complete setup before timing out and treating NDB as unavailable. The time is specified in seconds. The default value is 30.,0,0,others,mysql,0,0
5538,ndbcluster,"The NDBCLUSTER storage engine is necessary for using NDB Cluster. If a mysqld binary includes support for the NDBCLUSTER storage engine, the engine is disabled by default. Use the --ndbcluster option to enable it. Use --skip-ndbcluster to explicitly disable the engine.",0,0,others,mysql,0,0
5539,ndb-connectstring,"When using the NDBCLUSTER storage engine, this option specifies the management server that distributes cluster configuration data. See Section23.4.3.3, “NDB Cluster Connection Strings”, for syntax.",0,0,others,mysql,0,0
5540,ndbinfo,Enables the plugin for the ndbinfo information database. By default this is ON whenever NDBCLUSTER is enabled.,0,0,others,mysql,0,0
5541,ndbinfo_database,Shows the name used for the NDB information database; the default is ndbinfo. This is a read-only variable whose value is determined at compile time.,0,0,others,mysql,0,1
5542,ndbinfo_max_rows,Used in testing and debugging only.,0,0,others,mysql,0,0
5543,ndbinfo_offline,"Place the ndbinfo database into offline mode, in which tables and views can be opened even when they do not actually exist, or when they exist but have different definitions in NDB. No rows are returned from such tables (or views).",1,4,limited-side-effect,mysql,0,0
5544,ndbinfo_show_hidden,Whether or not the ndbinfo database's underlying internal tables are shown in the mysql client. The default is OFF.,0,0,others,mysql,0,0
5545,ndbinfo_table_prefix,"The prefix used in naming the ndbinfo database's base tables (normally hidden, unless exposed by setting ndbinfo_show_hidden). This is a read-only variable whose default value is ndb$; the prefix itself is determined at compile time.",0,0,others,mysql,0,0
5546,ndbinfo_version,Shows the version of the ndbinfo engine in use; read-only.,0,0,others,mysql,0,0
5547,ndb-mgmd-host,"Can be used to set the host and port number of a single management server for the program to connect to. If the program requires node IDs or references to multiple management servers (or both) in its connection information, use the --ndb-connectstring option instead.",0,0,others,mysql,0,1
5548,ndb-optimized-node-selection,Enable optimizations for selection of nodes for transactions. Enabled by default; use --skip-ndb-optimized-node-selection to disable.,0,0,others,mysql,0,0
5549,ndb-transid-mysql-connection-map,"Enables or disables the plugin that handles the ndb_transid_mysql_connection_map table in the INFORMATION_SCHEMA database. Takes one of the values ON, OFF, or FORCE. ON (the default) enables the plugin. OFF disables the plugin, which makes ndb_transid_mysql_connection_map inaccessible. FORCE keeps the MySQL Server from starting if the plugin fails to load and start.",0,0,others,mysql,0,1
5550,no-dd-upgrade,"Prevent automatic upgrade of the data dictionary tables during the MySQL server startup process. This option is typically used when starting the MySQL server following an in-place upgrade of an existing installation to a newer MySQL version, which may include changes to data dictionary table definitions.",0,0,others,mysql,0,0
5551,no-defaults,"Do not read any option files. If program startup fails due to reading unknown options from an option file, --no-defaults can be used to prevent them from being read. This must be the first option on the command line if it is used.",0,0,others,mysql,0,0
5552,no-monitor,"(Windows only). This option suppresses the forking that is used to implement the RESTART statement: Forking enables one process to act as a monitor to the other, which acts as the server. For a server started with this option, RESTART simply exits and does not restart.",0,0,others,mysql,0,0
5553,old-style-user-limits,"Enable old-style user limits. (Before MySQL 5.0.3, account resource limits were counted separately for each host from which a user connected rather than per account row in the user table.) See Section6.2.20, “Setting Account Resource Limits”.",0,0,others,mysql,0,0
5554,original_commit_timestamp,"For internal use by replication. When re-executing a transaction on a replica, this is set to the time when the transaction was committed on the original source, measured in microseconds since the epoch. This allows the original commit timestamp to be propagated throughout a replication topology.",0,0,others,mysql,0,0
5555,original_server_version,"For internal use by replication. This session system variable holds the MySQL Server release number of the server where a transaction was originally committed (for example, 80014 for a MySQL 8.0.14 server instance). If this original server is at a release that does not support the session system variable, the value of the variable is set to 0 (UNKNOWN_SERVER_VERSION). Note that when a release number is set by the original server, the value of the variable is reset to 0 if the immediate server or any other intervening server in the replication topology does not support the session system variable, and so does not replicate its value.",0,0,others,mysql,0,0
5556,performance_schema,"The value of this variable is ON or OFF to indicate whether the Performance Schema is enabled. By default, the value is ON. At server startup, you can specify this variable with no value or a value of ON or 1 to enable it, or with a value of OFF or 0 to disable it.",1,6,function-tradeoff,mysql,0,1
5557,performance_schema_accounts_size,"The number of rows in the accounts table. If this variable is 0, the Performance Schema does not maintain connection statistics in the accounts table or status variable information in the status_by_account table.",0,0,others,mysql,0,0
5558,performance_schema_digests_size,"The maximum number of rows in the events_statements_summary_by_digest table. If this maximum is exceeded such that a digest cannot be instrumented, the Performance Schema increments the Performance_schema_digest_lost status variable.",0,0,others,mysql,0,0
5559,performance_schema_error_size,"The number of instrumented server error codes. The default value is the actual number of server error codes. Although the value can be set anywhere from 0 to its maximum, the intended use is to set it to either its default (to instrument all errors) or 0 (to instrument no errors).",0,0,others,mysql,0,0
5560,performance_schema_events_stages_history_long_size,The number of rows in the events_stages_history_long table.,0,0,others,mysql,0,0
5561,performance_schema_events_stages_history_size,The number of rows per thread in the events_stages_history table.,0,0,others,mysql,0,0
5562,performance_schema_events_statements_history_long_size,The number of rows in the events_statements_history_long table.,0,0,others,mysql,0,0
5563,performance_schema_events_statements_history_size,The number of rows per thread in the events_statements_history table.,0,0,others,mysql,0,0
5564,performance_schema_events_transactions_history_long_size,The number of rows in the events_transactions_history_long table.,0,0,others,mysql,0,1
5565,performance_schema_events_transactions_history_size,The number of rows per thread in the events_transactions_history table.,0,0,others,mysql,0,0
5566,performance_schema_events_waits_history_long_size,The number of rows in the events_waits_history_long table.,0,0,others,mysql,0,0
5567,performance_schema_events_waits_history_size,The number of rows per thread in the events_waits_history table.,0,0,others,mysql,0,0
5568,performance_schema_hosts_size,"The number of rows in the hosts table. If this variable is 0, the Performance Schema does not maintain connection statistics in the hosts table or status variable information in the status_by_host table.",0,0,others,mysql,0,0
5569,performance_schema_max_cond_classes,The maximum number of condition instruments.,0,0,others,mysql,0,0
5570,performance_schema_max_cond_instances,The maximum number of instrumented condition objects.,0,0,others,mysql,0,0
5571,performance_schema_max_digest_length,The maximum number of bytes of memory reserved per statement for computation of normalized statement digest values in the Performance Schema.,0,0,others,mysql,0,0
5572,performance_schema_max_digest_sample_age,"This variable affects statement sampling for the events_statements_summary_by_digest table. When a new table row is inserted, the statement that produced the row digest value is stored as the current sample statement associated with the digest. Thereafter, when the server sees other statements with the same digest value, it determines whether to use the new statement to replace the current sample statement (that is, whether to resample). Resampling policy is based on the comparative wait times of the current sample statement and new statement and, optionally, the age of the current sample statement:",1,5,workload-specific,mysql,0,0
5573,performance_schema_max_file_classes,The maximum number of file instruments.,0,0,others,mysql,0,0
5574,performance_schema_max_file_handles,The maximum number of opened file objects.,0,0,others,mysql,0,0
5575,performance_schema_max_file_instances,The maximum number of instrumented file objects.,0,0,others,mysql,0,0
5576,performance_schema_max_index_stat,"The maximum number of indexes for which the Performance Schema maintains statistics. If this maximum is exceeded such that index statistics are lost, the Performance Schema increments the Performance_schema_index_stat_lost status variable. The default value is autosized using the value of performance_schema_max_table_instances.",0,0,others,mysql,0,0
5577,performance_schema_max_memory_classes,The maximum number of memory instruments.,0,0,others,mysql,0,1
5578,performance_schema_max_metadata_locks,"The maximum number of metadata lock instruments. This value controls the size of the metadata_locks table. If this maximum is exceeded such that a metadata lock cannot be instrumented, the Performance Schema increments the Performance_schema_metadata_lock_lost status variable.",0,0,others,mysql,0,1
5579,performance_schema_max_mutex_classes,The maximum number of mutex instruments.,0,0,others,mysql,0,0
5580,performance_schema_max_mutex_instances,The maximum number of instrumented mutex objects.,0,0,others,mysql,0,0
5581,performance_schema_max_prepared_statements_instances,"The maximum number of rows in the prepared_statements_instances table. If this maximum is exceeded such that a prepared statement cannot be instrumented, the Performance Schema increments the Performance_schema_prepared_statements_lost status variable.",0,0,others,mysql,0,0
5582,performance_schema_max_program_instances,"The maximum number of stored programs for which the Performance Schema maintains statistics. If this maximum is exceeded, the Performance Schema increments the Performance_schema_program_lost status variable.",0,0,others,mysql,0,0
5583,performance_schema_max_rwlock_classes,The maximum number of rwlock instruments.,0,0,others,mysql,0,0
5584,performance_schema_max_rwlock_instances,The maximum number of instrumented rwlock objects.,0,0,others,mysql,0,0
5585,performance_schema_max_socket_classes,The maximum number of socket instruments.,1,1,resource,mysql,0,1
5586,performance_schema_max_socket_instances,The maximum number of instrumented socket objects.,1,1,resource,mysql,0,0
5587,performance_schema_max_sql_text_length,The maximum number of bytes used to store SQL statements.,1,1,resource,mysql,0,0
5588,performance_schema_max_stage_classes,The maximum number of stage instruments.,0,0,others,mysql,0,1
5589,performance_schema_max_statement_classes,The maximum number of statement instruments.,0,0,others,mysql,0,1
5590,performance_schema_max_statement_stack,"The maximum depth of nested stored program calls for which the Performance Schema maintains statistics. When this maximum is exceeded, the Performance Schema increments the Performance_schema_nested_statement_lost status variable for each stored program statement executed.",1,5,workload-specific,mysql,0,0
5591,performance_schema_max_table_handles,"The maximum number of opened table objects. This value controls the size of the table_handles table. If this maximum is exceeded such that a table handle cannot be instrumented, the Performance Schema increments the Performance_schema_table_handles_lost status variable.",0,0,others,mysql,0,0
5592,performance_schema_max_table_instances,The maximum number of instrumented table objects.,0,0,others,mysql,0,0
5593,performance_schema_max_table_lock_stat,"The maximum number of tables for which the Performance Schema maintains lock statistics. If this maximum is exceeded such that table lock statistics are lost, the Performance Schema increments the Performance_schema_table_lock_stat_lost status variable.",0,0,others,mysql,0,0
5594,performance_schema_max_thread_classes,The maximum number of thread instruments.,1,1,resource,mysql,0,0
5595,performance_schema_max_thread_instances,"The maximum number of instrumented thread objects. The value controls the size of the threads table. If this maximum is exceeded such that a thread cannot be instrumented, the Performance Schema increments the Performance_schema_thread_instances_lost status variable.",1,1,resource,mysql,0,0
5596,performance_schema_session_connect_attrs_size,"The amount of preallocated memory per thread reserved to hold connection attribute key-value pairs. If the aggregate size of connection attribute data sent by a client is larger than this amount, the Performance Schema truncates the attribute data, increments the Performance_schema_session_connect_attrs_lost status variable, and writes a message to the error log indicating that truncation occurred if the log_error_verbosity system variable is greater than 1. A _truncated attribute is also added to the session attributes with a value indicating how many bytes were lost, if the attribute buffer has sufficient space. This enables the Performance Schema to expose per-connection truncation information in the connection attribute tables. This information can be examined without having to check the error log.",1,1,resource,mysql,0,0
5597,performance_schema_setup_actors_size,The number of rows in the setup_actors table.,1,1,resource,mysql,0,0
5598,performance_schema_setup_objects_size,The number of rows in the setup_objects table.,0,0,others,mysql,0,0
5599,performance_schema_show_processlist,"The SHOW PROCESSLIST statement provides process information by collecting thread data from all active threads. The performance_schema_show_processlist variable determines which SHOW PROCESSLIST implementation to use. The default implementation iterates across active threads from within the thread manager while holding a global mutex. This has negative performance consequences, particularly on busy systems. The alternative SHOW PROCESSLIST implementation is based on the Performance Schema processlist table. This implementation queries active thread data from the Performance Schema rather than the thread manager and does not require a mutex.",1,5,workload-specific,mysql,0,1
5600,performance_schema_users_size,"The number of rows in the users table. If this variable is 0, the Performance Schema does not maintain connection statistics in the users table or status variable information in the status_by_user table.",0,0,others,mysql,0,0
5601,performance-schema-consumer-events-stages-current,Configure the events-stages-current consumer.,0,0,others,mysql,0,0
5602,performance-schema-consumer-events-stages-history,Configure the events-stages-history consumer.,0,0,others,mysql,0,0
5603,performance-schema-consumer-events-stages-history-long,Configure the events-stages-history-long consumer.,0,0,others,mysql,0,1
5604,performance-schema-consumer-events-statements-current,Configure the events-statements-current consumer.,0,0,others,mysql,0,1
5605,performance-schema-consumer-events-statements-history,Configure the events-statements-history consumer.,0,0,others,mysql,0,0
5606,performance-schema-consumer-events-statements-history-long,Configure the events-statements-history-long consumer.,0,0,others,mysql,0,1
5607,performance-schema-consumer-events-transactions-current,Configure the Performance Schema events-transactions-current consumer.,0,0,others,mysql,0,0
5608,performance-schema-consumer-events-transactions-history,Configure the Performance Schema events-transactions-history consumer.,0,0,others,mysql,0,1
5609,performance-schema-consumer-events-transactions-history-long,Configure the Performance Schema events-transactions-history-long consumer.,0,0,others,mysql,0,0
5610,performance-schema-consumer-events-waits-current,Configure the events-waits-current consumer.,0,0,others,mysql,0,0
5611,performance-schema-consumer-events-waits-history,Configure the events-waits-history consumer.,0,0,others,mysql,0,0
5612,performance-schema-consumer-events-waits-history-long,Configure the events-waits-history-long consumer.,0,0,others,mysql,0,0
5613,performance-schema-consumer-global-instrumentation,Configure the global-instrumentation consumer.,0,0,others,mysql,0,0
5614,performance-schema-consumer-statements-digest,Configure the statements-digest consumer.,0,0,others,mysql,0,0
5615,performance-schema-consumer-thread-instrumentation,Configure the thread-instrumentation consumer.,0,0,others,mysql,0,0
5616,performance-schema-instrument,Configure a Performance Schema instrument. The name may be given as a pattern to configure instruments that match the pattern.,0,0,others,mysql,0,0
5617,plugin_load,"This option tells the server to load the named plugins at startup. If multiple --plugin-load options are given, only the last one applies. Additional plugins to load may be specified using --plugin-load-add options.",0,0,others,mysql,0,0
5618,plugin_load_add,This option complements the --plugin-load option. --plugin-load-add adds a plugin or plugins to the set of plugins to be loaded at startup. The argument format is the same as for --plugin-load. --plugin-load-add can be used to avoid specifying a large set of plugins as a single long unwieldy --plugin-load argument.,0,0,others,mysql,0,1
5619,plugin-xxx,"Specifies an option that pertains to a server plugin. For example, many storage engines can be built as plugins, and for such engines, options for them can be specified with a --plugin prefix. Thus, the --innodb-file-per-table option for InnoDB can be specified as --plugin-innodb-file-per-table.",0,0,others,mysql,0,0
5620,port,"The port number to use when listening for TCP/IP connections. On Unix and Unix-like systems, the port number must be 1024 or higher unless the server is started by the root operating system user. Setting this option to 0 causes the default value to be used.",0,0,others,mysql,0,0
5621,port-open-timeout,"On some systems, when the server is stopped, the TCP/IP port might not become available immediately. If the server is restarted quickly afterward, its attempt to reopen the port can fail. This option indicates how many seconds the server should wait for the TCP/IP port to become free if it cannot be opened. The default is not to wait.",0,0,others,mysql,0,0
5622,print-defaults,"Print the program name and all options that it gets from option files. Password values are masked. This must be the first option on the command line if it is used, except that it may be used immediately after --defaults-file or --defaults-extra-file.",0,0,others,mysql,0,0
5623,relay_log,"The base name for relay log files. For the default replication channel, the default base name for relay logs is host_name-relay-bin. For non-default replication channels, the default base name for relay logs is host_name-relay-bin-channel, where channel is the name of the replication channel recorded in this relay log.",0,0,others,mysql,0,1
5624,relay_log_basename,Holds the base name and complete path to the relay log file. The maximum variable length is 256. This variable is set by the server and is read only.,0,0,others,mysql,0,1
5625,relay_log_index,"The name for the relay log index file. The maximum variable length is 256. If you do not specify this variable, but the relay_log system variable is specified, its value is used as the default base name for the relay log index file. If relay_log is also not specified, then for the default replication channel, the default name is host_name-relay-bin.index, using the name of the host machine. For non-default replication channels, the default name is host_name-relay-bin-channel.index, where channel is the name of the replication channel recorded in this relay log index.",0,0,others,mysql,0,0
5626,relay_log_info_file,"The use of this system variable is now deprecated. It was used to set the file name for the replica's applier metadata repository if relay_log_info_repository=FILE was set. relay_log_info_file and the use of the relay_log_info_repository system variable are deprecated because the use of a file for the applier metadata repository has been superseded by crash-safe tables. For information about the applier metadata repository, see Section17.2.4.2, “Replication Metadata Repositories”.",0,0,others,mysql,0,0
5627,relay_log_info_repository,"The use of this system variable is now deprecated. The setting TABLE is the default, and is required when multiple replication channels are configured. The TABLE setting for the replica's applier metadata repository is also required to make replication resilient to unexpected halts. See Section17.4.2, “Handling an Unexpected Halt of a Replica” for more information. The alternative setting FILE was previously deprecated.",0,0,others,mysql,0,0
5628,relay_log_purge,Disables or enables automatic purging of relay log files as soon as they are not needed any more. The default value is 1 (ON).This is a global variable that can be changed dynamically with SET GLOBAL relay_log_purge = N. Disabling purging of relay logs when enabling the --relay-log-recovery option puts data consistency at risk.,1,3,reliability-tradeoff,mysql,0,0
5629,relay_log_recovery,"If enabled, this variable enables automatic relay log recovery immediately following server startup. The recovery process creates a new relay log file, initializes the SQL thread position to this new relay log, and initializes the I/O thread to the SQL thread position. Reading of the relay log from the source then continues.",0,0,others,mysql,0,0
5630,relay_log_space_limit,The maximum amount of space to use for all relay logs.,0,0,others,mysql,0,0
5631,remove,"(Windows only) Remove a MySQL Windows service. The default service name is MySQL if no service_name value is given. For more information, see Section2.3.4.8, “Starting MySQL as a Windows Service”.",0,0,others,mysql,0,0
5632,replica_allow_batching,"Whether or not batched updates are enabled on NDB Cluster replicas. From MySQL 8.0.26, use replica_allow_batching in place of slave_allow_batching, which is deprecated from that release. In releases before MySQL 8.0.26, use slave_allow_batching.",1,6,function-tradeoff,mysql,0,0
5633,replica_checkpoint_group,replica_checkpoint_group sets the maximum number of transactions that can be processed by a multithreaded replica before a checkpoint operation is called to update its status as shown by SHOW REPLICA STATUS. Setting this variable has no effect on replicas for which multithreading is not enabled. Setting this variable has no immediate effect. The state of the variable applies on all subsequent START REPLICA commands.,0,0,others,mysql,0,0
5634,replica_checkpoint_period,"replica_checkpoint_period sets the maximum time (in milliseconds) that is allowed to pass before a checkpoint operation is called to update the status of a multithreaded replica as shown by SHOW REPLICA STATUS. Setting this variable has no effect on replicas for which multithreading is not enabled. Setting this variable takes effect for all replication channels immediately, including running channels.",1,5,workload-specific,mysql,0,0
5635,replica_compressed_protocol,"replica_compressed_protocol specifies whether to use compression of the source/replica connection protocol if both source and replica support it. If this variable is disabled (the default), connections are uncompressed. Changes to this variable take effect on subsequent connection attempts; this includes after issuing a START REPLICA statement, as well as reconnections made by a running replication I/O thread.",1,4,limited-side-effect,mysql,0,1
5636,replica_exec_mode,replica_exec_mode controls how a replication thread resolves conflicts and errors during replication. IDEMPOTENT mode causes suppression of duplicate-key and no-key-found errors; STRICT means no such suppression takes place.,0,0,others,mysql,0,0
5637,replica_load_tmpdir,"replica_load_tmpdir specifies the name of the directory where the replica creates temporary files. Setting this variable takes effect for all replication channels immediately, including running channels. The variable value is by default equal to the value of the tmpdir system variable, or the default that applies when that system variable is not specified.",0,0,others,mysql,0,0
5638,replica_max_allowed_packet,"replica_max_allowed_packet sets the maximum packet size in bytes that the replication SQL and I/O threads can handle. Setting this variable takes effect for all replication channels immediately, including running channels. It is possible for a source to write binary log events longer than its max_allowed_packet setting once the event header is added. The setting for replica_max_allowed_packet must be larger than the max_allowed_packet setting on the source, so that large updates using row-based replication do not cause replication to fail.",0,0,others,mysql,0,0
5639,replica_net_timeout,"replica_net_timeout specifies the number of seconds to wait for more data or a heartbeat signal from the source before the replica considers the connection broken, aborts the read, and tries to reconnect. Setting this variable has no immediate effect. The state of the variable applies on all subsequent START REPLICA commands.",0,0,others,mysql,0,0
5640,replica_parallel_type,"For multithreaded replicas (replicas on which replica_parallel_workers or slave_parallel_workers is set to a value greater than 0), replica_parallel_type specifies the policy used to decide which transactions are allowed to execute in parallel on the replica. The variable has no effect on replicas for which multithreading is not enabled.",1,5,workload-specific,mysql,0,0
5641,replica_parallel_workers,"replica_parallel_workers enables multithreading on the replica and sets the number of applier threads for executing replication transactions in parallel. When the value is a number greater than 0, the replica is a multithreaded replica with the specified number of applier threads, plus a coordinator thread to manage them. If you are using multiple replication channels, each channel has this number of threads.",1,1,resource,mysql,0,0
5642,replica_pending_jobs_size_max,"For multithreaded replicas, this variable sets the maximum amount of memory (in bytes) available to applier queues holding events not yet applied. Setting this variable has no effect on replicas for which multithreading is not enabled. Setting this variable has no immediate effect. The state of the variable applies on all subsequent START REPLICA commands.",0,0,others,mysql,0,1
5643,replica_preserve_commit_order,"For multithreaded replicas (replicas on which replica_parallel_workers is set to a value greater than 0), setting replica_preserve_commit_order=1 ensures that transactions are executed and committed on the replica in the same order as they appear in the replica's relay log. This prevents gaps in the sequence of transactions that have been executed from the replica's relay log, and preserves the same transaction history on the replica as on the source (with the limitations listed below). This variable has no effect on replicas for which multithreading is not enabled.",1,3,reliability-tradeoff,mysql,0,1
5644,replica_skip_errors,"Normally, replication stops when an error occurs on the replica, which gives you the opportunity to resolve the inconsistency in the data manually. This variable causes the replication SQL thread to continue replication when a statement returns any of the errors listed in the variable value.",1,3,reliability-tradeoff,mysql,0,0
5645,replica_sql_verify_checksum,"slave_sql_verify_checksum causes the replication SQL thread to verify data using the checksums read from the relay log. In the event of a mismatch, the replica stops with an error. Setting this variable takes effect for all replication channels immediately, including running channels.",1,3,reliability-tradeoff,mysql,0,0
5646,replica_transaction_retries,"replica_transaction_retries sets the maximum number of times for replication SQL threads on a single-threaded or multithreaded replica to automatically retry failed transactions before stopping. Setting this variable takes effect for all replication channels immediately, including running channels. The default value is 10. Setting the variable to 0 disables automatic retrying of transactions.",0,0,others,mysql,0,0
5647,replica_type_conversions,"replica_type_conversions controls the type conversion mode in effect on the replica when using row-based replication. Its value is a comma-delimited set of zero or more elements from the list: ALL_LOSSY, ALL_NON_LOSSY, ALL_SIGNED, ALL_UNSIGNED. Set this variable to an empty string to disallow type conversions between the source and the replica. Setting this variable takes effect for all replication channels immediately, including running channels.",0,0,others,mysql,0,0
5648,replicate-do-db,Creates a replication filter using the name of a database. Such filters can also be created using CHANGE REPLICATION FILTER REPLICATE_DO_DB.,0,0,others,mysql,0,0
5649,replicate-do-table,"Creates a replication filter by telling the replication SQL thread to restrict replication to a given table. To specify more than one table, use this option multiple times, once for each table. This works for both cross-database updates and default database updates, in contrast to --replicate-do-db. See Section17.2.5, “How Servers Evaluate Replication Filtering Rules”. You can also create such a filter by issuing a CHANGE REPLICATION FILTER REPLICATE_DO_TABLE statement.",0,0,others,mysql,0,0
5650,replicate-ignore-db,Creates a replication filter using the name of a database. Such filters can also be created using CHANGE REPLICATION FILTER REPLICATE_IGNORE_DB.,0,0,others,mysql,0,0
5651,replicate-ignore-table,"Creates a replication filter by telling the replication SQL thread not to replicate any statement that updates the specified table, even if any other tables might be updated by the same statement. To specify more than one table to ignore, use this option multiple times, once for each table. This works for cross-database updates, in contrast to --replicate-ignore-db. See Section17.2.5, “How Servers Evaluate Replication Filtering Rules”. You can also create such a filter by issuing a CHANGE REPLICATION FILTER REPLICATE_IGNORE_TABLE statement.",0,0,others,mysql,0,0
5652,replicate-rewrite-db,"Tells the replica to create a replication filter that translates the specified database to to_name if it was from_name on the source. Only statements involving tables are affected, not statements such as CREATE DATABASE, DROP DATABASE, and ALTER DATABASE.",1,6,function-tradeoff,mysql,0,0
5653,replicate-same-server-id,"This option is for use on replicas. The default is 0 (FALSE). With this option set to 1 (TRUE), the replica does not skip events that have its own server ID. This setting is normally useful only in rare configurations.",0,0,others,mysql,0,0
5654,replicate-wild-do-table,"Creates a replication filter by telling the replication SQL thread to restrict replication to statements where any of the updated tables match the specified database and table name patterns. Patterns can contain the % and _ wildcard characters, which have the same meaning as for the LIKE pattern-matching operator. To specify more than one table, use this option multiple times, once for each table. This works for cross-database updates. See Section17.2.5, “How Servers Evaluate Replication Filtering Rules”. You can also create such a filter by issuing a CHANGE REPLICATION FILTER REPLICATE_WILD_DO_TABLE statement.",0,0,others,mysql,0,1
5655,replicate-wild-ignore-table,"Creates a replication filter which keeps the replication SQL thread from replicating a statement in which any table matches the given wildcard pattern. To specify more than one table to ignore, use this option multiple times, once for each table. This works for cross-database updates. See Section17.2.5, “How Servers Evaluate Replication Filtering Rules”. You can also create such a filter by issuing a CHANGE REPLICATION FILTER REPLICATE_WILD_IGNORE_TABLE statement.",0,0,others,mysql,0,1
5656,replication_optimize_for_static_plugin_config,"Use shared locks, and avoid unnecessary lock acquisitions, to improve performance for semisynchronous replication. This setting and replication_sender_observe_commit_only help as the number of replicas increases, because contention for locks can slow down performance. While this system variable is enabled, the semisynchronous replication plugin cannot be uninstalled, so you must disable the system variable before the uninstall can complete.",1,4,limited-side-effect,mysql,0,0
5657,replication_sender_observe_commit_only,"Limit callbacks to improve performance for semisynchronous replication. This setting and replication_optimize_for_static_plugin_config help as the number of replicas increases, because contention for locks can slow down performance.",1,4,limited-side-effect,mysql,0,0
5658,report_host,The host name or IP address of the replica to be reported to the source during replica registration. This value appears in the output of SHOW REPLICAS on the source server. Leave the value unset if you do not want the replica to register itself with the source.,0,0,others,mysql,0,0
5659,report_password,The account password of the replica to be reported to the source during replica registration. This value appears in the output of SHOW REPLICAS on the source server if the source was started with --show-replica-auth-info or --show-slave-auth-info.,0,0,others,mysql,0,0
5660,report_port,"The TCP/IP port number for connecting to the replica, to be reported to the source during replica registration. Set this only if the replica is listening on a nondefault port or if you have a special tunnel from the source or other clients to the replica. If you are not sure, do not use this option.",0,0,others,mysql,0,0
5661,report_user,The account user name of the replica to be reported to the source during replica registration. This value appears in the output of SHOW REPLICAS on the source server if the source was started with --show-replica-auth-info or --show-slave-auth-info.,0,0,others,mysql,0,0
5662,rewriter_enabled,Whether the Rewriter query rewrite plugin is enabled.,1,5,workload-specific,mysql,0,0
5663,Rewriter_number_loaded_rules,The number of rewrite plugin rewrite rules successfully loaded from the rewrite_rules table into memory for use by the Rewriter plugin.,0,0,others,mysql,0,0
5664,Rewriter_number_reloads,The number of times the rewrite_rules table has been loaded into the in-memory cache used by the Rewriter plugin.,1,1,resource,mysql,0,1
5665,Rewriter_number_rewritten_queries,The number of queries rewritten by the Rewriter query rewrite plugin since it was loaded.,1,1,resource,mysql,0,0
5666,Rewriter_reload_error,"Whether an error occurred the most recent time that the rewrite_rules table was loaded into the in-memory cache used by the Rewriter plugin. If the value is OFF, no error occurred. If the value is ON, an error occurred; check the message column of the rewriter_rules table for error messages.",0,0,others,mysql,0,0
5667,rewriter_verbose,For internal use.,0,0,others,mysql,0,1
5668,rpl_read_size,"The rpl_read_size system variable controls the minimum amount of data in bytes that is read from the binary log files and relay log files. If heavy disk I/O activity for these files is impeding performance for the database, increasing the read size might reduce file reads and I/O stalls when the file data is not currently cached by the operating system.",1,5,workload-specific,mysql,0,0
5669,rpl_semi_sync_master_enabled,"Controls whether semisynchronous replication is enabled on the source server. To enable or disable the plugin, set this variable to ON or OFF (or 1 or 0), respectively. The default is OFF.",0,0,others,mysql,0,0
5670,rpl_semi_sync_master_timeout,A value in milliseconds that controls how long the source waits on a commit for acknowledgment from a replica before timing out and reverting to asynchronous replication. The default value is 10000 (10 seconds).,0,0,others,mysql,0,1
5671,rpl_semi_sync_master_trace_level,The semisynchronous replication debug trace level on the source server. Four levels are defined:,0,0,others,mysql,0,0
5672,rpl_semi_sync_master_wait_for_slave_count,"The number of replica acknowledgments the source must receive per transaction before proceeding. By default rpl_semi_sync_master_wait_for_slave_count is 1, meaning that semisynchronous replication proceeds after receiving a single replica acknowledgment. Performance is best for small values of this variable.",1,1,resource,mysql,0,0
5673,rpl_semi_sync_master_wait_no_slave,"Controls whether the source waits for the timeout period configured by rpl_semi_sync_master_timeout to expire, even if the replica count drops to less than the number of replicas configured by rpl_semi_sync_master_wait_for_slave_count during the timeout period.",0,0,others,mysql,0,0
5674,rpl_semi_sync_master_wait_point,This variable controls the point at which a semisynchronous replication source server waits for replica acknowledgment of transaction receipt before returning a status to the client that committed the transaction. These values are permitted:,0,0,others,mysql,0,0
5675,rpl_semi_sync_replica_enabled,"rpl_semi_sync_replica_enabled is available when the rpl_semi_sync_replica (semisync_replica.so library) plugin was installed on the replica to set up semisynchronous replication. If the rpl_semi_sync_slave plugin (semisync_slave.so library) was installed, rpl_semi_sync_slave_enabled is available instead.",0,0,others,mysql,0,0
5676,rpl_semi_sync_replica_trace_level,"rpl_semi_sync_replica_trace_level is available when the rpl_semi_sync_replica (semisync_replica.so library) plugin was installed on the replica to set up semisynchronous replication. If the rpl_semi_sync_slave plugin (semisync_slave.so library) was installed, rpl_semi_sync_slave_trace_level is available instead.",0,0,others,mysql,0,1
5677,rpl_semi_sync_slave_enabled,"rpl_semi_sync_slave_enabled is available when the rpl_semi_sync_slave (semisync_slave.so library) plugin was installed on the replica to set up semisynchronous replication. If the rpl_semi_sync_replica plugin (semisync_replica.so library) was installed, rpl_semi_sync_replica_enabled is available instead.",0,0,others,mysql,0,0
5678,rpl_semi_sync_slave_trace_level,"rpl_semi_sync_slave_trace_level is available when the rpl_semi_sync_slave (semisync_slave.so library) plugin was installed on the replica to set up semisynchronous replication. If the rpl_semi_sync_replica plugin (semisync_replica.so library) was installed, rpl_semi_sync_replica_trace_level is available instead.",0,0,others,mysql,0,0
5679,rpl_semi_sync_source_enabled,"rpl_semi_sync_source_enabled is available when the rpl_semi_sync_source (semisync_source.so library) plugin was installed on the replica to set up semisynchronous replication. If the rpl_semi_sync_master plugin (semisync_master.so library) was installed, rpl_semi_sync_master_enabled is available instead.",0,0,others,mysql,0,1
5680,rpl_semi_sync_source_timeout,"rpl_semi_sync_source_timeout is available when the rpl_semi_sync_source (semisync_source.so library) plugin was installed on the replica to set up semisynchronous replication. If the rpl_semi_sync_master plugin (semisync_master.so library) was installed, rpl_semi_sync_master_timeout is available instead.",0,0,others,mysql,0,1
5681,rpl_semi_sync_source_trace_level,"rpl_semi_sync_source_trace_level is available when the rpl_semi_sync_source (semisync_source.so library) plugin was installed on the replica to set up semisynchronous replication. If the rpl_semi_sync_master plugin (semisync_master.so library) was installed, rpl_semi_sync_master_trace_level is available instead.",0,0,others,mysql,0,0
5682,rpl_semi_sync_source_wait_for_replica_count,"rpl_semi_sync_source_wait_for_replica_count is available when the rpl_semi_sync_source (semisync_source.so library) plugin was installed on the replica to set up semisynchronous replication. If the rpl_semi_sync_master plugin (semisync_master.so library) was installed, rpl_semi_sync_master_wait_for_slave_count is available instead.",0,0,others,mysql,0,0
5683,rpl_semi_sync_source_wait_no_replica,"rpl_semi_sync_source_wait_no_replica is available when the rpl_semi_sync_source (semisync_source.so library) plugin was installed on the replica to set up semisynchronous replication. If the rpl_semi_sync_master plugin (semisync_master.so library) was installed, rpl_semi_sync_source_wait_no_replica is available instead.",0,0,others,mysql,0,0
5684,rpl_semi_sync_source_wait_point,"rpl_semi_sync_source_wait_point is available when the rpl_semi_sync_source (semisync_source.so library) plugin was installed on the replica to set up semisynchronous replication. If the rpl_semi_sync_master plugin (semisync_master.so library) was installed, rpl_semi_sync_master_wait_point is available instead.",0,0,others,mysql,0,1
5685,rpl_stop_replica_timeout,You can control the length of time (in seconds) that STOP REPLICA waits before timing out by setting this variable. This can be used to avoid deadlocks between STOP REPLICA and other SQL statements using different client connections to the replica.,0,0,others,mysql,0,0
5686,rpl_stop_slave_timeout,You can control the length of time (in seconds) that STOP REPLICA waits before timing out by setting this variable. This can be used to avoid deadlocks between STOP REPLICA and other SQL statements using different client connections to the replica.,0,0,others,mysql,0,0
5687,safe-user-create,"If this option is enabled, a user cannot create new MySQL users by using the GRANT statement unless the user has the INSERT privilege for the mysql.user system table or any column in the table. If you want a user to have the ability to create new users that have those privileges that the user has the right to grant, you should grant the user the following privilege:",0,0,others,mysql,0,0
5688,server_id,"The following sections contain information about mysqld options and server variables that are used in replication and for controlling the binary log. Options and variables for use on sources and replicas are covered separately, as are options and variables relating to binary logging and global transaction identifiers (GTIDs). A set of quick-reference tables providing basic information about these options and variables is also included.",0,0,others,mysql,0,1
5689,server_id_bits,"This variable indicates the number of least significant bits within the 32-bit server_id which actually identify the server. Indicating that the server is actually identified by fewer than 32 bits makes it possible for some of the remaining bits to be used for other purposes, such as storing user data generated by applications using the NDB API's Event API within the AnyValue of an OperationOptions structure (NDB Cluster uses the AnyValue to store the server ID).",0,0,others,mysql,0,1
5690,server_uuid,"The following sections contain information about mysqld options and server variables that are used in replication and for controlling the binary log. Options and variables for use on sources and replicas are covered separately, as are options and variables relating to binary logging and global transaction identifiers (GTIDs). A set of quick-reference tables providing basic information about these options and variables is also included.",0,0,others,mysql,0,0
5691,show-replica-auth-info,"The options display replication user names and passwords in the output of SHOW REPLICAS (or before MySQL 8.0.22, SHOW SLAVE HOSTS) on the source for replicas started with the --report-user and --report-password options.",0,0,others,mysql,0,0
5692,show-slave-auth-info,Use this option before MySQL 8.0.26 rather than --show-replica-auth-info. Both options have the same effect.,0,0,others,mysql,0,1
5693,skip_replica_start,"--skip-replica-start tells the replica server not to start the replication I/O (receiver) and SQL (applier) threads when the server starts. To start the threads later, use a START REPLICA statement. You can use the skip_replica_start system variable in place of the command line option to allow access to this feature using MySQL Server’s privilege structure, so that database administrators do not need any privileged access to the operating system.",0,0,others,mysql,0,0
5694,skip_show_database,"This option sets the skip_show_database system variable that controls who is permitted to use the SHOW DATABASES statement. See Section5.1.8, “Server System Variables”.",0,0,others,mysql,0,0
5695,skip_slave_start,"Tells the replica server not to start the replication I/O and SQL threads when the server starts. To start the threads later, use a START REPLICA statement.",0,0,others,mysql,0,1
5696,skip-character-set-client-handshake,"Do not ignore character set information sent by the client. To ignore client information and use the default server character set, use --skip-character-set-client-handshake; this makes MySQL behave like MySQL 4.0.",0,0,others,mysql,0,0
5697,skip-grant-tables,"This option causes the server not to read the grant tables in the mysql system schema, and thus to start without using the privilege system at all. This gives anyone with access to the server unrestricted access to all databases.",0,0,others,mysql,0,1
5698,skip-host-cache,"Disable use of the internal host cache for faster name-to-IP resolution. With the cache disabled, the server performs a DNS lookup every time a client connects.",1,6,function-tradeoff,mysql,0,0
5699,skip-ndbcluster,"Disable the NDBCLUSTER storage engine. This is the default for binaries that were built with NDBCLUSTER storage engine support; the server allocates memory and other resources for this storage engine only if the --ndbcluster option is given explicitly. See Section23.4.1, “Quick Test Setup of NDB Cluster”, for an example.",0,0,others,mysql,0,0
5700,skip-new,"This option disables (what used to be considered) new, possibly unsafe behaviors. It results in these settings: delay_key_write=OFF, concurrent_insert=NEVER, automatic_sp_privileges=OFF. It also causes OPTIMIZE TABLE to be mapped to ALTER TABLE for storage engines for which OPTIMIZE TABLE is not supported.",1,2,security-tradeoff,mysql,0,0
5701,skip-ssl,The --ssl option specifies that the server permits but does not require encrypted connections on the main connection interface. This option is enabled by default.,1,2,security-tradeoff,mysql,0,0
5702,skip-stack-trace,"Do not write stack traces. This option is useful when you are running mysqld under a debugger. On some systems, you also must use this option to get a core file.",1,4,limited-side-effect,mysql,0,0
5703,slave_allow_batching,"Whether or not batched updates are enabled on NDB Cluster replicas. From MySQL 8.0.26, slave_allow_batching is deprecated and the alias replica_allow_batching should be used instead. In releases before MySQL 8.0.26, use slave_allow_batching.",1,6,function-tradeoff,mysql,0,0
5704,slave_checkpoint_group,slave_checkpoint_group sets the maximum number of transactions that can be processed by a multithreaded replica before a checkpoint operation is called to update its status as shown by SHOW REPLICA STATUS. Setting this variable has no effect on replicas for which multithreading is not enabled. Setting this variable has no immediate effect. The state of the variable applies on all subsequent START REPLICA commands.,0,0,others,mysql,0,0
5705,slave_checkpoint_period,"slave_checkpoint_period sets the maximum time (in milliseconds) that is allowed to pass before a checkpoint operation is called to update the status of a multithreaded replica as shown by SHOW REPLICA STATUS. Setting this variable has no effect on replicas for which multithreading is not enabled. Setting this variable takes effect for all replication channels immediately, including running channels.",0,0,others,mysql,0,0
5706,slave_compressed_protocol,"slave_compressed_protocol is deprecated, and from MySQL 8.0.26, the alias replica_compressed_protocol should be used instead. In releases before MySQL 8.0.26, use slave_compressed_protocol.",0,0,others,mysql,0,0
5707,slave_exec_mode,slave_exec_mode controls how a replication thread resolves conflicts and errors during replication. IDEMPOTENT mode causes suppression of duplicate-key and no-key-found errors; STRICT means no such suppression takes place.,0,0,others,mysql,0,0
5708,slave_load_tmpdir,"slave_load_tmpdir specifies the name of the directory where the replica creates temporary files. Setting this variable takes effect for all replication channels immediately, including running channels. The variable value is by default equal to the value of the tmpdir system variable, or the default that applies when that system variable is not specified.",0,0,others,mysql,0,0
5709,slave_max_allowed_packet,"slave_max_allowed_packet sets the maximum packet size in bytes that the replication SQL and I/O threads can handle. Setting this variable takes effect for all replication channels immediately, including running channels. It is possible for a source to write binary log events longer than its max_allowed_packet setting once the event header is added. The setting for slave_max_allowed_packet must be larger than the max_allowed_packet setting on the source, so that large updates using row-based replication do not cause replication to fail.",0,0,others,mysql,0,0
5710,slave_net_timeout,"slave_net_timeout specifies the number of seconds to wait for more data or a heartbeat signal from the source before the replica considers the connection broken, aborts the read, and tries to reconnect. Setting this variable has no immediate effect. The state of the variable applies on all subsequent START REPLICA commands.",0,0,others,mysql,0,0
5711,slave_parallel_type,"For multithreaded replicas (replicas on which replica_parallel_workers or slave_parallel_workers is set to a value greater than 0), slave_parallel_type specifies the policy used to decide which transactions are allowed to execute in parallel on the replica. The variable has no effect on replicas for which multithreading is not enabled.",0,0,others,mysql,0,0
5712,slave_parallel_workers,"slave_parallel_workers enables multithreading on the replica and sets the number of applier threads for executing replication transactions in parallel. When the value is a number greater than 0, the replica is a multithreaded replica with the specified number of applier threads, plus a coordinator thread to manage them. If you are using multiple replication channels, each channel has this number of threads.",1,1,resource,mysql,0,0
5713,slave_pending_jobs_size_max,"For multithreaded replicas, this variable sets the maximum amount of memory (in bytes) available to applier queues holding events not yet applied. Setting this variable has no effect on replicas for which multithreading is not enabled. Setting this variable has no immediate effect. The state of the variable applies on all subsequent START REPLICA commands.",0,0,others,mysql,0,0
5714,slave_preserve_commit_order,"For multithreaded replicas (replicas on which replica_parallel_workers or slave_parallel_workers is set to a value greater than 0), setting slave_preserve_commit_order=1 ensures that transactions are executed and committed on the replica in the same order as they appear in the replica's relay log. This prevents gaps in the sequence of transactions that have been executed from the replica's relay log, and preserves the same transaction history on the replica as on the source (with the limitations listed below). This variable has no effect on replicas for which multithreading is not enabled.",1,3,reliability-tradeoff,mysql,0,0
5715,slave_rows_search_algorithms,"When preparing batches of rows for row-based logging and replication, this system variable controls how the rows are searched for matches, in particular whether hash scans are used. The use of this system variable is now deprecated. The default setting INDEX_SCAN,HASH_SCAN is optimal for performance and works correctly in all scenarios.",0,0,others,mysql,0,0
5716,slave_skip_errors,"Normally, replication stops when an error occurs on the replica, which gives you the opportunity to resolve the inconsistency in the data manually. This option causes the replication SQL thread to continue replication when a statement returns any of the errors listed in the option value.",1,3,reliability-tradeoff,mysql,0,0
5717,slave_sql_verify_checksum,"slave_sql_verify_checksum causes the replication SQL thread to verify data using the checksums read from the relay log. In the event of a mismatch, the replica stops with an error. Setting this variable takes effect for all replication channels immediately, including running channels.",1,2,security-tradeoff,mysql,0,0
5718,slave_transaction_retries,"slave_transaction_retries sets the maximum number of times for replication SQL threads on a single-threaded or multithreaded replica to automatically retry failed transactions before stopping. Setting this variable takes effect for all replication channels immediately, including running channels. The default value is 10. Setting the variable to 0 disables automatic retrying of transactions.",0,0,others,mysql,0,0
5719,slave_type_conversions,"slave_type_conversions controls the type conversion mode in effect on the replica when using row-based replication. Its value is a comma-delimited set of zero or more elements from the list: ALL_LOSSY, ALL_NON_LOSSY, ALL_SIGNED, ALL_UNSIGNED. Set this variable to an empty string to disallow type conversions between the source and the replica. Setting this variable takes effect for all replication channels immediately, including running channels.",0,0,others,mysql,0,0
5720,slave-sql-verify-checksum,"When this option is enabled, the replica examines checksums read from the relay log. In the event of a mismatch, the replica stops with an error.",1,2,security-tradeoff,mysql,0,0
5721,slow-start-timeout,"This option controls the Windows service control manager's service start timeout. The value is the maximum number of milliseconds that the service control manager waits before trying to kill the windows service during startup. The default value is 15000 (15 seconds). If the MySQL service takes too long to start, you may need to increase this value. A value of 0 means there is no timeout.",0,0,others,mysql,0,0
5722,socket,"On Unix, this option specifies the Unix socket file to use when listening for local connections. The default value is /tmp/mysql.sock. If this option is given, the server creates the file in the data directory unless an absolute path name is given to specify a different directory. On Windows, the option specifies the pipe name to use when listening for local connections that use a named pipe. The default value is MySQL (not case-sensitive).",0,0,others,mysql,0,0
5723,source_verify_checksum,"Enabling source_verify_checksum causes the source to verify events read from the binary log by examining checksums, and to stop with an error in the event of a mismatch. source_verify_checksum is disabled by default; in this case, the source uses the event length from the binary log to verify events, so that only complete events are read from the binary log.",1,2,security-tradeoff,mysql,0,0
5724,sporadic-binlog-dump-fail,This option is used internally by the MySQL test suite for replication testing and debugging.,0,0,others,mysql,0,1
5725,sql_log_bin,"This variable controls whether logging to the binary log is enabled for the current session (assuming that the binary log itself is enabled). The default value is ON. To disable or enable binary logging for the current session, set the session sql_log_bin variable to OFF or ON.",0,0,others,mysql,0,0
5726,sql_mode,Set the SQL mode.,0,0,others,mysql,0,0
5727,sql_replica_skip_counter,"sql_replica_skip_counter specifies the number of events from the source that a replica should skip. Setting the option has no immediate effect. The variable applies to the next START REPLICA statement; the next START REPLICA statement also changes the value back to 0. When this variable is set to a nonzero value and there are multiple replication channels configured, the START REPLICA statement can only be used with the FOR CHANNEL channel clause.",0,0,others,mysql,0,0
5728,sql_slave_skip_counter,"sql_slave_skip_counter specifies the number of events from the source that a replica should skip. Setting the option has no immediate effect. The variable applies to the next START REPLICA statement; the next START REPLICA statement also changes the value back to 0. When this variable is set to a nonzero value and there are multiple replication channels configured, the START REPLICA statement can only be used with the FOR CHANNEL channel clause.",0,0,others,mysql,0,1
5729,ssl,The --ssl option specifies that the server permits but does not require encrypted connections on the main connection interface. This option is enabled by default.,1,2,security-tradeoff,mysql,0,1
5730,standalone,Available on Windows only; instructs the MySQL server not to run as a service.,0,0,others,mysql,0,0
5731,super-large-pages,"Standard use of large pages in MySQL attempts to use the largest size supported, up to 4MB. Under Solaris, a “super large pages” feature enables uses of pages up to 256MB. This feature is available for recent SPARC platforms. It can be enabled or disabled by using the --super-large-pages or --skip-super-large-pages option.",1,4,limited-side-effect,mysql,0,0
5732,symbolic-links,"Enable or disable symbolic link support. On Unix, enabling symbolic links means that you can link a MyISAM index file or data file to another directory with the INDEX DIRECTORY or DATA DIRECTORY option of the CREATE TABLE statement. If you delete or rename the table, the files that its symbolic links point to also are deleted or renamed. See Section8.12.2.2, “Using Symbolic Links for MyISAM Tables on Unix”.",0,0,others,mysql,0,0
5733,sync_binlog,Controls how often the MySQL server synchronizes the binary log to disk.,1,5,workload-specific,mysql,0,1
5734,sync_master_info,"sync_master_info specifies the number of events after which the replica updates the connection metadata repository. When the connection metadata repository is stored as an InnoDB table, which is the default from MySQL 8.0, it is updated after this number of events. If the connection metadata repository is stored as a file, which is deprecated from MySQL 8.0, the replica synchronizes its master.info file to disk (using fdatasync()) after this number of events. The default value is 10000, and a zero value means that the repository is never updated. Setting this variable takes effect for all replication channels immediately, including running channels.",1,3,reliability-tradeoff,mysql,0,0
5735,sync_relay_log,"If the value of this variable is greater than 0, the MySQL server synchronizes its relay log to disk (using fdatasync()) after every sync_relay_log events are written to the relay log. Setting this variable takes effect for all replication channels immediately, including running channels.",1,3,reliability-tradeoff,mysql,0,0
5736,sync_relay_log_info,"The number of transactions after which the replica updates the applier metadata repository. When the applier metadata repository is stored as an InnoDB table, which is the default from MySQL 8.0, it is updated after every transaction and this system variable is ignored. If the applier metadata repository is stored as a file, which is deprecated from MySQL 8.0, the replica synchronizes its relay-log.info file to disk (using fdatasync()) after this number of transactions. The default value for sync_relay_log_info is 10000, and a zero value means that the file contents are only flushed by the operating system. Setting this variable takes effect for all replication channels immediately, including running channels.",1,3,reliability-tradeoff,mysql,0,0
5737,sync_source_info,"sync_source_info specifies the number of events after which the replica updates the connection metadata repository. When the connection metadata repository is stored as an InnoDB table, which is the default from MySQL 8.0, it is updated after this number of events. If the connection metadata repository is stored as a file, which is deprecated from MySQL 8.0, the replica synchronizes its master.info file to disk (using fdatasync()) after this number of events. The default value is 10000, and a zero value means that the repository is never updated. Setting this variable takes effect for all replication channels immediately, including running channels.",1,3,reliability-tradeoff,mysql,0,0
5738,sysdate-is-now,"SYSDATE() by default returns the time at which it executes, not the time at which the statement in which it occurs begins executing. This differs from the behavior of NOW(). This option causes SYSDATE() to be a synonym for NOW(). For information about the implications for binary logging and replication, see the description for SYSDATE() in Section12.7, “Date and Time Functions” and for SET TIMESTAMP in Section5.1.8, “Server System Variables”.",0,0,others,mysql,0,1
5739,tc-heuristic-recover,The decision to use in a manual heuristic recovery.,1,2,security-tradeoff,mysql,0,0
5740,terminology_use_previous,"In MySQL 8.0.26, incompatible changes were made to instrumentation names containing the terms “master”, which is changed to “source”, “slave”, which is changed to “replica”, and “mts” (for “multithreaded slave”), which is changed to “mta” (for “multithreaded applier”). Monitoring tools that work with these instrumentation names might be impacted. If the incompatible changes have an impact for you, set the terminology_use_previous system variable to BEFORE_8_0_26 to make MySQL Server use the old versions of the names for the objects specified in the previous list. This enables monitoring tools that rely on the old names to continue working until they can be updated to use the new names.",0,0,others,mysql,0,0
5741,tmpdir,The path of the directory to use for creating temporary files. It might be useful if your default /tmp directory resides on a partition that is too small to hold temporary tables. This option accepts several paths that are used in round-robin fashion. Paths should be separated by colon characters (:) on Unix and semicolon characters (;) on Windows.,0,0,others,mysql,0,0
5742,transaction_allow_batching,"When set to 1 or ON, this variable enables batching of statements within the same transaction. To use this variable, autocommit must first be disabled by setting it to 0 or OFF; otherwise, setting transaction_allow_batching has no effect.",1,4,limited-side-effect,mysql,0,0
5743,transaction_isolation,"Sets the default transaction isolation level. The level value can be READ-UNCOMMITTED, READ-COMMITTED, REPEATABLE-READ, or SERIALIZABLE.",1,3,reliability-tradeoff,mysql,0,0
5744,transaction_read_only,"Sets the default transaction access mode. By default, read-only mode is disabled, so the mode is read/write.",0,0,others,mysql,0,0
5745,transaction_write_set_extraction,"This system variable specifies the algorithm used to hash the writes extracted during a transaction. The default in MySQL 8.0 is that transaction_write_set_extraction is set to XXHASH64. OFF means that write sets are not collected. transaction_write_set_extraction is deprecated from MySQL 8.0.26, and will be removed in a future MySQL release.",1,6,function-tradeoff,mysql,0,0
5746,upgrade,This option controls whether and how the server performs an automatic upgrade at startup. Automatic upgrade involves two steps:,0,0,others,mysql,0,0
5747,user,"Run the mysqld server as the user having the name user_name or the numeric user ID user_id. (“User” in this context refers to a system login account, not a MySQL user listed in the grant tables.)",0,0,others,mysql,0,0
5748,validate_password.check_user_name,Whether validate_password compares passwords to the user name part of the effective user account for the current session and rejects them if they match. This variable is unavailable unless validate_password is installed.,0,0,others,mysql,0,0
5749,validate_password.dictionary_file,The path name of the dictionary file that validate_password uses for checking passwords. This variable is unavailable unless validate_password is installed.,0,0,others,mysql,0,1
5750,validate_password.dictionary_file_last_parsed,When the dictionary file was last parsed. This variable is unavailable unless validate_password is installed.,0,0,others,mysql,0,0
5751,validate_password.dictionary_file_words_count,The number of words read from the dictionary file. This variable is unavailable unless validate_password is installed.,0,0,others,mysql,0,0
5752,validate_password.length,The minimum number of characters that validate_password requires passwords to have. This variable is unavailable unless validate_password is installed.,0,0,others,mysql,0,0
5753,validate_password.mixed_case_count,The minimum number of lowercase and uppercase characters that validate_password requires passwords to have if the password policy is MEDIUM or stronger. This variable is unavailable unless validate_password is installed.,0,0,others,mysql,0,0
5754,validate_password.number_count,The minimum number of numeric (digit) characters that validate_password requires passwords to have if the password policy is MEDIUM or stronger. This variable is unavailable unless validate_password is installed.,0,0,others,mysql,0,0
5755,validate_password.policy,The password policy enforced by validate_password. This variable is unavailable unless validate_password is installed.,0,0,others,mysql,0,1
5756,validate_password.special_char_count,The minimum number of nonalphanumeric characters that validate_password requires passwords to have if the password policy is MEDIUM or stronger. This variable is unavailable unless validate_password is installed.,0,0,others,mysql,0,0
5757,validate_password_check_user_name,This validate_password plugin system variable is deprecated; expect it to be removed in a future version of MySQL. Use the corresponding validate_password.check_user_name system variable of the validate_password component instead.,0,0,others,mysql,0,1
5758,validate_password_dictionary_file,This validate_password plugin system variable is deprecated; expect it to be removed in a future version of MySQL. Use the corresponding validate_password.dictionary_file system variable of the validate_password component instead.,0,0,others,mysql,0,1
5759,validate_password_dictionary_file_last_parsed,This validate_password plugin status variable is deprecated; expect it to be removed in a future version of MySQL. Use the corresponding validate_password.dictionary_file_last_parsed status variable of the validate_password component instead.,0,0,others,mysql,0,1
5760,validate_password_dictionary_file_words_count,This validate_password plugin status variable is deprecated; expect it to be removed in a future version of MySQL. Use the corresponding validate_password.dictionary_file_words_count status variable of the validate_password component instead.,0,0,others,mysql,0,0
5761,validate_password_length,This validate_password plugin system variable is deprecated; expect it to be removed in a future version of MySQL. Use the corresponding validate_password.length system variable of the validate_password component instead.,0,0,others,mysql,0,0
5762,validate_password_mixed_case_count,This validate_password plugin system variable is deprecated; expect it to be removed in a future version of MySQL. Use the corresponding validate_password.mixed_case_count system variable of the validate_password component instead.,0,0,others,mysql,0,0
5763,validate_password_number_count,This validate_password plugin system variable is deprecated; expect it to be removed in a future version of MySQL. Use the corresponding validate_password.number_count system variable of the validate_password component instead.,0,0,others,mysql,0,1
5764,validate_password_policy,This validate_password plugin system variable is deprecated; expect it to be removed in a future version of MySQL. Use the corresponding validate_password.policy system variable of the validate_password component instead.,0,0,others,mysql,0,0
5765,validate_password_special_char_count,This validate_password plugin system variable is deprecated; expect it to be removed in a future version of MySQL. Use the corresponding validate_password.special_char_count system variable of the validate_password component instead.,0,0,others,mysql,0,0
5766,validate-config,"Validate the server startup configuration. If no errors are found, the server terminates with an exit code of 0. If an error is found, the server displays a diagnostic message and terminates with an exit code of 1. Warning and information messages may also be displayed, depending on the log_error_verbosity value, but do not produce immediate validation termination or an exit code of 1.",0,0,others,mysql,0,0
5767,validate-password,"This option controls how the server loads the deprecated validate_password plugin at startup. The value should be one of those available for plugin-loading options, as described in Section5.6.1, “Installing and Uninstalling Plugins”. For example, --validate-password=FORCE_PLUS_PERMANENT tells the server to load the plugin at startup and prevents it from being removed while the server is running.",0,0,others,mysql,0,1
5768,validate-user-plugins,"If this option is enabled (the default), the server checks each user account and produces a warning if conditions are found that would make the account unusable. Enabling --validate-user-plugins slows down server initialization and FLUSH PRIVILEGES. If you do not require the additional checking, you can disable this option at startup to avoid the performance decrement.",1,2,security-tradeoff,mysql,0,0
5769,verbose,Use this option with the --help option for detailed help.,0,0,others,mysql,0,0
5770,version_tokens_session,The session value of this variable specifies the client version token list and indicates the tokens that the client session requires the server version token list to have.,0,0,others,mysql,0,1
5771,version_tokens_session_number,This variable is for internal use.,0,0,others,mysql,0,0
5772,absolute_redirect,"If disabled, redirects issued by nginx will be relative.",0,0,others,nginx,0,1
5773,accept_mutex,"If accept_mutex is enabled, worker processes will accept new connections by turn. Otherwise, all worker processes will be notified about new connections, and if volume of new connections is low, some of the worker processes may just waste system resources.",1,4,limited-side-effect,nginx,0,1
5774,accept_mutex_delay,"If accept_mutex is enabled, specifies the maximum time during which a worker process will try to restart accepting new connections if another worker process is currently accepting new connections.",0,0,others,nginx,0,0
5775,access_log.B,"Sets the path, format, and configuration for a buffered log write. Several logs can be specified on the same configuration level. Logging to syslog can be configured by specifying the syslog: prefix in the first parameter. The special value off cancels all access_log directives on the current level. If the format is not specified then the predefined combined format is used.",0,0,others,nginx,0,0
5776,access_log,"Sets the path, format, and configuration for a buffered log write. Several logs can be specified on the same configuration level. Logging to syslog can be configured by specifying the syslog: prefix in the first parameter. The special value off cancels all access_log directives on the current level.",0,0,others,nginx,0,0
5777,add_after_body,"Adds the text returned as a result of processing a given subrequest after the response body. An empty string ("""") as a parameter cancels addition inherited from the previous configuration level.",0,0,others,nginx,0,1
5778,add_before_body,"Adds the text returned as a result of processing a given subrequest before the response body. An empty string ("""") as a parameter cancels addition inherited from the previous configuration level.",0,0,others,nginx,0,0
5779,add_header,"Adds the specified field to a response header provided that the response code equals 200, 201 (1.3.10), 204, 206, 301, 302, 303, 304, 307 (1.1.16, 1.0.13), or 308 (1.13.0). Parameter value can contain variables.",0,0,others,nginx,0,0
5780,add_trailer,"Adds the specified field to the end of a response provided that the response code equals 200, 201, 206, 301, 302, 303, 307, or 308. Parameter value can contain variables.",0,0,others,nginx,0,1
5781,addition_types,"Allows adding text in responses with the specified MIME types, in addition to text/html. The special value * matches any MIME type (0.8.29).",0,0,others,nginx,0,0
5782,aio,Enables or disables the use of asynchronous file I/O (AIO) on FreeBSD and Linux:,1,6,function-tradeoff,nginx,0,0
5783,aio_write,"If aio is enabled, specifies whether it is used for writing files. Currently, this only works when using aio threads and is limited to writing temporary files with data received from proxied servers.",0,0,others,nginx,0,1
5784,alias,"Defines a replacement for the specified location. For example, with the following configuration",0,0,others,nginx,0,0
5785,allow.B,"Allows access for the specified network or address. If the special value unix: is specified (1.5.1), allows access for all UNIX-domain sockets.",0,0,others,nginx,0,0
5786,allow,"Allows access for the specified network or address. If the special value unix: is specified, allows access for all UNIX-domain sockets.",0,0,others,nginx,0,0
5787,ancient_browser,"If any of the specified substrings is found in the User-Agent request header field, the browser will be considered ancient. The special string netscape4 corresponds to the regular expression ^Mozilla/[1-4].",0,0,others,nginx,0,0
5788,ancient_browser_value,Sets a value for the $ancient_browser variables.,0,0,others,nginx,0,0
5789,api,Turns on the REST API interface in the surrounding location. Access to this location should be limited.,0,0,others,nginx,0,0
5790,auth_basic,"Enables validation of user name and password using the HTTP Basic Authentication protocol. The specified parameter is used as a realm. Parameter value can contain variables (1.3.10, 1.2.7). The special value off cancels the effect of the auth_basic directive inherited from the previous configuration level.",0,0,others,nginx,0,1
5791,auth_basic_user_file,"Specifies a file that keeps user names and passwords, in the following format:",0,0,others,nginx,0,1
5792,auth_delay,"Delays processing of unauthorized requests with 401 response code to prevent timing attacks when access is limited by password, by the result of subrequest, or by JWT.",0,0,others,nginx,0,1
5793,auth_http,Sets the URL of the HTTP authentication server. The protocol is described below.,0,0,others,nginx,0,0
5794,auth_http_header,Appends the specified header to requests sent to the authentication server. This header can be used as the shared secret to verify that the request comes from nginx. For example:,0,0,others,nginx,0,0
5795,auth_http_pass_client_cert,Appends the Auth-SSL-Cert header with the client certificate in the PEM format (urlencoded) to requests sent to the authentication server.,0,0,others,nginx,0,0
5796,auth_http_timeout,Sets the timeout for communication with the authentication server.,0,0,others,nginx,0,1
5797,auth_jwt,Enables validation of JSON Web Token. The specified string is used as a realm. Parameter value can contain variables.,1,2,security-tradeoff,nginx,0,0
5798,auth_jwt_claim_set,"Sets the variable to a JWT claim parameter identified by key names. Name matching starts from the top level of the JSON tree. For arrays, the variable keeps a list of array elements separated by commas.",0,0,others,nginx,0,0
5799,auth_jwt_header_set,"Sets the variable to a JOSE header parameter identified by key names. Name matching starts from the top level of the JSON tree. For arrays, the variable keeps a list of array elements separated by commas.",0,0,others,nginx,0,1
5800,auth_jwt_key_file,Specifies a file in JSON Web Key Set format for validating JWT signature. Parameter value can contain variables.,0,0,others,nginx,0,0
5801,auth_jwt_key_request,"Allows retrieving a JSON Web Key Set file from a subrequest for validating JWT signature and sets the URI where the subrequest will be sent to. Parameter value can contain variables. To avoid validation overhead, it is recommended to cache the key file:",0,0,others,nginx,0,0
5802,auth_jwt_leeway,Sets the maximum allowable leeway to compensate clock skew when verifying the exp and nbf JWT claims.,0,0,others,nginx,0,1
5803,auth_jwt_type,Specifies which type of JSON Web Token to expect: JWS (signed) or JWE (encrypted).,1,2,security-tradeoff,nginx,0,0
5804,auth_request,Enables authorization based on the result of a subrequest and sets the URI to which the subrequest will be sent.,0,0,others,nginx,0,1
5805,auth_request_set,"Sets the request variable to the given value after the authorization request completes. The value may contain variables from the authorization request, such as $upstream_http_*.",0,0,others,nginx,0,0
5806,autoindex,Enables or disables the directory listing output.,0,0,others,nginx,0,0
5807,autoindex_exact_size,"For the HTML format, specifies whether exact file sizes should be output in the directory listing, or rather rounded to kilobytes, megabytes, and gigabytes.",0,0,others,nginx,0,0
5808,autoindex_format,Sets the format of a directory listing.,0,0,others,nginx,0,0
5809,autoindex_localtime,"For the HTML format, specifies whether times in the directory listing should be output in the local time zone or UTC.",0,0,others,nginx,0,0
5810,break,Stops processing the current set of ngx_http_rewrite_module directives.,0,0,others,nginx,0,0
5811,charset,"Adds the specified charset to the Content-Type response header field. If this charset is different from the charset specified in the source_charset directive, a conversion is performed.",0,0,others,nginx,0,0
5812,charset_map,"Describes the conversion table from one charset to another. A reverse conversion table is built using the same data. Character codes are given in hexadecimal. Missing characters in the range 80-FF are replaced with ?. When converting from UTF-8, characters missing in a one-byte charset are replaced with &#XXXX;.",0,0,others,nginx,0,0
5813,charset_types,Enables module processing in responses with the specified MIME types in addition to text/html. The special value * matches any MIME type (0.8.29).,0,0,others,nginx,0,0
5814,chunked_transfer_encoding,Allows disabling chunked transfer encoding in HTTP/1.1. It may come in handy when using a software failing to support chunked encoding despite the standards requirement.,0,0,others,nginx,0,0
5815,client_body_buffer_size,"Sets buffer size for reading client request body. In case the request body is larger than the buffer, the whole body or only its part is written to a temporary file. By default, buffer size is equal to two memory pages. This is 8K on x86, other 32-bit platforms, and x86-64. It is usually 16K on other 64-bit platforms.",1,1,resource,nginx,0,1
5816,client_body_in_file_only,"Determines whether nginx should save the entire client request body into a file. This directive can be used during debugging, or when using the $request_body_file variable, or the $r->request_body_file method of the module ngx_http_perl_module.",0,0,others,nginx,0,0
5817,client_body_in_single_buffer,"Determines whether nginx should save the entire client request body in a single buffer. The directive is recommended when using the $request_body variable, to save the number of copy operations involved.",1,6,function-tradeoff,nginx,0,0
5818,client_body_temp_path,"Defines a directory for storing temporary files holding client request bodies. Up to three-level subdirectory hierarchy can be used under the specified directory. For example, in the following configuration",0,0,others,nginx,0,1
5819,client_body_timeout,"Defines a timeout for reading client request body. The timeout is set only for a period between two successive read operations, not for the transmission of the whole request body. If a client does not transmit anything within this time, the request is terminated with the 408 (Request Time-out) error.",0,0,others,nginx,0,0
5820,client_header_buffer_size,"Sets buffer size for reading client request header. For most requests, a buffer of 1K bytes is enough. However, if a request includes long cookies, or comes from a WAP client, it may not fit into 1K. If a request line or a request header field does not fit into this buffer then larger buffers, configured by the large_client_header_buffers directive, are allocated.",1,1,resource,nginx,0,1
5821,client_header_timeout,"Defines a timeout for reading client request header. If a client does not transmit the entire header within this time, the request is terminated with the 408 (Request Time-out) error.",0,0,others,nginx,0,1
5822,client_max_body_size,"Sets the maximum allowed size of the client request body. If the size in a request exceeds the configured value, the 413 (Request Entity Too Large) error is returned to the client. Please be aware that browsers cannot correctly display this error. Setting size to 0 disables checking of client request body size.",1,1,resource,nginx,0,0
5823,connection_pool_size,"Allows accurate tuning of per-connection memory allocations. This directive has minimal impact on performance and should not generally be used. By default, the size is equal to 256 bytes on 32-bit platforms and 512 bytes on 64-bit platforms.",1,5,workload-specific,nginx,0,0
5824,create_full_put_path,The WebDAV specification only allows creating files in already existing directories. This directive allows creating all needed intermediate directories.,0,0,others,nginx,0,0
5825,daemon,Determines whether nginx should become a daemon. Mainly used during development.,0,0,others,nginx,0,0
5826,dav_access,"Sets access permissions for newly created files and directories, e.g.:",0,0,others,nginx,0,1
5827,dav_methods,"Allows the specified HTTP and WebDAV methods. The parameter off denies all methods processed by this module. The following methods are supported: PUT, DELETE, MKCOL, COPY, and MOVE.",0,0,others,nginx,0,0
5828,debug_connection,"Enables debugging log for selected client connections. Other connections will use logging level set by the error_log directive. Debugged connections are specified by IPv4 or IPv6 (1.3.0, 1.2.1) address or network. A connection may also be specified using a hostname. For connections using UNIX-domain sockets (1.3.0, 1.2.1), debugging log is enabled by the unix: parameter.",0,0,others,nginx,0,0
5829,debug_points,This directive is used for debugging.,0,0,others,nginx,0,0
5830,default_type,Defines the default MIME type of a response. Mapping of file name extensions to MIME types can be set with the types directive.,0,0,others,nginx,0,0
5831,deny.B,"Denies access for the specified network or address. If the special value unix: is specified (1.5.1), denies access for all UNIX-domain sockets.",0,0,others,nginx,0,1
5832,deny,"Denies access for the specified network or address. If the special value unix: is specified, denies access for all UNIX-domain sockets.",0,0,others,nginx,0,1
5833,directio,"Enables the use of the O_DIRECT flag (FreeBSD, Linux), the F_NOCACHE flag (macOS), or the directio() function (Solaris), when reading files that are larger than or equal to the specified size. The directive automatically disables (0.7.15) the use of sendfile for a given request. It can be useful for serving large files:",0,0,others,nginx,0,0
5834,directio_alignment,"Sets the alignment for directio. In most cases, a 512-byte alignment is enough. However, when using XFS under Linux, it needs to be increased to 4K.",0,0,others,nginx,0,0
5835,disable_symlinks,Determines how symbolic links should be treated when opening files:,0,0,others,nginx,0,0
5836,empty_gif,Turns on module processing in a surrounding location.,0,0,others,nginx,0,1
5837,env,"By default, nginx removes all environment variables inherited from its parent process except the TZ variable. This directive allows preserving some of the inherited variables, changing their values, or creating new environment variables. These variables are then:",0,0,others,nginx,0,0
5838,error_log,"Configures logging. Several logs can be specified on the same configuration level (1.5.2). If on the main configuration level writing a log to a file is not explicitly defined, the default file will be used.",0,0,others,nginx,0,0
5839,error_page,Defines the URI that will be shown for the specified errors. A uri value can contain variables.,0,0,others,nginx,0,0
5840,etag,Enables or disables automatic generation of the ETag response header field for static resources.,0,0,others,nginx,0,0
5841,events,Provides the configuration file context in which the directives that affect connection processing are specified.,0,0,others,nginx,0,1
5842,expires,"Enables or disables adding or modifying the Expires and Cache-Control response header fields provided that the response code equals 200, 201 (1.3.10), 204, 206, 301, 302, 303, 304, 307 (1.1.16, 1.0.13), or 308 (1.13.0). The parameter can be a positive or negative time.",0,0,others,nginx,0,0
5843,f4f,Turns on module processing in the surrounding location.,1,6,function-tradeoff,nginx,0,0
5844,f4f_buffer_size,Sets the size of the buffer used for reading the .f4x index file.,1,1,resource,nginx,0,0
5845,fastcgi_bind,"Makes outgoing connections to a FastCGI server originate from the specified local IP address with an optional port (1.11.2). Parameter value can contain variables (1.3.12). The special value off (1.3.12) cancels the effect of the fastcgi_bind directive inherited from the previous configuration level, which allows the system to auto-assign the local IP address and port.",1,4,limited-side-effect,nginx,0,0
5846,fastcgi_buffer_size,"Sets the size of the buffer used for reading the first part of the response received from the FastCGI server. This part usually contains a small response header. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform. It can be made smaller, however.",1,1,resource,nginx,0,0
5847,fastcgi_buffering,Enables or disables buffering of responses from the FastCGI server.,1,6,function-tradeoff,nginx,0,0
5848,fastcgi_buffers,"Sets the number and size of the buffers used for reading a response from the FastCGI server, for a single connection. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform.",1,1,resource,nginx,0,0
5849,fastcgi_busy_buffers_size,"When buffering of responses from the FastCGI server is enabled, limits the total size of buffers that can be busy sending a response to the client while the response is not yet fully read. In the meantime, the rest of the buffers can be used for reading the response and, if needed, buffering part of the response to a temporary file. By default, size is limited by the size of two buffers set by the fastcgi_buffer_size and fastcgi_buffers directives.",1,1,resource,nginx,0,0
5850,fastcgi_cache,Defines a shared memory zone used for caching. The same zone can be used in several places. Parameter value can contain variables (1.7.9). The off parameter disables caching inherited from the previous configuration level.,0,0,others,nginx,0,1
5851,fastcgi_cache_background_update,"Allows starting a background subrequest to update an expired cache item, while a stale cached response is returned to the client. Note that it is necessary to allow the usage of a stale cached response when it is being updated.",1,6,function-tradeoff,nginx,0,0
5852,fastcgi_cache_bypass,Defines conditions under which the response will not be taken from a cache. If at least one value of the string parameters is not empty and is not equal to 0 then the response will not be taken from the cache:,0,0,others,nginx,0,0
5853,fastcgi_cache_key,"Defines a key for caching, for example",0,0,others,nginx,0,0
5854,fastcgi_cache_lock,"When enabled, only one request at a time will be allowed to populate a new cache element identified according to the fastcgi_cache_key directive by passing a request to a FastCGI server. Other requests of the same cache element will either wait for a response to appear in the cache or the cache lock for this element to be released, up to the time set by the fastcgi_cache_lock_timeout directive.",1,4,limited-side-effect,nginx,0,0
5855,fastcgi_cache_lock_age,"If the last request passed to the FastCGI server for populating a new cache element has not completed for the specified time, one more request may be passed to the FastCGI server.",0,0,others,nginx,0,0
5856,fastcgi_cache_lock_timeout,"Sets a timeout for fastcgi_cache_lock. When the time expires, the request will be passed to the FastCGI server, however, the response will not be cached.",0,0,others,nginx,0,0
5857,fastcgi_cache_max_range_offset,"Sets an offset in bytes for byte-range requests. If the range is beyond the offset, the range request will be passed to the FastCGI server and the response will not be cached.",1,5,workload-specific,nginx,0,1
5858,fastcgi_cache_methods,"If the client request method is listed in this directive then the response will be cached. GET and HEAD methods are always added to the list, though it is recommended to specify them explicitly. See also the fastcgi_no_cache directive.",0,0,others,nginx,0,0
5859,fastcgi_cache_min_uses,Sets the number of requests after which the response will be cached.,1,5,workload-specific,nginx,0,0
5860,fastcgi_cache_path,"Sets the path and other parameters of a cache. Cache data are stored in files. Both the key and file name in a cache are a result of applying the MD5 function to the proxied URL. The levels parameter defines hierarchy levels of a cache: from 1 to 3, each level accepts values 1 or 2. For example, in the following configuration",0,0,others,nginx,0,0
5861,fastcgi_cache_purge,Defines conditions under which the request will be considered a cache purge request. If at least one value of the string parameters is not empty and is not equal to 0 then the cache entry with a corresponding cache key is removed. The result of successful operation is indicated by returning the 204 (No Content) response.,0,0,others,nginx,0,0
5862,fastcgi_cache_revalidate,Enables revalidation of expired cache items using conditional requests with the If-Modified-Since and If-None-Match header fields.,1,2,security-tradeoff,nginx,0,1
5863,fastcgi_cache_use_stale,Determines in which cases a stale cached response can be used when an error occurs during communication with the FastCGI server. The directives parameters match the parameters of the fastcgi_next_upstream directive.,0,0,others,nginx,0,0
5864,fastcgi_cache_valid,"Sets caching time for different response codes. For example, the following directives",0,0,others,nginx,0,1
5865,fastcgi_catch_stderr,"Sets a string to search for in the error stream of a response received from a FastCGI server. If the string is found then it is considered that the FastCGI server has returned an invalid response. This allows handling application errors in nginx, for example:",0,0,others,nginx,0,0
5866,fastcgi_connect_timeout,Defines a timeout for establishing a connection with a FastCGI server. It should be noted that this timeout cannot usually exceed 75 seconds.,0,0,others,nginx,0,0
5867,fastcgi_force_ranges,Enables byte-range support for both cached and uncached responses from the FastCGI server regardless of the Accept-Ranges field in these responses.,1,4,limited-side-effect,nginx,0,0
5868,fastcgi_hide_header,"By default, nginx does not pass the header fields Status and X-Accel-... from the response of a FastCGI server to a client. The fastcgi_hide_header directive sets additional fields that will not be passed. If, on the contrary, the passing of fields needs to be permitted, the fastcgi_pass_header directive can be used.",0,0,others,nginx,0,0
5869,fastcgi_ignore_client_abort,Determines whether the connection with a FastCGI server should be closed when a client closes the connection without waiting for a response.,0,0,others,nginx,0,0
5870,fastcgi_ignore_headers,"Disables processing of certain response header fields from the FastCGI server. The following fields can be ignored: X-Accel-Redirect, X-Accel-Expires, X-Accel-Limit-Rate (1.1.6), X-Accel-Buffering (1.1.6), X-Accel-Charset (1.1.6), Expires, Cache-Control, Set-Cookie (0.8.44), and Vary (1.7.7).",0,0,others,nginx,0,0
5871,fastcgi_index,"Sets a file name that will be appended after a URI that ends with a slash, in the value of the $fastcgi_script_name variable. For example, with these settings",0,0,others,nginx,0,0
5872,fastcgi_intercept_errors,Determines whether FastCGI server responses with codes greater than or equal to 300 should be passed to a client or be intercepted and redirected to nginx for processing with the error_page directive.,0,0,others,nginx,0,0
5873,fastcgi_keep_conn,"By default, a FastCGI server will close a connection right after sending the response. However, when this directive is set to the value on, nginx will instruct a FastCGI server to keep connections open. This is necessary, in particular, for keepalive connections to FastCGI servers to function.",0,0,others,nginx,0,0
5874,fastcgi_limit_rate,"Limits the speed of reading the response from the FastCGI server. The rate is specified in bytes per second. The zero value disables rate limiting. The limit is set per a request, and so if nginx simultaneously opens two connections to the FastCFI server, the overall rate will be twice as much as the specified limit. The limitation works only if buffering of responses from the FastCGI server is enabled.",0,0,others,nginx,0,1
5875,fastcgi_max_temp_file_size,"When buffering of responses from the FastCGI server is enabled, and the whole response does not fit into the buffers set by the fastcgi_buffer_size and fastcgi_buffers directives, a part of the response can be saved to a temporary file. This directive sets the maximum size of the temporary file. The size of data written to the temporary file at a time is set by the fastcgi_temp_file_write_size directive.",1,5,workload-specific,nginx,0,0
5876,fastcgi_next_upstream,Specifies in which cases a request should be passed to the next server:,0,0,others,nginx,0,1
5877,fastcgi_next_upstream_timeout,Limits the time during which a request can be passed to the next server. The 0 value turns off this limitation.,0,0,others,nginx,0,0
5878,fastcgi_next_upstream_tries,Limits the number of possible tries for passing a request to the next server. The 0 value turns off this limitation.,0,0,others,nginx,0,0
5879,fastcgi_no_cache,Defines conditions under which the response will not be saved to a cache. If at least one value of the string parameters is not empty and is not equal to 0 then the response will not be saved:,0,0,others,nginx,0,1
5880,fastcgi_param,"Sets a parameter that should be passed to the FastCGI server. The value can contain text, variables, and their combination. These directives are inherited from the previous configuration level if and only if there are no fastcgi_param directives defined on the current level.",0,0,others,nginx,0,0
5881,fastcgi_pass,"Sets the address of a FastCGI server. The address can be specified as a domain name or IP address, and a port:",0,0,others,nginx,0,0
5882,fastcgi_pass_header,Permits passing otherwise disabled header fields from a FastCGI server to a client.,0,0,others,nginx,0,0
5883,fastcgi_pass_request_body,Indicates whether the original request body is passed to the FastCGI server. See also the fastcgi_pass_request_headers directive.,0,0,others,nginx,0,0
5884,fastcgi_pass_request_headers,Indicates whether the header fields of the original request are passed to the FastCGI server. See also the fastcgi_pass_request_body directive.,0,0,others,nginx,0,0
5885,fastcgi_read_timeout,"Defines a timeout for reading a response from the FastCGI server. The timeout is set only between two successive read operations, not for the transmission of the whole response. If the FastCGI server does not transmit anything within this time, the connection is closed.",0,0,others,nginx,0,0
5886,fastcgi_request_buffering,Enables or disables buffering of a client request body.,1,6,function-tradeoff,nginx,0,0
5887,fastcgi_send_lowat,"If the directive is set to a non-zero value, nginx will try to minimize the number of send operations on outgoing connections to a FastCGI server by using either NOTE_LOWAT flag of the kqueue method, or the SO_SNDLOWAT socket option, with the specified size.",1,4,limited-side-effect,nginx,0,0
5888,fastcgi_send_timeout,"Sets a timeout for transmitting a request to the FastCGI server. The timeout is set only between two successive write operations, not for the transmission of the whole request. If the FastCGI server does not receive anything within this time, the connection is closed.",0,0,others,nginx,0,0
5889,fastcgi_socket_keepalive,"Configures the TCP keepalive behavior for outgoing connections to a FastCGI server. By default, the operating systems settings are in effect for the socket. If the directive is set to the value on, the SO_KEEPALIVE socket option is turned on for the socket.",0,0,others,nginx,0,1
5890,fastcgi_split_path_info,"Defines a regular expression that captures a value for the $fastcgi_path_info variable. The regular expression should have two captures: the first becomes a value of the $fastcgi_script_name variable, the second becomes a value of the $fastcgi_path_info variable. For example, with these settings",0,0,others,nginx,0,0
5891,fastcgi_store,"Enables saving of files to a disk. The on parameter saves files with paths corresponding to the directives alias or root. The off parameter disables saving of files. In addition, the file name can be set explicitly using the string with variables:",1,6,function-tradeoff,nginx,0,0
5892,fastcgi_store_access,"Sets access permissions for newly created files and directories, e.g.:",0,0,others,nginx,0,1
5893,fastcgi_temp_file_write_size,"Limits the size of data written to a temporary file at a time, when buffering of responses from the FastCGI server to temporary files is enabled. By default, size is limited by two buffers set by the fastcgi_buffer_size and fastcgi_buffers directives. The maximum size of a temporary file is set by the fastcgi_max_temp_file_size directive.",1,5,workload-specific,nginx,0,1
5894,fastcgi_temp_path,"Defines a directory for storing temporary files with data received from FastCGI servers. Up to three-level subdirectory hierarchy can be used underneath the specified directory. For example, in the following configuration",0,0,others,nginx,0,0
5895,flv,Turns on module processing in a surrounding location.,1,6,function-tradeoff,nginx,0,0
5896,geo.B,"Describes the dependency of values of the specified variable on the client IP address. By default, the address is taken from the $remote_addr variable, but it can also be taken from another variable (0.7.27), for example:",0,0,others,nginx,0,0
5897,geo,"Describes the dependency of values of the specified variable on the client IP address. By default, the address is taken from the $remote_addr variable, but it can also be taken from another variable, for example:",0,0,others,nginx,0,0
5898,geoip_city,"Specifies a database used to determine the country, region, and city depending on the client IP address. The following variables are available when using this database:",0,0,others,nginx,0,0
5899,geoip_country,Specifies a database used to determine the country depending on the client IP address. The following variables are available when using this database:,0,0,others,nginx,0,0
5900,geoip_org,Specifies a database used to determine the organization depending on the client IP address. The following variable is available when using this database:,0,0,others,nginx,0,0
5901,geoip_proxy,"Defines trusted addresses. When a request comes from a trusted address, an address from the X-Forwarded-For request header field will be used instead.",0,0,others,nginx,0,0
5902,geoip_proxy_recursive,"If recursive search is disabled then instead of the original client address that matches one of the trusted addresses, the last address sent in X-Forwarded-For will be used. If recursive search is enabled then instead of the original client address that matches one of the trusted addresses, the last non-trusted address sent in X-Forwarded-For will be used.",0,0,others,nginx,0,1
5903,google_perftools_profiles,"Sets a file name that keeps profiling information of nginx worker process. The ID of the worker process is always a part of the file name and is appended to the end of the file name, after a dot.",0,0,others,nginx,0,0
5904,grpc_bind,"Makes outgoing connections to a gRPC server originate from the specified local IP address with an optional port. Parameter value can contain variables. The special value off cancels the effect of the grpc_bind directive inherited from the previous configuration level, which allows the system to auto-assign the local IP address and port.",0,0,others,nginx,0,0
5905,grpc_buffer_size,"Sets the size of the buffer used for reading the response received from the gRPC server. The response is passed to the client synchronously, as soon as it is received.",1,1,resource,nginx,0,0
5906,grpc_connect_timeout,Defines a timeout for establishing a connection with a gRPC server. It should be noted that this timeout cannot usually exceed 75 seconds.,0,0,others,nginx,0,0
5907,grpc_hide_header,"By default, nginx does not pass the header fields Date, Server, and X-Accel-... from the response of a gRPC server to a client. The grpc_hide_header directive sets additional fields that will not be passed. If, on the contrary, the passing of fields needs to be permitted, the grpc_pass_header directive can be used.",0,0,others,nginx,0,0
5908,grpc_ignore_headers,Disables processing of certain response header fields from the gRPC server. The following fields can be ignored: X-Accel-Redirect and X-Accel-Charset.,0,0,others,nginx,0,0
5909,grpc_intercept_errors,Determines whether gRPC server responses with codes greater than or equal to 300 should be passed to a client or be intercepted and redirected to nginx for processing with the error_page directive.,0,0,others,nginx,0,0
5910,grpc_next_upstream,Specifies in which cases a request should be passed to the next server:,0,0,others,nginx,0,0
5911,grpc_next_upstream_timeout,Limits the time during which a request can be passed to the next server. The 0 value turns off this limitation.,0,0,others,nginx,0,0
5912,grpc_next_upstream_tries,Limits the number of possible tries for passing a request to the next server. The 0 value turns off this limitation.,0,0,others,nginx,0,0
5913,grpc_pass,"Sets the gRPC server address. The address can be specified as a domain name or IP address, and a port:",0,0,others,nginx,0,0
5914,grpc_pass_header,Permits passing otherwise disabled header fields from a gRPC server to a client.,0,0,others,nginx,0,0
5915,grpc_read_timeout,"Defines a timeout for reading a response from the gRPC server. The timeout is set only between two successive read operations, not for the transmission of the whole response. If the gRPC server does not transmit anything within this time, the connection is closed.",0,0,others,nginx,0,1
5916,grpc_send_timeout,"Sets a timeout for transmitting a request to the gRPC server. The timeout is set only between two successive write operations, not for the transmission of the whole request. If the gRPC server does not receive anything within this time, the connection is closed.",0,0,others,nginx,0,0
5917,grpc_set_header,"Allows redefining or appending fields to the request header passed to the gRPC server. The value can contain text, variables, and their combinations. These directives are inherited from the previous configuration level if and only if there are no grpc_set_header directives defined on the current level.",0,0,others,nginx,0,1
5918,grpc_socket_keepalive,"Configures the TCP keepalive behavior for outgoing connections to a gRPC server. By default, the operating systems settings are in effect for the socket. If the directive is set to the value on, the SO_KEEPALIVE socket option is turned on for the socket.",0,0,others,nginx,0,0
5919,grpc_ssl_certificate,Specifies a file with the certificate in the PEM format used for authentication to a gRPC SSL server.,0,0,others,nginx,0,1
5920,grpc_ssl_certificate_key,Specifies a file with the secret key in the PEM format used for authentication to a gRPC SSL server.,0,0,others,nginx,0,0
5921,grpc_ssl_ciphers,Specifies the enabled ciphers for requests to a gRPC SSL server. The ciphers are specified in the format understood by the OpenSSL library.,0,0,others,nginx,0,0
5922,grpc_ssl_conf_command,Sets arbitrary OpenSSL configuration commands when establishing a connection with the gRPC SSL server.,0,0,others,nginx,0,0
5923,grpc_ssl_crl,Specifies a file with revoked certificates (CRL) in the PEM format used to verify the certificate of the gRPC SSL server.,0,0,others,nginx,0,0
5924,grpc_ssl_name,Allows overriding the server name used to verify the certificate of the gRPC SSL server and to be passed through SNI when establishing a connection with the gRPC SSL server.,0,0,others,nginx,0,0
5925,grpc_ssl_password_file,Specifies a file with passphrases for secret keys where each passphrase is specified on a separate line. Passphrases are tried in turn when loading the key.,0,0,others,nginx,0,0
5926,grpc_ssl_protocols,Enables the specified protocols for requests to a gRPC SSL server.,0,0,others,nginx,0,1
5927,grpc_ssl_server_name,"Enables or disables passing of the server name through TLS Server Name Indication extension (SNI, RFC 6066) when establishing a connection with the gRPC SSL server.",0,0,others,nginx,0,1
5928,grpc_ssl_session_reuse,"Determines whether SSL sessions can be reused when working with the gRPC server. If the errors SSL3_GET_FINISHED:digest check failed appear in the logs, try disabling session reuse.",1,4,limited-side-effect,nginx,0,0
5929,grpc_ssl_trusted_certificate,Specifies a file with trusted CA certificates in the PEM format used to verify the certificate of the gRPC SSL server.,0,0,others,nginx,0,0
5930,grpc_ssl_verify,Enables or disables verification of the gRPC SSL server certificate.,1,2,security-tradeoff,nginx,0,0
5931,grpc_ssl_verify_depth,Sets the verification depth in the gRPC SSL server certificates chain.,1,2,security-tradeoff,nginx,0,0
5932,gunzip,"Enables or disables decompression of gzipped responses for clients that lack gzip support. If enabled, the following directives are also taken into account when determining if clients support gzip: gzip_http_version, gzip_proxied, and gzip_disable. See also the gzip_vary directive.",1,6,function-tradeoff,nginx,0,0
5933,gunzip_buffers,"Sets the number and size of buffers used to decompress a response. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform.",1,1,resource,nginx,0,0
5934,gzip,Enables or disables gzipping of responses.,1,6,function-tradeoff,nginx,0,1
5935,gzip_buffers,"Sets the number and size of buffers used to compress a response. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform.",1,1,resource,nginx,0,0
5936,gzip_comp_level,Sets a gzip compression level of a response. Acceptable values are in the range from 1 to 9.,1,6,function-tradeoff,nginx,0,0
5937,gzip_disable,Disables gzipping of responses for requests with User-Agent header fields matching any of the specified regular expressions.,0,0,others,nginx,0,0
5938,gzip_http_version,Sets the minimum HTTP version of a request required to compress a response.,0,0,others,nginx,0,0
5939,gzip_min_length,Sets the minimum length of a response that will be gzipped. The length is determined only from the Content-Length response header field.,0,0,others,nginx,0,0
5940,gzip_proxied,Enables or disables gzipping of responses for proxied requests depending on the request and response. The fact that the request is proxied is determined by the presence of the Via request header field. The directive accepts multiple parameters:,0,0,others,nginx,0,1
5941,gzip_static,"Enables (on) or disables (off) checking the existence of precompressed files. The following directives are also taken into account: gzip_http_version, gzip_proxied, gzip_disable, and gzip_vary.",0,0,others,nginx,0,0
5942,gzip_types,Enables gzipping of responses for the specified MIME types in addition to text/html. The special value * matches any MIME type (0.8.29). Responses with the text/html type are always compressed.,0,0,others,nginx,0,0
5943,gzip_vary,"Enables or disables inserting the Vary: Accept-Encoding response header field if the directives gzip, gzip_static, or gunzip are active.",0,0,others,nginx,0,0
5944,hash.B,"Specifies a load balancing method for a server group where the client-server mapping is based on the hashed key value. The key can contain text, variables, and their combinations (1.11.2). Usage example:",0,0,others,nginx,0,0
5945,hash,"Specifies a load balancing method for a server group where the client-server mapping is based on the hashed key value. The key can contain text, variables, and their combinations. Note that adding or removing a server from the group may result in remapping most of the keys to different servers. The method is compatible with the Cache::Memcached Perl library.",0,0,others,nginx,0,0
5946,health_check.B,Enables periodic health checks of the servers in a group referenced in the surrounding location.,0,0,others,nginx,0,0
5947,health_check,Enables periodic health checks of the servers in a group.,0,0,others,nginx,0,0
5948,health_check_timeout,Overrides the proxy_timeout value for health checks.,0,0,others,nginx,0,1
5949,hls,Turns on HLS streaming in the surrounding location.,0,0,others,nginx,0,0
5950,hls_buffers,Sets the maximum number and size of buffers that are used for reading and writing data frames.,1,1,resource,nginx,0,0
5951,hls_forward_args,"Adds arguments from a playlist request to URIs of fragments. This may be useful for performing client authorization at the moment of requesting a fragment, or when protecting an HLS stream with the ngx_http_secure_link_module module.",0,0,others,nginx,0,0
5952,hls_fragment,Defines the default fragment length for playlist URIs requested without the len argument.,0,0,others,nginx,0,0
5953,hls_mp4_buffer_size,Sets the initial size of the buffer used for processing MP4 and MOV files.,1,1,resource,nginx,0,0
5954,hls_mp4_max_buffer_size,"During metadata processing, a larger buffer may become necessary. Its size cannot exceed the specified size, or else nginx will return the server error 500 (Internal Server Error), and log the following message:",1,1,resource,nginx,0,0
5955,http,Provides the configuration file context in which the HTTP server directives are specified.,0,0,others,nginx,0,0
5956,http2_body_preread_size,Sets the size of the buffer per each request in which the request body may be saved before it is started to be processed.,1,5,workload-specific,nginx,0,1
5957,http2_chunk_size,Sets the maximum size of chunks into which the response body is sliced. A too low value results in higher overhead. A too high value impairs prioritization due to HOL blocking.,1,5,workload-specific,nginx,0,0
5958,http2_idle_timeout,Sets the timeout of inactivity after which the connection is closed.,1,6,function-tradeoff,nginx,0,1
5959,http2_max_concurrent_pushes,Limits the maximum number of concurrent push requests in a connection.,1,5,workload-specific,nginx,0,0
5960,http2_max_concurrent_streams,Sets the maximum number of concurrent HTTP/2 streams in a connection.,1,5,workload-specific,nginx,0,1
5961,http2_max_field_size,"Limits the maximum size of an HPACK-compressed request header field. The limit applies equally to both name and value. Note that if Huffman encoding is applied, the actual size of decompressed name and value strings may be larger. For most requests, the default limit should be enough.",1,5,workload-specific,nginx,0,0
5962,http2_max_header_size,"Limits the maximum size of the entire request header list after HPACK decompression. For most requests, the default limit should be enough.",0,0,others,nginx,0,0
5963,http2_max_requests,"Sets the maximum number of requests (including push requests) that can be served through one HTTP/2 connection, after which the next client request will lead to connection closing and the need of establishing a new connection. Closing connections periodically is necessary to free per-connection memory allocations. Therefore, using too high maximum number of requests could result in excessive memory usage and not recommended.",1,1,resource,nginx,0,0
5964,http2_push,"Pre-emptively sends (pushes) a request to the specified uri along with the response to the original request. Only relative URIs with absolute path will be processed, for example:",0,0,others,nginx,0,1
5965,http2_push_preload,Enables automatic conversion of preload links specified in the Link response header fields into push requests.,0,0,others,nginx,0,0
5966,http2_recv_buffer_size,Sets the size of the per worker input buffer.,1,1,resource,nginx,0,1
5967,http2_recv_timeout,"Sets the timeout for expecting more data from the client, after which the connection is closed.",0,0,others,nginx,0,1
5968,if,"The specified condition is evaluated. If true, this module directives specified inside the braces are executed, and the request is assigned the configuration inside the if directive. Configurations inside the if directives are inherited from the previous configuration level.",0,0,others,nginx,0,1
5969,if_modified_since,Specifies how to compare modification time of a response with the time in the If-Modified-Since request header field:,0,0,others,nginx,0,0
5970,ignore_invalid_headers,"Controls whether header fields with invalid names should be ignored. Valid names are composed of English letters, digits, hyphens, and possibly underscores (as controlled by the underscores_in_headers directive).",0,0,others,nginx,0,0
5971,image_filter,Sets the type of transformation to perform on images:,1,6,function-tradeoff,nginx,0,0
5972,image_filter_buffer,Sets the maximum size of the buffer used for reading images. When the size is exceeded the server returns error 415 (Unsupported Media Type).,1,1,resource,nginx,0,0
5973,image_filter_interlace,"If enabled, final images will be interlaced. For JPEG, final images will be in progressive JPEG format.",1,4,limited-side-effect,nginx,0,0
5974,image_filter_jpeg_quality,Sets the desired quality of the transformed JPEG images. Acceptable values are in the range from 1 to 100. Lesser values usually imply both lower image quality and less data to transfer. The maximum recommended value is 95. Parameter value can contain variables.,0,0,others,nginx,0,0
5975,image_filter_sharpen,Increases sharpness of the final image. The sharpness percentage can exceed 100. The zero value disables sharpening. Parameter value can contain variables.,1,5,workload-specific,nginx,0,0
5976,image_filter_transparency,Defines whether transparency should be preserved when transforming GIF images or PNG images with colors specified by a palette. The loss of transparency results in images of a better quality. The alpha channel transparency in PNG is always preserved.,1,6,function-tradeoff,nginx,0,0
5977,image_filter_webp_quality,Sets the desired quality of the transformed WebP images. Acceptable values are in the range from 1 to 100. Lesser values usually imply both lower image quality and less data to transfer. Parameter value can contain variables.,1,6,function-tradeoff,nginx,0,0
5978,imap_auth,Sets permitted methods of authentication for IMAP clients. Supported methods are:,1,2,security-tradeoff,nginx,0,0
5979,imap_capabilities,Sets the IMAP protocol extensions list that is passed to the client in response to the CAPABILITY command. The authentication methods specified in the imap_auth directive and STARTTLS are automatically added to this list depending on the starttls directive value.,0,0,others,nginx,0,1
5980,imap_client_buffer,"Sets the size of the buffer used for reading IMAP commands. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform.",1,1,resource,nginx,0,0
5981,include,"Includes another file, or files matching the specified mask, into configuration. Included files should consist of syntactically correct directives and blocks.",0,0,others,nginx,0,0
5982,index,Defines files that will be used as an index. The file name can contain variables. Files are checked in the specified order. The last element of the list can be a file with an absolute path. Example:,0,0,others,nginx,0,0
5983,internal,"Specifies that a given location can only be used for internal requests. For external requests, the client error 404 (Not Found) is returned. Internal requests are the following:",0,0,others,nginx,0,1
5984,ip_hash,"Specifies that a group should use a load balancing method where requests are distributed between servers based on client IP addresses. The first three octets of the client IPv4 address, or the entire IPv6 address, are used as a hashing key. The method ensures that requests from the same client will always be passed to the same server except when this server is unavailable. In the latter case client requests will be passed to another server. Most probably, it will always be the same server as well.",0,0,others,nginx,0,0
5985,js_access,"Sets an njs function which will be called at the access phase. Since 0.4.0, a module function can be referenced.",0,0,others,nginx,0,0
5986,js_body_filter,Sets an njs function as a response body filter. The filter function is called for each data chunk of a response body with the following arguments:,0,0,others,nginx,0,0
5987,js_content,"Sets an njs function as a location content handler. Since 0.4.0, a module function can be referenced.",0,0,others,nginx,0,0
5988,js_filter,"Sets a data filter. Since 0.4.0, a module function can be referenced.",0,0,others,nginx,0,0
5989,js_header_filter,Sets an njs function as a response header filter. The directive allows changing arbitrary header fields of a response header.,0,0,others,nginx,0,0
5990,js_import,"Imports a module that implements location and variable handlers in njs. The export_name is used as a namespace to access module functions. If the export_name is not specified, the module name will be used as a namespace.",0,0,others,nginx,0,1
5991,js_include.B,Specifies a file that implements location and variable handlers in njs:,0,0,others,nginx,0,0
5992,js_include,Specifies a file that implements server and variable handlers in njs:,0,0,others,nginx,0,0
5993,js_path,Sets an additional path for njs modules.,0,0,others,nginx,0,0
5994,js_preread,"Sets an njs function which will be called at the preread phase. Since 0.4.0, a module function can be referenced.",0,0,others,nginx,0,0
5995,js_set,"Sets an njs function for the specified variable. Since 0.4.0, a module function can be referenced.",0,0,others,nginx,0,0
5996,js_var.B,"Declares a writable variable. The value can contain text, variables, and their combination.",0,0,others,nginx,0,0
5997,js_var,"Declares a writable variable. The value can contain text, variables, and their combination. The variable is not overwritten after a redirect unlike variables created with the set directive.",0,0,others,nginx,0,0
5998,keepalive,Activates the cache for connections to upstream servers.,0,0,others,nginx,0,0
5999,keepalive_disable,"Disables keep-alive connections with misbehaving browsers. The browser parameters specify which browsers will be affected. The value msie6 disables keep-alive connections with old versions of MSIE, once a POST request is received. The value safari disables keep-alive connections with Safari and Safari-like browsers on macOS and macOS-like operating systems. The value none enables keep-alive connections with all browsers.",0,0,others,nginx,0,1
6000,keepalive_requests.B,"Sets the maximum number of requests that can be served through one keep-alive connection. After the maximum number of requests are made, the connection is closed.",1,5,workload-specific,nginx,0,0
6001,keepalive_requests,"Sets the maximum number of requests that can be served through one keepalive connection. After the maximum number of requests is made, the connection is closed.",1,5,workload-specific,nginx,0,0
6002,keepalive_time.B,"Limits the maximum time during which requests can be processed through one keepalive connection. After this time is reached, the connection is closed following the subsequent request processing.",0,0,others,nginx,0,0
6003,keepalive_time,"Limits the maximum time during which requests can be processed through one keep-alive connection. After this time is reached, the connection is closed following the subsequent request processing.",0,0,others,nginx,0,0
6004,keepalive_timeout.B,Sets a timeout during which an idle keepalive connection to an upstream server will stay open.,0,0,others,nginx,0,1
6005,keepalive_timeout,The first parameter sets a timeout during which a keep-alive client connection will stay open on the server side. The zero value disables keep-alive client connections. The optional second parameter sets a value in the Keep-Alive: timeout=time response header field. Two parameters may differ.,0,0,others,nginx,0,0
6006,keyval,Creates a new $variable whose value is looked up by the key in the key-value database. Matching rules are defined by the type parameter of the keyval_zone directive. The database is stored in a shared memory zone specified by the zone parameter.,0,0,others,nginx,0,0
6007,keyval_zone,Sets the name and size of the shared memory zone that keeps the key-value database. Key-value pairs are managed by the API.,1,5,workload-specific,nginx,0,0
6008,large_client_header_buffers,"Sets the maximum number and size of buffers used for reading large client request header. A request line cannot exceed the size of one buffer, or the 414 (Request-URI Too Large) error is returned to the client. A request header field cannot exceed the size of one buffer as well, or the 400 (Bad Request) error is returned to the client. Buffers are allocated only on demand. By default, the buffer size is equal to 8K bytes. If after the end of request processing a connection is transitioned into the keep-alive state, these buffers are released.",1,1,resource,nginx,0,1
6009,least_conn.B,"Specifies that a group should use a load balancing method where a connection is passed to the server with the least number of active connections, taking into account weights of servers. If there are several such servers, they are tried in turn using a weighted round-robin balancing method.",0,0,others,nginx,0,1
6010,least_conn,"Specifies that a group should use a load balancing method where a request is passed to the server with the least number of active connections, taking into account weights of servers. If there are several such servers, they are tried in turn using a weighted round-robin balancing method.",0,0,others,nginx,0,0
6011,least_time.B,"Specifies that a group should use a load balancing method where a connection is passed to the server with the least average time and least number of active connections, taking into account weights of servers. If there are several such servers, they are tried in turn using a weighted round-robin balancing method.",0,0,others,nginx,0,0
6012,least_time,"Specifies that a group should use a load balancing method where a request is passed to the server with the least average response time and least number of active connections, taking into account weights of servers. If there are several such servers, they are tried in turn using a weighted round-robin balancing method.",0,0,others,nginx,0,0
6013,limit_conn.B,"Sets the shared memory zone and the maximum allowed number of connections for a given key value. When this limit is exceeded, the server will close the connection. For example, the directives",1,1,resource,nginx,0,1
6014,limit_conn,"Sets the shared memory zone and the maximum allowed number of connections for a given key value. When this limit is exceeded, the server will return the error in reply to a request. For example, the directives",1,1,resource,nginx,0,0
6015,limit_conn_dry_run,"Enables the dry run mode. In this mode, the number of connections is not limited, however, in the shared memory zone, the number of excessive connections is accounted as usual.",1,6,function-tradeoff,nginx,0,0
6016,limit_conn_log_level,Sets the desired logging level for cases when the server limits the number of connections.,1,6,function-tradeoff,nginx,0,0
6017,limit_conn_status,Sets the status code to return in response to rejected requests.,0,0,others,nginx,0,0
6018,limit_conn_zone.B,"Sets parameters for a shared memory zone that will keep states for various keys. In particular, the state includes the current number of connections. The key can contain text, variables, and their combination. Requests with an empty key value are not accounted.",1,1,resource,nginx,0,0
6019,limit_conn_zone,"Sets parameters for a shared memory zone that will keep states for various keys. In particular, the state includes the current number of connections. The key can contain text, variables, and their combinations (1.11.2). Connections with an empty key value are not accounted. Usage example:",1,1,resource,nginx,0,0
6020,limit_except,"Limits allowed HTTP methods inside a location. The method parameter can be one of the following: GET, HEAD, POST, PUT, DELETE, MKCOL, COPY, MOVE, OPTIONS, PROPFIND, PROPPATCH, LOCK, UNLOCK, or PATCH. Allowing the GET method makes the HEAD method also allowed. Access to other methods can be limited using the ngx_http_access_module, ngx_http_auth_basic_module, and ngx_http_auth_jwt_module (1.13.10) modules directives:",0,0,others,nginx,0,1
6021,limit_rate,"Limits the rate of response transmission to a client. The rate is specified in bytes per second. The zero value disables rate limiting. The limit is set per a request, and so if a client simultaneously opens two connections, the overall rate will be twice as much as the specified limit.",0,0,others,nginx,0,0
6022,limit_rate_after,Sets the initial amount after which the further transmission of a response to a client will be rate limited. Parameter value can contain variables (1.17.0).,0,0,others,nginx,0,0
6023,limit_req,"Sets the shared memory zone and the maximum burst size of requests. If the requests rate exceeds the rate configured for a zone, their processing is delayed such that requests are processed at a defined rate. Excessive requests are delayed until their number exceeds the maximum burst size in which case the request is terminated with an error. By default, the maximum burst size is equal to zero. For example, the directives",1,1,resource,nginx,0,0
6024,limit_req_dry_run,"Enables the dry run mode. In this mode, requests processing rate is not limited, however, in the shared memory zone, the number of excessive requests is accounted as usual.",1,6,function-tradeoff,nginx,0,0
6025,limit_req_log_level,"Sets the desired logging level for cases when the server refuses to process requests due to rate exceeding, or delays request processing. Logging level for delays is one point less than for refusals; for example, if limit_req_log_level notice is specified, delays are logged with the info level.",1,6,function-tradeoff,nginx,0,0
6026,limit_req_status,Sets the status code to return in response to rejected requests.,0,0,others,nginx,0,0
6027,limit_req_zone,"Sets parameters for a shared memory zone that will keep states for various keys. In particular, the state stores the current number of excessive requests. The key can contain text, variables, and their combination. Requests with an empty key value are not accounted.",1,1,resource,nginx,0,0
6028,limit_zone,This directive was made obsolete in version 1.1.8 and was removed in version 1.7.6. An equivalent limit_conn_zone directive with a changed syntax should be used instead:,0,0,others,nginx,0,0
6029,lingering_close,Controls how nginx closes client connections.,1,6,function-tradeoff,nginx,0,1
6030,lingering_time,"When lingering_close is in effect, this directive specifies the maximum time during which nginx will process (read and ignore) additional data coming from a client. After that, the connection will be closed, even if there will be more data.",0,0,others,nginx,0,1
6031,lingering_timeout,"When lingering_close is in effect, this directive specifies the maximum waiting time for more client data to arrive. If data are not received during this time, the connection is closed. Otherwise, the data are read and ignored, and nginx starts waiting for more data again. The wait-read-ignore cycle is repeated, but no longer than specified by the lingering_time directive.",0,0,others,nginx,0,0
6032,listen.B,"Sets the address and port for IP, or the path for a UNIX-domain socket on which the server will accept requests. Both address and port, or only address or only port can be specified. An address may also be a hostname, for example:",0,0,others,nginx,0,1
6033,listen.C,"Sets the address and port for the socket on which the server will accept connections. It is possible to specify just the port. The address can also be a hostname, for example:",0,0,others,nginx,0,0
6034,listen,"Sets the address and port for the socket on which the server will accept requests. It is possible to specify just the port. The address can also be a hostname, for example:",0,0,others,nginx,0,0
6035,load_module,Loads a dynamic module.,0,0,others,nginx,0,0
6036,location,Sets configuration depending on a request URI.,0,0,others,nginx,0,0
6037,lock_file,"nginx uses the locking mechanism to implement accept_mutex and serialize access to shared memory. On most systems the locks are implemented using atomic operations, and this directive is ignored. On other systems the lock file mechanism is used. This directive specifies a prefix for the names of lock files.",0,0,others,nginx,0,0
6038,log_format.B,Specifies log format.,0,0,others,nginx,0,0
6039,log_format,"Specifies the log format, for example:",1,6,function-tradeoff,nginx,0,0
6040,log_not_found,Enables or disables logging of errors about not found files into error_log.,1,6,function-tradeoff,nginx,0,0
6041,log_subrequest,Enables or disables logging of subrequests into access_log.,1,6,function-tradeoff,nginx,0,1
6042,mail,Provides the configuration file context in which the mail server directives are specified.,0,0,others,nginx,0,0
6043,map,Creates a new variable whose value depends on values of one or more of the source variables specified in the first parameter.,0,0,others,nginx,0,0
6044,map_hash_bucket_size,Sets the bucket size for the map variables hash tables. Default value depends on the processors cache line size. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx,0,0
6045,map_hash_max_size,Sets the maximum size of the map variables hash tables. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx,0,0
6046,master_process,Determines whether worker processes are started. This directive is intended for nginx developers.,0,0,others,nginx,0,0
6047,match.B,Defines the named test set used to verify responses to health check requests.,0,0,others,nginx,0,1
6048,match,Defines the named test set used to verify server responses to health checks.,0,0,others,nginx,0,0
6049,max_errors,Sets the number of protocol errors after which the connection is closed.,0,0,others,nginx,0,0
6050,max_ranges,"Limits the maximum allowed number of ranges in byte-range requests. Requests that exceed the limit are processed as if there were no byte ranges specified. By default, the number of ranges is not limited. The zero value disables the byte-range support completely.",1,1,resource,nginx,0,0
6051,memcached_bind,"Makes outgoing connections to a memcached server originate from the specified local IP address with an optional port (1.11.2). Parameter value can contain variables (1.3.12). The special value off (1.3.12) cancels the effect of the memcached_bind directive inherited from the previous configuration level, which allows the system to auto-assign the local IP address and port.",0,0,others,nginx,0,0
6052,memcached_buffer_size,"Sets the size of the buffer used for reading the response received from the memcached server. The response is passed to the client synchronously, as soon as it is received.",1,1,resource,nginx,0,1
6053,memcached_connect_timeout,Defines a timeout for establishing a connection with a memcached server. It should be noted that this timeout cannot usually exceed 75 seconds.,0,0,others,nginx,0,0
6054,memcached_force_ranges,Enables byte-range support for both cached and uncached responses from the memcached server regardless of the Accept-Ranges field in these responses.,1,4,limited-side-effect,nginx,0,0
6055,memcached_gzip_flag,Enables the test for the flag presence in the memcached server response and sets the Content-Encoding response header field to gzip if the flag is set.,0,0,others,nginx,0,0
6056,memcached_next_upstream,Specifies in which cases a request should be passed to the next server:,0,0,others,nginx,0,1
6057,memcached_next_upstream_timeout,Limits the time during which a request can be passed to the next server. The 0 value turns off this limitation.,0,0,others,nginx,0,0
6058,memcached_next_upstream_tries,Limits the number of possible tries for passing a request to the next server. The 0 value turns off this limitation.,0,0,others,nginx,0,0
6059,memcached_pass,"Sets the memcached server address. The address can be specified as a domain name or IP address, and a port:",0,0,others,nginx,0,0
6060,memcached_read_timeout,"Defines a timeout for reading a response from the memcached server. The timeout is set only between two successive read operations, not for the transmission of the whole response. If the memcached server does not transmit anything within this time, the connection is closed.",0,0,others,nginx,0,0
6061,memcached_send_timeout,"Sets a timeout for transmitting a request to the memcached server. The timeout is set only between two successive write operations, not for the transmission of the whole request. If the memcached server does not receive anything within this time, the connection is closed.",0,0,others,nginx,0,0
6062,memcached_socket_keepalive,"Configures the TCP keepalive behavior for outgoing connections to a memcached server. By default, the operating systems settings are in effect for the socket. If the directive is set to the value on, the SO_KEEPALIVE socket option is turned on for the socket.",0,0,others,nginx,0,0
6063,merge_slashes,Enables or disables compression of two or more adjacent slashes in a URI into a single slash.,1,4,limited-side-effect,nginx,0,0
6064,min_delete_depth,"Allows the DELETE method to remove files provided that the number of elements in a request path is not less than the specified number. For example, the directive",1,5,workload-specific,nginx,0,0
6065,mirror,Sets the URI to which an original request will be mirrored. Several mirrors can be specified on the same configuration level.,0,0,others,nginx,0,1
6066,mirror_request_body,"Indicates whether the client request body is mirrored. When enabled, the client request body will be read prior to creating mirror subrequests. In this case, unbuffered client request body proxying set by the proxy_request_buffering, fastcgi_request_buffering, scgi_request_buffering, and uwsgi_request_buffering directives will be disabled.",0,0,others,nginx,0,1
6067,modern_browser,"Specifies a version starting from which a browser is considered modern. A browser can be any one of the following: msie, gecko (browsers based on Mozilla), opera, safari, or konqueror.",0,0,others,nginx,0,0
6068,modern_browser_value,Sets a value for the $modern_browser variables.,0,0,others,nginx,0,0
6069,mp4,Turns on module processing in a surrounding location.,0,0,others,nginx,0,0
6070,mp4_buffer_size,Sets the initial size of the buffer used for processing MP4 files.,1,1,resource,nginx,0,1
6071,mp4_limit_rate,"Limits the rate of response transmission to a client. The rate is limited based on the average bitrate of the MP4 file served. To calculate the rate, the bitrate is multiplied by the specified factor. The special value on corresponds to the factor of 1.1. The special value off disables rate limiting. The limit is set per a request, and so if a client simultaneously opens two connections, the overall rate will be twice as much as the specified limit.",0,0,others,nginx,0,1
6072,mp4_limit_rate_after,Sets the initial amount of media data (measured in playback time) after which the further transmission of the response to a client will be rate limited.,0,0,others,nginx,0,0
6073,mp4_max_buffer_size,"During metadata processing, a larger buffer may become necessary. Its size cannot exceed the specified size, or else nginx will return the 500 (Internal Server Error) server error, and log the following message:",1,1,resource,nginx,0,0
6074,msie_padding,Enables or disables adding comments to responses for MSIE clients with status greater than 400 to increase the response size to 512 bytes.,1,6,function-tradeoff,nginx,0,0
6075,msie_refresh,Enables or disables issuing refreshes instead of redirects for MSIE clients.,1,4,limited-side-effect,nginx,0,0
6076,multi_accept,"If multi_accept is disabled, a worker process will accept one new connection at a time. Otherwise, a worker process will accept all new connections at a time.",1,4,limited-side-effect,nginx,0,0
6077,ntlm,"Allows proxying requests with NTLM Authentication. The upstream connection is bound to the client connection once the client sends a request with the Authorization header field value starting with Negotiate or NTLM. Further client requests will be proxied through the same upstream connection, keeping the authentication context.",1,2,security-tradeoff,nginx,0,1
6078,open_file_cache,Configures a cache that can store:,1,6,function-tradeoff,nginx,0,0
6079,open_file_cache_errors,Enables or disables caching of file lookup errors by open_file_cache.,1,4,limited-side-effect,nginx,0,0
6080,open_file_cache_min_uses,"Sets the minimum number of file accesses during the period configured by the inactive parameter of the open_file_cache directive, required for a file descriptor to remain open in the cache.",0,0,others,nginx,0,1
6081,open_file_cache_valid,Sets a time after which open_file_cache elements should be validated.,0,0,others,nginx,0,1
6082,open_log_file_cache,Defines a cache that stores the file descriptors of frequently used logs whose names contain variables. The directive has the following parameters:,1,4,limited-side-effect,nginx,0,0
6083,output_buffers,Sets the number and size of the buffers used for reading a response from a disk.,1,1,resource,nginx,0,0
6084,override_charset,"Determines whether a conversion should be performed for answers received from a proxied or a FastCGI/uwsgi/SCGI/gRPC server when the answers already carry a charset in the Content-Type response header field. If conversion is enabled, a charset specified in the received response is used as a source charset.",0,0,others,nginx,0,0
6085,pcre_jit,Enables or disables the use of just-in-time compilation (PCRE JIT) for the regular expressions known by the time of configuration parsing.,1,4,limited-side-effect,nginx,0,1
6086,perl,Sets a Perl handler for the given location.,0,0,others,nginx,0,0
6087,perl_modules,Sets an additional path for Perl modules.,0,0,others,nginx,0,0
6088,perl_require,Defines the name of a module that will be loaded during each reconfiguration. Several perl_require directives can be present.,0,0,others,nginx,0,0
6089,perl_set,Installs a Perl handler for the specified variable.,0,0,others,nginx,0,0
6090,pid,Defines a file that will store the process ID of the main process.,0,0,others,nginx,0,0
6091,pop3_auth,Sets permitted methods of authentication for POP3 clients. Supported methods are:,1,2,security-tradeoff,nginx,0,1
6092,pop3_capabilities,Sets the POP3 protocol extensions list that is passed to the client in response to the CAPA command. The authentication methods specified in the pop3_auth directive (SASL extension) and STLS are automatically added to this list depending on the starttls directive value.,1,2,security-tradeoff,nginx,0,0
6093,port_in_redirect,Enables or disables specifying the port in absolute redirects issued by nginx.,0,0,others,nginx,0,1
6094,postpone_output,"If possible, the transmission of client data will be postponed until nginx has at least size bytes of data to send. The zero value disables postponing data transmission.",0,0,others,nginx,0,0
6095,preread_buffer_size,Specifies a size of the preread buffer.,1,1,resource,nginx,0,1
6096,preread_timeout,Specifies a timeout of the preread phase.,0,0,others,nginx,0,0
6097,protocol,"Sets the protocol for a proxied server. Supported protocols are IMAP, POP3, and SMTP.",0,0,others,nginx,0,0
6098,proxy_bind.B,"Makes outgoing connections to a proxied server originate from the specified local IP address with an optional port (1.11.2). Parameter value can contain variables (1.3.12). The special value off (1.3.12) cancels the effect of the proxy_bind directive inherited from the previous configuration level, which allows the system to auto-assign the local IP address and port.",0,0,others,nginx,0,0
6099,proxy_bind,"Makes outgoing connections to a proxied server originate from the specified local IP address. Parameter value can contain variables (1.11.2). The special value off cancels the effect of the proxy_bind directive inherited from the previous configuration level, which allows the system to auto-assign the local IP address.",0,0,others,nginx,0,0
6100,proxy_buffer,"Sets the size of the buffer used for proxying. By default, the buffer size is equal to one memory page. Depending on a platform, it is either 4K or 8K.",1,1,resource,nginx,0,0
6101,proxy_buffer_size.B,Sets the size of the buffer used for reading data from the proxied server. Also sets the size of the buffer used for reading data from the client.,1,1,resource,nginx,0,1
6102,proxy_buffer_size,"Sets the size of the buffer used for reading the first part of the response received from the proxied server. This part usually contains a small response header. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform. It can be made smaller, however.",1,1,resource,nginx,0,0
6103,proxy_buffering,Enables or disables buffering of responses from the proxied server.,1,6,function-tradeoff,nginx,0,0
6104,proxy_buffers,"Sets the number and size of the buffers used for reading a response from the proxied server, for a single connection. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform.",1,1,resource,nginx,0,0
6105,proxy_busy_buffers_size,"When buffering of responses from the proxied server is enabled, limits the total size of buffers that can be busy sending a response to the client while the response is not yet fully read. In the meantime, the rest of the buffers can be used for reading the response and, if needed, buffering part of the response to a temporary file. By default, size is limited by the size of two buffers set by the proxy_buffer_size and proxy_buffers directives.",1,1,resource,nginx,0,0
6106,proxy_cache,Defines a shared memory zone used for caching. The same zone can be used in several places. Parameter value can contain variables (1.7.9). The off parameter disables caching inherited from the previous configuration level.,1,4,limited-side-effect,nginx,0,0
6107,proxy_cache_background_update,"Allows starting a background subrequest to update an expired cache item, while a stale cached response is returned to the client. Note that it is necessary to allow the usage of a stale cached response when it is being updated.",1,6,function-tradeoff,nginx,0,0
6108,proxy_cache_bypass,Defines conditions under which the response will not be taken from a cache. If at least one value of the string parameters is not empty and is not equal to 0 then the response will not be taken from the cache:,0,0,others,nginx,0,0
6109,proxy_cache_convert_head,"Enables or disables the conversion of the HEAD method to GET for caching. When the conversion is disabled, the cache key should be configured to include the $request_method.",1,5,workload-specific,nginx,0,0
6110,proxy_cache_key,"Defines a key for caching, for example",0,0,others,nginx,0,0
6111,proxy_cache_lock,"When enabled, only one request at a time will be allowed to populate a new cache element identified according to the proxy_cache_key directive by passing a request to a proxied server. Other requests of the same cache element will either wait for a response to appear in the cache or the cache lock for this element to be released, up to the time set by the proxy_cache_lock_timeout directive.",1,6,function-tradeoff,nginx,0,0
6112,proxy_cache_lock_age,"If the last request passed to the proxied server for populating a new cache element has not completed for the specified time, one more request may be passed to the proxied server.",0,0,others,nginx,0,1
6113,proxy_cache_lock_timeout,"Sets a timeout for proxy_cache_lock. When the time expires, the request will be passed to the proxied server, however, the response will not be cached.",0,0,others,nginx,0,0
6114,proxy_cache_max_range_offset,"Sets an offset in bytes for byte-range requests. If the range is beyond the offset, the range request will be passed to the proxied server and the response will not be cached.",0,0,others,nginx,0,1
6115,proxy_cache_methods,"If the client request method is listed in this directive then the response will be cached. GET and HEAD methods are always added to the list, though it is recommended to specify them explicitly. See also the proxy_no_cache directive.",0,0,others,nginx,0,0
6116,proxy_cache_min_uses,Sets the number of requests after which the response will be cached.,1,5,workload-specific,nginx,0,1
6117,proxy_cache_path,"Sets the path and other parameters of a cache. Cache data are stored in files. The file name in a cache is a result of applying the MD5 function to the cache key. The levels parameter defines hierarchy levels of a cache: from 1 to 3, each level accepts values 1 or 2. For example, in the following configuration",0,0,others,nginx,0,0
6118,proxy_cache_purge,Defines conditions under which the request will be considered a cache purge request. If at least one value of the string parameters is not empty and is not equal to 0 then the cache entry with a corresponding cache key is removed. The result of successful operation is indicated by returning the 204 (No Content) response.,0,0,others,nginx,0,0
6119,proxy_cache_revalidate,Enables revalidation of expired cache items using conditional requests with the If-Modified-Since and If-None-Match header fields.,1,2,security-tradeoff,nginx,0,0
6120,proxy_cache_use_stale,Determines in which cases a stale cached response can be used during communication with the proxied server. The directives parameters match the parameters of the proxy_next_upstream directive.,0,0,others,nginx,0,0
6121,proxy_cache_valid,"Sets caching time for different response codes. For example, the following directives",0,0,others,nginx,0,0
6122,proxy_connect_timeout.B,Defines a timeout for establishing a connection with a proxied server.,0,0,others,nginx,0,1
6123,proxy_connect_timeout,Defines a timeout for establishing a connection with a proxied server. It should be noted that this timeout cannot usually exceed 75 seconds.,0,0,others,nginx,0,0
6124,proxy_cookie_domain,Sets a text that should be changed in the domain attribute of the Set-Cookie header fields of a proxied server response. Suppose a proxied server returned the Set-Cookie header field with the attribute domain=localhost. The directive,0,0,others,nginx,0,1
6125,proxy_cookie_flags,"Sets one or more flags for the cookie. The cookie can contain text, variables, and their combinations. The flag can contain text, variables, and their combinations (1.19.8). The secure, httponly, samesite=strict, samesite=lax, samesite=none parameters add the corresponding flags. The nosecure, nohttponly, nosamesite parameters remove the corresponding flags.",0,0,others,nginx,0,0
6126,proxy_cookie_path,Sets a text that should be changed in the path attribute of the Set-Cookie header fields of a proxied server response. Suppose a proxied server returned the Set-Cookie header field with the attribute path=/two/some/uri/. The directive,0,0,others,nginx,0,1
6127,proxy_download_rate,"Limits the speed of reading the data from the proxied server. The rate is specified in bytes per second. The zero value disables rate limiting. The limit is set per a connection, so if nginx simultaneously opens two connections to the proxied server, the overall rate will be twice as much as the specified limit.",0,0,others,nginx,0,0
6128,proxy_force_ranges,Enables byte-range support for both cached and uncached responses from the proxied server regardless of the Accept-Ranges field in these responses.,1,4,limited-side-effect,nginx,0,0
6129,proxy_headers_hash_bucket_size,Sets the bucket size for hash tables used by the proxy_hide_header and proxy_set_header directives. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx,0,0
6130,proxy_headers_hash_max_size,Sets the maximum size of hash tables used by the proxy_hide_header and proxy_set_header directives. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx,0,1
6131,proxy_hide_header,"By default, nginx does not pass the header fields Date, Server, X-Pad, and X-Accel-... from the response of a proxied server to a client. The proxy_hide_header directive sets additional fields that will not be passed. If, on the contrary, the passing of fields needs to be permitted, the proxy_pass_header directive can be used.",1,2,security-tradeoff,nginx,0,0
6132,proxy_http_version,"Sets the HTTP protocol version for proxying. By default, version 1.0 is used. Version 1.1 is recommended for use with keepalive connections and NTLM authentication.",0,0,others,nginx,0,0
6133,proxy_ignore_client_abort,Determines whether the connection with a proxied server should be closed when a client closes the connection without waiting for a response.,0,0,others,nginx,0,0
6134,proxy_ignore_headers,"Disables processing of certain response header fields from the proxied server. The following fields can be ignored: X-Accel-Redirect, X-Accel-Expires, X-Accel-Limit-Rate (1.1.6), X-Accel-Buffering (1.1.6), X-Accel-Charset (1.1.6), Expires, Cache-Control, Set-Cookie (0.8.44), and Vary (1.7.7).",0,0,others,nginx,0,0
6135,proxy_intercept_errors,Determines whether proxied responses with codes greater than or equal to 300 should be passed to a client or be intercepted and redirected to nginx for processing with the error_page directive.,1,6,function-tradeoff,nginx,0,0
6136,proxy_limit_rate,"Limits the speed of reading the response from the proxied server. The rate is specified in bytes per second. The zero value disables rate limiting. The limit is set per a request, and so if nginx simultaneously opens two connections to the proxied server, the overall rate will be twice as much as the specified limit. The limitation works only if buffering of responses from the proxied server is enabled.",0,0,others,nginx,0,0
6137,proxy_max_temp_file_size,"When buffering of responses from the proxied server is enabled, and the whole response does not fit into the buffers set by the proxy_buffer_size and proxy_buffers directives, a part of the response can be saved to a temporary file. This directive sets the maximum size of the temporary file. The size of data written to the temporary file at a time is set by the proxy_temp_file_write_size directive.",1,5,workload-specific,nginx,0,1
6138,proxy_method,Specifies the HTTP method to use in requests forwarded to the proxied server instead of the method from the client request. Parameter value can contain variables (1.11.6).,0,0,others,nginx,0,1
6139,proxy_next_upstream.B,Specifies in which cases a request should be passed to the next server:,0,0,others,nginx,0,0
6140,proxy_next_upstream,"When a connection to the proxied server cannot be established, determines whether a client connection will be passed to the next server.",0,0,others,nginx,0,0
6141,proxy_next_upstream_timeout.B,Limits the time allowed to pass a connection to the next server. The 0 value turns off this limitation.,0,0,others,nginx,0,1
6142,proxy_next_upstream_timeout,Limits the time during which a request can be passed to the next server. The 0 value turns off this limitation.,0,0,others,nginx,0,0
6143,proxy_next_upstream_tries.B,Limits the number of possible tries for passing a connection to the next server. The 0 value turns off this limitation.,0,0,others,nginx,0,0
6144,proxy_next_upstream_tries,Limits the number of possible tries for passing a request to the next server. The 0 value turns off this limitation.,0,0,others,nginx,0,0
6145,proxy_no_cache,Defines conditions under which the response will not be saved to a cache. If at least one value of the string parameters is not empty and is not equal to 0 then the response will not be saved:,0,0,others,nginx,0,0
6146,proxy_pass.B,"Sets the address of a proxied server. The address can be specified as a domain name or IP address, and a port:",0,0,others,nginx,0,0
6147,proxy_pass,"Sets the protocol and address of a proxied server and an optional URI to which a location should be mapped. As a protocol, http or https can be specified. The address can be specified as a domain name or IP address, and an optional port:",0,0,others,nginx,0,0
6148,proxy_pass_error_message,Indicates whether to pass the error message obtained during the authentication on the backend to the client.,1,6,function-tradeoff,nginx,0,0
6149,proxy_pass_header,Permits passing otherwise disabled header fields from a proxied server to a client.,0,0,others,nginx,0,0
6150,proxy_pass_request_body,Indicates whether the original request body is passed to the proxied server.,0,0,others,nginx,0,0
6151,proxy_pass_request_headers,Indicates whether the header fields of the original request are passed to the proxied server.,0,0,others,nginx,0,0
6152,proxy_protocol.B,Enables the PROXY protocol for connections to a backend.,1,6,function-tradeoff,nginx,0,0
6153,proxy_protocol,Enables the PROXY protocol for connections to a proxied server.,0,0,others,nginx,0,0
6154,proxy_protocol_timeout,"Specifies a timeout for reading the PROXY protocol header to complete. If no entire header is transmitted within this time, the connection is closed.",0,0,others,nginx,0,1
6155,proxy_read_timeout,"Defines a timeout for reading a response from the proxied server. The timeout is set only between two successive read operations, not for the transmission of the whole response. If the proxied server does not transmit anything within this time, the connection is closed.",0,0,others,nginx,0,0
6156,proxy_redirect,Sets the text that should be changed in the Location and Refresh header fields of a proxied server response. Suppose a proxied server returned the header field Location: http://localhost:8000/two/some/uri/. The directive,0,0,others,nginx,0,0
6157,proxy_request_buffering,Enables or disables buffering of a client request body.,1,6,function-tradeoff,nginx,0,1
6158,proxy_requests,"Sets the number of client datagrams at which binding between a client and existing UDP stream session is dropped. After receiving the specified number of datagrams, next datagram from the same client starts a new session. The session terminates when all client datagrams are transmitted to a proxied server and the expected number of responses is received, or when it reaches a timeout.",1,5,workload-specific,nginx,0,0
6159,proxy_responses,"Sets the number of datagrams expected from the proxied server in response to a client datagram if the UDP protocol is used. The number serves as a hint for session termination. By default, the number of datagrams is not limited.",1,5,workload-specific,nginx,0,0
6160,proxy_send_lowat,"If the directive is set to a non-zero value, nginx will try to minimize the number of send operations on outgoing connections to a proxied server by using either NOTE_LOWAT flag of the kqueue method, or the SO_SNDLOWAT socket option, with the specified size.",1,5,workload-specific,nginx,0,0
6161,proxy_send_timeout,"Sets a timeout for transmitting a request to the proxied server. The timeout is set only between two successive write operations, not for the transmission of the whole request. If the proxied server does not receive anything within this time, the connection is closed.",0,0,others,nginx,0,1
6162,proxy_session_drop,Enables terminating all sessions to a proxied server after it was removed from the group or marked as permanently unavailable. This can occur because of re-resolve or with the API DELETE command. A server can be marked as permanently unavailable if it is considered unhealthy or with the API PATCH command. Each session is terminated when the next read or write event is processed for the client or proxied server.,0,0,others,nginx,0,0
6163,proxy_set_body,"Allows redefining the request body passed to the proxied server. The value can contain text, variables, and their combination.",0,0,others,nginx,0,1
6164,proxy_set_header,"Allows redefining or appending fields to the request header passed to the proxied server. The value can contain text, variables, and their combinations. These directives are inherited from the previous configuration level if and only if there are no proxy_set_header directives defined on the current level. By default, only two fields are redefined:",0,0,others,nginx,0,0
6165,proxy_smtp_auth,Enables or disables user authentication on the SMTP backend using the AUTH command.,1,2,security-tradeoff,nginx,0,0
6166,proxy_socket_keepalive,"Configures the TCP keepalive behavior for outgoing connections to a proxied server. By default, the operating systems settings are in effect for the socket. If the directive is set to the value on, the SO_KEEPALIVE socket option is turned on for the socket.",0,0,others,nginx,0,0
6167,proxy_ssl,Enables the SSL/TLS protocol for connections to a proxied server.,1,2,security-tradeoff,nginx,0,1
6168,proxy_ssl_certificate.B,Specifies a file with the certificate in the PEM format used for authentication to a proxied HTTPS server.,0,0,others,nginx,0,1
6169,proxy_ssl_certificate,Specifies a file with the certificate in the PEM format used for authentication to a proxied server.,0,0,others,nginx,0,0
6170,proxy_ssl_certificate_key.B,Specifies a file with the secret key in the PEM format used for authentication to a proxied HTTPS server.,0,0,others,nginx,0,0
6171,proxy_ssl_certificate_key,Specifies a file with the secret key in the PEM format used for authentication to a proxied server.,0,0,others,nginx,0,0
6172,proxy_ssl_ciphers.B,Specifies the enabled ciphers for connections to a proxied server. The ciphers are specified in the format understood by the OpenSSL library.,0,0,others,nginx,0,1
6173,proxy_ssl_ciphers,Specifies the enabled ciphers for requests to a proxied HTTPS server. The ciphers are specified in the format understood by the OpenSSL library.,0,0,others,nginx,0,0
6174,proxy_ssl_conf_command.B,Sets arbitrary OpenSSL configuration commands when establishing a connection with the proxied HTTPS server.,0,0,others,nginx,0,0
6175,proxy_ssl_conf_command,Sets arbitrary OpenSSL configuration commands when establishing a connection with the proxied server.,0,0,others,nginx,0,0
6176,proxy_ssl_crl.B,Specifies a file with revoked certificates (CRL) in the PEM format used to verify the certificate of the proxied HTTPS server.,0,0,others,nginx,0,0
6177,proxy_ssl_crl,Specifies a file with revoked certificates (CRL) in the PEM format used to verify the certificate of the proxied server.,0,0,others,nginx,0,1
6178,proxy_ssl_name.B,Allows overriding the server name used to verify the certificate of the proxied HTTPS server and to be passed through SNI when establishing a connection with the proxied HTTPS server.,0,0,others,nginx,0,1
6179,proxy_ssl_name,Allows overriding the server name used to verify the certificate of the proxied server and to be passed through SNI when establishing a connection with the proxied server. The server name can also be specified using variables (1.11.3).,0,0,others,nginx,0,0
6180,proxy_ssl_password_file,Specifies a file with passphrases for secret keys where each passphrase is specified on a separate line. Passphrases are tried in turn when loading the key.,0,0,others,nginx,0,0
6181,proxy_ssl_protocols.B,Enables the specified protocols for connections to a proxied server.,0,0,others,nginx,0,0
6182,proxy_ssl_protocols,Enables the specified protocols for requests to a proxied HTTPS server.,1,2,security-tradeoff,nginx,0,0
6183,proxy_ssl_server_name.B,"Enables or disables passing of the server name through TLS Server Name Indication extension (SNI, RFC 6066) when establishing a connection with the proxied HTTPS server.",0,0,others,nginx,0,0
6184,proxy_ssl_server_name,"Enables or disables passing of the server name through TLS Server Name Indication extension (SNI, RFC 6066) when establishing a connection with the proxied server.",0,0,others,nginx,0,0
6185,proxy_ssl_session_reuse,"Determines whether SSL sessions can be reused when working with the proxied server. If the errors SSL3_GET_FINISHED:digest check failed appear in the logs, try disabling session reuse.",1,4,limited-side-effect,nginx,0,0
6186,proxy_ssl_trusted_certificate.B,Specifies a file with trusted CA certificates in the PEM format used to verify the certificate of the proxied HTTPS server.,0,0,others,nginx,0,0
6187,proxy_ssl_trusted_certificate,Specifies a file with trusted CA certificates in the PEM format used to verify the certificate of the proxied server.,0,0,others,nginx,0,0
6188,proxy_ssl_verify.B,Enables or disables verification of the proxied HTTPS server certificate.,1,2,security-tradeoff,nginx,0,0
6189,proxy_ssl_verify,Enables or disables verification of the proxied server certificate.,1,2,security-tradeoff,nginx,0,0
6190,proxy_ssl_verify_depth.B,Sets the verification depth in the proxied HTTPS server certificates chain.,1,2,security-tradeoff,nginx,0,0
6191,proxy_ssl_verify_depth,Sets the verification depth in the proxied server certificates chain.,1,2,security-tradeoff,nginx,0,0
6192,proxy_store,"Enables saving of files to a disk. The on parameter saves files with paths corresponding to the directives alias or root. The off parameter disables saving of files. In addition, the file name can be set explicitly using the string with variables:",0,0,others,nginx,0,0
6193,proxy_store_access,"Sets access permissions for newly created files and directories, e.g.:",0,0,others,nginx,0,0
6194,proxy_temp_file_write_size,"Limits the size of data written to a temporary file at a time, when buffering of responses from the proxied server to temporary files is enabled. By default, size is limited by two buffers set by the proxy_buffer_size and proxy_buffers directives. The maximum size of a temporary file is set by the proxy_max_temp_file_size directive.",1,5,workload-specific,nginx,0,1
6195,proxy_temp_path,"Defines a directory for storing temporary files with data received from proxied servers. Up to three-level subdirectory hierarchy can be used underneath the specified directory. For example, in the following configuration",0,0,others,nginx,0,0
6196,proxy_timeout,"Sets the timeout between two successive read or write operations on client or proxied server connections. If no data is transmitted within this time, the connection is closed.",0,0,others,nginx,0,1
6197,proxy_upload_rate,"Limits the speed of reading the data from the client. The rate is specified in bytes per second. The zero value disables rate limiting. The limit is set per a connection, so if the client simultaneously opens two connections, the overall rate will be twice as much as the specified limit.",0,0,others,nginx,0,0
6198,queue,"If an upstream server cannot be selected immediately while processing a request, the request will be placed into the queue. The directive specifies the maximum number of requests that can be in the queue at the same time. If the queue is filled up, or the server to pass the request to cannot be selected within the time period specified in the timeout parameter, the 502 (Bad Gateway) error will be returned to the client.",0,0,others,nginx,0,0
6199,random.B,"Specifies that a group should use a load balancing method where a connection is passed to a randomly selected server, taking into account weights of servers.",0,0,others,nginx,0,0
6200,random,"Specifies that a group should use a load balancing method where a request is passed to a randomly selected server, taking into account weights of servers.",0,0,others,nginx,0,0
6201,random_index,Enables or disables module processing in a surrounding location.,0,0,others,nginx,0,0
6202,read_ahead,Sets the amount of pre-reading for the kernel when working with file.,1,5,workload-specific,nginx,0,0
6203,real_ip_header,Defines the request header field whose value will be used to replace the client address.,0,0,others,nginx,0,1
6204,real_ip_recursive,"If recursive search is disabled, the original client address that matches one of the trusted addresses is replaced by the last address sent in the request header field defined by the real_ip_header directive. If recursive search is enabled, the original client address that matches one of the trusted addresses is replaced by the last non-trusted address sent in the request header field.",0,0,others,nginx,0,0
6205,recursive_error_pages,Enables or disables doing several redirects using the error_page directive. The number of such redirects is limited.,1,4,limited-side-effect,nginx,0,0
6206,referer_hash_bucket_size,Sets the bucket size for the valid referers hash tables. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx,0,0
6207,referer_hash_max_size,Sets the maximum size of the valid referers hash tables. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx,0,0
6208,request_pool_size,Allows accurate tuning of per-request memory allocations. This directive has minimal impact on performance and should not generally be used.,1,5,workload-specific,nginx,0,0
6209,reset_timedout_connection,"Enables or disables resetting timed out connections and connections closed with the non-standard code 444 (1.15.2). The reset is performed as follows. Before closing a socket, the SO_LINGER option is set on it with a timeout value of 0. When the socket is closed, TCP RST is sent to the client, and all memory occupied by this socket is released. This helps avoid keeping an already closed socket with filled buffers in a FIN_WAIT1 state for a long time.",0,0,others,nginx,0,0
6210,resolver,"Configures name servers used to find the clients hostname to pass it to the authentication server, and in the XCLIENT command when proxying SMTP. For example:",0,0,others,nginx,0,0
6211,resolver_timeout.B,"Sets a timeout for DNS operations, for example:",0,0,others,nginx,0,0
6212,resolver_timeout,"Sets a timeout for name resolution, for example:",0,0,others,nginx,0,1
6213,return.B,"Specifies a value to send to the client. The value can contain text, variables, and their combination.",0,0,others,nginx,0,0
6214,return,Stops processing and returns the specified code to a client. The non-standard code 444 closes a connection without sending a response header.,0,0,others,nginx,0,0
6215,rewrite,"If the specified regular expression matches a request URI, URI is changed as specified in the replacement string. The rewrite directives are executed sequentially in order of their appearance in the configuration file. It is possible to terminate further processing of the directives using flags. If a replacement string starts with http://, https://, or $scheme, the processing stops and the redirect is returned to a client.",1,2,security-tradeoff,nginx,0,0
6216,rewrite_log,Enables or disables logging of ngx_http_rewrite_module module directives processing results into the error_log at the notice level.,1,6,function-tradeoff,nginx,0,0
6217,root,"Sets the root directory for requests. For example, with the following configuration",0,0,others,nginx,0,0
6218,satisfy,"Allows access if all (all) or at least one (any) of the ngx_http_access_module, ngx_http_auth_basic_module, ngx_http_auth_request_module, or ngx_http_auth_jwt_module modules allow access.",0,0,others,nginx,0,1
6219,scgi_bind,"Makes outgoing connections to an SCGI server originate from the specified local IP address with an optional port (1.11.2). Parameter value can contain variables (1.3.12). The special value off (1.3.12) cancels the effect of the scgi_bind directive inherited from the previous configuration level, which allows the system to auto-assign the local IP address and port.",0,0,others,nginx,0,0
6220,scgi_buffer_size,"Sets the size of the buffer used for reading the first part of the response received from the SCGI server. This part usually contains a small response header. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform. It can be made smaller, however.",1,1,resource,nginx,0,0
6221,scgi_buffering,Enables or disables buffering of responses from the SCGI server.,1,6,function-tradeoff,nginx,0,0
6222,scgi_buffers,"Sets the number and size of the buffers used for reading a response from the SCGI server, for a single connection. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform.",1,1,resource,nginx,0,0
6223,scgi_busy_buffers_size,"When buffering of responses from the SCGI server is enabled, limits the total size of buffers that can be busy sending a response to the client while the response is not yet fully read. In the meantime, the rest of the buffers can be used for reading the response and, if needed, buffering part of the response to a temporary file. By default, size is limited by the size of two buffers set by the scgi_buffer_size and scgi_buffers directives.",1,1,resource,nginx,0,0
6224,scgi_cache,Defines a shared memory zone used for caching. The same zone can be used in several places. Parameter value can contain variables (1.7.9). The off parameter disables caching inherited from the previous configuration level.,1,4,limited-side-effect,nginx,0,0
6225,scgi_cache_background_update,"Allows starting a background subrequest to update an expired cache item, while a stale cached response is returned to the client. Note that it is necessary to allow the usage of a stale cached response when it is being updated.",1,6,function-tradeoff,nginx,0,0
6226,scgi_cache_bypass,Defines conditions under which the response will not be taken from a cache. If at least one value of the string parameters is not empty and is not equal to 0 then the response will not be taken from the cache:,0,0,others,nginx,0,0
6227,scgi_cache_key,"Defines a key for caching, for example",0,0,others,nginx,0,1
6228,scgi_cache_lock,"When enabled, only one request at a time will be allowed to populate a new cache element identified according to the scgi_cache_key directive by passing a request to an SCGI server. Other requests of the same cache element will either wait for a response to appear in the cache or the cache lock for this element to be released, up to the time set by the scgi_cache_lock_timeout directive.",0,0,others,nginx,0,0
6229,scgi_cache_lock_age,"If the last request passed to the SCGI server for populating a new cache element has not completed for the specified time, one more request may be passed to the SCGI server.",0,0,others,nginx,0,1
6230,scgi_cache_lock_timeout,"Sets a timeout for scgi_cache_lock. When the time expires, the request will be passed to the SCGI server, however, the response will not be cached.",0,0,others,nginx,0,1
6231,scgi_cache_max_range_offset,"Sets an offset in bytes for byte-range requests. If the range is beyond the offset, the range request will be passed to the SCGI server and the response will not be cached.",1,5,workload-specific,nginx,0,0
6232,scgi_cache_methods,"If the client request method is listed in this directive then the response will be cached. GET and HEAD methods are always added to the list, though it is recommended to specify them explicitly. See also the scgi_no_cache directive.",0,0,others,nginx,0,0
6233,scgi_cache_min_uses,Sets the number of requests after which the response will be cached.,1,5,workload-specific,nginx,0,0
6234,scgi_cache_path,"Sets the path and other parameters of a cache. Cache data are stored in files. The file name in a cache is a result of applying the MD5 function to the cache key. The levels parameter defines hierarchy levels of a cache: from 1 to 3, each level accepts values 1 or 2. For example, in the following configuration",0,0,others,nginx,0,1
6235,scgi_cache_purge,Defines conditions under which the request will be considered a cache purge request. If at least one value of the string parameters is not empty and is not equal to 0 then the cache entry with a corresponding cache key is removed. The result of successful operation is indicated by returning the 204 (No Content) response.,0,0,others,nginx,0,0
6236,scgi_cache_revalidate,Enables revalidation of expired cache items using conditional requests with the If-Modified-Since and If-None-Match header fields.,1,2,security-tradeoff,nginx,0,0
6237,scgi_cache_use_stale,Determines in which cases a stale cached response can be used when an error occurs during communication with the SCGI server. The directives parameters match the parameters of the scgi_next_upstream directive.,0,0,others,nginx,0,1
6238,scgi_cache_valid,"Sets caching time for different response codes. For example, the following directives",0,0,others,nginx,0,0
6239,scgi_connect_timeout,Defines a timeout for establishing a connection with an SCGI server. It should be noted that this timeout cannot usually exceed 75 seconds.,0,0,others,nginx,0,1
6240,scgi_force_ranges,Enables byte-range support for both cached and uncached responses from the SCGI server regardless of the Accept-Ranges field in these responses.,0,0,others,nginx,0,0
6241,scgi_hide_header,"By default, nginx does not pass the header fields Status and X-Accel-... from the response of an SCGI server to a client. The scgi_hide_header directive sets additional fields that will not be passed. If, on the contrary, the passing of fields needs to be permitted, the scgi_pass_header directive can be used.",0,0,others,nginx,0,0
6242,scgi_ignore_client_abort,Determines whether the connection with an SCGI server should be closed when a client closes the connection without waiting for a response.,0,0,others,nginx,0,0
6243,scgi_ignore_headers,"Disables processing of certain response header fields from the SCGI server. The following fields can be ignored: X-Accel-Redirect, X-Accel-Expires, X-Accel-Limit-Rate (1.1.6), X-Accel-Buffering (1.1.6), X-Accel-Charset (1.1.6), Expires, Cache-Control, Set-Cookie (0.8.44), and Vary (1.7.7).",0,0,others,nginx,0,0
6244,scgi_intercept_errors,Determines whether an SCGI server responses with codes greater than or equal to 300 should be passed to a client or be intercepted and redirected to nginx for processing with the error_page directive.,0,0,others,nginx,0,1
6245,scgi_limit_rate,"Limits the speed of reading the response from the SCGI server. The rate is specified in bytes per second. The zero value disables rate limiting. The limit is set per a request, and so if nginx simultaneously opens two connections to the SCGI server, the overall rate will be twice as much as the specified limit. The limitation works only if buffering of responses from the SCGI server is enabled.",0,0,others,nginx,0,0
6246,scgi_max_temp_file_size,"When buffering of responses from the SCGI server is enabled, and the whole response does not fit into the buffers set by the scgi_buffer_size and scgi_buffers directives, a part of the response can be saved to a temporary file. This directive sets the maximum size of the temporary file. The size of data written to the temporary file at a time is set by the scgi_temp_file_write_size directive.",1,5,workload-specific,nginx,0,0
6247,scgi_next_upstream,Specifies in which cases a request should be passed to the next server:,0,0,others,nginx,0,1
6248,scgi_next_upstream_timeout,Limits the time during which a request can be passed to the next server. The 0 value turns off this limitation.,0,0,others,nginx,0,0
6249,scgi_next_upstream_tries,Limits the number of possible tries for passing a request to the next server. The 0 value turns off this limitation.,0,0,others,nginx,0,0
6250,scgi_no_cache,Defines conditions under which the response will not be saved to a cache. If at least one value of the string parameters is not empty and is not equal to 0 then the response will not be saved:,0,0,others,nginx,0,0
6251,scgi_param,"Sets a parameter that should be passed to the SCGI server. The value can contain text, variables, and their combination. These directives are inherited from the previous configuration level if and only if there are no scgi_param directives defined on the current level.",0,0,others,nginx,0,0
6252,scgi_pass,"Sets the address of an SCGI server. The address can be specified as a domain name or IP address, and a port:",0,0,others,nginx,0,0
6253,scgi_pass_header,Permits passing otherwise disabled header fields from an SCGI server to a client.,0,0,others,nginx,0,1
6254,scgi_pass_request_body,Indicates whether the original request body is passed to the SCGI server. See also the scgi_pass_request_headers directive.,0,0,others,nginx,0,0
6255,scgi_pass_request_headers,Indicates whether the header fields of the original request are passed to the SCGI server. See also the scgi_pass_request_body directive.,0,0,others,nginx,0,0
6256,scgi_read_timeout,"Defines a timeout for reading a response from the SCGI server. The timeout is set only between two successive read operations, not for the transmission of the whole response. If the SCGI server does not transmit anything within this time, the connection is closed.",0,0,others,nginx,0,0
6257,scgi_request_buffering,Enables or disables buffering of a client request body.,1,6,function-tradeoff,nginx,0,0
6258,scgi_send_timeout,"Sets a timeout for transmitting a request to the SCGI server. The timeout is set only between two successive write operations, not for the transmission of the whole request. If the SCGI server does not receive anything within this time, the connection is closed.",0,0,others,nginx,0,0
6259,scgi_socket_keepalive,"Configures the TCP keepalive behavior for outgoing connections to an SCGI server. By default, the operating systems settings are in effect for the socket. If the directive is set to the value on, the SO_KEEPALIVE socket option is turned on for the socket.",0,0,others,nginx,0,0
6260,scgi_store,"Enables saving of files to a disk. The on parameter saves files with paths corresponding to the directives alias or root. The off parameter disables saving of files. In addition, the file name can be set explicitly using the string with variables:",0,0,others,nginx,0,1
6261,scgi_store_access,"Sets access permissions for newly created files and directories, e.g.:",0,0,others,nginx,0,0
6262,scgi_temp_file_write_size,"Limits the size of data written to a temporary file at a time, when buffering of responses from the SCGI server to temporary files is enabled. By default, size is limited by two buffers set by the scgi_buffer_size and scgi_buffers directives. The maximum size of a temporary file is set by the scgi_max_temp_file_size directive.",1,5,workload-specific,nginx,0,0
6263,scgi_temp_path,"Defines a directory for storing temporary files with data received from SCGI servers. Up to three-level subdirectory hierarchy can be used underneath the specified directory. For example, in the following configuration",0,0,others,nginx,0,0
6264,secure_link,Defines a string with variables from which the checksum value and lifetime of a link will be extracted.,0,0,others,nginx,0,0
6265,secure_link_md5,Defines an expression for which the MD5 hash value will be computed and compared with the value passed in a request.,0,0,others,nginx,0,0
6266,secure_link_secret,Defines a secret word used to check authenticity of requested links.,0,0,others,nginx,0,0
6267,send_lowat,"If the directive is set to a non-zero value, nginx will try to minimize the number of send operations on client sockets by using either NOTE_LOWAT flag of the kqueue method or the SO_SNDLOWAT socket option. In both cases the specified size is used.",1,5,workload-specific,nginx,0,0
6268,send_timeout,"Sets a timeout for transmitting a response to the client. The timeout is set only between two successive write operations, not for the transmission of the whole response. If the client does not receive anything within this time, the connection is closed.",0,0,others,nginx,0,0
6269,sendfile,Enables or disables the use of sendfile().,0,0,others,nginx,0,0
6270,sendfile_max_chunk,"When set to a non-zero value, limits the amount of data that can be transferred in a single sendfile() call. Without the limit, one fast connection may seize the worker process entirely.",1,4,limited-side-effect,nginx,0,0
6271,server.B,"Defines the address and other parameters of a server. The address can be specified as a domain name or IP address with an obligatory port, or as a UNIX-domain socket path specified after the unix: prefix. A domain name that resolves to several IP addresses defines multiple servers at once.",0,0,others,nginx,0,0
6272,server.C,"Defines the address and other parameters of a server. The address can be specified as a domain name or IP address, with an optional port, or as a UNIX-domain socket path specified after the unix: prefix. If a port is not specified, the port 80 is used. A domain name that resolves to several IP addresses defines multiple servers at once.",0,0,others,nginx,0,0
6273,server.D,"Sets configuration for a virtual server. There is no clear separation between IP-based (based on the IP address) and name-based (based on the Host request header field) virtual servers. Instead, the listen directives describe all addresses and ports that should accept connections for the server, and the server_name directive lists all server names. Example configurations are provided in the How nginx processes a request document.",0,0,others,nginx,0,1
6274,server,Sets the configuration for a server.,0,0,others,nginx,0,0
6275,server_name.B,"Sets names of a virtual server, for example:",0,0,others,nginx,0,0
6276,server_name,Sets the server name that is used:,0,0,others,nginx,0,0
6277,server_name_in_redirect,"Enables or disables the use of the primary server name, specified by the server_name directive, in absolute redirects issued by nginx. When the use of the primary server name is disabled, the name from the Host request header field is used. If this field is not present, the IP address of the server is used.",0,0,others,nginx,0,1
6278,server_names_hash_bucket_size,Sets the bucket size for the server names hash tables. The default value depends on the size of the processors cache line. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx,0,1
6279,server_names_hash_max_size,Sets the maximum size of the server names hash tables. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx,0,0
6280,server_tokens,Enables or disables emitting nginx version on error pages and in the Server response header field.,1,6,function-tradeoff,nginx,0,0
6281,session_log,Enables the use of the specified session log. The special value off cancels the effect of the session_log directives inherited from the previous configuration level.,0,0,others,nginx,0,1
6282,session_log_format,Specifies the output format of a log. The value of the $body_bytes_sent variable is aggregated across all requests in a session. The values of all other variables available for logging correspond to the first request in a session.,0,0,others,nginx,0,0
6283,session_log_zone,Sets the path to a log file and configures the shared memory zone that is used to store currently active sessions.,0,0,others,nginx,0,0
6284,set,"Sets a value for the specified variable. The value can contain text, variables, and their combination.",0,0,others,nginx,0,0
6285,set_real_ip_from.B,"Defines trusted addresses that are known to send correct replacement addresses. If the special value unix: is specified, all UNIX-domain sockets will be trusted.",0,0,others,nginx,0,0
6286,set_real_ip_from,"Defines trusted addresses that are known to send correct replacement addresses. If the special value unix: is specified, all UNIX-domain sockets will be trusted. Trusted addresses may also be specified using a hostname (1.13.1).",0,0,others,nginx,0,0
6287,slice,Sets the size of the slice. The zero value disables splitting responses into slices. Note that a too low value may result in excessive memory usage and opening a large number of files.,1,5,workload-specific,nginx,0,0
6288,smtp_auth,Sets permitted methods of SASL authentication for SMTP clients. Supported methods are:,1,2,security-tradeoff,nginx,0,0
6289,smtp_capabilities,Sets the SMTP protocol extensions list that is passed to the client in response to the EHLO command. The authentication methods specified in the smtp_auth directive and STARTTLS are automatically added to this list depending on the starttls directive value.,1,2,security-tradeoff,nginx,0,0
6290,smtp_client_buffer,"Sets the size of the buffer used for reading SMTP commands. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform.",1,1,resource,nginx,0,1
6291,smtp_greeting_delay,Allows setting a delay before sending an SMTP greeting in order to reject clients who fail to wait for the greeting before sending SMTP commands.,0,0,others,nginx,0,0
6292,source_charset,"Defines the source charset of a response. If this charset is different from the charset specified in the charset directive, a conversion is performed.",0,0,others,nginx,0,0
6293,spdy_chunk_size,Sets the maximum size of chunks into which the response body is sliced. A too low value results in higher overhead. A too high value impairs prioritization due to HOL blocking.,1,5,workload-specific,nginx,0,0
6294,spdy_headers_comp,"Sets the header compression level of a response in a range from 1 (fastest, less compression) to 9 (slowest, best compression). The special value 0 turns off the header compression.",1,6,function-tradeoff,nginx,0,0
6295,split_clients,"Creates a variable for A/B testing, for example:",0,0,others,nginx,0,0
6296,ssi,Enables or disables processing of SSI commands in responses.,1,6,function-tradeoff,nginx,0,0
6297,ssi_last_modified,Allows preserving the Last-Modified header field from the original response during SSI processing to facilitate response caching.,1,4,limited-side-effect,nginx,0,0
6298,ssi_min_file_chunk,"Sets the minimum size for parts of a response stored on disk, starting from which it makes sense to send them using sendfile.",1,5,workload-specific,nginx,0,0
6299,ssi_silent_errors,"If enabled, suppresses the output of the [an error occurred while processing the directive] string if an error occurred during SSI processing.",0,0,others,nginx,0,1
6300,ssi_types,Enables processing of SSI commands in responses with the specified MIME types in addition to text/html. The special value * matches any MIME type (0.8.29).,0,0,others,nginx,0,0
6301,ssi_value_length,Sets the maximum length of parameter values in SSI commands.,0,0,others,nginx,0,0
6302,ssl,This directive was made obsolete in version 1.15.0. The ssl parameter of the listen directive should be used instead.,1,2,security-tradeoff,nginx,0,0
6303,ssl_buffer_size,Sets the size of the buffer used for sending data.,1,1,resource,nginx,0,0
6304,ssl_certificate.B,"Specifies a file with the certificate in the PEM format for the given server. If intermediate certificates should be specified in addition to a primary certificate, they should be specified in the same file in the following order: the primary certificate comes first, then the intermediate certificates. A secret key in the PEM format may be placed in the same file.",0,0,others,nginx,0,0
6305,ssl_certificate,"Specifies a file with the certificate in the PEM format for the given virtual server. If intermediate certificates should be specified in addition to a primary certificate, they should be specified in the same file in the following order: the primary certificate comes first, then the intermediate certificates. A secret key in the PEM format may be placed in the same file.",0,0,others,nginx,0,0
6306,ssl_certificate_key.B,Specifies a file with the secret key in the PEM format for the given server.,0,0,others,nginx,0,1
6307,ssl_certificate_key,Specifies a file with the secret key in the PEM format for the given virtual server.,0,0,others,nginx,0,0
6308,ssl_ciphers,"Specifies the enabled ciphers. The ciphers are specified in the format understood by the OpenSSL library, for example:",0,0,others,nginx,0,0
6309,ssl_client_certificate.B,Specifies a file with trusted CA certificates in the PEM format used to verify client certificates and OCSP responses if ssl_stapling is enabled.,0,0,others,nginx,0,1
6310,ssl_client_certificate,Specifies a file with trusted CA certificates in the PEM format used to verify client certificates.,0,0,others,nginx,0,0
6311,ssl_conf_command,Sets arbitrary OpenSSL configuration commands.,0,0,others,nginx,0,0
6312,ssl_crl,Specifies a file with revoked certificates (CRL) in the PEM format used to verify client certificates.,0,0,others,nginx,0,0
6313,ssl_dhparam,Specifies a file with DH parameters for DHE ciphers.,0,0,others,nginx,0,0
6314,ssl_early_data,Enables or disables TLS 1.3 early data.,1,2,security-tradeoff,nginx,0,0
6315,ssl_ecdh_curve,Specifies a curve for ECDHE ciphers.,0,0,others,nginx,0,0
6316,ssl_engine,Defines the name of the hardware SSL accelerator.,1,6,function-tradeoff,nginx,0,1
6317,ssl_handshake_timeout,Specifies a timeout for the SSL handshake to complete.,0,0,others,nginx,0,0
6318,ssl_ocsp,Enables OCSP validation of the client certificate chain. The leaf parameter enables validation of the client certificate only.,1,2,security-tradeoff,nginx,0,0
6319,ssl_ocsp_cache,Sets name and size of the cache that stores client certificates status for OCSP validation. The cache is shared between all worker processes. A cache with the same name can be used in several virtual servers.,1,1,resource,nginx,0,0
6320,ssl_ocsp_responder,Overrides the URL of the OCSP responder specified in the Authority Information Access certificate extension for validation of client certificates.,0,0,others,nginx,0,0
6321,ssl_password_file,Specifies a file with passphrases for secret keys where each passphrase is specified on a separate line. Passphrases are tried in turn when loading the key.,0,0,others,nginx,0,0
6322,ssl_prefer_server_ciphers.B,Specifies that server ciphers should be preferred over client ciphers when the SSLv3 and TLS protocols are used.,0,0,others,nginx,0,0
6323,ssl_prefer_server_ciphers,Specifies that server ciphers should be preferred over client ciphers when using the SSLv3 and TLS protocols.,0,0,others,nginx,0,1
6324,ssl_preread,Enables extracting information from the ClientHello message at the preread phase.,1,6,function-tradeoff,nginx,0,0
6325,ssl_protocols,Enables the specified protocols.,1,6,function-tradeoff,nginx,0,1
6326,ssl_reject_handshake,"If enabled, SSL handshakes in the server block will be rejected.",0,0,others,nginx,0,0
6327,ssl_session_cache.B,Sets the types and sizes of caches that store session parameters.,1,1,resource,nginx,0,0
6328,ssl_session_cache,Sets the types and sizes of caches that store session parameters. A cache can be of any of the following types:,0,0,others,nginx,0,0
6329,ssl_session_ticket_key,"Sets a file with the secret key used to encrypt and decrypt TLS session tickets. The directive is necessary if the same key has to be shared between multiple servers. By default, a randomly generated key is used.",0,0,others,nginx,0,0
6330,ssl_session_tickets,Enables or disables session resumption through TLS session tickets.,0,0,others,nginx,0,0
6331,ssl_session_timeout,Specifies a time during which a client may reuse the session parameters.,0,0,others,nginx,0,0
6332,ssl_stapling,Enables or disables stapling of OCSP responses by the server.,0,0,others,nginx,0,0
6333,ssl_stapling_file,"When set, the stapled OCSP response will be taken from the specified file instead of querying the OCSP responder specified in the server certificate.",0,0,others,nginx,0,0
6334,ssl_stapling_responder,Overrides the URL of the OCSP responder specified in the Authority Information Access certificate extension.,0,0,others,nginx,0,0
6335,ssl_stapling_verify,Enables or disables verification of OCSP responses by the server.,1,2,security-tradeoff,nginx,0,0
6336,ssl_trusted_certificate.B,Specifies a file with trusted CA certificates in the PEM format used to verify client certificates and OCSP responses if ssl_stapling is enabled.,0,0,others,nginx,0,0
6337,ssl_trusted_certificate,Specifies a file with trusted CA certificates in the PEM format used to verify client certificates.,0,0,others,nginx,0,0
6338,ssl_verify_client.B,Enables verification of client certificates. The verification result is passed in the Auth-SSL-Verify header of the authentication request.,0,0,others,nginx,0,0
6339,ssl_verify_client.C,Enables verification of client certificates. The verification result is stored in the $ssl_client_verify variable.,1,2,security-tradeoff,nginx,0,1
6340,ssl_verify_client,"Enables verification of client certificates. The verification result is stored in the $ssl_client_verify variable. If an error has occurred during the client certificate verification or a client has not presented the required certificate, the connection is closed.",1,2,security-tradeoff,nginx,0,0
6341,ssl_verify_depth,Sets the verification depth in the client certificates chain.,1,2,security-tradeoff,nginx,0,0
6342,state,Specifies a file that keeps the state of the dynamically configurable group.,0,0,others,nginx,0,0
6343,status,The status information will be accessible from the surrounding location. Access to this location should be limited.,0,0,others,nginx,0,0
6344,status_format,"By default, status information is output in the JSON format.",0,0,others,nginx,0,1
6345,status_zone.B,Enables collection of virtual http or stream (1.7.11) server status information in the specified zone. Several servers may share the same zone.,0,0,others,nginx,0,0
6346,status_zone,Enables collection of virtual http or stream server status information in the specified zone. Several servers may share the same zone.,0,0,others,nginx,0,0
6347,sticky,"Enables session affinity, which causes requests from the same client to be passed to the same server in a group of servers. Three methods are available:",0,0,others,nginx,0,1
6348,sticky_cookie_insert,This directive is obsolete since version 1.5.7. An equivalent sticky directive with a new syntax should be used instead:,0,0,others,nginx,0,1
6349,stream,Provides the configuration file context in which the stream server directives are specified.,0,0,others,nginx,0,1
6350,stub_status,The basic status information will be accessible from the surrounding location.,0,0,others,nginx,0,0
6351,sub_filter,Sets a string to replace and a replacement string. The string to replace is matched ignoring the case. The string to replace (1.9.4) and replacement string can contain variables. Several sub_filter directives can be specified on the same configuration level (1.9.4). These directives are inherited from the previous configuration level if and only if there are no sub_filter directives defined on the current level.,0,0,others,nginx,0,0
6352,sub_filter_last_modified,Allows preserving the Last-Modified header field from the original response during replacement to facilitate response caching.,0,0,others,nginx,0,0
6353,sub_filter_once,Indicates whether to look for each string to replace once or repeatedly.,0,0,others,nginx,0,0
6354,sub_filter_types,Enables string replacement in responses with the specified MIME types in addition to text/html. The special value * matches any MIME type (0.8.29).,0,0,others,nginx,0,0
6355,subrequest_output_buffer_size,"Sets the size of the buffer used for storing the response body of a subrequest. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform. It can be made smaller, however.",1,1,resource,nginx,0,0
6356,tcp_nodelay.B,Enables or disables the use of the TCP_NODELAY option. The option is enabled for both client and proxied server connections.,0,0,others,nginx,0,0
6357,tcp_nodelay,"Enables or disables the use of the TCP_NODELAY option. The option is enabled when a connection is transitioned into the keep-alive state. Additionally, it is enabled on SSL connections, for unbuffered proxying, and for WebSocket proxying.",1,6,function-tradeoff,nginx,0,0
6358,tcp_nopush,Enables or disables the use of the TCP_NOPUSH socket option on FreeBSD or the TCP_CORK socket option on Linux. The options are enabled only when sendfile is used. Enabling the option allows,1,6,function-tradeoff,nginx,0,0
6359,thread_pool,Defines the name and parameters of a thread pool used for multi-threaded reading and sending of files without blocking worker processes.,0,0,others,nginx,0,0
6360,timeout,Sets the timeout that is used before proxying to the backend starts.,0,0,others,nginx,0,1
6361,timer_resolution,"Reduces timer resolution in worker processes, thus reducing the number of gettimeofday() system calls made. By default, gettimeofday() is called each time a kernel event is received. With reduced resolution, gettimeofday() is only called once per specified interval.",0,0,others,nginx,0,0
6362,try_files,"Checks the existence of files in the specified order and uses the first found file for request processing; the processing is performed in the current context. The path to a file is constructed from the file parameter according to the root and alias directives. It is possible to check directorys existence by specifying a slash at the end of a name, e.g. $uri/. If none of the files were found, an internal redirect to the uri specified in the last parameter is made. For example:",0,0,others,nginx,0,0
6363,types,"Maps file name extensions to MIME types of responses. Extensions are case-insensitive. Several extensions can be mapped to one type, for example:",0,0,others,nginx,0,0
6364,types_hash_bucket_size,Sets the bucket size for the types hash tables. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx,0,0
6365,types_hash_max_size,Sets the maximum size of the types hash tables. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx,0,0
6366,underscores_in_headers,"Enables or disables the use of underscores in client request header fields. When the use of underscores is disabled, request header fields whose names contain underscores are marked as invalid and become subject to the ignore_invalid_headers directive.",0,0,others,nginx,0,0
6367,uninitialized_variable_warn,Controls whether warnings about uninitialized variables are logged.,1,6,function-tradeoff,nginx,0,0
6368,upstream,"Defines a group of servers. Servers can listen on different ports. In addition, servers listening on TCP and UNIX-domain sockets can be mixed.",0,0,others,nginx,0,0
6369,upstream_conf,Turns on the HTTP interface of upstream configuration in the surrounding location. Access to this location should be limited.,0,0,others,nginx,0,0
6370,use,"Specifies the connection processing method to use. There is normally no need to specify it explicitly, because nginx will by default use the most efficient method.",0,0,others,nginx,0,0
6371,user,"Defines user and group credentials used by worker processes. If group is omitted, a group whose name equals that of user is used.",0,0,others,nginx,0,0
6372,userid,Enables or disables setting cookies and logging the received cookies:,1,6,function-tradeoff,nginx,0,1
6373,userid_domain,Defines a domain for which the cookie is set. The none parameter disables setting of a domain for the cookie.,0,0,others,nginx,0,0
6374,userid_expires,Sets a time during which a browser should keep the cookie. The parameter max will cause the cookie to expire on 31 Dec 2037 23:55:55 GMT. The parameter off will cause the cookie to expire at the end of a browser session.,0,0,others,nginx,0,0
6375,userid_flags,"If the parameter is not off, defines one or more additional flags for the cookie: secure, httponly, samesite=strict, samesite=lax, samesite=none.",0,0,others,nginx,0,0
6376,userid_mark,"If the parameter is not off, enables the cookie marking mechanism and sets the character used as a mark. This mechanism is used to add or change userid_p3p and/or a cookie expiration time while preserving the client identifier. A mark can be any letter of the English alphabet (case-sensitive), digit, or the = character.",0,0,others,nginx,0,0
6377,userid_name,Sets the cookie name.,1,6,function-tradeoff,nginx,0,0
6378,userid_p3p,"Sets a value for the P3P header field that will be sent along with the cookie. If the directive is set to the special value none, the P3P header will not be sent in a response.",0,0,others,nginx,0,1
6379,userid_path,Defines a path for which the cookie is set.,0,0,others,nginx,0,0
6380,userid_service,"If identifiers are issued by multiple servers (services), each service should be assigned its own number to ensure that client identifiers are unique. For version 1 cookies, the default value is zero. For version 2 cookies, the default value is the number composed from the last four octets of the servers IP address.",0,0,others,nginx,0,0
6381,uwsgi_bind,"Makes outgoing connections to a uwsgi server originate from the specified local IP address with an optional port (1.11.2). Parameter value can contain variables (1.3.12). The special value off (1.3.12) cancels the effect of the uwsgi_bind directive inherited from the previous configuration level, which allows the system to auto-assign the local IP address and port.",1,4,limited-side-effect,nginx,0,1
6382,uwsgi_buffer_size,"Sets the size of the buffer used for reading the first part of the response received from the uwsgi server. This part usually contains a small response header. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform. It can be made smaller, however.",1,1,resource,nginx,0,0
6383,uwsgi_buffering,Enables or disables buffering of responses from the uwsgi server.,1,6,function-tradeoff,nginx,0,0
6384,uwsgi_buffers,"Sets the number and size of the buffers used for reading a response from the uwsgi server, for a single connection. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform.",1,1,resource,nginx,0,1
6385,uwsgi_busy_buffers_size,"When buffering of responses from the uwsgi server is enabled, limits the total size of buffers that can be busy sending a response to the client while the response is not yet fully read. In the meantime, the rest of the buffers can be used for reading the response and, if needed, buffering part of the response to a temporary file. By default, size is limited by the size of two buffers set by the uwsgi_buffer_size and uwsgi_buffers directives.",1,1,resource,nginx,0,0
6386,uwsgi_cache,Defines a shared memory zone used for caching. The same zone can be used in several places. Parameter value can contain variables (1.7.9). The off parameter disables caching inherited from the previous configuration level.,1,4,limited-side-effect,nginx,0,0
6387,uwsgi_cache_background_update,"Allows starting a background subrequest to update an expired cache item, while a stale cached response is returned to the client. Note that it is necessary to allow the usage of a stale cached response when it is being updated.",1,6,function-tradeoff,nginx,0,0
6388,uwsgi_cache_bypass,Defines conditions under which the response will not be taken from a cache. If at least one value of the string parameters is not empty and is not equal to 0 then the response will not be taken from the cache:,0,0,others,nginx,0,0
6389,uwsgi_cache_key,"Defines a key for caching, for example",0,0,others,nginx,0,1
6390,uwsgi_cache_lock,"When enabled, only one request at a time will be allowed to populate a new cache element identified according to the uwsgi_cache_key directive by passing a request to a uwsgi server. Other requests of the same cache element will either wait for a response to appear in the cache or the cache lock for this element to be released, up to the time set by the uwsgi_cache_lock_timeout directive.",0,0,others,nginx,0,0
6391,uwsgi_cache_lock_age,"If the last request passed to the uwsgi server for populating a new cache element has not completed for the specified time, one more request may be passed to the uwsgi server.",0,0,others,nginx,0,0
6392,uwsgi_cache_lock_timeout,"Sets a timeout for uwsgi_cache_lock. When the time expires, the request will be passed to the uwsgi server, however, the response will not be cached.",0,0,others,nginx,0,1
6393,uwsgi_cache_max_range_offset,"Sets an offset in bytes for byte-range requests. If the range is beyond the offset, the range request will be passed to the uwsgi server and the response will not be cached.",1,5,workload-specific,nginx,0,0
6394,uwsgi_cache_methods,"If the client request method is listed in this directive then the response will be cached. GET and HEAD methods are always added to the list, though it is recommended to specify them explicitly. See also the uwsgi_no_cache directive.",0,0,others,nginx,0,1
6395,uwsgi_cache_min_uses,Sets the number of requests after which the response will be cached.,1,5,workload-specific,nginx,0,0
6396,uwsgi_cache_path,"Sets the path and other parameters of a cache. Cache data are stored in files. The file name in a cache is a result of applying the MD5 function to the cache key. The levels parameter defines hierarchy levels of a cache: from 1 to 3, each level accepts values 1 or 2. For example, in the following configuration",0,0,others,nginx,0,0
6397,uwsgi_cache_purge,Defines conditions under which the request will be considered a cache purge request. If at least one value of the string parameters is not empty and is not equal to 0 then the cache entry with a corresponding cache key is removed. The result of successful operation is indicated by returning the 204 (No Content) response.,0,0,others,nginx,0,0
6398,uwsgi_cache_revalidate,Enables revalidation of expired cache items using conditional requests with the If-Modified-Since and If-None-Match header fields.,1,2,security-tradeoff,nginx,0,1
6399,uwsgi_cache_use_stale,Determines in which cases a stale cached response can be used when an error occurs during communication with the uwsgi server. The directives parameters match the parameters of the uwsgi_next_upstream directive.,0,0,others,nginx,0,0
6400,uwsgi_cache_valid,"Sets caching time for different response codes. For example, the following directives",0,0,others,nginx,0,0
6401,uwsgi_connect_timeout,Defines a timeout for establishing a connection with a uwsgi server. It should be noted that this timeout cannot usually exceed 75 seconds.,0,0,others,nginx,0,0
6402,uwsgi_force_ranges,Enables byte-range support for both cached and uncached responses from the uwsgi server regardless of the Accept-Ranges field in these responses.,0,0,others,nginx,0,0
6403,uwsgi_hide_header,"By default, nginx does not pass the header fields Status and X-Accel-... from the response of a uwsgi server to a client. The uwsgi_hide_header directive sets additional fields that will not be passed. If, on the contrary, the passing of fields needs to be permitted, the uwsgi_pass_header directive can be used.",0,0,others,nginx,0,1
6404,uwsgi_ignore_client_abort,Determines whether the connection with a uwsgi server should be closed when a client closes the connection without waiting for a response.,0,0,others,nginx,0,0
6405,uwsgi_ignore_headers,"Disables processing of certain response header fields from the uwsgi server. The following fields can be ignored: X-Accel-Redirect, X-Accel-Expires, X-Accel-Limit-Rate (1.1.6), X-Accel-Buffering (1.1.6), X-Accel-Charset (1.1.6), Expires, Cache-Control, Set-Cookie (0.8.44), and Vary (1.7.7).",0,0,others,nginx,0,0
6406,uwsgi_intercept_errors,Determines whether a uwsgi server responses with codes greater than or equal to 300 should be passed to a client or be intercepted and redirected to nginx for processing with the error_page directive.,0,0,others,nginx,0,0
6407,uwsgi_limit_rate,"Limits the speed of reading the response from the uwsgi server. The rate is specified in bytes per second. The zero value disables rate limiting. The limit is set per a request, and so if nginx simultaneously opens two connections to the uwsgi server, the overall rate will be twice as much as the specified limit. The limitation works only if buffering of responses from the uwsgi server is enabled.",0,0,others,nginx,0,0
6408,uwsgi_max_temp_file_size,"When buffering of responses from the uwsgi server is enabled, and the whole response does not fit into the buffers set by the uwsgi_buffer_size and uwsgi_buffers directives, a part of the response can be saved to a temporary file. This directive sets the maximum size of the temporary file. The size of data written to the temporary file at a time is set by the uwsgi_temp_file_write_size directive.",1,5,workload-specific,nginx,0,0
6409,uwsgi_modifier1,Sets the value of the modifier1 field in the uwsgi packet header.,0,0,others,nginx,0,0
6410,uwsgi_modifier2,Sets the value of the modifier2 field in the uwsgi packet header.,0,0,others,nginx,0,0
6411,uwsgi_next_upstream,Specifies in which cases a request should be passed to the next server:,0,0,others,nginx,0,0
6412,uwsgi_next_upstream_timeout,Limits the time during which a request can be passed to the next server. The 0 value turns off this limitation.,0,0,others,nginx,0,0
6413,uwsgi_next_upstream_tries,Limits the number of possible tries for passing a request to the next server. The 0 value turns off this limitation.,0,0,others,nginx,0,0
6414,uwsgi_no_cache,Defines conditions under which the response will not be saved to a cache. If at least one value of the string parameters is not empty and is not equal to 0 then the response will not be saved:,0,0,others,nginx,0,0
6415,uwsgi_param,"Sets a parameter that should be passed to the uwsgi server. The value can contain text, variables, and their combination. These directives are inherited from the previous configuration level if and only if there are no uwsgi_param directives defined on the current level.",0,0,others,nginx,0,1
6416,uwsgi_pass,"Sets the protocol and address of a uwsgi server. As a protocol, uwsgi or suwsgi (secured uwsgi, uwsgi over SSL) can be specified. The address can be specified as a domain name or IP address, and a port:",0,0,others,nginx,0,0
6417,uwsgi_pass_header,Permits passing otherwise disabled header fields from a uwsgi server to a client.,0,0,others,nginx,0,0
6418,uwsgi_pass_request_body,Indicates whether the original request body is passed to the uwsgi server. See also the uwsgi_pass_request_headers directive.,0,0,others,nginx,0,0
6419,uwsgi_pass_request_headers,Indicates whether the header fields of the original request are passed to the uwsgi server. See also the uwsgi_pass_request_body directive.,0,0,others,nginx,0,0
6420,uwsgi_read_timeout,"Defines a timeout for reading a response from the uwsgi server. The timeout is set only between two successive read operations, not for the transmission of the whole response. If the uwsgi server does not transmit anything within this time, the connection is closed.",0,0,others,nginx,0,0
6421,uwsgi_request_buffering,Enables or disables buffering of a client request body.,1,6,function-tradeoff,nginx,0,0
6422,uwsgi_send_timeout,"Sets a timeout for transmitting a request to the uwsgi server. The timeout is set only between two successive write operations, not for the transmission of the whole request. If the uwsgi server does not receive anything within this time, the connection is closed.",0,0,others,nginx,0,0
6423,uwsgi_socket_keepalive,"Configures the TCP keepalive behavior for outgoing connections to a uwsgi server. By default, the operating systems settings are in effect for the socket. If the directive is set to the value on, the SO_KEEPALIVE socket option is turned on for the socket.",0,0,others,nginx,0,0
6424,uwsgi_ssl_certificate,Specifies a file with the certificate in the PEM format used for authentication to a secured uwsgi server.,0,0,others,nginx,0,0
6425,uwsgi_ssl_certificate_key,Specifies a file with the secret key in the PEM format used for authentication to a secured uwsgi server.,0,0,others,nginx,0,1
6426,uwsgi_ssl_ciphers,Specifies the enabled ciphers for requests to a secured uwsgi server. The ciphers are specified in the format understood by the OpenSSL library.,0,0,others,nginx,0,1
6427,uwsgi_ssl_conf_command,Sets arbitrary OpenSSL configuration commands when establishing a connection with the secured uwsgi server.,0,0,others,nginx,0,0
6428,uwsgi_ssl_crl,Specifies a file with revoked certificates (CRL) in the PEM format used to verify the certificate of the secured uwsgi server.,0,0,others,nginx,0,0
6429,uwsgi_ssl_name,Allows overriding the server name used to verify the certificate of the secured uwsgi server and to be passed through SNI when establishing a connection with the secured uwsgi server.,0,0,others,nginx,0,1
6430,uwsgi_ssl_password_file,Specifies a file with passphrases for secret keys where each passphrase is specified on a separate line. Passphrases are tried in turn when loading the key.,0,0,others,nginx,0,0
6431,uwsgi_ssl_protocols,Enables the specified protocols for requests to a secured uwsgi server.,1,6,function-tradeoff,nginx,0,0
6432,uwsgi_ssl_server_name,"Enables or disables passing of the server name through TLS Server Name Indication extension (SNI, RFC 6066) when establishing a connection with the secured uwsgi server.",0,0,others,nginx,0,0
6433,uwsgi_ssl_session_reuse,"Determines whether SSL sessions can be reused when working with a secured uwsgi server. If the errors SSL3_GET_FINISHED:digest check failed appear in the logs, try disabling session reuse.",1,6,function-tradeoff,nginx,0,0
6434,uwsgi_ssl_trusted_certificate,Specifies a file with trusted CA certificates in the PEM format used to verify the certificate of the secured uwsgi server.,0,0,others,nginx,0,1
6435,uwsgi_ssl_verify,Enables or disables verification of the secured uwsgi server certificate.,1,2,security-tradeoff,nginx,0,0
6436,uwsgi_ssl_verify_depth,Sets the verification depth in the secured uwsgi server certificates chain.,1,2,security-tradeoff,nginx,0,0
6437,uwsgi_store,"Enables saving of files to a disk. The on parameter saves files with paths corresponding to the directives alias or root. The off parameter disables saving of files. In addition, the file name can be set explicitly using the string with variables:",1,6,function-tradeoff,nginx,0,1
6438,uwsgi_store_access,"Sets access permissions for newly created files and directories, e.g.:",0,0,others,nginx,0,0
6439,uwsgi_temp_file_write_size,"Limits the size of data written to a temporary file at a time, when buffering of responses from the uwsgi server to temporary files is enabled. By default, size is limited by two buffers set by the uwsgi_buffer_size and uwsgi_buffers directives. The maximum size of a temporary file is set by the uwsgi_max_temp_file_size directive.",1,5,workload-specific,nginx,0,0
6440,uwsgi_temp_path,"Defines a directory for storing temporary files with data received from uwsgi servers. Up to three-level subdirectory hierarchy can be used underneath the specified directory. For example, in the following configuration",0,0,others,nginx,0,0
6441,valid_referers,"Specifies the Referer request header field values that will cause the embedded $invalid_referer variable to be set to an empty string. Otherwise, the variable will be set to 1. Search for a match is case-insensitive.",0,0,others,nginx,0,0
6442,variables_hash_bucket_size,Sets the bucket size for the variables hash table. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx,0,0
6443,variables_hash_max_size,Sets the maximum size of the variables hash table. The details of setting up hash tables are provided in a separate document.,1,1,resource,nginx,0,0
6444,worker_aio_requests,"When using aio with the epoll connection processing method, sets the maximum number of outstanding asynchronous I/O operations for a single worker process.",0,0,others,nginx,0,1
6445,worker_connections,Sets the maximum number of simultaneous connections that can be opened by a worker process.,1,5,workload-specific,nginx,0,0
6446,worker_cpu_affinity,"Binds worker processes to the sets of CPUs. Each CPU set is represented by a bitmask of allowed CPUs. There should be a separate set defined for each of the worker processes. By default, worker processes are not bound to any specific CPUs.",0,0,others,nginx,0,0
6447,worker_priority,Defines the scheduling priority for worker processes like it is done by the nice command: a negative number means higher priority. Allowed range normally varies from -20 to 20.,0,0,others,nginx,0,0
6448,worker_processes,Defines the number of worker processes.,1,5,workload-specific,nginx,0,0
6449,worker_rlimit_core,Changes the limit on the largest size of a core file (RLIMIT_CORE) for worker processes. Used to increase the limit without restarting the main process.,1,5,workload-specific,nginx,0,0
6450,worker_rlimit_nofile,Changes the limit on the maximum number of open files (RLIMIT_NOFILE) for worker processes. Used to increase the limit without restarting the main process.,1,1,resource,nginx,0,1
6451,worker_shutdown_timeout,"Configures a timeout for a graceful shutdown of worker processes. When the time expires, nginx will try to close all the connections currently open to facilitate shutdown.",0,0,others,nginx,0,0
6452,working_directory,"Defines the current working directory for a worker process. It is primarily used when writing a core-file, in which case a worker process should have write permission for the specified directory.",0,0,others,nginx,0,0
6453,xclient,Enables or disables the passing of the XCLIENT command with client parameters when connecting to the SMTP backend.,0,0,others,nginx,0,1
6454,xml_entities,"Specifies the DTD file that declares character entities. This file is compiled at the configuration stage. For technical reasons, the module is unable to use the external subset declared in the processed XML, so it is ignored and a specially defined file is used instead. This file should not describe the XML structure. It is enough to declare just the required character entities, for example:",0,0,others,nginx,0,0
6455,xslt_last_modified,Allows preserving the Last-Modified header field from the original response during XSLT transformations to facilitate response caching.,0,0,others,nginx,0,1
6456,xslt_param,"Defines the parameters for XSLT stylesheets. The value is treated as an XPath expression. The value can contain variables. To pass a string value to a stylesheet, the xslt_string_param directive can be used.",0,0,others,nginx,0,0
6457,xslt_string_param,Defines the string parameters for XSLT stylesheets. XPath expressions in the value are not interpreted. The value can contain variables.,0,0,others,nginx,0,0
6458,xslt_stylesheet,Defines the XSLT stylesheet and its optional parameters. A stylesheet is compiled at the configuration stage.,0,0,others,nginx,0,1
6459,xslt_types,"Enables transformations in responses with the specified MIME types in addition to text/xml. The special value * matches any MIME type (0.8.29). If the transformation result is an HTML response, its MIME type is changed to text/html.",0,0,others,nginx,0,0
6460,zone,"Defines the name and size of the shared memory zone that keeps the groups configuration and run-time state that are shared between worker processes. Several groups may share the same zone. In this case, it is enough to specify the size only once.",0,0,others,nginx,0,0
6461,zone_sync,Enables the synchronization of shared memory zones between cluster nodes. Cluster nodes are defined using zone_sync_server directives.,1,3,reliability-tradeoff,nginx,0,0
6462,zone_sync_buffers,"Sets the number and size of the per-zone buffers used for pushing zone contents. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform.",1,1,resource,nginx,0,0
6463,zone_sync_connect_retry_interval,Defines an interval between connection attempts to another cluster node.,0,0,others,nginx,0,0
6464,zone_sync_connect_timeout,Defines a timeout for establishing a connection with another cluster node.,0,0,others,nginx,0,0
6465,zone_sync_interval,Defines an interval for polling updates in a shared memory zone.,0,0,others,nginx,0,0
6466,zone_sync_recv_buffer_size,"Sets size of a per-connection receive buffer used to parse incoming stream of synchronization messages. The buffer size must be equal or greater than one of the zone_sync_buffers. By default, the buffer size is equal to zone_sync_buffers size multiplied by number.",1,1,resource,nginx,0,0
6467,zone_sync_server,"Defines the address of a cluster node. The address can be specified as a domain name or IP address with a mandatory port, or as a UNIX-domain socket path specified after the unix: prefix. A domain name that resolves to several IP addresses defines multiple nodes at once.",0,0,others,nginx,0,0
6468,zone_sync_ssl,Enables the SSL/TLS protocol for connections to another cluster server.,1,2,security-tradeoff,nginx,0,1
6469,zone_sync_ssl_certificate,Specifies a file with the certificate in the PEM format used for authentication to another cluster server.,0,0,others,nginx,0,0
6470,zone_sync_ssl_certificate_key,Specifies a file with the secret key in the PEM format used for authentication to another cluster server.,0,0,others,nginx,0,0
6471,zone_sync_ssl_ciphers,Specifies the enabled ciphers for connections to another cluster server. The ciphers are specified in the format understood by the OpenSSL library.,0,0,others,nginx,0,0
6472,zone_sync_ssl_conf_command,Sets arbitrary OpenSSL configuration commands when establishing a connection with another cluster server.,0,0,others,nginx,0,0
6473,zone_sync_ssl_crl,Specifies a file with revoked certificates (CRL) in the PEM format used to verify the certificate of another cluster server.,0,0,others,nginx,0,1
6474,zone_sync_ssl_name,Allows overriding the server name used to verify the certificate of a cluster server and to be passed through SNI when establishing a connection with the cluster server.,0,0,others,nginx,0,0
6475,zone_sync_ssl_password_file,Specifies a file with passphrases for secret keys where each passphrase is specified on a separate line. Passphrases are tried in turn when loading the key.,0,0,others,nginx,0,0
6476,zone_sync_ssl_protocols,Enables the specified protocols for connections to another cluster server.,0,0,others,nginx,0,0
6477,zone_sync_ssl_server_name,"Enables or disables passing of the server name through TLS Server Name Indication extension (SNI, RFC 6066) when establishing a connection with another cluster server.",0,0,others,nginx,0,0
6478,zone_sync_ssl_trusted_certificate,Specifies a file with trusted CA certificates in the PEM format used to verify the certificate of another cluster server.,0,0,others,nginx,0,0
6479,zone_sync_ssl_verify,Enables or disables verification of another cluster server certificate.,1,2,security-tradeoff,nginx,0,0
6480,zone_sync_ssl_verify_depth,Sets the verification depth in another cluster server certificates chain.,1,2,security-tradeoff,nginx,0,0
6481,zone_sync_timeout,"Sets the timeout between two successive read or write operations on connection to another cluster node. If no data is transmitted within this time, the connection is closed.",0,0,others,nginx,0,0
6483,agent_enabled,Enable the SPICE guest agent support on the instances.,1,6,function-tradeoff,nova,0,0
6485,agent_resetnetwork_timeout,Number of seconds to wait for agent's reply to resetnetwork request.,0,0,others,nova,0,0
6486,agent_timeout,Number of seconds to wait for agent's reply to a request.,0,0,others,nova,0,0
6487,agent_version_timeout,Number of seconds to wait for agent't reply to version request.,0,0,others,nova,0,0
6490,alias,An alias for a PCI passthrough device requirement.,0,0,others,nova,0,1
6491,allow_credentials,Indicate that the actual request can include user credentials,0,0,others,nova,0,0
6492,allow_headers,Indicate which header field names may be used during the actual request.,0,0,others,nova,0,0
6493,allow_methods,Indicate which methods can be used during the actual request.,0,0,others,nova,0,0
6495,allow_same_net_traffic,Determine whether to allow network traffic from same network.,0,0,others,nova,0,0
6499,amqp_auto_delete,Auto-delete queues in AMQP.,1,5,workload-specific,nova,0,0
6500,amqp_durable_queues,Use durable queues in AMQP.,1,5,workload-specific,nova,0,0
6502,api_endpoint,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",0,0,others,nova,0,0
6503,api_max_retries,"The number of times to retry when a request conflicts. If set to 0, only try once, no retries.",0,0,others,nova,0,0
6505,api_retry_count,"Number of times VMware vCenter server API must be retried on connection failures, e.g. socket error, etc.",0,0,others,nova,0,0
6506,api_retry_interval,The number of seconds to wait before retrying the request.,0,0,others,nova,0,0
6507,api_servers,List of glance api servers endpoints available to nova.,0,0,others,nova,0,0
6508,approle_role_id,AppRole role_id for authentication with vault,0,0,others,nova,0,1
6509,approle_secret_id,AppRole secret_id for authentication with vault,0,0,others,nova,0,1
6511,auth_schemes,The authentication schemes to use with the compute node.,1,2,security-tradeoff,nova,0,0
6512,auth_section,Config Section from which to load plugin specific options,0,0,others,nova,0,0
6513,auth_strategy,Determine the strategy to use for authentication.,1,2,security-tradeoff,nova,0,0
6514,auth_type.B,Authentication type to load,1,2,security-tradeoff,nova,0,0
6515,auth_type,"The type of authentication credential to create. Possible values are 'token', 'password', 'keystone_token', and 'keystone_password'. Required if no context is passed to the credential factory.",0,0,others,nova,0,0
6516,auth_uri,"Complete 'public' Identity API endpoint. This endpoint should not be an 'admin' endpoint, as it should be accessible by all end users. Unauthenticated clients are redirected to this endpoint to authenticate. Although this endpoint should ideally be unversioned, client support in the wild varies. If you're using a versioned v2 endpoint here, then this should not be the same endpoint the service user utilizes for validating tokens, because normal end users may not be able to reach that endpoint. This option is deprecated in favor of www_authenticate_uri and will be removed in the S release.",0,0,others,nova,0,0
6520,auto_assign_floating_ip,Autoassigning floating IP to VM,0,0,others,nova,0,0
6521,available_filters,Filters that the scheduler can use.,0,0,others,nova,0,0
6522,backdoor_port,"Enable eventlet backdoor. Acceptable values are 0, <port>, and <start>:<end>, where 0 results in listening on a random tcp port number; <port> results in listening on the specified port number (and not enabling backdoor if that port is in use); and <start>:<end> results in listening on the smallest unused port number within the specified range of port numbers. The chosen port is displayed in the service's log file.",0,0,others,nova,0,0
6524,backend.B,"Cache backend module. For eventlet-based or environments with hundreds of threaded servers, Memcache with pooling (oslo_cache.memcache_pool) is recommended. For environments with less than 100 threaded servers, Memcached (dogpile.cache.memcached) or Redis (dogpile.cache.redis) is recommended. Test environments with a single instance of the server can use the dogpile.cache.memory backend.",0,0,others,nova,0,0
6525,backend.C,Specify the key manager implementation. Options are 'barbican' and 'vault'. Default is 'barbican'. Will support the values earlier set using [key_manager]/api_class for some time.,0,0,others,nova,0,0
6526,backend,The back end to use for the database.,0,0,others,nova,0,0
6527,backend_argument,Arguments supplied to the backend module. Specify this option once per argument to be passed to the dogpile.cache backend. Example format: '<argname>:<value>'.,0,0,others,nova,0,0
6528,backends,Additional backends that can perform health checks and report that information back as part of a request.,0,0,others,nova,0,0
6529,bandwidth_poll_interval,Interval to pull network bandwidth usage info.,0,0,others,nova,0,1
6530,bandwidth_update_interval,Bandwidth update interval.,0,0,others,nova,0,1
6531,barbican_api_version,"Version of the Barbican API, for example: 'v1'",0,0,others,nova,0,0
6535,baseapi,Base API RPC API version cap.,0,0,others,nova,0,1
6536,bdms_in_notifications,"If enabled, include block device information in the versioned notification payload. Sending block device information is disabled by default as providing that information can incur some overhead on the system since the information may need to be loaded from the database.",1,6,function-tradeoff,nova,0,0
6538,block_device_allocate_retries,"Number of times to retry block device allocation on failures. Starting with Liberty, Cinder can use image volume cache. This may help with block device allocation performance. Look at the cinder image_volume_cache_enabled configuration option.",0,0,others,nova,0,0
6539,block_device_allocate_retries_interval,Interval (in seconds) between block device allocation retries on failures.,0,0,others,nova,0,0
6540,block_device_creation_timeout,Time in secs to wait for a block device to be created,0,0,others,nova,0,0
6542,build_failure_weight_multiplier,Multiplier used for weighing hosts that have had recent build failures.,0,0,others,nova,0,0
6543,ca_file.B,CA certificate file to be verified in httpd server with TLS enabled,0,0,others,nova,0,0
6544,ca_file,Specifies the CA bundle file to be used in verifying the vCenter server certificate.,0,0,others,nova,0,0
6545,cache,"Request environment key where the Swift cache object is stored. When auth_token middleware is deployed with a Swift cache, use this option to have the middleware share a caching backend with swift. Otherwise, use the memcached_servers option instead.",0,0,others,nova,0,1
6546,cache_images,Cache glance images locally.,1,4,limited-side-effect,nova,0,0
6547,cache_prefix,This option adds a prefix to the folder where cached images are stored,0,0,others,nova,0,0
6548,cafile.B,A PEM encoded Certificate Authority to use when verifying HTTPs connections. Defaults to system CAs.,0,0,others,nova,0,0
6549,cafile,PEM encoded Certificate Authority to use when verifying HTTPs connections.,0,0,others,nova,0,0
6550,call_timeout,Call timeout.,0,0,others,nova,0,0
6551,capabilities.B,Cell capabilities.,0,0,others,nova,0,0
6552,capabilities,List of Linux capabilities retained by the privsep daemon.,0,0,others,nova,0,0
6553,catalog_info,Info to match when looking for cinder in the service catalog.,0,0,others,nova,0,0
6554,cell_type,Type of cell.,0,0,others,nova,0,1
6555,cells,Cells RPC API version cap.,0,0,others,nova,0,0
6556,cells_config,Optional cells configuration.,0,0,others,nova,0,0
6557,cert.B,Cert RPC API version cap.,0,0,others,nova,0,0
6558,cert,Path to SSL certificate file.,0,0,others,nova,0,1
6561,check_host,Ensure compute service is running on host XenAPI connects to. This option must be set to false if the 'independent_compute' option is set to true.,1,3,reliability-tradeoff,nova,0,0
6562,checksum_base_images,Write a checksum for files in _base to disk,1,6,function-tradeoff,nova,0,0
6563,checksum_interval_seconds,How frequently to checksum base images,0,0,others,nova,0,0
6564,cipher,Cipher-mode string to be used.,0,0,others,nova,0,0
6565,client_socket_timeout,This option specifies the timeout for client connections' socket operations. If an incoming connection is idle for this number of seconds it will be closed. It indicates timeout on individual read/writes on the socket connection. To wait forever set to 0.,0,0,others,nova,0,0
6566,cloud_connector_url,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",0,0,others,nova,0,0
6568,cnt_vpn_clients,This option represents the number of IP addresses to reserve at the top of the address range for VPN clients. It also will be ignored if the configuration option for network_manager is not set to the default of 'nova.network.manager.VlanManager'.,0,0,others,nova,0,0
6569,collect_timing,Collect per-API call timing information.,1,6,function-tradeoff,nova,0,1
6570,compute,Compute RPC API version cap.,0,0,others,nova,0,0
6574,conductor,Conductor RPC API version cap.,0,0,others,nova,0,1
6575,config_drive_cdrom,Configuration drive cdrom,0,0,others,nova,0,1
6576,config_drive_format,Configuration drive format,0,0,others,nova,0,1
6577,config_drive_inject_password,Configuration drive inject password,0,0,others,nova,0,0
6578,config_drive_skip_versions,"When gathering the existing metadata for a config drive, the EC2-style metadata is returned for all versions that don't appear in this option. As of the Liberty release, the available versions are:",0,0,others,nova,0,0
6579,config_prefix,Prefix for building the configuration dictionary for the cache region. This should not need to be changed unless there is another dogpile.cache region with the same configuration name.,0,0,others,nova,0,1
6580,conn_pool_min_size,The pool size limit for connections expiration policy,1,1,resource,nova,0,0
6581,conn_pool_ttl,The time-to-live in sec of idle connections in the pool,0,0,others,nova,0,0
6582,connection.B,The SQLAlchemy connection string to use to connect to the database.,0,0,others,nova,0,0
6583,connection,The SQLAlchemy connection string to use to connect to the database. Do not set this for the nova-compute service.,0,0,others,nova,0,0
6584,connection_concurrent,Maximum number of concurrent XenAPI connections.,1,1,resource,nova,0,1
6585,connection_debug,"Verbosity of SQL debugging information: 0=None, 100=Everything.",1,6,function-tradeoff,nova,0,0
6587,connection_password,Password for connection to XenServer/Xen Cloud Platform,0,0,others,nova,0,0
6588,connection_pool_size,This option sets the http connection pool size,1,1,resource,nova,0,0
6589,connection_recycle_time,Connections which have been present in the connection pool longer than this number of seconds will be replaced with a new one the next time they are checked out from the pool.,0,0,others,nova,0,1
6590,connection_retry_backoff,Increase the connection_retry_interval by this many seconds after each unsuccessful failover attempt.,0,0,others,nova,0,0
6591,connection_retry_interval,Seconds to pause before attempting to re-connect.,0,0,others,nova,0,0
6592,connection_retry_interval_max,Maximum limit for connection_retry_interval + connection_retry_backoff,0,0,others,nova,0,0
6593,connection_string,Connection string for a notifier backend.,0,0,others,nova,0,1
6594,connection_trace,Add Python stack traces to SQL as comment strings.,0,0,others,nova,0,0
6597,connection_username,Username for connection to XenServer/Xen Cloud Platform,0,0,others,nova,0,0
6598,consecutive_build_service_disable_threshold,Enables reporting of build failures to the scheduler.,1,6,function-tradeoff,nova,0,1
6599,console,Console RPC API version cap.,0,0,others,nova,0,0
6600,console_delay_seconds,Set this value if affected by an increased network latency causing repeated characters when typing in a remote console.,0,0,others,nova,0,0
6601,console_host,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",0,0,others,nova,0,0
6602,console_public_hostname,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",0,0,others,nova,0,0
6604,console_xvp_conf_template,XVP conf template,0,0,others,nova,0,0
6605,console_xvp_log,XVP log file,0,0,others,nova,0,0
6608,consoleauth,Consoleauth RPC API version cap.,0,0,others,nova,0,1
6612,cores,The number of instance cores or vCPUs allowed per project.,1,1,resource,nova,0,1
6613,cpu_allocation_ratio,This option helps you specify virtual CPU to physical CPU allocation ratio.,1,5,workload-specific,nova,0,0
6614,cpu_mode,Is used to set the CPU mode an instance should have.,1,5,workload-specific,nova,0,0
6615,cpu_model,Set the name of the libvirt CPU model the instance should use.,1,6,function-tradeoff,nova,0,1
6616,cpu_model_extra_flags,"This allows specifying granular CPU feature flags when configuring CPU models. For example, to explicitly specify the pcid (Process-Context ID, an Intel processor feature -- which is now required to address the guest performance degradation as a result of applying the 'Meltdown' CVE fixes to certain Intel CPU models) flag to the 'IvyBridge' virtual CPU model:",1,6,function-tradeoff,nova,0,0
6617,cpu_shared_set,Defines which physical CPUs (pCPUs) will be used for best-effort guest vCPU resources.,0,0,others,nova,0,0
6618,cpu_weight_multiplier,CPU weight multiplier ratio.,1,5,workload-specific,nova,0,1
6620,cross_az_attach,Allow attach between instance and volume in different availability zones.,0,0,others,nova,0,0
6621,daemon,Run as a background process.,1,4,limited-side-effect,nova,0,0
6623,db_check_interval,DB check interval.,0,0,others,nova,0,1
6624,db_inc_retry_interval,"If True, increases the interval between retries of a database operation up to db_max_retry_interval.",0,0,others,nova,0,0
6625,db_max_retries,Maximum retries in case of connection error or deadlock error before error is raised. Set to -1 to specify an infinite retry count.,0,0,others,nova,0,0
6626,db_max_retry_interval,"If db_inc_retry_interval is set, the maximum seconds between retries of a database operation.",0,0,others,nova,0,1
6627,db_retry_interval,Seconds between retries of a database transaction.,0,0,others,nova,0,0
6628,debug.B,Enable or disable debug logging with glanceclient.,1,6,function-tradeoff,nova,0,0
6629,debug.C,Enable/disables guestfs logging.,1,6,function-tradeoff,nova,0,0
6630,debug,"If set to true, the logging level will be set to DEBUG instead of the default INFO level.",1,6,function-tradeoff,nova,0,1
6631,debug_cache_backend,"Extra debugging from the cache backend (cache keys, get/set/delete/etc calls). This is only really useful if you need to see the specific cache-backend get/set/delete calls with the keys/values. Typically this should be left set to false.",0,0,others,nova,0,1
6633,default_availability_zone,Default availability zone for compute services.,0,0,others,nova,0,0
6634,default_domain_id,Optional domain ID to use with v3 and v2 parameters. It will be used for both the user and project domain in v3 and ignored in v2 authentication.,0,0,others,nova,0,0
6635,default_domain_name,Optional domain name to use with v3 API and v2 parameters. It will be used for both the user and project domain in v3 and ignored in v2 authentication.,0,0,others,nova,0,1
6636,default_ephemeral_format,The default format an ephemeral_volume will be formatted with on creation.,0,0,others,nova,0,0
6637,default_flavor,Default flavor to use for the EC2 API only. The Nova API does not support a default flavor.,0,0,others,nova,0,0
6638,default_floating_pool,Default pool for floating IPs.,0,0,others,nova,0,0
6639,default_level,Default notification level for outgoing notifications.,0,0,others,nova,0,0
6640,default_log_levels,List of package logging levels in logger=LEVEL pairs. This option is ignored if log_config_append is set.,1,6,function-tradeoff,nova,0,0
6642,default_notify_timeout,The deadline for a sent notification message delivery. Only used when caller does not provide a timeout expiry.,0,0,others,nova,0,0
6643,default_os_type,Default OS type used when uploading an image to glance,0,0,others,nova,0,1
6644,default_pool_size,This option specifies the size of the pool of greenthreads used by wsgi. It is possible to limit the number of concurrent connections using this option.,1,1,resource,nova,0,0
6645,default_reply_retry,The maximum number of attempts to re-send a reply message which failed due to a recoverable error.,0,0,others,nova,0,0
6646,default_reply_timeout,The deadline for an rpc reply message delivery.,0,0,others,nova,0,0
6648,default_schedule_zone,Default availability zone for instances.,0,0,others,nova,0,1
6649,default_send_timeout,The deadline for an rpc cast or call message delivery. Only used when caller does not provide a timeout expiry.,0,0,others,nova,0,0
6651,default_trusted_certificate_ids,List of certificate IDs for certificates that should be trusted.,0,0,others,nova,0,0
6652,defer_iptables_apply,Defer application of IPTables rules until after init phase.,0,0,others,nova,0,0
6653,delay_auth_decision,"Do not handle authorization requests within the middleware, but delegate the authorization decision to downstream WSGI components.",1,2,security-tradeoff,nova,0,0
6654,detailed,Show more detailed information as part of the response. Security note: Enabling this option may expose sensitive details about the service being monitored. Be sure to verify that it will not violate your security policies.,1,2,security-tradeoff,nova,0,1
6659,disable_agent,Disables the use of XenAPI agent.,1,6,function-tradeoff,nova,0,0
6660,disable_by_file_path,Check the presence of a file to determine if an application is running on a port. Used by DisableByFileHealthcheck plugin.,0,0,others,nova,0,0
6661,disable_by_file_paths,Check the presence of a file based on a port to determine if an application is running on a port. Expects a 'port:path' list of strings. Used by DisableByFilesPortsHealthcheck plugin.,0,0,others,nova,0,0
6662,disable_group_policy_check_upcall,Disable the server group policy check upcall in compute.,1,6,function-tradeoff,nova,0,0
6663,disable_libvirt_livesnapshot,Disable live snapshots when using the libvirt driver.,1,6,function-tradeoff,nova,0,0
6664,disable_process_locking,Enables or disables inter-process locks.,0,0,others,nova,0,0
6665,disable_rootwrap,Use sudo instead of rootwrap.,1,6,function-tradeoff,nova,0,0
6666,discover_hosts_in_cells_interval,Periodic task interval.,0,0,others,nova,0,0
6667,disk_allocation_ratio,This option helps you specify virtual disk to physical disk allocation ratio.,1,5,workload-specific,nova,0,0
6668,disk_cachemodes,Specific cache modes to use for different disk types.,1,5,workload-specific,nova,0,0
6670,disk_prefix,Override the default disk prefix for the devices attached to an instance.,0,0,others,nova,0,0
6671,disk_weight_multiplier,Disk weight multipler ratio.,1,5,workload-specific,nova,0,0
6674,dns_update_periodic_interval,"This option determines the time, in seconds, to wait between refreshing DNS entries for the network.",0,0,others,nova,0,0
6683,dynamic_memory_ratio,Dynamic memory ratio,1,5,workload-specific,nova,0,0
6684,ebtables_exec_attempts,This option determines the number of times to retry ebtables commands before giving up. The minimum number of retries is 1.,0,0,others,nova,0,0
6685,ebtables_retry_interval,"This option determines the time, in seconds, that the system will sleep in between ebtables retries. Note that each successive retry waits a multiple of this value, so for example, if this is set to the default of 1.0 seconds, and ebtables_exec_attempts is 4, after the first failure, the system will sleep for 1 * 1.0 seconds, after the second failure it will sleep 2 * 1.0 seconds, and after the third failure it will sleep 3 * 1.0 seconds.",0,0,others,nova,0,0
6686,enable,Enable cell v1 functionality.,1,6,function-tradeoff,nova,0,1
6687,enable_auto_commit,Enable asynchronous consumer commits,1,4,limited-side-effect,nova,0,0
6688,enable_certificate_validation,Enable certificate validation for image signature verification.,1,6,function-tradeoff,nova,0,0
6689,enable_consoleauth,Enable the consoleauth service to avoid resetting unexpired consoles.,1,6,function-tradeoff,nova,0,0
6690,enable_instance_metrics_collection,Enable instance metrics collection,1,6,function-tradeoff,nova,0,0
6692,enable_network_quota,This option is used to enable or disable quota checking for tenant networks.,1,6,function-tradeoff,nova,0,0
6693,enable_new_services,Enable new nova-compute services on this host automatically.,1,6,function-tradeoff,nova,0,0
6694,enable_numa_live_migration,Enable live migration of instances with NUMA topologies.,1,6,function-tradeoff,nova,0,0
6695,enable_proxy_headers_parsing,Whether the application is behind a proxy or not. This determines if the middleware should parse the headers or not.,0,0,others,nova,0,0
6696,enable_remotefx,Enable RemoteFX feature,1,6,function-tradeoff,nova,0,0
6697,enabled.B,Enable Remote Desktop Protocol (RDP) related features.,1,6,function-tradeoff,nova,0,0
6698,enabled.C,Enable SPICE related features.,1,6,function-tradeoff,nova,0,0
6699,enabled.D,Enable the profiling for all services on this node.,1,6,function-tradeoff,nova,0,0
6700,enabled.E,Enable the serial console feature.,1,6,function-tradeoff,nova,0,1
6701,enabled.F,Enable VNC related features.,1,6,function-tradeoff,nova,0,0
6702,enabled.G,Enables graphical console access for virtual machines.,0,0,others,nova,0,0
6703,enabled.H,Enables/disables LVM ephemeral storage encryption.,1,2,security-tradeoff,nova,0,1
6704,enabled,Global toggle for caching.,1,4,limited-side-effect,nova,0,1
6705,enabled_apis,List of APIs to be enabled by default.,0,0,others,nova,0,0
6706,enabled_filters,Filters that the scheduler will use.,0,0,others,nova,0,0
6708,enabled_ssl_apis,List of APIs with enabled SSL.,0,0,others,nova,0,0
6709,enabled_vgpu_types,The vGPU types enabled in the compute node.,0,0,others,nova,0,1
6712,enforce_scope,"This option controls whether or not to enforce scope when evaluating policies. If True, the scope of the token used in the request is compared to the scope_types of the policy being enforced. If the scopes do not match, an InvalidScope exception will be raised. If False, a message will be logged informing operators that policies are being invoked with mismatching scope.",0,0,others,nova,0,0
6713,enforce_token_bind,Used to control the use and type of token binding. Can be set to: 'disabled' to not check token binding. 'permissive' (default) to validate binding information if the bind type is of a form known to the server and ignore it if not. 'strict' like 'permissive' but if the bind type is unknown the token will be rejected. 'required' any form of token binding is needed to be allowed. Finally the name of a binding method that must be present in tokens.,0,0,others,nova,0,1
6715,es_doc_type,Document type for notification indexing in elasticsearch.,0,0,others,nova,0,1
6716,es_scroll_size,Elasticsearch splits large requests in batches. This parameter defines maximum size of each batch (for example: es_scroll_size=10000).,1,5,workload-specific,nova,0,0
6717,es_scroll_time,"This parameter is a time value parameter (for example: es_scroll_time=2m), indicating for how long the nodes that participate in the search will maintain relevant resources in order to continue and support it.",0,0,others,nova,0,1
6718,executor_thread_pool_size,Size of executor thread pool when executor is threading or eventlet.,1,1,resource,nova,0,0
6719,expiration_time,"Default TTL, in seconds, for any cached item in the dogpile.cache region. This applies to any cached method that doesn't have an explicit cache expiration time defined for it.",0,0,others,nova,0,0
6720,expose_headers,Indicate which headers are safe to expose to the API. Defaults to HTTP Simple Headers.,0,0,others,nova,0,0
6721,extension_sync_interval,Integer value representing the number of seconds to wait before querying Neutron for extensions. After this number of seconds the next time Nova needs to create a resource in Neutron it will requery Neutron for the extensions that it has loaded. Setting value to 0 will refresh the extensions with no wait.,1,3,reliability-tradeoff,nova,0,0
6722,fake_network,This option is used mainly in testing to avoid calls to the underlying network utilities.,0,0,others,nova,0,0
6723,fatal_deprecations,Enables or disables fatal status of deprecations.,0,0,others,nova,0,1
6724,file_backed_memory,Available capacity in MiB for file-backed memory.,1,1,resource,nova,0,1
6725,filter_error_trace,Enable filter traces that contain error/exception to a separated place.,1,6,function-tradeoff,nova,0,0
6727,fixed_ip_disassociate_timeout,"This is the number of seconds to wait before disassociating a deallocated fixed IP address. This is only used with the nova-network service, and has no effect when using neutron for networking.",0,0,others,nova,0,0
6728,fixed_ips,The number of fixed IPs allowed per project.,0,0,others,nova,0,0
6729,fixed_key,"Fixed key returned by key manager, specified in hex.",0,0,others,nova,0,0
6733,flat_network_bridge,This option determines the bridge used for simple network interfaces when no bridge is specified in the VM creation request.,0,0,others,nova,0,0
6736,floating_ips,The number of floating IPs allowed per project.,0,0,others,nova,0,1
6737,force_config_drive,Force injection to take place on a config drive,0,0,others,nova,0,1
6738,force_dhcp_release,"When this option is True, a call is made to release the DHCP for the instance when that instance is terminated.",0,0,others,nova,0,0
6739,force_raw_images,Force conversion of backing images to raw format.,1,6,function-tradeoff,nova,0,0
6740,force_snat_range,"This is a list of zero or more IP ranges that traffic from the routing_source_ip will be SNATted to. If the list is empty, then no SNAT rules are created.",0,0,others,nova,0,1
6741,forward_bridge_interface,"One or more interfaces that bridges can forward traffic to. If any of the items in this list is the special keyword 'all', then all traffic will be forwarded.",0,0,others,nova,0,1
6742,gateway,This is the default IPv4 gateway. It is used only in the testing suite.,0,0,others,nova,0,0
6743,gateway_v6,This is the default IPv6 gateway. It is used only in the testing suite.,0,0,others,nova,0,0
6747,group,Group that the privsep daemon should run as.,0,0,others,nova,0,1
6749,handle_virt_lifecycle_events,Enable handling of events emitted from compute drivers.,1,6,function-tradeoff,nova,0,1
6750,hash_algorithms,"Hash algorithms to use for hashing PKI tokens. This may be a single algorithm or multiple. The algorithms are those supported by Python standard hashlib.new(). The hashes will be tried in the order given, so put the preferred one first for performance. The result of the first hash will be stored in the cache. This will typically be set to multiple values only while migrating from a less secure algorithm to a more secure one. Once all the old tokens are expired this option should be set to a single value for better performance.",1,4,limited-side-effect,nova,0,0
6751,heal_instance_info_cache_interval,Interval between instance network information cache updates.,0,0,others,nova,0,1
6752,heartbeat_rate,How often times during the heartbeat_timeout_threshold we check the heartbeat.,0,0,others,nova,0,0
6753,heartbeat_timeout_threshold,Number of seconds after which the Rabbit broker is considered down if heartbeat's keep-alive fails (0 disable the heartbeat). EXPERIMENTAL,0,0,others,nova,0,1
6754,helper_command,"Command to invoke to start the privsep daemon if not using the 'fork' method. If not specified, a default is generated using 'sudo privsep-helper' and arguments designed to recreate the current configuration. This command must accept suitable --privsep_context and --privsep_sock_path arguments.",0,0,others,nova,0,1
6755,hmac_keys,Secret key(s) to use for encrypting context data for performance profiling.,0,0,others,nova,0,0
6760,host_subset_size,Size of subset of best hosts selected by scheduler.,0,0,others,nova,0,0
6768,http_retries.B,Number of times cinderclient should retry on any failed http call. 0 means connection is attempted only once. Setting it to any positive integer means that on failure connection is retried that many times e.g. setting it to 3 means total attempts to connect will be 4.,0,0,others,nova,0,0
6769,http_retries,Number of times neutronclient should retry on any failed http call.,0,0,others,nova,0,0
6770,hw_disk_discard,Discard option for nova managed disks.,0,0,others,nova,0,0
6771,hw_machine_type,"For qemu or KVM guests, set this option to specify a default machine type per host architecture. You can find a list of supported machine types in your environment by checking the output of the 'virsh capabilities' command. The format of the value for this config option is host-arch=machine-type. For example: x86_64=machinetype1,armv7l=machinetype2",0,0,others,nova,0,0
6772,idle_timeout,Timeout for inactive connections (in seconds),0,0,others,nova,0,1
6773,image_cache_manager_interval,Number of seconds to wait between runs of the image cache manager.,0,0,others,nova,0,0
6774,image_cache_subdirectory_name,Location of cached images.,0,0,others,nova,0,0
6775,image_compression_level,Compression level for images.,1,6,function-tradeoff,nova,0,0
6776,image_handler,The plugin used to handle image uploads and downloads.,0,0,others,nova,0,0
6777,image_info_filename_pattern,Allows image information files to be stored in non-standard locations,0,0,others,nova,0,0
6778,image_properties_default_architecture,The default architecture to be used when using the image properties filter.,0,0,others,nova,0,0
6779,image_tmp_path,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",0,0,others,nova,0,1
6782,images_rbd_pool,The RADOS pool in which rbd volumes are stored,0,0,others,nova,0,0
6783,images_type,VM Images format.,0,0,others,nova,0,1
6784,images_volume_group,"LVM Volume Group that is used for VM images, when you specify images_type=lvm",0,0,others,nova,0,1
6785,include_service_catalog,"(Optional) Indicate whether to set the X-Service-Catalog header. If False, middleware will not ask for service catalog on token validation and will not set the X-Service-Catalog header.",0,0,others,nova,0,0
6786,incomplete_consumer_project_id,"Early API microversions (<1.8) allowed creating allocations and not specifying a project or user identifier for the consumer. In cleaning up the data modeling, we no longer allow missing project and user information. If an older client makes an allocation, we'll use this in place of the information it doesn't provide.",0,0,others,nova,0,1
6787,incomplete_consumer_user_id,"Early API microversions (<1.8) allowed creating allocations and not specifying a project or user identifier for the consumer. In cleaning up the data modeling, we no longer allow missing project and user information. If an older client makes an allocation, we'll use this in place of the information it doesn't provide.",0,0,others,nova,0,0
6789,initial_cpu_allocation_ratio,This option helps you specify initial virtual CPU to physical CPU allocation ratio.,1,5,workload-specific,nova,0,0
6790,initial_disk_allocation_ratio,This option helps you specify initial virtual disk to physical disk allocation ratio.,1,5,workload-specific,nova,0,0
6791,initial_ram_allocation_ratio,This option helps you specify initial virtual RAM to physical RAM allocation ratio.,1,5,workload-specific,nova,0,0
6792,inject_key,Allow the injection of an SSH key at boot time.,1,2,security-tradeoff,nova,0,0
6794,inject_password,Allow the injection of an admin password for instance only at create and rebuild process.,1,6,function-tradeoff,nova,0,0
6795,injected_file_content_bytes,The number of bytes allowed per injected file.,1,1,resource,nova,0,0
6797,injected_files,The number of injected files allowed.,0,0,others,nova,0,0
6799,insecure.B,"If true, the vCenter server certificate is not verified. If false, then the default CA truststore is used for verification.",1,2,security-tradeoff,nova,0,0
6800,insecure,Verify HTTPS connections.,1,2,security-tradeoff,nova,0,0
6801,instance_build_timeout,Maximum time in seconds that an instance can take to build.,0,0,others,nova,0,0
6802,instance_delete_interval,Interval for retrying failed instance file deletes.,0,0,others,nova,0,0
6803,instance_dns_domain,"If specified, Nova checks if the availability_zone of every instance matches what the database says the availability_zone should be for the specified dns_domain.",0,0,others,nova,0,0
6805,instance_format,The format for an instance that is passed with the log message.,0,0,others,nova,0,0
6806,instance_list_cells_batch_fixed_size,"This controls the batch size of instances requested from each cell database if instance_list_cells_batch_strategy` is set to fixed. This integral value will define the limit issued to each cell every time a batch of instances is requested, regardless of the number of cells in the system or any other factors. Per the general logic called out in the documentation for instance_list_cells_batch_strategy, the minimum value for this is 100 records per batch.",1,5,workload-specific,nova,0,0
6807,instance_list_cells_batch_strategy,"This controls the method by which the API queries cell databases in smaller batches during large instance list operations. If batching is performed, a large instance list operation will request some fraction of the overall API limit from each cell database initially, and will re-request that same batch size as records are consumed (returned) from each cell as necessary. Larger batches mean less chattiness between the API and the database, but potentially more wasted effort processing the results from the database which will not be returned to the user. Any strategy will yield a batch size of at least 100 records, to avoid a user causing many tiny database queries in their request.",0,0,others,nova,0,1
6808,instance_list_per_project_cells,"When enabled, this will cause the API to only query cell databases in which the tenant has mapped instances. This requires an additional (fast) query in the API database before each list, but also (potentially) limits the number of cell databases that must be queried to provide the result. If you have a small number of cells, or tenants are likely to have instances in all cells, then this should be False. If you have many cells, especially if you confine tenants to a small subset of those cells, this should be True.",0,0,others,nova,0,0
6809,instance_name_template,Template string to be used to generate instance names.,0,0,others,nova,0,0
6810,instance_update_num_instances,Instance update num instances,1,5,workload-specific,nova,0,0
6811,instance_update_sync_database_limit,Instance update sync database limit.,1,3,reliability-tradeoff,nova,0,0
6812,instance_updated_at_threshold,Number of seconds after an instance was updated or deleted to continue to update cells. This option lets cells manager to only attempt to sync instances that have been updated recently.,1,5,workload-specific,nova,0,0
6813,instance_usage_audit,This option enables periodic compute.instance.exists notifications. Each compute node must be configured to generate system usage data. These notifications are consumed by OpenStack Telemetry service.,1,6,function-tradeoff,nova,0,0
6814,instance_usage_audit_period,Time period to generate instance usages for. It is possible to define optional offset to given period by appending @ character followed by a number defining offset.,0,0,others,nova,0,1
6815,instance_uuid_format,The format for an instance UUID that is passed with the log message.,0,0,others,nova,0,0
6816,instances,The number of instances allowed per project.,0,0,others,nova,0,1
6817,instances_path,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",0,0,others,nova,0,0
6820,intercell,Intercell RPC API version cap.,0,0,others,nova,0,0
6821,internal_service_availability_zone,Availability zone for internal services.,0,0,others,nova,0,0
6822,introduce_vdi_retry_wait,Number of seconds to wait for SR to settle if the VDI does not exist when first introduced.,0,0,others,nova,0,0
6823,io_ops_weight_multiplier,IO operations weight multipler ratio.,1,5,workload-specific,nova,0,0
6824,iptables_bottom_regex,"This expression, if defined, will select any matching iptables rules and place them at the bottom when applying metadata changes to the rules.",0,0,others,nova,0,0
6825,iptables_drop_action,"By default, packets that do not pass the firewall are DROPped. In many cases, though, an operator may find it more useful to change this from DROP to REJECT, so that the user issuing those packets may have a better idea as to what's going on, or LOGDROP in order to record the blocked traffic before DROPping.",0,0,others,nova,0,0
6826,iptables_top_regex,"This expression, if defined, will select any matching iptables rules and place them at the top when applying metadata changes to the rules.",0,0,others,nova,0,0
6831,iscsi_iface,The iSCSI transport iface to use to connect to target in case offload support is desired.,0,0,others,nova,0,0
6832,iscsi_initiator_list,List of iSCSI initiators that will be used for estabilishing iSCSI sessions.,0,0,others,nova,0,0
6833,iser_use_multipath,Use multipath connection of the iSER volume.,0,0,others,nova,0,0
6834,isolated_hosts,List of hosts that can only run certain images.,0,0,others,nova,0,1
6835,isolated_images,List of UUIDs for images that can only be run on certain hosts.,0,0,others,nova,0,0
6836,kafka_consumer_timeout,Default timeout(s) for Kafka consumers,0,0,others,nova,0,0
6837,kafka_max_fetch_bytes,Max fetch bytes of Kafka consumer,1,1,resource,nova,0,0
6838,keep_alive,"This option allows using the same TCP connection to send and receive multiple HTTP requests/responses, as opposed to opening a new one for every single request/response pair. HTTP keep-alive indicates HTTP connection reuse.",0,0,others,nova,0,1
6839,key,SSL key file (if separate from cert).,0,0,others,nova,0,0
6840,key_pairs,The maximum number of key pairs allowed per user.,0,0,others,nova,0,0
6841,key_size,Encryption key length in bits.,1,2,security-tradeoff,nova,0,0
6844,keymap.B,A keyboard layout which is supported by the underlying hypervisor on this node.,0,0,others,nova,0,0
6845,keymap,Keymap for VNC.,0,0,others,nova,0,1
6846,kombu_compression,"EXPERIMENTAL: Possible values are: gzip, bz2. If not set compression will not be used. This option may not be available in future versions.",1,6,function-tradeoff,nova,0,0
6847,kombu_failover_strategy,Determines how the next RabbitMQ node is chosen in case the one we are currently connected to becomes unavailable. Takes effect only if more than one RabbitMQ node is provided in config.,0,0,others,nova,0,0
6848,kombu_missing_consumer_retry_timeout,How long to wait a missing client before abandoning to send it its replies. This value should not be longer than rpc_response_timeout.,0,0,others,nova,0,0
6849,kombu_reconnect_delay,How long to wait before reconnecting in response to an AMQP consumer cancel notification.,0,0,others,nova,0,1
6850,kv_mountpoint,"Mountpoint of KV store in Vault to use, for example: secret",0,0,others,nova,0,0
6851,l3_lib,This option allows you to specify the L3 management library to be used.,0,0,others,nova,0,1
6862,limit_cpu_features,Limit CPU features,1,6,function-tradeoff,nova,0,0
6863,limit_tenants_to_placement_aggregate,"This setting causes the scheduler to look up a host aggregate with the metadata key of filter_tenant_id set to the project of an incoming request, and request results from placement be limited to that aggregate. Multiple tenants may be added to a single aggregate by appending a serial number to the key, such as filter_tenant_id:123.",0,0,others,nova,0,0
6864,link_retry_delay,Time to pause between re-connecting an AMQP 1.0 link that failed due to a recoverable error.,0,0,others,nova,0,1
6867,list_records_by_skipping_down_cells,"When set to False, this will cause the API to return a 500 error if there is an infrastructure failure like non-responsive cells. If you want the API to skip the down cells and return the results from the up cells set this option to True.",0,0,others,nova,0,1
6868,live_migration_bandwidth,Maximum bandwidth(in MiB/s) to be used during migration.,1,1,resource,nova,0,0
6869,live_migration_completion_timeout,"Time to wait, in seconds, for migration to successfully complete transferring data before aborting the operation.",0,0,others,nova,0,0
6870,live_migration_downtime,"Maximum permitted downtime, in milliseconds, for live migration switchover.",0,0,others,nova,0,1
6871,live_migration_downtime_delay,"Time to wait, in seconds, between each step increase of the migration downtime.",0,0,others,nova,0,0
6872,live_migration_downtime_steps,Number of incremental steps to reach max downtime value.,0,0,others,nova,0,0
6873,live_migration_inbound_addr,Target used for live migration traffic.,0,0,others,nova,0,0
6874,live_migration_permit_auto_converge,This option allows nova to start live migration with auto converge on.,0,0,others,nova,0,0
6875,live_migration_permit_post_copy,"This option allows nova to switch an on-going live migration to post-copy mode, i.e., switch the active VM to the one on the destination node before the migration is complete, therefore ensuring an upper bound on the memory that needs to be transferred. Post-copy requires libvirt>=1.3.3 and QEMU>=2.5.0.",0,0,others,nova,0,0
6876,live_migration_retry_count,Maximum number of 1 second retries in live_migration. It specifies number of retries to iptables when it complains. It happens when an user continuously sends live-migration request to same host leading to concurrent request to iptables.,0,0,others,nova,0,0
6878,live_migration_timeout_action,"This option will be used to determine what action will be taken against a VM after live_migration_completion_timeout expires. By default, the live migrate operation will be aborted after completion timeout. If it is set to force_complete, the compute service will either pause the VM or trigger post-copy depending on if post copy is enabled and available (live_migration_permit_post_copy is set to True).",0,0,others,nova,0,0
6879,live_migration_tunnelled,Enable tunnelled migration.,1,6,function-tradeoff,nova,0,0
6882,live_migration_with_native_tls,Use QEMU-native TLS encryption when live migrating.,1,2,security-tradeoff,nova,0,0
6883,local_metadata_per_cell,"Indicates that the nova-metadata API service has been deployed per-cell, so that we can have better performance and data isolation in a multi-cell deployment. Users should consider the use of this configuration depending on how neutron is setup. If you have networks that span cells, you might need to run nova-metadata API service globally. If your networks are segmented along cell boundaries, then you can run nova-metadata API service per cell. When running nova-metadata API service per cell, you should also configure each Neutron metadata-agent to point to the corresponding nova-metadata API service.",0,0,others,nova,0,0
6884,lock_path,"Directory to use for lock files. For security, the specified directory should only be writable by the user running the processes that need locking. Defaults to environment variable OSLO_LOCK_PATH. If external locks are used, a lock path must be set.",0,0,others,nova,0,1
6885,log_config_append,"The name of a logging configuration file. This file is appended to any existing logging configuration files. For details about logging configuration files, see the Python logging module documentation. Note that when logging configuration files are used then all logging configuration is set in the configuration file and other logging configuration options are ignored (for example, log-date-format).",0,0,others,nova,0,0
6886,log_date_format,Defines the format string for %(asctime)s in log records. Default: the value above . This option is ignored if log_config_append is set.,0,0,others,nova,0,0
6888,log_file,"(Optional) Name of log file to send logging output to. If no default is set, logging will go to stderr as defined by use_stderr. This option is ignored if log_config_append is set.",0,0,others,nova,0,0
6889,log_options,Enables or disables logging values of all registered options when starting a service (at DEBUG level).,1,6,function-tradeoff,nova,0,0
6890,log_rotate_interval,The amount of time before the log files are rotated. This option is ignored unless log_rotation_type is setto 'interval'.,0,0,others,nova,0,0
6892,log_rotation_type,Log rotation type.,0,0,others,nova,0,0
6893,logging_context_format_string,Format string to use for log messages with context. Used by oslo_log.formatters.ContextFormatter,0,0,others,nova,0,0
6894,logging_debug_format_suffix,Additional data to append to log message when logging level for the message is DEBUG. Used by oslo_log.formatters.ContextFormatter,1,6,function-tradeoff,nova,0,0
6895,logging_default_format_string,Format string to use for log messages when context is undefined. Used by oslo_log.formatters.ContextFormatter,0,0,others,nova,0,0
6896,logging_exception_prefix,Prefix each line of exception output with this format. Used by oslo_log.formatters.ContextFormatter,0,0,others,nova,0,0
6897,logging_user_identity_format,Defines the format string for %(user_identity)s that is used in logging_context_format_string. Used by oslo_log.formatters.ContextFormatter,0,0,others,nova,0,1
6898,login_timeout,Timeout in seconds for XenAPI login.,0,0,others,nova,0,0
6899,long_rpc_timeout,"This option allows setting an alternate timeout value for RPC calls that have the potential to take a long time. If set, RPC calls to other services will use this value for the timeout (in seconds) instead of the global rpc_response_timeout value.",0,0,others,nova,0,1
6900,max_age,Maximum cache age of CORS preflight requests.,1,5,workload-specific,nova,0,0
6901,max_attempts,"This is the maximum number of attempts that will be made for a given instance build/move operation. It limits the number of alternate hosts returned by the scheduler. When that list of hosts is exhausted, a MaxRetriesExceeded exception is raised and the instance is set to an error state.",0,0,others,nova,0,0
6902,max_concurrent_builds,"Limits the maximum number of instance builds to run concurrently by nova-compute. Compute service can attempt to build an infinite number of instances, if asked to do so. This limit is enforced to avoid building unlimited instance concurrently on a compute node. This value can be set per compute node.",1,1,resource,nova,0,0
6903,max_concurrent_disk_ops,"Number of concurrent disk-IO-intensive operations (glance image downloads, image format conversions, etc.) that we will do in parallel. If this is set too high then response time suffers. The default value of 0 means no limit.",1,1,resource,nova,0,0
6904,max_concurrent_live_migrations,Maximum number of live migrations to run concurrently. This limit is enforced to avoid outbound live migrations overwhelming the host/network and causing failures. It is not recommended that you change this unless you are very sure that doing so is safe and stable in your environment.,1,1,resource,nova,0,0
6905,max_disk_devices_to_attach,"Maximum number of disk devices allowed to attach to a single server. Note that the number of disks supported by an server depends on the bus used. For example, the ide disk bus is limited to 4 attached devices. The configured maximum is enforced during server create, rebuild, evacuate, unshelve, live migrate, and attach volume.",1,1,resource,nova,0,0
6906,max_header_line,This option specifies the maximum line size of message headers to be accepted. max_header_line may need to be increased when using large tokens (typically those generated by the Keystone v3 API with big service catalogs).,0,0,others,nova,0,0
6907,max_hop_count,Maximum hop count,0,0,others,nova,0,1
6908,max_instances_per_host,Maximum number of instances that be active on a host.,1,1,resource,nova,0,0
6909,max_io_ops_per_host,The number of instances that can be actively performing IO on a host.,1,1,resource,nova,0,0
6910,max_kernel_ramdisk_size,Maximum size in bytes of kernel or ramdisk images.,1,1,resource,nova,0,1
6911,max_limit,"As a query can potentially return many thousands of items, you can limit the maximum number of items in a single response by setting this option.",1,5,workload-specific,nova,0,0
6912,max_local_block_devices,Maximum number of devices that will result in a local image being created on the hypervisor node.,1,1,resource,nova,0,0
6913,max_logfile_count,Maximum number of rotated log files.,0,0,others,nova,0,0
6914,max_logfile_size_mb,Log file maximum size in MB. This option is ignored if 'log_rotation_type' is not set to 'size'.,0,0,others,nova,0,0
6915,max_overflow,"If set, use this value for max_overflow with SQLAlchemy.",0,0,others,nova,0,0
6916,max_placement_results,This setting determines the maximum limit on results received from the placement service during a scheduling operation. It effectively limits the number of hosts that may be considered for scheduling requests that match a large number of candidates.,0,0,others,nova,0,1
6917,max_poll_records,The maximum number of records returned in a poll call,1,1,resource,nova,0,0
6918,max_pool_size,Maximum number of SQL connections to keep open in a pool. Setting a value of 0 indicates no limit.,1,1,resource,nova,0,0
6919,max_request_body_size,"The maximum body size for each request, in bytes.",1,1,resource,nova,0,0
6920,max_retries,Maximum number of database connection retries during startup. Set to -1 to specify an infinite retry count.,0,0,others,nova,0,0
6921,maximum_instance_delete_attempts,The number of times to attempt to reap an instance's files.,0,0,others,nova,0,0
6922,maximum_objects,This option specifies the limit on the maximum number of objects to return in a single result.,0,0,others,nova,0,0
6923,mem_stats_period_seconds,A number of seconds to memory usage statistics period. Zero or negative value mean to disable memory usage statistics.,0,0,others,nova,0,0
6924,memcache_dead_retry,Number of seconds memcached server is considered dead before it is tried again. (dogpile.cache.memcache and oslo_cache.memcache_pool backends only).,0,0,others,nova,0,1
6925,memcache_pool_conn_get_timeout,(Optional) Number of seconds that an operation will wait to get a memcached client connection from the pool.,0,0,others,nova,0,1
6926,memcache_pool_connection_get_timeout,Number of seconds that an operation will wait to get a memcache client connection.,0,0,others,nova,0,0
6927,memcache_pool_dead_retry,(Optional) Number of seconds memcached server is considered dead before it is tried again.,0,0,others,nova,0,0
6928,memcache_pool_maxsize.B,(Optional) Maximum total number of open connections to every memcached server.,1,1,resource,nova,0,0
6929,memcache_pool_maxsize,Max total number of open connections to every memcached server. (oslo_cache.memcache_pool backend only).,1,1,resource,nova,0,0
6930,memcache_pool_socket_timeout,(Optional) Socket timeout in seconds for communicating with a memcached server.,0,0,others,nova,0,0
6931,memcache_pool_unused_timeout.B,(Optional) Number of seconds a connection to memcached is held unused in the pool before it is closed.,0,0,others,nova,0,0
6932,memcache_pool_unused_timeout,Number of seconds a connection to memcached is held unused in the pool before it is closed. (oslo_cache.memcache_pool backend only).,0,0,others,nova,0,0
6933,memcache_secret_key,"(Optional, mandatory if memcache_security_strategy is defined) This string is used for key derivation.",0,0,others,nova,0,0
6934,memcache_security_strategy,"(Optional) If defined, indicate whether token data should be authenticated or authenticated and encrypted. If MAC, token data is authenticated (with HMAC) in the cache. If ENCRYPT, token data is encrypted and authenticated in the cache. If the value is not one of these options or empty, auth_token will raise an exception on initialization.",1,2,security-tradeoff,nova,0,0
6935,memcache_servers,Memcache servers in the format of 'host:port'. (dogpile.cache.memcache and oslo_cache.memcache_pool backends only).,0,0,others,nova,0,1
6936,memcache_socket_timeout,Timeout in seconds for every call to a server. (dogpile.cache.memcache and oslo_cache.memcache_pool backends only).,0,0,others,nova,0,0
6937,memcache_use_advanced_pool,(Optional) Use the advanced (eventlet safe) memcached client pool. The advanced pool will only work under python 2.x.,0,0,others,nova,0,0
6938,memcached_servers,"Optionally specify a list of memcached server(s) to use for caching. If left undefined, tokens will instead be cached in-process.",0,0,others,nova,0,0
6939,metadata_cache_expiration,"This option is the time (in seconds) to cache metadata. When set to 0, metadata caching is disabled entirely; this is generally not recommended for performance reasons. Increasing this setting should improve response times of the metadata API when under heavy load. Higher values may increase memory usage, and result in longer times for host metadata changes to take effect.",1,5,workload-specific,nova,0,0
6941,metadata_items,The number of metadata items allowed per instance.,0,0,others,nova,0,0
6945,metadata_proxy_shared_secret,"This option holds the shared secret string used to validate proxy requests to Neutron metadata requests. In order to be used, the 'X-Metadata-Provider-Signature' header must be supplied in the request.",1,2,security-tradeoff,nova,0,0
6946,metadata_workers,Number of workers for metadata service. If not specified the number of available CPUs will be used.,1,1,resource,nova,0,0
6947,migrate_max_retries,Number of times to retry live-migration before failing.,0,0,others,nova,0,1
6948,min_pool_size,Minimum number of SQL connections to keep open in a pool.,1,1,resource,nova,0,0
6951,mounted_disk_query_retry_count,Mounted disk query retry count,0,0,others,nova,0,0
6952,mounted_disk_query_retry_interval,Mounted disk query retry interval,0,0,others,nova,0,0
6953,multi_host,Default value for multi_host in networks.,0,0,others,nova,0,0
6955,mute_child_interval,Mute child interval.,0,0,others,nova,0,0
6956,mute_weight_multiplier,Mute weight multiplier.,0,0,others,nova,0,0
6958,my_ip,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",0,0,others,nova,0,1
6959,mysql_enable_ndb,"If True, transparently enables support for handling MySQL Cluster (NDB).",0,0,others,nova,0,0
6960,mysql_sql_mode,"The SQL mode to be used for MySQL sessions. This option, including the default, overrides any server-set SQL mode. To use whatever SQL mode is set by the server configuration, set this to no value. Example: mysql_sql_mode=",1,6,function-tradeoff,nova,0,0
6962,network,Network RPC API version cap.,0,0,others,nova,0,0
6963,network_allocate_retries,Number of times to retry network allocation. It is required to attempt network allocation retries if the virtual interface plug fails.,0,0,others,nova,0,0
6966,network_size,This option determines the number of addresses in each private subnet.,0,0,others,nova,0,1
6969,nfs_mount_options,Mount options passed to the NFS client. See section of the nfs man page for details.,0,0,others,nova,0,0
6971,non_inheritable_image_properties,Image properties that should not be inherited from the instance when taking a snapshot.,0,0,others,nova,0,1
6972,notification_format,Specifies which notification format shall be used by nova.,0,0,others,nova,0,0
6974,notify_on_state_change,"If set, send compute.instance.update notifications on instance state changes.",0,0,others,nova,0,0
6975,notify_server_credit,Window size for incoming Notification messages,1,1,resource,nova,0,0
6979,num_aoe_discover_tries,Number of times to rediscover AoE target to find volume.,0,0,others,nova,0,0
6980,num_iser_scan_tries,Number of times to scan iSER target to find volume.,0,0,others,nova,0,0
6981,num_networks,"This option represents the number of networks to create if not explicitly specified when the network is created. The only time this is used is if a CIDR is specified, but an explicit network_size is not. In that case, the subnets are created by diving the IP address space of the CIDR by num_networks. The resulting subnet sizes cannot be larger than the configuration option network_size; in that event, they are reduced to network_size, and a warning is logged.",0,0,others,nova,0,0
6982,num_nvme_discover_tries,Number of times to rediscover NVMe target to find volume,0,0,others,nova,0,0
6983,num_pcie_ports,The number of PCIe ports an instance will get.,0,0,others,nova,0,0
6984,num_retries,Enable glance operation retries.,0,0,others,nova,0,1
6985,num_vbd_unplug_retries,"Maximum number of retries to unplug VBD. If set to 0, should try once, no retries.",0,0,others,nova,0,0
6986,num_volume_scan_tries,Number of times to scan given storage protocol to find volume.,0,0,others,nova,0,0
6987,number_of_retries,Number of times to retry poll for key creation completion,0,0,others,nova,0,1
6988,offset_weight_multiplier,Offset weight multiplier,0,0,others,nova,0,1
6992,osapi_compute_unique_server_name_scope,Sets the scope of the check for unique instance names.,1,6,function-tradeoff,nova,0,0
6993,osapi_compute_workers,Number of workers for OpenStack API service. The default will be the number of CPUs available.,1,1,resource,nova,0,0
6996,ovs_vsctl_timeout,"This option represents the period of time, in seconds, that the ovs_vsctl calls will wait for a response from the database before timing out. A setting of 0 means that the utility should wait forever for a response.",0,0,others,nova,0,0
6997,partition_key,"Case-insensitive key to limit the set of nodes that may be managed by this service to the set of nodes in Ironic which have a matching conductor_group property. If unset, all available nodes will be eligible to be managed by this service. Note that setting this to the empty string ("""") will match the default conductor group, and is different than leaving the option unset.",0,0,others,nova,0,1
6998,passthrough_whitelist,White list of PCI devices available to VMs.,0,0,others,nova,0,0
7001,password_length,Length of generated instance admin passwords.,0,0,others,nova,0,0
7003,pbm_default_policy,This option specifies the default policy to be used.,1,6,function-tradeoff,nova,0,0
7004,pbm_enabled,This option enables or disables storage policy based placement of instances.,1,6,function-tradeoff,nova,0,0
7006,pci_weight_multiplier,PCI device affinity weight multiplier.,0,0,others,nova,0,0
7008,periodic_enable,Enable periodic tasks.,1,6,function-tradeoff,nova,0,0
7009,periodic_fuzzy_delay,Number of seconds to randomly delay when starting the periodic task scheduler to reduce stampeding.,1,5,workload-specific,nova,0,0
7010,periodic_task_interval,Periodic task interval.,1,3,reliability-tradeoff,nova,0,0
7012,placement_aggregate_required_for_tenants,"This setting, when limit_tenants_to_placement_aggregate=True, will control whether or not a tenant with no aggregate affinity will be allowed to schedule to any available node. If aggregates are used to limit some tenants but not all, then this should be False. If all tenants should be confined via aggregate, then this should be True to prevent them from receiving unrestricted scheduling to any available node.",1,4,limited-side-effect,nova,0,0
7013,pointer_model,Generic property to specify the pointer type.,0,0,others,nova,0,0
7014,policy_default_rule,Default rule. Enforced when a requested rule is not found.,0,0,others,nova,0,0
7018,pool_size,Pool Size for Kafka Consumers,1,1,resource,nova,0,0
7019,pool_timeout,"If set, use this value for pool_timeout with SQLAlchemy.",0,0,others,nova,0,0
7021,port_range,A range of TCP ports a guest can use for its backend.,0,0,others,nova,0,0
7022,power_state_check_timeframe,Power state check timeframe,0,0,others,nova,0,0
7023,power_state_event_polling_interval,Power state event polling interval,0,0,others,nova,0,0
7024,pre_settled,Send messages of this type pre-settled. Pre-settled messages will not receive acknowledgement from the peer. Note well: pre-settled messages may be silently discarded if the delivery fails. Permitted values: 'rpc-call' - send RPC Calls pre-settled 'rpc-reply'- send RPC Replies pre-settled 'rpc-cast' - Send RPC Casts pre-settled 'notify' - Send Notifications pre-settled,0,0,others,nova,0,1
7025,preallocate_images,The image preallocation mode to use.,1,6,function-tradeoff,nova,0,1
7026,proc_units_factor,"Factor used to calculate the amount of physical processor compute power given to each vCPU. E.g. A value of 1.0 means a whole physical processor, whereas 0.05 means 1/20th of a physical processor.",1,1,resource,nova,0,0
7027,producer_batch_size,Size of batch for the producer async send,1,5,workload-specific,nova,0,0
7028,producer_batch_timeout,Upper bound on the delay for KafkaProducer batching in seconds,0,0,others,nova,0,0
7035,project_id_regex,"This option is a string representing a regular expression (regex) that matches the project_id as contained in URLs. If not set, it will match normal UUIDs created by keystone.",0,0,others,nova,0,0
7038,proxies,Proxy classes to import that will affect the way the dogpile.cache backend functions. See the dogpile.cache documentation on changing-backend-behavior.,0,0,others,nova,0,1
7042,publish_errors,Enables or disables publication of error events.,1,6,function-tradeoff,nova,0,0
7043,pybasedir,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",0,0,others,nova,0,1
7044,qemu_img_cmd,qemu-img command,0,0,others,nova,0,1
7048,quota_networks,This option controls the number of private networks that can be created per project (or per tenant).,0,0,others,nova,0,0
7049,rabbit_ha_queues,"Try to use HA queues in RabbitMQ (x-ha-policy: all). If you change this option, you must wipe the RabbitMQ database. In RabbitMQ 3.0, queue mirroring is no longer controlled by the x-ha-policy argument when declaring a queue. If you just want to make sure that all queues (except those with auto-generated names) are mirrored across all nodes, run: 'rabbitmqctl set_policy HA '^(?!amq.).*' '{'ha-mode': 'all'}' '",0,0,others,nova,0,0
7050,rabbit_interval_max,Maximum interval of RabbitMQ connection retries. Default is 30 seconds.,0,0,others,nova,0,1
7051,rabbit_login_method,The RabbitMQ login method.,0,0,others,nova,0,0
7052,rabbit_qos_prefetch_count,Specifies the number of messages to prefetch. Setting to zero allows unlimited messages.,1,1,resource,nova,0,1
7053,rabbit_retry_backoff,How long to backoff for between retries when connecting to RabbitMQ.,0,0,others,nova,0,0
7054,rabbit_retry_interval,How frequently to retry connecting with RabbitMQ.,0,0,others,nova,0,0
7056,ram,The number of megabytes of instance RAM allowed per project.,1,1,resource,nova,0,0
7057,ram_allocation_ratio,This option helps you specify virtual RAM to physical RAM allocation ratio.,1,5,workload-specific,nova,0,0
7058,ram_weight_multiplier.B,RAM weight multipler ratio.,1,5,workload-specific,nova,0,0
7059,ram_weight_multiplier,Ram weight multiplier.,0,0,others,nova,0,0
7060,randomize_allocation_candidates,"If True, when limiting allocation candidate results, the results will be a random sampling of the full result set. If False, allocation candidates are returned in a deterministic but undefined order. That is, all things being equal, two requests for allocation candidates will return the same results in the same order; but no guarantees are made as to how that order is determined.",0,0,others,nova,0,0
7061,rate_limit_burst,Maximum number of logged messages per rate_limit_interval.,0,0,others,nova,0,1
7062,rate_limit_except_level,"Log level name used by rate limiting: CRITICAL, ERROR, INFO, WARNING, DEBUG or empty string. Logs with level greater or equal to rate_limit_except_level are not filtered. An empty string means that all levels are filtered.",1,6,function-tradeoff,nova,0,0
7063,rate_limit_interval,"Interval, number of seconds, of log rate limiting.",0,0,others,nova,0,1
7064,rbd_secret_uuid,The libvirt UUID of the secret for the rbd_user volumes.,0,0,others,nova,0,0
7066,reachable_timeout,Timeout (seconds) to wait for an instance to start.,0,0,others,nova,0,0
7067,realtime_scheduler_priority,In a realtime host context vCPUs for guest will run in that scheduling priority. Priority depends on the host kernel (usually 1-99),1,5,workload-specific,nova,0,1
7068,reauthenticate,Allow fetching a new token if the current one is going to expire. Optional for 'keystone_token' and 'keystone_password' auth_type.,1,6,function-tradeoff,nova,0,0
7069,reboot_timeout,Time interval after which an instance is hard rebooted automatically.,0,0,others,nova,0,1
7070,recheck_quota,Recheck quota after resource creation to prevent allowing quota to be exceeded.,1,6,function-tradeoff,nova,0,0
7071,reclaim_instance_interval,Interval for reclaiming deleted instances.,0,0,others,nova,0,0
7075,remote_content_type,Content Type to send and receive data for REST based policy check,0,0,others,nova,0,0
7077,remote_ssl_ca_crt_file,Absolute path to ca cert file for REST based policy check,0,0,others,nova,0,0
7078,remote_ssl_client_crt_file,Absolute path to client cert for REST based policy check,0,0,others,nova,0,0
7079,remote_ssl_client_key_file,Absolute path client key file REST based policy check,0,0,others,nova,0,0
7080,remote_ssl_verify_server_crt,server identity verification for REST based policy check,1,2,security-tradeoff,nova,0,0
7081,remove_unused_base_images,Should unused base images be removed?,1,4,limited-side-effect,nova,0,0
7084,reply_link_credit,Window size for incoming RPC Reply messages.,1,1,resource,nova,0,0
7085,report_interval,Number of seconds indicating how frequently the state of services on a given hypervisor is reported. Nova needs to know this to determine the overall health of the deployment.,0,0,others,nova,0,0
7086,required,"This setting determines how any unavailable metrics are treated. If this option is set to True, any hosts for which a metric is unavailable will raise an exception, so it is recommended to also use the MetricFilter to filter out those hosts before weighing.",0,0,others,nova,0,0
7090,rescue_timeout,Interval to wait before un-rescuing an instance stuck in RESCUE.,0,0,others,nova,0,0
7091,reserve_percent,Reserve percentage,1,5,workload-specific,nova,0,0
7092,reserved_host_cpus,"Number of physical CPUs to reserve for the host. The host resources usage is reported back to the scheduler continuously from nova-compute running on the compute node. To prevent the host CPU from being considered as available, this option is used to reserve random pCPU(s) for the host.",1,1,resource,nova,0,0
7093,reserved_host_disk_mb,"Amount of disk resources in MB to make them always available to host. The disk usage gets reported back to the scheduler from nova-compute running on the compute nodes. To prevent the disk resources from being considered as available, this option can be used to reserve disk space for that host.",1,1,resource,nova,0,1
7094,reserved_host_memory_mb,"Amount of memory in MB to reserve for the host so that it is always available to host processes. The host resources usage is reported back to the scheduler continuously from nova-compute running on the compute node. To prevent the host memory from being considered as available, this option is used to reserve memory for the host.",1,1,resource,nova,0,0
7095,reserved_huge_pages,Number of huge/large memory pages to reserved per NUMA host cell.,1,1,resource,nova,0,0
7096,resize_confirm_window,Automatically confirm resizes after N seconds.,0,0,others,nova,0,0
7097,resize_fs_using_block_device,Enable resizing of filesystems via a block device.,1,5,workload-specific,nova,0,0
7098,resource_provider_association_refresh,"Interval for updating nova-compute-side cache of the compute node resource provider's inventories, aggregates, and traits.",1,5,workload-specific,nova,0,0
7099,restrict_isolated_hosts_to_isolated_images,Prevent non-isolated images from being built on isolated hosts.,0,0,others,nova,0,0
7100,resume_guests_state_on_host_boot,This option specifies whether to start guests that were running before the host rebooted. It ensures that all of the instances on a Nova compute node resume their state each time the compute node boots or restarts.,1,6,function-tradeoff,nova,0,0
7101,retry,"The maximum number of attempts to re-send a notification message which failed to be delivered due to a recoverable error. 0 - No retry, -1 - indefinite",0,0,others,nova,0,0
7102,retry_delay,Number of seconds to wait before retrying poll for key creation completion,0,0,others,nova,0,0
7103,retry_interval,Interval between retries of opening a SQL connection.,0,0,others,nova,0,0
7104,rng_dev_path,"The path to an RNG (Random Number Generator) device that will be used as the source of entropy on the host. Since libvirt 1.3.4, any path (that returns random numbers when read) is accepted. The recommended source of entropy is /dev/urandom -- it is non-blocking, therefore relatively fast; and avoids the limitations of /dev/random, which is a legacy interface. For more details (and comparision between different RNG sources), refer to the 'Usage' section in the Linux kernel API documentation for [u]random: http://man7.org/linux/man-pages/man4/urandom.4.html and http://man7.org/linux/man-pages/man7/random.7.html.",0,0,others,nova,0,0
7109,rpc_conn_pool_size,Size of RPC connection pool.,1,1,resource,nova,0,0
7111,rpc_response_timeout,Seconds to wait for a response from a call.,0,0,others,nova,0,0
7112,rpc_server_credit,Window size for incoming RPC Request messages,1,1,resource,nova,0,0
7113,run_external_periodic_tasks,Some periodic tasks can be run in a separate process. Should we run them here,0,0,others,nova,0,0
7114,running_deleted_instance_action,The compute service periodically checks for instances that have been deleted in the database but remain running on the compute node. The above option enables action to be taken when such instances are identified.,0,0,others,nova,0,1
7115,running_deleted_instance_poll_interval,"Time interval in seconds to wait between runs for the clean up action. If set to 0, above check will be disabled. If 'running_deleted_instance _action' is set to 'log' or 'reap', a value greater than 0 must be set.",0,0,others,nova,0,0
7116,running_deleted_instance_timeout,Time interval in seconds to wait for the instances that have been marked as deleted in database to be eligible for cleanup.,1,3,reliability-tradeoff,nova,0,0
7117,running_timeout,Wait time for instances to go to running state.,0,0,others,nova,0,1
7118,rx_queue_size,Configure virtio rx queue size.,1,1,resource,nova,0,0
7121,sasl_default_realm,SASL realm to use if no realm present in username,0,0,others,nova,0,0
7122,sasl_mechanism,Mechanism when security protocol is SASL,0,0,others,nova,0,1
7123,sasl_mechanisms,Space separated list of acceptable SASL mechanisms,0,0,others,nova,0,0
7124,scheduler.B,Cells scheduler.,0,0,others,nova,0,0
7125,scheduler,Scheduler RPC API version cap.,0,0,others,nova,0,0
7126,scheduler_filter_classes,Scheduler filter classes.,0,0,others,nova,0,0
7127,scheduler_instance_sync_interval,Interval between sending the scheduler a list of current instance UUIDs to verify that its view of instances is in sync with nova.,1,3,reliability-tradeoff,nova,0,0
7128,scheduler_retries,Scheduler retries.,0,0,others,nova,0,1
7129,scheduler_retry_delay,Scheduler retry delay.,0,0,others,nova,0,0
7130,scheduler_weight_classes,Scheduler weight classes.,0,0,others,nova,0,0
7131,secure_proxy_ssl_header.B,"The HTTP Header that will be used to determine what the original request protocol scheme was, even if it was hidden by a SSL termination proxy.",0,0,others,nova,0,0
7132,secure_proxy_ssl_header,"This option specifies the HTTP header used to determine the protocol scheme for the original request, even if it was removed by a SSL terminating proxy.",0,0,others,nova,0,0
7133,security_group_rules,The number of security rules per security group.,0,0,others,nova,0,0
7134,security_groups,The number of security groups per project.,0,0,others,nova,0,0
7135,security_protocol,Protocol used to communicate with brokers,0,0,others,nova,0,0
7136,send_arp_for_ha,"When True, when a device starts up, and upon binding floating IP addresses, arp messages will be sent to ensure that the arp caches on the compute hosts are up-to-date.",0,0,others,nova,0,1
7137,send_arp_for_ha_count,"When arp messages are configured to be sent, they will be sent with the count set to the value of this option. Of course, if this is set to zero, no arp messages will be sent.",0,0,others,nova,0,0
7138,send_service_user_token,"When True, if sending a user token to a REST API, also send a service token.",1,6,function-tradeoff,nova,0,1
7140,serial_console_state_timeout,Timeout (seconds) to wait for node serial console state changed. Set to 0 to disable timeout.,0,0,others,nova,0,1
7141,serial_log_dir,Specifies the directory where the Virtual Serial Port Concentrator is storing console log files. It should match the 'serial_log_dir' config value of VSPC.,0,0,others,nova,0,0
7142,serial_port_proxy_uri,Identifies a proxy service that provides network access to the serial_port_service_uri.,0,0,others,nova,0,0
7145,serialproxy_port,The port number which is used by the nova-serialproxy service to listen for incoming requests.,0,0,others,nova,0,0
7146,server_group_members,The maximum number of servers per server group.,0,0,others,nova,0,0
7147,server_groups,The maxiumum number of server groups per project.,0,0,others,nova,0,1
7153,service_down_time,Maximum time in seconds since last check-in for up service,0,0,others,nova,0,0
7156,service_token_roles,A choice of roles that must be present in a service token. Service tokens are allowed to request that an expired token can be used and so this check should tightly control that only actual services should be sending this token. Roles here are applied as an ANY check so any role in this list must be present. For backwards compatibility reasons this currently only affects the allow_expired check.,0,0,others,nova,0,0
7157,service_token_roles_required,For backwards compatibility reasons we must let valid service tokens pass that don't pass the service_token_roles check as valid. Setting this true will become the default in a future release and should be enabled if possible.,0,0,others,nova,0,0
7160,share_dhcp_address,THIS VALUE SHOULD BE SET WHEN CREATING THE NETWORK.,0,0,others,nova,0,0
7162,shelved_poll_interval,Interval for polling shelved instances to offload.,0,0,others,nova,0,0
7163,shuffle_best_same_weighed_hosts,Enable spreading the instances between hosts with the same best weight.,0,0,others,nova,0,0
7164,shutdown_retry_interval,Time to wait in seconds before resending an ACPI shutdown signal to instances.,0,0,others,nova,0,0
7165,shutdown_timeout,Total time to wait in seconds for an instance to perform a clean shutdown.,0,0,others,nova,0,1
7166,signing_dir,Directory used to cache files related to PKI tokens. This option has been deprecated in the Ocata release and will be removed in the P release.,0,0,others,nova,0,0
7167,slave_connection,The SQLAlchemy connection string to use to connect to the slave database.,0,0,others,nova,0,0
7168,smbfs_mount_options,Mount options passed to the SMBFS client.,0,0,others,nova,0,0
7170,snapshot_compression,Enable snapshot compression for qcow2 images.,1,4,limited-side-effect,nova,0,0
7171,snapshot_image_format,Determine the snapshot image format when sending to the image service.,0,0,others,nova,0,0
7173,socket_timeout,Redissentinel provides a timeout option on the connections. This parameter defines that timeout (for example: socket_timeout=0.1).,0,0,others,nova,0,0
7174,soft_affinity_weight_multiplier,Multiplier used for weighing hosts for group soft-affinity.,0,0,others,nova,0,0
7175,soft_anti_affinity_weight_multiplier,Multiplier used for weighing hosts for group soft-anti-affinity.,0,0,others,nova,0,0
7177,sparse_copy,Whether to use sparse_copy for copying data on a resize down. (False will use standard dd). This speeds up resizes down considerably since large runs of zeros won't have to be rsynced.,1,4,limited-side-effect,nova,0,0
7178,sparse_logical_volumes,Create sparse logical volumes (with virtualsize) if this flag is set to True.,1,4,limited-side-effect,nova,0,0
7179,split_loggers,Log requests to multiple loggers.,1,6,function-tradeoff,nova,0,1
7180,sqlite_synchronous,"If True, SQLite uses synchronous mode.",1,3,reliability-tradeoff,nova,0,1
7182,sr_matching_filter,Filter for finding the SR to be used to install guest instances on.,0,0,others,nova,0,1
7183,ssl.B,"Attempt to connect via SSL. If no other ssl-related parameters are given, it will use the system's CA-bundle to verify the server's certificate.",1,2,security-tradeoff,nova,0,1
7184,ssl,Connect over SSL.,1,2,security-tradeoff,nova,0,1
7186,ssl_ca_file.B,CA certificate PEM file used to verify the server's certificate,0,0,others,nova,0,0
7187,ssl_ca_file.C,SSL certification authority file (valid only if SSL enabled).,0,0,others,nova,0,0
7188,ssl_ca_file,This option allows setting path to the CA certificate file that should be used to verify connecting clients.,0,0,others,nova,0,0
7189,ssl_cafile,CA certificate PEM file used to verify the server certificate,0,0,others,nova,0,1
7190,ssl_cert_file.B,Self-identifying certificate PEM file for client authentication,0,0,others,nova,0,0
7191,ssl_cert_file.C,SSL cert file (valid only if SSL enabled).,0,0,others,nova,0,0
7192,ssl_cert_file,This option allows setting path to the SSL certificate of API server.,0,0,others,nova,0,0
7197,ssl_only,Disallow non-encrypted connections.,1,2,security-tradeoff,nova,0,0
7198,ssl_verify_vhost,"By default SSL checks that the name in the server's certificate matches the hostname in the transport_url. In some configurations it may be preferable to use the virtual hostname instead, for example if the server uses the Server Name Indication TLS extension (rfc6066) to provide a certificate per virtual host. Set ssl_verify_vhost to True if the server's SSL certificate uses the virtual host name instead of the DNS name.",1,2,security-tradeoff,nova,0,1
7199,ssl_version,"SSL version to use (valid only if SSL enabled). Valid values are TLSv1 and SSLv23. SSLv2, SSLv3, TLSv1_1, and TLSv1_2 may be available on some distributions.",0,0,others,nova,0,0
7201,sync_power_state_interval,Interval to sync power states between the database and the hypervisor.,1,3,reliability-tradeoff,nova,0,1
7202,sync_power_state_pool_size,Number of greenthreads available for use to sync power states.,1,1,resource,nova,0,0
7203,sysinfo_serial,"The data source used to the populate the host 'serial' UUID exposed to guest in the virtual BIOS. All choices except unique will change the serial when migrating the instance to another host. Changing the choice of this option will also affect existing instances on this host once they are stopped and started again. It is recommended to use the default choice (unique) since that will not change when an instance is migrated. However, if you have a need for per-host serials in addition to per-instance serial numbers, then consider restricting flavors via host aggregates.",0,0,others,nova,0,0
7204,syslog_log_facility,Syslog facility to receive log lines. This option is ignored if log_config_append is set.,0,0,others,nova,0,0
7205,system_scope,Scope for system operations,0,0,others,nova,0,0
7210,teardown_unused_network_gateway,"Determines whether unused gateway devices, both VLAN and bridge, are deleted if the network is in nova-network VLAN mode and is multi-hosted.",1,4,limited-side-effect,nova,0,0
7214,thread_pool_size,The number of threads available for privsep to concurrently run processes. Defaults to the number of CPU cores in the system.,1,1,resource,nova,0,0
7215,timeout,Timeout value for http requests,0,0,others,nova,0,0
7216,timeout_nbd,"Amount of time, in seconds, to wait for NBD device start up.",0,0,others,nova,0,0
7217,token,Token for authentication. Required for 'token' and 'keystone_token' auth_type if no context is passed to the credential factory.,0,0,others,nova,0,0
7218,token_cache_time,"In order to prevent excessive effort spent validating tokens, the middleware caches previously-seen tokens for a configurable duration (in seconds). Set to -1 to disable caching completely.",1,5,workload-specific,nova,0,1
7219,token_ttl,The lifetime of a console auth token (in seconds).,0,0,others,nova,0,1
7220,topics,AMQP topic used for OpenStack notifications.,0,0,others,nova,0,0
7221,trace,Debug: dump AMQP frames to stdout,1,6,function-tradeoff,nova,0,1
7222,trace_sqlalchemy,Enable SQL requests profiling in services.,1,6,function-tradeoff,nova,0,0
7223,track_instance_changes,Enable querying of individual hosts for instance information.,1,6,function-tradeoff,nova,0,0
7228,tx_queue_size,Configure virtio tx queue size.,1,1,resource,nova,0,0
7231,update_dns_entries,"When this option is True, whenever a DNS entry must be updated, a fanout cast message is sent to all network hosts to update their DNS entries in multi-host mode.",0,0,others,nova,0,0
7232,update_resources_interval,Interval for updating compute resources.,1,3,reliability-tradeoff,nova,0,0
7233,url,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",0,0,others,nova,0,0
7234,use_agent_default,Whether or not to use the agent by default when its usage is enabled but not indicated by the image.,0,0,others,nova,0,0
7235,use_cow_images,Enable use of copy-on-write (cow) images.,1,4,limited-side-effect,nova,0,0
7236,use_db_reconnect,Enable the experimental use of database reconnect on connection lost.,0,0,others,nova,0,0
7237,use_eventlog,Log output to Windows Event Log.,1,6,function-tradeoff,nova,0,0
7239,use_ipv6,Assign IPv6 and IPv4 addresses when creating instances.,0,0,others,nova,0,0
7241,use_journal,Enable journald for logging. If running in a systemd environment you may wish to enable journal support. Doing so will use the journal native protocol which includes structured metadata in addition to log messages.This option is ignored if log_config_append is set.,1,6,function-tradeoff,nova,0,0
7242,use_json,Use JSON formatting for logging. This option is ignored if log_config_append is set.,1,6,function-tradeoff,nova,0,0
7243,use_linked_clone,This option enables/disables the use of linked clone.,1,6,function-tradeoff,nova,0,0
7244,use_multipath_io,Use multipath connections when attaching iSCSI or FC disks.,1,4,limited-side-effect,nova,0,0
7245,use_network_dns_servers,"When this option is set to True, the dns1 and dns2 servers for the network specified by the user on boot will be used for DNS, as well as any specified in the dns_server option.",0,0,others,nova,0,0
7246,use_neutron,Enable neutron as the backend for networking.,0,0,others,nova,0,0
7247,use_neutron_default_nets,"When True, the TenantNetworkController will query the Neutron API to get the default networks to use.",0,0,others,nova,0,0
7248,use_rootwrap_daemon,Start and use a daemon that can run the commands that need to be run with root privileges. This option is usually enabled on nodes that run nova compute processes.,1,6,function-tradeoff,nova,0,1
7250,use_ssl,SSL Enabled/Disabled,1,2,security-tradeoff,nova,0,0
7251,use_stderr,Log output to standard error. This option is ignored if log_config_append is set.,1,6,function-tradeoff,nova,0,0
7252,use_syslog,Use syslog for logging. Existing syslog format is DEPRECATED and will be changed later to honor RFC5424. This option is ignored if log_config_append is set.,1,6,function-tradeoff,nova,0,0
7253,use_tpool,Enable the experimental use of thread pooling for all DB API calls,1,4,limited-side-effect,nova,0,0
7254,use_usb_tablet,Enable a mouse cursor within a graphical VNC or SPICE sessions.,0,0,others,nova,0,0
7255,use_virtio_for_bridges,Use virtio for bridge interfaces with KVM/QEMU,0,0,others,nova,0,0
7256,user,User that the privsep daemon should run as.,0,0,others,nova,0,0
7263,username.B,Username,0,0,others,nova,0,0
7264,username,Username for authentication. Required for 'password' auth_type. Optional for the 'keystone_password' auth_type.,0,0,others,nova,0,0
7267,vcpu_pin_set,Defines which physical CPUs (pCPUs) can be used by instance virtual CPUs (vCPUs).,0,0,others,nova,0,1
7271,vendordata_dynamic_connect_timeout,Maximum wait time for an external REST service to connect.,0,0,others,nova,0,0
7272,vendordata_dynamic_failure_fatal,Should failures to fetch dynamic vendordata be fatal to instance boot?,0,0,others,nova,0,0
7273,vendordata_dynamic_read_timeout,Maximum wait time for an external REST service to return data once connected.,0,0,others,nova,0,0
7274,vendordata_dynamic_ssl_certfile,Path to an optional certificate file or CA bundle to verify dynamic vendordata REST services ssl certificates against.,0,0,others,nova,0,0
7277,vendordata_providers,A list of vendordata providers.,0,0,others,nova,0,0
7278,verify_glance_signatures,Enable image signature verification.,1,6,function-tradeoff,nova,0,0
7279,verify_ssl,"Specifies if insecure TLS (https) requests. If False, the server's certificate will not be validated",1,2,security-tradeoff,nova,0,0
7280,versioned_notifications_topics,Specifies the topics for the versioned notifications issued by nova.,0,0,others,nova,0,0
7281,vhd_coalesce_max_attempts,Max number of times to poll for VHD to coalesce.,0,0,others,nova,0,0
7282,vhd_coalesce_poll_interval,The interval used for polling of coalescing vhds.,0,0,others,nova,0,0
7283,vif_plugging_is_fatal,Determine if instance should boot or fail on VIF plugging timeout.,0,0,others,nova,0,0
7284,vif_plugging_timeout,Timeout for Neutron VIF plugging event message arrival.,0,0,others,nova,0,1
7289,vlan_start,"This is the VLAN number used for private networks. Note that the when creating the networks, if the specified number has already been assigned, nova-network will increment this number until it finds an available VLAN.",1,1,resource,nova,0,0
7290,vnc_keymap,Keymap for VNC.,0,0,others,nova,0,0
7292,vnc_port_total,Total number of VNC ports.,0,0,others,nova,0,0
7293,volume_attach_retry_count,Volume attach retry count,0,0,others,nova,0,1
7294,volume_attach_retry_interval,Volume attach retry interval,0,0,others,nova,0,0
7295,volume_clear,Method used to wipe ephemeral disks when they are deleted. Only takes effect if LVM is set as backing storage.,0,0,others,nova,0,0
7296,volume_clear_size,"Size of area in MiB, counting from the beginning of the allocated volume, that will be cleared using method set in volume_clear option.",1,1,resource,nova,0,1
7297,volume_group_name,"Volume Group to use for block device operations. If disk_driver is localdisk, then this attribute must be specified. It is strongly recommended NOT to use rootvg since that is used by the management partition and filling it will cause failures.",0,0,others,nova,0,0
7298,volume_usage_poll_interval,Interval for gathering volume usages.,0,0,others,nova,0,1
7299,volume_use_multipath,Use multipath connection of the iSCSI or FC volume,0,0,others,nova,0,0
7301,vpn_start,This is the port number to use as the first VPN port for private networks.,0,0,others,nova,0,1
7303,vzstorage_cache_path,Path to the SSD cache file.,0,0,others,nova,0,0
7304,vzstorage_log_path,Path to vzstorage client log.,0,0,others,nova,0,0
7306,vzstorage_mount_opts,Extra mount options for pstorage-mount,0,0,others,nova,0,0
7307,vzstorage_mount_perms,Mount access mode.,0,0,others,nova,0,1
7310,wait_soft_reboot_seconds.B,Number of seconds to wait for instance to shut down after soft reboot request is made. We fall back to hard reboot if instance does not shutdown within this window.,0,0,others,nova,0,0
7311,wait_soft_reboot_seconds,Wait soft reboot seconds,0,0,others,nova,0,0
7312,watch_log_file,Uses logging handler designed to watch file system. When log file is moved or removed this handler will open a new log file with specified path instantaneously. It makes sense only if log_file option is specified and Linux platform is used. This option is ignored if log_config_append is set.,1,6,function-tradeoff,nova,0,0
7314,weight_classes,Weighers that the scheduler will use.,0,0,others,nova,0,0
7316,weight_of_unavailable,"When any of the following conditions are met, this value will be used in place of any actual metric value:",0,0,others,nova,0,0
7318,workers.B,Number of workers for OpenStack Conductor service. The default will be the number of CPUs available.,1,1,resource,nova,0,0
7319,workers,"Number of workers for the nova-scheduler service. The default will be the number of CPUs available if using the 'filter_scheduler' scheduler driver, otherwise the default will be 1.",1,1,resource,nova,0,0
7320,wsgi_log_format,"It represents a python format string that is used as the template to generate log lines. The following values can be formatted into it: client_ip, date_time, request_line, status_code, body_length, wall_seconds.",0,0,others,nova,0,0
7321,www_authenticate_uri,"Complete 'public' Identity API endpoint. This endpoint should not be an 'admin' endpoint, as it should be accessible by all end users. Unauthenticated clients are redirected to this endpoint to authenticate. Although this endpoint should ideally be unversioned, client support in the wild varies. If you're using a versioned v2 endpoint here, then this should not be the same endpoint the service user utilizes for validating tokens, because normal end users may not be able to reach that endpoint.",0,0,others,nova,0,1